from dataclasses import dataclass, field
import numpy as np
import astropy.convolution as cnv
import skimage
from collections import defaultdict
import inspect
from enum import Enum


class VariableToBin(Enum):
    XY = 1
    DIR = 2
    SPEED = 3
    XY_TIME = 4
    SPEED_DIR = 5
    EGO_BOUNDARY = 6
    TIME = 7


class MapType(Enum):
    RATE = 1
    POS = 2
    SPK = 3
    ADAPTIVE = 4
    AUTO_CORR = 5
    CROSS_CORR = 6


"""
The following docstring is mostly generated by Copilot...

A dataclass to store binned data. The binned data is stored in a list of numpy
arrays. The bin edges are stored in a list of numpy arrays. The variable to bin
is stored as an instance of the VariableToBin enum. The map type is stored as an
instance of the MapType enum. The binned data and bin edges are initialized as
empty lists. The set_nan_indices method is used to set the values of the binned
data at the specified indices to NaN.

The __truediv__ method is used to divide the binned data by the binned data of 
another BinnedData instance i.e. spike data / pos data to get a rate map. I've
added a check to this for instances where the length of the binned data in the 
numerator (i.e. binned spike arrays) is greater than the length of the denominator
(i.e.the binned position array). This is because when either binning up >1 cluster
in a trial, or creating shuffled data, the position data is invariant and so it's
pointless to generate replicates. I don't think this will trip me up in the future
but I guess it's theoretically possible that the denominator might be different in
some cicrcumstances (maybe binning up runs along a linear track or something...)

The __add__ method is used to add the binned data of another BinnedData instance 
to the binned data of this instance.

Both these overloaded operators check if the bin edges of the two BinnedData
instances match before performing the operation.

The BinnedData class is the output of the main binning function in the
ephysiopy.common.binning.RateMap class. It is used to store the binned data as a
convenience mostly for easily iterating over the binned data and using the bin_edges
to plot the data. As such, it is used as a convenience for plotting as the bin edges
are used when calling pcolormesh in the plotting functions.
"""


@dataclass
class BinnedData:
    variable: VariableToBin = VariableToBin.XY
    map_type: MapType = MapType.RATE
    binned_data: list[np.ndarray] = field(default_factory=list)
    bin_edges: list[np.ndarray] = field(default_factory=list)

    def __iter__(self):
        yield from self.binned_data

    def __assert_equal_bin_edges__(self, other):
        assert np.all(
            [np.all(sbe == obe) for sbe, obe in zip(self.bin_edges, other.bin_edges)]
        ), "Bin edges do not match"

    def __len__(self):
        return len(self.binned_data)

    def __getitem__(self, i):
        return BinnedData(
            variable=self.variable,
            map_type=self.map_type,
            binned_data=self.binned_data[i],
            bin_edges=self.bin_edges,
        )

    def __truediv__(self, other):
        if isinstance(other, BinnedData):
            self.__assert_equal_bin_edges__(other)
            if len(self.binned_data) > len(other.binned_data):
                if (other.map_type.value == MapType.POS.value) and (
                    self.map_type.value == MapType.SPK.value
                ):
                    if len(other.binned_data) == 1:
                        return BinnedData(
                            variable=self.variable,
                            map_type=MapType.RATE,
                            binned_data=[
                                a / b
                                for a in self.binned_data
                                for b in other.binned_data
                            ],
                            bin_edges=self.bin_edges,
                        )

            return BinnedData(
                variable=self.variable,
                map_type=MapType.RATE,
                binned_data=[
                    a / b for a, b in zip(self.binned_data, other.binned_data)
                ],
                bin_edges=self.bin_edges,
            )

    def __add__(self, other):
        if isinstance(other, BinnedData):
            self.__assert_equal_bin_edges__(other)
            return BinnedData(
                variable=self.variable,
                map_type=self.map_type,
                binned_data=self.binned_data + other.binned_data,
                bin_edges=self.bin_edges,
            )

    def __eq__(self, other) -> bool:
        assert isinstance(other, BinnedData)
        self.__assert_equal_bin_edges__(other)
        if np.all(
            [
                np.all(np.isfinite(sbd) == np.isfinite(obd))
                for sbd, obd in zip(self.binned_data, other.binned_data)
            ]
        ):
            if np.all(
                [
                    np.all(sbd[np.isfinite(sbd)] == obd[np.isfinite(obd)])
                    for sbd, obd in zip(self.binned_data, other.binned_data)
                ]
            ):
                return True
            else:
                return False
        else:
            return False

    def set_nan_indices(self, indices):
        for i in range(len(self.binned_data)):
            self.binned_data[i][indices] = np.nan

    def T(self):
        return BinnedData(
            variable=self.variable,
            map_type=self.map_type,
            binned_data=[a.T for a in self.binned_data],
            bin_edges=self.bin_edges[::-1],
        )

    def correlate(self, other=None, as_matrix=False) -> list[float] | np.ndarray:
        """
        This method is used to correlate the binned data of this BinnedData
        instance with the binned data of another BinnedData instance.

        Args:
            other (BinnedData): The other BinnedData instance to correlate with.
                If None, then correlations are performed between all the data held
                in the list self.binned_data
            as_matrix (bool): If True will return the full correlation matrix for
                all of the correlations in the list of data in self.binned_data. If
                False, a list of the unique correlations for the comparisons in
                self.binned_data are returned.

        Returns:
            BinnedData: A new BinnedData instance with the correlation of the
                binned data of this instance and the other instance.
        """
        if other is not None:
            assert isinstance(other, BinnedData)
            self.__assert_equal_bin_edges__(other)
        if other is not None:
            result = np.reshape(
                [corr_maps(a, b) for a in self.binned_data for b in other.binned_data],
                newshape=(len(self.binned_data), len(other.binned_data)),
            )
        else:
            result = np.reshape(
                [corr_maps(a, b) for a in self.binned_data for b in self.binned_data],
                newshape=(len(self.binned_data), len(self.binned_data)),
            )
        if as_matrix:
            return result
        else:
            # pick out the relevant diagonal
            k = -1
            if len(self.binned_data) == 1:
                k = 0
            if other is not None:
                if len(other.binned_data) == 1:
                    k = 0
                idx = np.tril_indices(
                    n=len(self.binned_data), m=len(other.binned_data), k=k
                )
            else:
                idx = np.tril_indices(n=len(self.binned_data), k=k)
            return result[idx]


# a basic dataclass for holding filter values
@dataclass(eq=True)
class TrialFilter:
    name: str
    start: float | str
    end: float | str

    def __init__(self, name: str, start: float | str, end: float | str):
        assert name in [
            "time",
            "dir",
            "speed",
            "xrange",
            "yrange",
        ], "name must be one of 'time', 'dir', 'speed', 'xrange', 'yrange'"
        self.name = name
        self.start = start
        self.end = end


def clean_kwargs(func, kwargs):
    """
    This function is used to remove any keyword arguments that are not
    accepted by the function. It is useful for passing keyword arguments
    to other functions without having to worry about whether they are
    accepted by the function or not.

    Args:
        func (function): The function to check for keyword arguments.
        kwargs (dict): The keyword arguments to check.

    Returns:
        dict: A dictionary containing only the keyword arguments that are
        accepted by the function.
    """
    valid_kwargs = inspect.getfullargspec(func).kwonlyargs
    return {k: v for k, v in kwargs.items() if k in valid_kwargs}


def get_z_score(x: np.ndarray, mean=None, sd=None, axis=0) -> np.ndarray:
    """
    Calculate the z-scores for array x based on the mean
    and standard deviation in that sample, unless stated
    """
    if mean is None:
        mean = np.nanmean(x, axis=axis)
    if sd is None:
        sd = np.nanstd(x, axis=axis)
    if axis == -1:
        return (x - mean[..., None]) / sd[..., None]
    return (x - mean) / sd


def mean_norm(x: np.ndarray, mn=None, axis=0) -> np.ndarray:
    if mn is None:
        mn = np.mean(x, axis)
    x = (x - mn) / (np.max(x, axis) - np.min(x, axis))
    return x


def min_max_norm(x: np.ndarray, min=None, max=None, axis=0) -> np.ndarray:
    if min is None:
        min = np.nanmin(x, axis)
    if max is None:
        max = np.nanmax(x, axis)
    if axis == -1:
        return (x - np.array(min)[..., None]) / np.array(max - min)[..., None]
    return (x - min) / (max - min)


def remap_to_range(x: np.ndarray, new_min=0, new_max=1, axis=0) -> np.ndarray:
    """
    Remap the values of x to the range [new_min, new_max].
    """
    min = np.nanmin(x, axis)
    max = np.nanmax(x, axis)
    if axis == -1:
        return np.array((x.T - min) / (max - min) * (new_max - new_min) + new_min).T
    return (x - min) / (max - min) * (new_max - new_min) + new_min


def flatten_list(list_to_flatten: list) -> list:
    try:
        return [item for sublist in list_to_flatten for item in sublist]
    except TypeError:
        return list_to_flatten


def smooth(x, window_len=9, window="hanning"):
    """
    Smooth the data using a window with requested size.

    This method is based on the convolution of a scaled window with the signal.
    The signal is prepared by introducing reflected copies of the signal
    (with the window size) in both ends so that transient parts are minimized
    in the beginning and end part of the output signal.

    Args:
        x (array_like): The input signal.
        window_len (int): The length of the smoothing window.
        window (str): The type of window from 'flat', 'hanning', 'hamming',
            'bartlett', 'blackman'. 'flat' window will produce a moving average
            smoothing.

    Returns:
        out (array_like): The smoothed signal.

    Example:
        >>> t=linspace(-2,2,0.1)
        >>> x=sin(t)+randn(len(t))*0.1
        >>> y=smooth(x)

    See Also:
        numpy.hanning, numpy.hamming, numpy.bartlett, numpy.blackman,
        numpy.convolve, scipy.signal.lfilter

    Notes:
        The window parameter could be the window itself if an array instead of
        a string.
    """

    if isinstance(x, list):
        x = np.array(x)

    if x.ndim != 1:
        raise ValueError("smooth only accepts 1 dimension arrays.")

    if len(x) < window_len:
        print("length of x: ", len(x))
        print("window_len: ", window_len)
        raise ValueError("Input vector needs to be bigger than window size.")
    if window_len < 3:
        return x

    if (window_len % 2) == 0:
        window_len = window_len + 1

    if window not in ["flat", "hanning", "hamming", "bartlett", "blackman"]:
        raise ValueError(
            "Window is on of 'flat', 'hanning', \
                'hamming', 'bartlett', 'blackman'"
        )

    if window == "flat":  # moving average
        w = np.ones(window_len, "d")
    else:
        w = eval("np." + window + "(window_len)")
    y = cnv.convolve(x, w / w.sum(), normalize_kernel=False, boundary="extend")
    # return the smoothed signal
    return y


def blur_image(
    im: BinnedData, n: int, ny: int = 0, ftype: str = "boxcar", **kwargs
) -> BinnedData:
    """
    Smooths a 2D image by convolving with a filter.

    Args:
        im (BinnedData): Contains the array to smooth.
        n, ny (int): The size of the smoothing kernel.
        ftype (str): The type of smoothing kernel.
            Either 'boxcar' or 'gaussian'.

    Returns:
        res (BinnedData): BinnedData instance with the smoothed data.

    Notes:
        This essentially does the smoothing in-place
    """
    stddev = kwargs.pop("stddev", 5)
    boundary = kwargs.pop("boundary", "extend")
    n = int(n)
    if n % 2 == 0:
        n += 1
    if ny == 0:
        ny = n
    else:
        ny = int(ny)
        if ny % 2 == 0:
            ny += 1
    ndims = len(im.bin_edges)
    g = cnv.Box2DKernel(n)
    if "box" in ftype:
        if ndims == 1:
            g = cnv.Box1DKernel(n)
        if ndims == 2:
            g = np.atleast_2d(g)
    elif "gaussian" in ftype:
        if ndims == 1:
            g = cnv.Gaussian1DKernel(stddev, x_size=n)
        if ndims == 2:
            g = cnv.Gaussian2DKernel(stddev, x_size=n, y_size=ny)
    g = np.array(g)
    for i, m in enumerate(im.binned_data):
        im.binned_data[i] = cnv.convolve(m, g, boundary=boundary)
    return im


def shift_vector(v, shift, maxlen=None):
    """
    Shifts the elements of a vector by a given amount.
    A bit like numpys roll function but when the shift goes
    beyond some limit that limit is subtracted from the shift.
    The result is then sorted and returned.

    Args:
        v (array_like): The input vector.
        shift (int): The amount to shift the elements.
        fill_value (int): The value to fill the empty spaces.

    Returns:
        array_like: The shifted vector.
    """
    if shift == 0:
        return v
    if maxlen is None:
        return v
    if shift > 0:
        shifted = v + shift
        shifted[shifted >= maxlen] -= maxlen
        return np.sort(shifted)


def count_to(n):
    """
    This function is equivalent to hstack((arange(n_i) for n_i in n)).
    It seems to be faster for some possible inputs and encapsulates
    a task in a function.

    Example:
        Given n = [0, 0, 3, 0, 0, 2, 0, 2, 1],
        the result would be [0, 1, 2, 0, 1, 0, 1, 0].
    """
    if n.ndim != 1:
        raise Exception("n is supposed to be 1d array.")

    n_mask = n.astype(bool)
    n_cumsum = np.cumsum(n)
    ret = np.ones(n_cumsum[-1] + 1, dtype=int)
    ret[n_cumsum[n_mask]] -= n[n_mask]
    ret[0] -= 1
    return np.cumsum(ret)[:-1]


def repeat_ind(n: np.ndarray) -> np.ndarray:
    """
    Examples:
        >>> n = [0, 0, 3, 0, 0, 2, 0, 2, 1]
        >>> res = repeat_ind(n)
        >>> res = [2, 2, 2, 5, 5, 7, 7, 8]

    The input specifies how many times to repeat the given index.
    It is equivalent to something like this:

        hstack((zeros(n_i,dtype=int)+i for i, n_i in enumerate(n)))

    But this version seems to be faster, and probably scales better.
    At any rate, it encapsulates a task in a function.
    """
    if n.ndim != 1:
        raise Exception("n is supposed to be 1d array.")

    res = [[idx] * a for idx, a in enumerate(n) if a != 0]
    return np.concatenate(res)


def rect(r, w, deg=False):
    """
    Convert from polar (r,w) to rectangular (x,y)
    x = r cos(w)
    y = r sin(w)
    """
    # radian if deg=0; degree if deg=1
    if deg:
        w = np.pi * w / 180.0
    return r * np.cos(w), r * np.sin(w)


def polar(x, y, deg=False):
    """
    Converts from rectangular coordinates to polar ones.

    Args:
        x, y (array_like, list_like): The x and y coordinates.
        deg (int): Radian if deg=0; degree if deg=1.

    Returns:
        p (array_like): The polar version of x and y.
    """
    if deg:
        return np.hypot(x, y), 180.0 * np.arctan2(y, x) / np.pi
    else:
        return np.hypot(x, y), np.arctan2(y, x)


def bwperim(bw, n=4):
    """
    Finds the perimeter of objects in binary images.

    A pixel is part of an object perimeter if its value is one and there
    is at least one zero-valued pixel in its neighborhood.

    By default the neighborhood of a pixel is 4 nearest pixels, but
    if `n` is set to 8 the 8 nearest pixels will be considered.

    Args:
        bw (array_like): A black-and-white image.
        n (int, optional): Connectivity. Must be 4 or 8. Default is 8.

    Returns:
        perim (array_like): A boolean image.
    """

    if n not in (4, 8):
        raise ValueError("mahotas.bwperim: n must be 4 or 8")
    rows, cols = bw.shape

    # Translate image by one pixel in all directions
    north = np.zeros((rows, cols))
    south = np.zeros((rows, cols))
    west = np.zeros((rows, cols))
    east = np.zeros((rows, cols))

    north[:-1, :] = bw[1:, :]
    south[1:, :] = bw[:-1, :]
    west[:, :-1] = bw[:, 1:]
    east[:, 1:] = bw[:, :-1]
    idx = (north == bw) & (south == bw) & (west == bw) & (east == bw)
    if n == 8:
        north_east = np.zeros((rows, cols))
        north_west = np.zeros((rows, cols))
        south_east = np.zeros((rows, cols))
        south_west = np.zeros((rows, cols))
        north_east[:-1, 1:] = bw[1:, :-1]
        north_west[:-1, :-1] = bw[1:, 1:]
        south_east[1:, 1:] = bw[:-1, :-1]
        south_west[1:, :-1] = bw[:-1, 1:]
        idx &= (
            (north_east == bw)
            & (south_east == bw)
            & (south_west == bw)
            & (north_west == bw)
        )
    return ~idx * bw


def count_runs_and_unique_numbers(arr: np.ndarray) -> tuple:
    """
    Counts the number of continuous runs of numbers in a 1D numpy array
    and returns the count of runs for each unique number and the unique
    numbers.

    Args:
        arr (np.ndarray): The input 1D numpy array of numbers.

    Returns:
        tuple: A tuple containing a dictionary with the count of runs for
        each unique number and the set of unique numbers in the array.
    """
    if arr.size == 0:
        return {}, set()

    unique_numbers = set(arr)
    runs_count = defaultdict(int)
    for num in unique_numbers:
        runs = np.diff(np.where(arr == num)) != 1
        # Add 1 because diff reduces the size by 1
        runs_count[num] = np.count_nonzero(runs) + 1

    return runs_count, unique_numbers


def corr_maps(map1, map2, maptype="normal") -> float:
    """
    correlates two ratemaps together ignoring areas that have zero sampling
    """
    if map1.shape > map2.shape:
        map2 = skimage.transform.resize(map2, map1.shape, mode="reflect")
    elif map1.shape < map2.shape:
        map1 = skimage.transform.resize(map1, map2.shape, mode="reflect")
    map1 = map1.flatten()
    map2 = map2.flatten()
    valid_map1 = np.zeros_like(map1)
    valid_map2 = np.zeros_like(map2)
    if "normal" in maptype:
        np.logical_or((map1 > 0), ~np.isnan(map1), out=valid_map1)
        np.logical_or((map2 > 0), ~np.isnan(map2), out=valid_map2)
    elif "grid" in maptype:
        np.isfinite(map1, out=valid_map1)
        np.isfinite(map2, out=valid_map2)
    valid = np.logical_and(valid_map1, valid_map2)
    r = np.corrcoef(map1[valid], map2[valid])
    return r[1][0]
