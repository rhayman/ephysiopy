{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to ephysiopy","text":""},{"location":"#installation","title":"Installation","text":"<pre><code>pip install ephysiopy\n</code></pre>"},{"location":"#basic-usage","title":"Basic usage","text":"<p>python<code>from ephysiopy.io.recording import OpenEphysBase from pathlib import Path trial = OpenEphysBase(Path(\"/path/to/top/folder\")) trial.load_pos_data() trial.load_neural_data() trial.rate_map(1, 1)</code></p> <p>This will load the data recorded with openephys contained in the folder \"/path/to/top/folder\", load the position data and the neural data (KiloSort output) and plot the cluster 1 on channel 1</p>"},{"location":"reference/","title":"Reference","text":""},{"location":"reference/#recording-data","title":"Recording data","text":""},{"location":"reference/#ephysiopy.io.recording.AxonaTrial","title":"<code>AxonaTrial</code>","text":"<p>               Bases: <code>TrialInterface</code></p> Source code in <code>ephysiopy/io/recording.py</code> <pre><code>class AxonaTrial(TrialInterface):\n    def __init__(self, pname: Path, **kwargs) -&gt; None:\n        use_volts = kwargs.pop(\"volts\", True)\n        pname = Path(pname)\n        super().__init__(pname, **kwargs)\n        self._settings = None\n        self.TETRODE = TetrodeDict(str(self.pname.with_suffix(\"\")), volts=use_volts)\n        self.load_settings()\n\n    def load_lfp(self, *args, **kwargs):\n        from ephysiopy.axona.axonaIO import EEG\n\n        if \"egf\" in args:\n            lfp = EEG(self.pname, egf=1)\n        else:\n            lfp = EEG(self.pname)\n        if lfp is not None:\n            self.EEGCalcs = EEGCalcsGeneric(lfp.sig, lfp.sample_rate)\n\n    def load_neural_data(self, *args, **kwargs):\n        if \"tetrode\" in kwargs.keys():\n            use_volts = kwargs.get(\"volts\", True)\n            self.TETRODE[kwargs[\"tetrode\"], use_volts]  # lazy load\n\n    def load_cluster_data(self, *args, **kwargs):\n        return False\n\n    def get_available_clusters_channels(self) -&gt; dict:\n        clust_chans = {}\n        for k in self.TETRODE.keys():\n            if isinstance(k, int):  # only other key is 'volts'\n                clust_chans[k] = self.TETRODE[k].clusters.tolist()\n        return clust_chans\n\n    def load_settings(self, *args, **kwargs):\n        if self._settings is None:\n            try:\n                settings_io = IO()\n                self.settings = settings_io.getHeader(str(self.pname))\n            except IOError:\n                print(\".set file not loaded\")\n                self.settings = None\n\n    def load_pos_data(\n        self, ppm: int = 300, jumpmax: int = 100, *args, **kwargs\n    ) -&gt; None:\n        try:\n            AxonaPos = Pos(Path(self.pname))\n            P = PosCalcsGeneric(\n                AxonaPos.led_pos[:, 0],\n                AxonaPos.led_pos[:, 1],\n                cm=True,\n                ppm=ppm,\n                jumpmax=jumpmax,\n            )\n            P.sample_rate = AxonaPos.getHeaderVal(AxonaPos.header, \"sample_rate\")\n            P.xyTS = AxonaPos.ts / P.sample_rate  # in seconds now\n            P.postprocesspos(tracker_params={\"SampleRate\": P.sample_rate})\n            print(\"Loaded pos data\")\n            self.PosCalcs = P\n        except IOError:\n            print(\"Couldn't load the pos data\")\n\n    def load_ttl(self, *args, **kwargs) -&gt; bool:\n        from ephysiopy.axona.axonaIO import Stim\n\n        try:\n            self.ttl_data = Stim(self.pname)\n            # ttl times in Stim are in ms\n        except IOError:\n            return False\n        print(\"Loaded ttl data\")\n        return True\n\n    def get_spike_times(\n        self, cluster: int | list = None, tetrode: int | list = None, *args, **kwargs\n    ) -&gt; list | np.ndarray:\n        \"\"\"\n        Parameters:\n            tetrode (int | list):\n            cluster (int | list):\n\n        Returns:\n            spike_times (np.ndarray):\n        \"\"\"\n        if tetrode is not None:\n            if isinstance(cluster, int):\n                return self.TETRODE.get_spike_samples(int(tetrode), int(cluster))\n            elif isinstance(cluster, list) and isinstance(tetrode, list):\n                if len(cluster) == 1:\n                    tetrode = tetrode[0]\n                    cluster = cluster[0]\n                    return self.TETRODE.get_spike_samples(int(tetrode), int(cluster))\n                else:\n                    spikes = []\n                    for tc in zip(tetrode, cluster):\n                        spikes.append(self.TETRODE.get_spike_samples(tc[0], tc[1]))\n                    return spikes\n\n    def apply_filter(self, *trial_filter: TrialFilter) -&gt; np.ndarray:\n        \"\"\"Apply a mask to the data\n\n        Parameters\n        ----------\n        trial_filter (TrialFilter): A namedtuple containing the filter\n            name, start and end values:\n                name (str): The name of the filter\n                start (float): The start value of the filter\n                end (float): The end value of the filter\n\n            Valid names are:\n                'dir' - the directional range to filter for\n                'speed' - min and max speed to filter for\n                'xrange' - min and max values to filter x pos values\n                'yrange' - same as xrange but for y pos\n                'time' - the times to keep / remove specified in ms\n\n            Values are pairs specifying the range of values to filter for\n            from the namedtuple TrialFilter that has fields 'start' and 'end'\n            where 'start' and 'end' are the ranges to filter for\n\n            See ephysiopy.common.utils.TrialFilter for more details\n\n        Returns\n        -------\n        np.ndarray: An array of bools that is True where the mask is applied\n        \"\"\"\n        mask = super().apply_filter(*trial_filter)\n        for tetrode in self.TETRODE.keys():\n            if self.TETRODE[tetrode] is not None:\n                self.TETRODE[tetrode].apply_mask(\n                    mask, sample_rate=self.PosCalcs.sample_rate\n                )\n        return mask\n</code></pre>"},{"location":"reference/#ephysiopy.io.recording.AxonaTrial.apply_filter","title":"<code>apply_filter(*trial_filter)</code>","text":"<p>Apply a mask to the data</p> <p>Parameters:</p> Name Type Description Default <code>trial_filter</code> <code>TrialFilter</code> <p>name, start and end values:     name (str): The name of the filter     start (float): The start value of the filter     end (float): The end value of the filter</p> <p>Valid names are:     'dir' - the directional range to filter for     'speed' - min and max speed to filter for     'xrange' - min and max values to filter x pos values     'yrange' - same as xrange but for y pos     'time' - the times to keep / remove specified in ms</p> <p>Values are pairs specifying the range of values to filter for from the namedtuple TrialFilter that has fields 'start' and 'end' where 'start' and 'end' are the ranges to filter for</p> <p>See ephysiopy.common.utils.TrialFilter for more details</p> <code>()</code> <p>Returns:</p> Type Description <code>np.ndarray: An array of bools that is True where the mask is applied</code> Source code in <code>ephysiopy/io/recording.py</code> <pre><code>def apply_filter(self, *trial_filter: TrialFilter) -&gt; np.ndarray:\n    \"\"\"Apply a mask to the data\n\n    Parameters\n    ----------\n    trial_filter (TrialFilter): A namedtuple containing the filter\n        name, start and end values:\n            name (str): The name of the filter\n            start (float): The start value of the filter\n            end (float): The end value of the filter\n\n        Valid names are:\n            'dir' - the directional range to filter for\n            'speed' - min and max speed to filter for\n            'xrange' - min and max values to filter x pos values\n            'yrange' - same as xrange but for y pos\n            'time' - the times to keep / remove specified in ms\n\n        Values are pairs specifying the range of values to filter for\n        from the namedtuple TrialFilter that has fields 'start' and 'end'\n        where 'start' and 'end' are the ranges to filter for\n\n        See ephysiopy.common.utils.TrialFilter for more details\n\n    Returns\n    -------\n    np.ndarray: An array of bools that is True where the mask is applied\n    \"\"\"\n    mask = super().apply_filter(*trial_filter)\n    for tetrode in self.TETRODE.keys():\n        if self.TETRODE[tetrode] is not None:\n            self.TETRODE[tetrode].apply_mask(\n                mask, sample_rate=self.PosCalcs.sample_rate\n            )\n    return mask\n</code></pre>"},{"location":"reference/#ephysiopy.io.recording.AxonaTrial.get_spike_times","title":"<code>get_spike_times(cluster=None, tetrode=None, *args, **kwargs)</code>","text":"<p>Parameters:     tetrode (int | list):     cluster (int | list):</p> <p>Returns:     spike_times (np.ndarray):</p> Source code in <code>ephysiopy/io/recording.py</code> <pre><code>def get_spike_times(\n    self, cluster: int | list = None, tetrode: int | list = None, *args, **kwargs\n) -&gt; list | np.ndarray:\n    \"\"\"\n    Parameters:\n        tetrode (int | list):\n        cluster (int | list):\n\n    Returns:\n        spike_times (np.ndarray):\n    \"\"\"\n    if tetrode is not None:\n        if isinstance(cluster, int):\n            return self.TETRODE.get_spike_samples(int(tetrode), int(cluster))\n        elif isinstance(cluster, list) and isinstance(tetrode, list):\n            if len(cluster) == 1:\n                tetrode = tetrode[0]\n                cluster = cluster[0]\n                return self.TETRODE.get_spike_samples(int(tetrode), int(cluster))\n            else:\n                spikes = []\n                for tc in zip(tetrode, cluster):\n                    spikes.append(self.TETRODE.get_spike_samples(tc[0], tc[1]))\n                return spikes\n</code></pre>"},{"location":"reference/#ephysiopy.io.recording.OpenEphysBase","title":"<code>OpenEphysBase</code>","text":"<p>               Bases: <code>TrialInterface</code></p> Source code in <code>ephysiopy/io/recording.py</code> <pre><code>class OpenEphysBase(TrialInterface):\n    def __init__(self, pname: Path, **kwargs) -&gt; None:\n        pname = Path(pname)\n        super().__init__(pname, **kwargs)\n        setattr(self, \"sync_message_file\", None)\n        self.load_settings()\n        # The numbers after the strings in this list are the node id's\n        # in openephys\n        record_methods = [\n            \"Acquisition Board [0-9][0-9][0-9]\",\n            \"Acquisition Board\",\n            \"Neuropix-PXI [0-9][0-9][0-9]\",\n            \"Neuropix-PXI\",\n            \"Sources/Neuropix-PXI [0-9][0-9][0-9]\",\n            \"Rhythm FPGA [0-9][0-9][0-9]\",\n            \"Rhythm\",\n            \"Sources/Rhythm FPGA [0-9][0-9][0-9]\",\n        ]\n        rec_method = [\n            re.search(m, k).string\n            for k in self.settings.processors.keys()\n            for m in record_methods\n            if re.search(m, k) is not None\n        ][0]\n        if \"Sources/\" in rec_method:\n            rec_method = rec_method.lstrip(\"Sources/\")\n\n        self.rec_kind = Xml2RecordingKind[rec_method.rpartition(\" \")[0]]\n\n        # Attempt to find the files contained in the parent directory\n        # related to the recording with the default experiment and\n        # recording name\n        self.find_files(pname, **kwargs)\n        self.sample_rate = None\n        self.sample_rate = self.settings.processors[rec_method].sample_rate\n        if self.sample_rate is None:\n            if self.rec_kind == RecordingKind.NEUROPIXELS:\n                self.sample_rate = 30000\n        else:  # rubbish fix - many strs need casting to int/float\n            self.sample_rate = float(self.sample_rate)\n        self.channel_count = self.settings.processors[rec_method].channel_count\n        if self.channel_count is None:\n            if self.rec_kind == RecordingKind.NEUROPIXELS:\n                self.channel_count = 384\n        self.kilodata = None\n        self.template_model = None\n\n    def _get_recording_start_time(self) -&gt; float:\n        \"\"\"\n        Get the recording start time from the sync_messages.txt file\n\n        Returns\n        -------\n        start_time (float) - in seconds\n        \"\"\"\n        recording_start_time = 0.0\n        if self.sync_message_file is not None:\n            with open(self.sync_message_file, \"r\") as f:\n                sync_strs = f.read()\n            sync_lines = sync_strs.split(\"\\n\")\n            for line in sync_lines:\n                if \"Start Time\" in line:\n                    tokens = line.split(\":\")\n                    start_time = int(tokens[-1])\n                    sample_rate = int(tokens[0].split(\"@\")[-1].strip().split()[0])\n                    recording_start_time = start_time / float(sample_rate)\n        self.recording_start_time = recording_start_time\n        return recording_start_time\n\n    def get_spike_times(\n        self, cluster: int | list = None, tetrode: int | list = None, *args, **kwargs\n    ) -&gt; list | np.ndarray:\n        \"\"\"\n        Parameters\n        ----------\n        cluster (int| list)\n        tetrode (int | list)\n\n        Returns\n        -------\n        spike_times (list | np.ndarray): in seconds\n        \"\"\"\n        if not self.clusterData:\n            self.load_cluster_data()\n        if isinstance(cluster, int) and isinstance(tetrode, int):\n            if cluster in self.clusterData.spk_clusters:\n                times = self.clusterData.get_cluster_spike_times(cluster)\n                return times.astype(np.int64) / self.sample_rate\n            else:\n                warnings.warn(\"Cluster not present\")\n        elif isinstance(cluster, list) and isinstance(tetrode, list):\n            times = []\n            for c in cluster:\n                if c in self.clusterData.spk_clusters:\n                    t = self.clusterData.get_cluster_spike_times(c)\n                    times.append(t.astype(np.int64) / self.sample_rate)\n                else:\n                    warnings.warn(\"Cluster not present\")\n            return times\n\n    def load_lfp(self, *args, **kwargs):\n        \"\"\"\n        Valid kwargs are:\n        'target_sample_rate' - int\n            the sample rate to downsample to from the original\n        \"\"\"\n        from scipy import signal\n\n        if self.path2LFPdata is not None:\n            lfp = memmapBinaryFile(\n                os.path.join(self.path2LFPdata, \"continuous.dat\"),\n                n_channels=self.channel_count,\n            )\n            channel = kwargs.get(\"channel\", 0)\n            # set the target sample rate to 250Hz by default to match\n            # Axona EEG data\n            target_sample_rate = kwargs.get(\"target_sample_rate\", 250)\n            denom = np.gcd(int(target_sample_rate), int(self.sample_rate))\n            data = lfp[channel, :]\n            sig = signal.resample_poly(\n                data.astype(float),\n                target_sample_rate / denom,\n                self.sample_rate / denom,\n                0,\n            )\n            self.EEGCalcs = EEGCalcsGeneric(sig, target_sample_rate)\n\n    def load_neural_data(self, *args, **kwargs):\n        if \"path2APdata\" in kwargs.keys():\n            self.path2APdata: Path = Path(kwargs[\"path2APdata\"])\n        n_channels: int = self.channel_count or kwargs[\"nChannels\"]\n        try:\n            self.template_model = TemplateModel(\n                dir_path=self.path2APdata,\n                sample_rate=3e4,\n                dat_path=Path(self.path2APdata).joinpath(\"continuous.dat\"),\n                n_channels_dat=int(n_channels),\n            )\n            print(\"Loaded neural data\")\n        except Exception:\n            warnings.warn(\"Could not find raw data file\")\n\n    def load_settings(self, *args, **kwargs):\n        if self._settings is None:\n            # pname_root gets walked through and over-written with\n            # correct location of settings.xml\n            self.settings = Settings(self.pname)\n            print(\"Loaded settings data\\n\")\n\n    def get_available_clusters_channels(self) -&gt; dict:\n        if self.template_model is None:\n            self.load_neural_data()\n        unique_clusters = np.unique(self.template_model.spike_clusters)\n        clust_chans = dict.fromkeys(np.unique(self.template_model.clusters_channels))\n        for clust_id, chan in enumerate(self.template_model.clusters_channels):\n            if clust_id in unique_clusters:\n                clust_chans[chan] = [\n                    ch\n                    for ch, cl in enumerate(self.template_model.clusters_channels)\n                    if cl == chan\n                ]\n\n        return clust_chans\n\n    def load_cluster_data(self, removeNoiseClusters=True, *args, **kwargs) -&gt; bool:\n        if self.path2KiloSortData is not None:\n            clusterData = KiloSortSession(self.pname)\n        else:\n            return False\n        if clusterData is not None:\n            if clusterData.load():\n                print(\"Loaded KiloSort data\")\n                if removeNoiseClusters:\n                    try:\n                        clusterData.removeKSNoiseClusters()\n                        print(\"Removed noise clusters\")\n                    except Exception:\n                        pass\n        else:\n            return False\n        self.clusterData = clusterData\n        return True\n\n    def load_pos_data(\n        self, ppm: int = 300, jumpmax: int = 100, *args, **kwargs\n    ) -&gt; None:\n        # kwargs valid keys = \"loadTTLPos\" - if present loads the ttl\n        # timestamps not the ones in the plugin folder\n\n        # Only sub-class that doesn't use this is OpenEphysNWB\n        # which needs updating\n        # TODO: Update / overhaul OpenEphysNWB\n        # Load the start time from the sync_messages file\n        if \"cm\" in kwargs:\n            cm = kwargs[\"cm\"]\n        else:\n            cm = True\n\n        recording_start_time = self._get_recording_start_time()\n\n        if self.path2PosData is not None:\n            pos_method = [\n                \"Pos Tracker [0-9][0-9][0-9]\",\n                \"PosTracker [0-9][0-9][0-9]\",\n                \"TrackMe [0-9][0-9][0-9]\",\n                \"TrackingPlugin [0-9][0-9][0-9]\",\n                \"Tracking Port\",\n            ]\n            pos_plugin_name = [\n                re.search(m, k).string\n                for k in self.settings.processors.keys()\n                for m in pos_method\n                if re.search(m, k) is not None\n            ][0]\n            if \"Sources/\" in pos_plugin_name:\n                pos_plugin_name = pos_plugin_name.lstrip(\"Sources/\")\n\n            self.pos_plugin_name = pos_plugin_name\n\n            if \"Tracker\" in pos_plugin_name:\n                print(\"Loading Tracker data...\")\n                pos_data = np.load(os.path.join(self.path2PosData, \"data_array.npy\"))\n            if \"Tracking Port\" in pos_plugin_name:\n                print(\"Loading Tracking Port data...\")\n                pos_data = loadTrackingPluginData(\n                    os.path.join(self.path2PosData, \"data_array.npy\")\n                )\n            if \"TrackingPlugin\" in pos_plugin_name:\n                print(\"Loading TrackingPlugin data...\")\n                pos_data = loadTrackingPluginData(\n                    os.path.join(self.path2PosData, \"data_array.npy\")\n                )\n\n            pos_ts = np.load(os.path.join(self.path2PosData, \"timestamps.npy\"))\n            # pos_ts in seconds\n            pos_ts = np.ravel(pos_ts)\n            if \"TrackMe\" in pos_plugin_name:\n                print(\"Loading TrackMe data...\")\n                n_pos_chans = int(\n                    self.settings.processors[pos_plugin_name].channel_count\n                )\n                pos_data = loadTrackMePluginData(\n                    Path(os.path.join(self.path2PosData, \"continuous.dat\")),\n                    n_channels=n_pos_chans,\n                )\n                if \"loadTTLPos\" in kwargs.keys():\n                    pos_ts = loadTrackMeTTLTimestamps(Path(self.path2EventsData))\n                else:\n                    pos_ts = loadTrackMeTimestamps(Path(self.path2PosData))\n                pos_ts = pos_ts[0 : len(pos_data)]\n            sample_rate = self.settings.processors[pos_plugin_name].sample_rate\n            sample_rate = float(sample_rate) if sample_rate is not None else 50\n            # the timestamps for the Tracker Port plugin are fucked so\n            # we have to infer from the shape of the position data\n            if \"Tracking Port\" in pos_plugin_name:\n                sample_rate = kwargs[\"sample_rate\"] or 50\n                # pos_ts in seconds\n                pos_ts = np.arange(\n                    0, pos_data.shape[0] / sample_rate, 1.0 / sample_rate\n                )\n            if \"TrackMe\" not in pos_plugin_name:\n                xyTS = pos_ts - recording_start_time\n            else:\n                xyTS = pos_ts\n            if self.sync_message_file is not None:\n                recording_start_time = xyTS[0]\n            P = PosCalcsGeneric(\n                pos_data[:, 0],\n                pos_data[:, 1],\n                cm=cm,\n                ppm=ppm,\n                jumpmax=jumpmax,\n            )\n            P.xyTS = xyTS\n            P.sample_rate = sample_rate\n            P.postprocesspos({\"SampleRate\": sample_rate})\n            print(\"Loaded pos data\")\n            self.PosCalcs = P\n        else:\n            warnings.warn(\n                \"Could not find the pos data. \\\n                Make sure there is a pos_data folder with data_array.npy \\\n                and timestamps.npy in\"\n            )\n        self.recording_start_time = recording_start_time\n\n    def load_ttl(self, *args, **kwargs) -&gt; bool:\n        \"\"\"\n        Returns\n        -------\n        loaded (bool) - whether the data was loaded or not\n\n        Notes\n        -----\n        Valid kwargs:\n            StimControl_id (str): This is the string\n                \"StimControl [0-9][0-9][0-9]\" where the numbers\n                are the node id in the openephys signal chain\n            TTL_channel_number (int): The integer value in the \"states.npy\"\n                file that corresponds to the\n                identity of the TTL input on the Digital I/O board on the\n                openephys recording system. i.e. if there is input to BNC\n                port 3 on the digital I/O board then values of 3 in the\n                states.npy file are high TTL values on this input and -3\n                are low TTL values. NB This is important as there could well\n                be other TTL lines that are active and so the states vector\n                will then contain a mix of integer values\n            RippleDetector (str): Loads up the TTL data from the Ripple Detector\n                plugin\n\n        Sets some keys/values in a dict on 'self'\n        called ttl_data, namely:\n\n        ttl_timestamps (list): the times of high ttl pulses in ms\n        stim_duration (int): the duration of the ttl pulse in ms\n        \"\"\"\n        if not Path(self.path2EventsData).exists:\n            return False\n        ttl_ts = np.load(os.path.join(self.path2EventsData, \"timestamps.npy\"))\n        states = np.load(os.path.join(self.path2EventsData, \"states.npy\"))\n        recording_start_time = self._get_recording_start_time()\n        self.ttl_data = {}\n        if \"StimControl_id\" in kwargs.keys():\n            stim_id = kwargs[\"StimControl_id\"]\n            if stim_id in self.settings.processors.keys():\n                duration = getattr(self.settings.processors[stim_id], \"Duration\")\n            else:\n                return False\n            self.ttl_data[\"stim_duration\"] = int(duration)\n        if \"TTL_channel_number\" in kwargs.keys():\n            chan = kwargs[\"TTL_channel_number\"]\n            high_ttl = ttl_ts[states == chan]\n            # get into seconds\n            high_ttl = (high_ttl * 1000.0) - recording_start_time\n            self.ttl_data[\"ttl_timestamps\"] = high_ttl / 1000.0  # in seconds now\n        if \"RippleDetector\" in args:\n            if self.path2RippleDetector:\n                detector_settings = self.settings.get_processor(\"Ripple\")\n                ttl_ts = (\n                    np.load(Path(self.path2RippleDetector) / \"timestamps.npy\")\n                    - self.recording_start_time\n                )\n                ttl_states = np.load(Path(self.path2RippleDetector) / \"states.npy\")\n                save_ttl = int(detector_settings.Ripple_save)\n                out_ttl = int(detector_settings.Ripple_Out)\n                indices_to_throw = []\n\n                for i in range(len(ttl_states) - 2):\n                    i_pair = ttl_states[i : i + 2]\n                    if np.all(i_pair == np.array([save_ttl, out_ttl])):\n                        # be extra sure this is a zero time difference\n                        if np.diff(ttl_ts[i : i + 2]) == 0:\n                            indices_to_throw.append(i)\n\n                mask = np.ones_like(ttl_states, dtype=bool)\n                mask[indices_to_throw] = False\n\n                ttl_ts = ttl_ts[mask]\n                ttl_states = ttl_states[mask]\n\n                laser_ons = ttl_ts[ttl_states == out_ttl]\n                laser_offs = ttl_ts[ttl_states == -out_ttl]\n                no_laser_ons = ttl_ts[ttl_states == save_ttl]\n                self.ttl_data[\"ttl_timestamps\"] = laser_ons\n                self.ttl_data[\"ttl_timestamps_off\"] = laser_offs\n                mean_duration = np.nanmean(laser_offs - laser_ons)\n                self.ttl_data[\"stim_duration\"] = mean_duration\n                self.ttl_data[\"no_laser_ttls\"] = no_laser_ons\n\n        if not self.ttl_data:\n            return False\n        print(\"Loaded ttl data\")\n        return True\n\n    def load_accelerometer(self, target_freq: int = 50) -&gt; bool:\n        if not self.path2LFPdata:\n            return False\n        \"\"\"\n        Need to figure out which of the channels are AUX if we want to load\n        the accelerometer data with minimal user input...\n        Annoyingly, there could also be more than one RecordNode which means\n        the channels might get represented more than once in the structure.oebin\n        file\n\n        Parameters\n        ----------\n        target_freq (int) - the desired frequency when downsampling the aux data\n\n        \"\"\"\n        from ephysiopy.openephys2py.OESettings import OEStructure\n        from ephysiopy.common.ephys_generic import downsample_aux\n\n        oebin = OEStructure(self.pname)\n        aux_chan_nums = []\n        aux_bitvolts = 0\n        for record_node_key in oebin.data.keys():\n            for channel_key in oebin.data[record_node_key].keys():\n                # this thing is a 1-item list\n                if \"continuous\" in channel_key:\n                    for chan_keys in oebin.data[record_node_key][channel_key][0]:\n                        for chan_idx, i_chan in enumerate(\n                            oebin.data[record_node_key][channel_key][0][\"channels\"]\n                        ):\n                            if \"AUX\" in i_chan[\"channel_name\"]:\n                                aux_chan_nums.append(chan_idx)\n                                aux_bitvolts = i_chan[\"bit_volts\"]\n\n        if len(aux_chan_nums) &gt; 0:\n            aux_chan_nums = np.unique(np.array(aux_chan_nums))\n            if self.path2LFPdata is not None:\n                data = memmapBinaryFile(\n                    os.path.join(self.path2LFPdata, \"continuous.dat\"),\n                    n_channels=self.channel_count,\n                )\n                s = slice(min(aux_chan_nums), max(aux_chan_nums) + 1)\n                aux_data = data[s, :]\n                # now downsample the aux data a lot\n                # might take a while so print a message to console\n                print(\n                    f\"Downsampling {aux_data.shape[1]} samples over {aux_data.shape[0]} channels...\"\n                )\n                aux_data = downsample_aux(aux_data, target_freq=target_freq)\n                self.aux_data = aux_data\n                self.aux_data_fs = target_freq\n                self.aux_bitvolts = aux_bitvolts\n                return True\n        else:\n            warnings.warn(\"No AUX data found in structure.oebin file, so not loaded\")\n        return False\n\n    def apply_filter(self, *trial_filter: TrialFilter) -&gt; np.ndarray:\n        \"\"\"Apply a mask to the data\n\n        Parameters\n        ----------\n        trial_filter (TrialFilter): A namedtuple containing the filter\n            name, start and end values\n\n        Returns\n        -------\n            np.array: An array of bools that is True where the mask is applied\n        \"\"\"\n        mask = super().apply_filter(*trial_filter)\n        return mask\n\n    def find_files(\n        self,\n        pname_root: str | Path,\n        experiment_name: str = \"experiment1\",\n        rec_name: str = \"recording1\",\n        **kwargs,\n    ):\n        exp_name = Path(experiment_name)\n        PosTracker_match = (\n            exp_name / rec_name / \"events\" / \"*Pos_Tracker*/BINARY_group*\"\n        )\n        TrackingPlugin_match = (\n            exp_name / rec_name / \"events\" / \"*Tracking_Port*/BINARY_group*\"\n        )\n        TrackMe_match = (\n            exp_name / rec_name / \"continuous\" / \"TrackMe-[0-9][0-9][0-9].TrackingNode\"\n        )\n        RippleDetector_match = (\n            exp_name / rec_name / \"events\" / \"Ripple_Detector*\" / \"TTL\"\n        )\n        sync_file_match = exp_name / rec_name\n        acq_method = \"\"\n        if self.rec_kind == RecordingKind.NEUROPIXELS:\n            # the old OE NPX plugins saved two forms of the data,\n            # one for AP @30kHz and one for LFP @??Hz\n            # the newer plugin saves only the 30kHz data. Also, the\n            # 2.0 probes are saved with Probe[A-Z] appended to the end\n            # of the folder\n            # the older way:\n            acq_method = \"Neuropix-PXI-[0-9][0-9][0-9].\"\n            APdata_match = exp_name / rec_name / \"continuous\" / (acq_method + \"0\")\n            LFPdata_match = exp_name / rec_name / \"continuous\" / (acq_method + \"1\")\n            # the new way:\n            Rawdata_match = (\n                exp_name / rec_name / \"continuous\" / (acq_method + \"Probe[A-Z]\")\n            )\n        elif self.rec_kind == RecordingKind.FPGA:\n            acq_method = \"Rhythm_FPGA-[0-9][0-9][0-9].\"\n            APdata_match = exp_name / rec_name / \"continuous\" / (acq_method + \"0\")\n            LFPdata_match = exp_name / rec_name / \"continuous\" / (acq_method + \"1\")\n            Rawdata_match = (\n                exp_name / rec_name / \"continuous\" / (acq_method + \"Probe[A-Z]\")\n            )\n        else:\n            acq_method = \"Acquisition_Board-[0-9][0-9][0-9].*\"\n            APdata_match = exp_name / rec_name / \"continuous\" / acq_method\n            LFPdata_match = exp_name / rec_name / \"continuous\" / acq_method\n            Rawdata_match = (\n                exp_name / rec_name / \"continuous\" / (acq_method + \"Probe[A-Z]\")\n            )\n        Events_match = (\n            # only dealing with a single TTL channel at the moment\n            exp_name\n            / rec_name\n            / \"events\"\n            / acq_method\n            / \"TTL\"\n        )\n\n        if pname_root is None:\n            pname_root = self.pname_root\n\n        verbose = kwargs.get(\"verbose\", False)\n\n        for d, c, f in os.walk(pname_root):\n            for ff in f:\n                if \".\" not in c:  # ignore hidden directories\n                    if \"data_array.npy\" in ff:\n                        if PurePath(d).match(str(PosTracker_match)):\n                            if self.path2PosData is None:\n                                self.path2PosData = os.path.join(d)\n                                if verbose:\n                                    print(f\"Pos data at: {self.path2PosData}\\n\")\n                            self.path2PosOEBin = Path(d).parents[1]\n                        if PurePath(d).match(\"*pos_data*\"):\n                            if self.path2PosData is None:\n                                self.path2PosData = os.path.join(d)\n                                if verbose:\n                                    print(f\"Pos data at: {self.path2PosData}\\n\")\n                        if PurePath(d).match(str(TrackingPlugin_match)):\n                            if self.path2PosData is None:\n                                self.path2PosData = os.path.join(d)\n                                if verbose:\n                                    print(f\"Pos data at: {self.path2PosData}\\n\")\n                    if \"continuous.dat\" in ff:\n                        if PurePath(d).match(str(APdata_match)):\n                            self.path2APdata = os.path.join(d)\n                            if verbose:\n                                print(f\"Continuous AP data at: {self.path2APdata}\\n\")\n                            self.path2APOEBin = Path(d).parents[1]\n                        if PurePath(d).match(str(LFPdata_match)):\n                            self.path2LFPdata = os.path.join(d)\n                            if verbose:\n                                print(f\"Continuous LFP data at: {self.path2LFPdata}\\n\")\n                        if PurePath(d).match(str(Rawdata_match)):\n                            self.path2APdata = os.path.join(d)\n                            self.path2LFPdata = os.path.join(d)\n                        if PurePath(d).match(str(TrackMe_match)):\n                            self.path2PosData = os.path.join(d)\n                            if verbose:\n                                print(f\"TrackMe posdata at: {self.path2PosData}\\n\")\n                    if \"sync_messages.txt\" in ff:\n                        if PurePath(d).match(str(sync_file_match)):\n                            sync_file = os.path.join(d, \"sync_messages.txt\")\n                            if fileContainsString(sync_file, \"Start Time\"):\n                                self.sync_message_file = sync_file\n                                if verbose:\n                                    print(f\"sync_messages file at: {sync_file}\\n\")\n                    if \"full_words.npy\" in ff:\n                        if PurePath(d).match(str(Events_match)):\n                            self.path2EventsData = os.path.join(d)\n                            if verbose:\n                                print(f\"Event data at: {self.path2EventsData}\\n\")\n                        if PurePath(d).match(str(RippleDetector_match)):\n                            self.path2RippleDetector = os.path.join(d)\n                            if verbose:\n                                print(\n                                    f\"Ripple Detector plugin found at {self.path2RippleDetector}\\n\"\n                                )\n                    if \".nwb\" in ff:\n                        self.path2NWBData = os.path.join(d, ff)\n                        if verbose:\n                            print(f\"nwb data at: {self.path2NWBData}\\n\")\n                    if \"spike_templates.npy\" in ff:\n                        self.path2KiloSortData = os.path.join(d)\n                        if verbose:\n                            print(f\"Found KiloSort data at {self.path2KiloSortData}\\n\")\n</code></pre>"},{"location":"reference/#ephysiopy.io.recording.OpenEphysBase._get_recording_start_time","title":"<code>_get_recording_start_time()</code>","text":"<p>Get the recording start time from the sync_messages.txt file</p> <p>Returns:</p> Type Description <code>start_time (float) - in seconds</code> Source code in <code>ephysiopy/io/recording.py</code> <pre><code>def _get_recording_start_time(self) -&gt; float:\n    \"\"\"\n    Get the recording start time from the sync_messages.txt file\n\n    Returns\n    -------\n    start_time (float) - in seconds\n    \"\"\"\n    recording_start_time = 0.0\n    if self.sync_message_file is not None:\n        with open(self.sync_message_file, \"r\") as f:\n            sync_strs = f.read()\n        sync_lines = sync_strs.split(\"\\n\")\n        for line in sync_lines:\n            if \"Start Time\" in line:\n                tokens = line.split(\":\")\n                start_time = int(tokens[-1])\n                sample_rate = int(tokens[0].split(\"@\")[-1].strip().split()[0])\n                recording_start_time = start_time / float(sample_rate)\n    self.recording_start_time = recording_start_time\n    return recording_start_time\n</code></pre>"},{"location":"reference/#ephysiopy.io.recording.OpenEphysBase.apply_filter","title":"<code>apply_filter(*trial_filter)</code>","text":"<p>Apply a mask to the data</p> <p>Parameters:</p> Name Type Description Default <code>trial_filter</code> <code>TrialFilter</code> <p>name, start and end values</p> <code>()</code> <p>Returns:</p> Type Description <code>    np.array: An array of bools that is True where the mask is applied</code> Source code in <code>ephysiopy/io/recording.py</code> <pre><code>def apply_filter(self, *trial_filter: TrialFilter) -&gt; np.ndarray:\n    \"\"\"Apply a mask to the data\n\n    Parameters\n    ----------\n    trial_filter (TrialFilter): A namedtuple containing the filter\n        name, start and end values\n\n    Returns\n    -------\n        np.array: An array of bools that is True where the mask is applied\n    \"\"\"\n    mask = super().apply_filter(*trial_filter)\n    return mask\n</code></pre>"},{"location":"reference/#ephysiopy.io.recording.OpenEphysBase.get_spike_times","title":"<code>get_spike_times(cluster=None, tetrode=None, *args, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>cluster</code> <code>int | list</code> <code>None</code> <code>tetrode</code> <code>int | list</code> <code>None</code> <p>Returns:</p> Type Description <code>spike_times (list | np.ndarray): in seconds</code> Source code in <code>ephysiopy/io/recording.py</code> <pre><code>def get_spike_times(\n    self, cluster: int | list = None, tetrode: int | list = None, *args, **kwargs\n) -&gt; list | np.ndarray:\n    \"\"\"\n    Parameters\n    ----------\n    cluster (int| list)\n    tetrode (int | list)\n\n    Returns\n    -------\n    spike_times (list | np.ndarray): in seconds\n    \"\"\"\n    if not self.clusterData:\n        self.load_cluster_data()\n    if isinstance(cluster, int) and isinstance(tetrode, int):\n        if cluster in self.clusterData.spk_clusters:\n            times = self.clusterData.get_cluster_spike_times(cluster)\n            return times.astype(np.int64) / self.sample_rate\n        else:\n            warnings.warn(\"Cluster not present\")\n    elif isinstance(cluster, list) and isinstance(tetrode, list):\n        times = []\n        for c in cluster:\n            if c in self.clusterData.spk_clusters:\n                t = self.clusterData.get_cluster_spike_times(c)\n                times.append(t.astype(np.int64) / self.sample_rate)\n            else:\n                warnings.warn(\"Cluster not present\")\n        return times\n</code></pre>"},{"location":"reference/#ephysiopy.io.recording.OpenEphysBase.load_lfp","title":"<code>load_lfp(*args, **kwargs)</code>","text":"<p>Valid kwargs are: 'target_sample_rate' - int     the sample rate to downsample to from the original</p> Source code in <code>ephysiopy/io/recording.py</code> <pre><code>def load_lfp(self, *args, **kwargs):\n    \"\"\"\n    Valid kwargs are:\n    'target_sample_rate' - int\n        the sample rate to downsample to from the original\n    \"\"\"\n    from scipy import signal\n\n    if self.path2LFPdata is not None:\n        lfp = memmapBinaryFile(\n            os.path.join(self.path2LFPdata, \"continuous.dat\"),\n            n_channels=self.channel_count,\n        )\n        channel = kwargs.get(\"channel\", 0)\n        # set the target sample rate to 250Hz by default to match\n        # Axona EEG data\n        target_sample_rate = kwargs.get(\"target_sample_rate\", 250)\n        denom = np.gcd(int(target_sample_rate), int(self.sample_rate))\n        data = lfp[channel, :]\n        sig = signal.resample_poly(\n            data.astype(float),\n            target_sample_rate / denom,\n            self.sample_rate / denom,\n            0,\n        )\n        self.EEGCalcs = EEGCalcsGeneric(sig, target_sample_rate)\n</code></pre>"},{"location":"reference/#ephysiopy.io.recording.OpenEphysBase.load_ttl","title":"<code>load_ttl(*args, **kwargs)</code>","text":"<p>Returns:</p> Type Description <code>loaded (bool) - whether the data was loaded or not</code> Notes <p>Valid kwargs:     StimControl_id (str): This is the string         \"StimControl [0-9][][0-9]\" where the numbers         are the node id in the openephys signal chain     TTL_channel_number (int): The integer value in the \"states.npy\"         file that corresponds to the         identity of the TTL input on the Digital I/O board on the         openephys recording system. i.e. if there is input to BNC         port 3 on the digital I/O board then values of 3 in the         states.npy file are high TTL values on this input and -3         are low TTL values. NB This is important as there could well         be other TTL lines that are active and so the states vector         will then contain a mix of integer values     RippleDetector (str): Loads up the TTL data from the Ripple Detector         plugin</p> <p>Sets some keys/values in a dict on 'self' called ttl_data, namely:</p> <p>ttl_timestamps (list): the times of high ttl pulses in ms stim_duration (int): the duration of the ttl pulse in ms</p> Source code in <code>ephysiopy/io/recording.py</code> <pre><code>def load_ttl(self, *args, **kwargs) -&gt; bool:\n    \"\"\"\n    Returns\n    -------\n    loaded (bool) - whether the data was loaded or not\n\n    Notes\n    -----\n    Valid kwargs:\n        StimControl_id (str): This is the string\n            \"StimControl [0-9][0-9][0-9]\" where the numbers\n            are the node id in the openephys signal chain\n        TTL_channel_number (int): The integer value in the \"states.npy\"\n            file that corresponds to the\n            identity of the TTL input on the Digital I/O board on the\n            openephys recording system. i.e. if there is input to BNC\n            port 3 on the digital I/O board then values of 3 in the\n            states.npy file are high TTL values on this input and -3\n            are low TTL values. NB This is important as there could well\n            be other TTL lines that are active and so the states vector\n            will then contain a mix of integer values\n        RippleDetector (str): Loads up the TTL data from the Ripple Detector\n            plugin\n\n    Sets some keys/values in a dict on 'self'\n    called ttl_data, namely:\n\n    ttl_timestamps (list): the times of high ttl pulses in ms\n    stim_duration (int): the duration of the ttl pulse in ms\n    \"\"\"\n    if not Path(self.path2EventsData).exists:\n        return False\n    ttl_ts = np.load(os.path.join(self.path2EventsData, \"timestamps.npy\"))\n    states = np.load(os.path.join(self.path2EventsData, \"states.npy\"))\n    recording_start_time = self._get_recording_start_time()\n    self.ttl_data = {}\n    if \"StimControl_id\" in kwargs.keys():\n        stim_id = kwargs[\"StimControl_id\"]\n        if stim_id in self.settings.processors.keys():\n            duration = getattr(self.settings.processors[stim_id], \"Duration\")\n        else:\n            return False\n        self.ttl_data[\"stim_duration\"] = int(duration)\n    if \"TTL_channel_number\" in kwargs.keys():\n        chan = kwargs[\"TTL_channel_number\"]\n        high_ttl = ttl_ts[states == chan]\n        # get into seconds\n        high_ttl = (high_ttl * 1000.0) - recording_start_time\n        self.ttl_data[\"ttl_timestamps\"] = high_ttl / 1000.0  # in seconds now\n    if \"RippleDetector\" in args:\n        if self.path2RippleDetector:\n            detector_settings = self.settings.get_processor(\"Ripple\")\n            ttl_ts = (\n                np.load(Path(self.path2RippleDetector) / \"timestamps.npy\")\n                - self.recording_start_time\n            )\n            ttl_states = np.load(Path(self.path2RippleDetector) / \"states.npy\")\n            save_ttl = int(detector_settings.Ripple_save)\n            out_ttl = int(detector_settings.Ripple_Out)\n            indices_to_throw = []\n\n            for i in range(len(ttl_states) - 2):\n                i_pair = ttl_states[i : i + 2]\n                if np.all(i_pair == np.array([save_ttl, out_ttl])):\n                    # be extra sure this is a zero time difference\n                    if np.diff(ttl_ts[i : i + 2]) == 0:\n                        indices_to_throw.append(i)\n\n            mask = np.ones_like(ttl_states, dtype=bool)\n            mask[indices_to_throw] = False\n\n            ttl_ts = ttl_ts[mask]\n            ttl_states = ttl_states[mask]\n\n            laser_ons = ttl_ts[ttl_states == out_ttl]\n            laser_offs = ttl_ts[ttl_states == -out_ttl]\n            no_laser_ons = ttl_ts[ttl_states == save_ttl]\n            self.ttl_data[\"ttl_timestamps\"] = laser_ons\n            self.ttl_data[\"ttl_timestamps_off\"] = laser_offs\n            mean_duration = np.nanmean(laser_offs - laser_ons)\n            self.ttl_data[\"stim_duration\"] = mean_duration\n            self.ttl_data[\"no_laser_ttls\"] = no_laser_ons\n\n    if not self.ttl_data:\n        return False\n    print(\"Loaded ttl data\")\n    return True\n</code></pre>"},{"location":"reference/#ephysiopy.io.recording.TrialInterface","title":"<code>TrialInterface</code>","text":"<p>               Bases: <code>FigureMaker</code></p> <p>Defines a minimal and required set of methods for loading electrophysiology data recorded using Axona or OpenEphys (OpenEphysNWB is there but not used)</p> <p>Attributes:</p> Name Type Description <code>pname</code> <code>str</code> <p>the absolute pathname of the top-level data directory</p> <code>settings</code> <code>dict</code> <p>contains metadata about the trial</p> <code>PosCalcs</code> <code>PosCalcsGeneric</code> <p>contains the positional data for the trial</p> <code>RateMap</code> <code>RateMap</code> <p>methods for binning data mostly</p> <code>EEGCalcs</code> <code>EEGCalcs</code> <p>methods for dealing with LFP data</p> <code>clusterData</code> <code>clusterData</code> <p>contains results of a spike sorting session (i.e. KiloSort)</p> <code>recording_start_time</code> <code>float</code> <p>the start time of the recording in seconds</p> <code>sync_message_file</code> <code>Path</code> <p>the location of the sync_message_file (OpenEphys)</p> <code>ttl_data</code> <code>dict</code> <p>ttl data including timestamps, ids and states</p> <code>accelerometer_data</code> <code>ndarray</code> <p>data relating to headstage accelerometers</p> <code>path2PosData</code> <code>Path</code> <p>location of the positional data</p> <code>mask_array</code> <code>MaskedArray</code> <p>contains the mask (if applied) for positional data</p> <code>filter</code> <code>TrialFilter</code> <p>contains details of the filter applied to the positional data</p> Source code in <code>ephysiopy/io/recording.py</code> <pre><code>class TrialInterface(FigureMaker, metaclass=abc.ABCMeta):\n    \"\"\"\n    Defines a minimal and required set of methods for loading\n    electrophysiology data recorded using Axona or OpenEphys\n    (OpenEphysNWB is there but not used)\n\n    Attributes\n    ----------\n    pname : str\n        the absolute pathname of the top-level data directory\n    settings : dict\n        contains metadata about the trial\n    PosCalcs : PosCalcsGeneric\n        contains the positional data for the trial\n    RateMap : RateMap\n        methods for binning data mostly\n    EEGCalcs : EEGCalcs\n        methods for dealing with LFP data\n    clusterData : clusterData\n        contains results of a spike sorting session (i.e. KiloSort)\n    recording_start_time : float\n        the start time of the recording in seconds\n    sync_message_file : Path\n        the location of the sync_message_file (OpenEphys)\n    ttl_data : dict\n        ttl data including timestamps, ids and states\n    accelerometer_data : np.ndarray\n        data relating to headstage accelerometers\n    path2PosData : Path\n        location of the positional data\n    mask_array : np.ma.MaskedArray\n        contains the mask (if applied) for positional data\n    filter : TrialFilter\n        contains details of the filter applied to the positional data\n\n    \"\"\"\n\n    def __init__(self, pname: Path, **kwargs) -&gt; None:\n        assert Path(pname).exists(), f\"Path provided doesnt exist: {pname}\"\n        self._pname = pname\n        self._settings = None\n        self._PosCalcs = None\n        self._RateMap = None\n        self._EEGCalcs = None\n        self._sync_message_file = None\n        self._clusterData = None  # KiloSortSession\n        self._recording_start_time = None  # float\n        self._ttl_data = None  # dict\n        self._accelerometer_data = None\n        self._path2PosData = None  # Path or str\n        self._filter: list = []\n        self._mask_array = None\n\n    @classmethod\n    def __subclasshook__(cls, subclass):\n        return (\n            hasattr(subclass, \"load_neural_data\")\n            and callable(subclass.load_neural_data)\n            and hasattr(subclass, \"load_lfp\")\n            and callable(subclass.load_lfp)\n            and hasattr(subclass, \"load_pos\")\n            and callable(subclass.load_pos)\n            and hasattr(subclass, \"load_cluster_data\")\n            and callable(subclass.load_cluster_data)\n            and hasattr(subclass, \"load_settings\")\n            and callable(subclass.load_settings)\n            and hasattr(subclass, \"get_spike_times\")\n            and callable(subclass.get_spike_times)\n            and hasattr(subclass, \"get_available_clusters_channels\")\n            and callable(subclass.get_available_clusters_channels)\n            and hasattr(subclass, \"load_ttl\")\n            and callable(subclass.load_ttl)\n            or NotImplemented\n        )\n\n    @property\n    def pname(self):\n        return self._pname\n\n    @pname.setter\n    def pname(self, val):\n        self._pname = val\n\n    @property\n    def settings(self):\n        return self._settings\n\n    @settings.setter\n    def settings(self, val):\n        self._settings = val\n\n    @property\n    def PosCalcs(self):\n        return self._PosCalcs\n\n    @PosCalcs.setter\n    def PosCalcs(self, val):\n        self._PosCalcs = val\n\n    @property\n    def RateMap(self):\n        return self._RateMap\n\n    @RateMap.setter\n    def RateMap(self, value):\n        self._RateMap = value\n\n    @property\n    def EEGCalcs(self):\n        return self._EEGCalcs\n\n    @EEGCalcs.setter\n    def EEGCalcs(self, val):\n        self._EEGCalcs = val\n\n    @property\n    def clusterData(self):\n        return self._clusterData\n\n    @clusterData.setter\n    def clusterData(self, val):\n        self._clusterData = val\n\n    @property\n    def recording_start_time(self):\n        return self._recording_start_time\n\n    @recording_start_time.setter\n    def recording_start_time(self, val):\n        self._recording_start_time = val\n\n    @property\n    def sync_message_file(self):\n        return self._sync_message_file\n\n    @sync_message_file.setter\n    def sync_message_file(self, val):\n        self._sync_message_file = val\n\n    @property\n    def ttl_data(self):\n        return self._ttl_data\n\n    @ttl_data.setter\n    def ttl_data(self, val):\n        self._ttl_data = val\n\n    @property\n    def accelerometer_data(self):\n        return self._accelerometer_data\n\n    @accelerometer_data.setter\n    def accelerometer_data(self, val):\n        self._accelerometer_data = val\n\n    @property\n    def path2PosData(self):\n        return self._path2PosData\n\n    @path2PosData.setter\n    def path2PosData(self, val):\n        self._path2PosData = val\n\n    @property\n    def mask_array(self):\n        if self._mask_array is None:\n            if self.PosCalcs:\n                self._mask_array = np.ma.MaskedArray(\n                    np.zeros(shape=(1, self.PosCalcs.npos), dtype=bool)\n                )\n            else:\n                self._mask_array = np.ma.MaskedArray(np.zeros(shape=(1, 1), dtype=bool))\n            return self._mask_array\n        else:\n            return self._mask_array\n\n    @mask_array.setter\n    def mask_array(self, val):\n        if self._mask_array is None:\n            self._mask_array = np.ma.MaskedArray(np.zeros(shape=(1, 1), dtype=bool))\n        if not isinstance(val, np.ma.MaskedArray):\n            if isinstance(val, (np.ndarray, list)):\n                self._mask_array = np.ma.MaskedArray(val)\n            elif isinstance(val, bool):\n                self._mask_array.fill(val)\n            else:\n                raise TypeError(\"Need an array-like input\")\n        else:\n            self._mask_array = val\n\n    @property\n    def filter(self):\n        return self._filter\n\n    def _update_filter(self, val: TrialFilter | None):\n        if val is None:\n            self._filter = []\n        else:\n            if val not in self._filter:\n                self.filter.append(val)\n        return self._filter\n\n    @abc.abstractmethod\n    def load_lfp(self, *args, **kwargs):\n        \"\"\"Load the LFP data\"\"\"\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def load_neural_data(self, *args, **kwargs):\n        \"\"\"Load the neural data\"\"\"\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def load_pos_data(self, ppm: int = 300, jumpmax: int = 100, *args, **kwargs):\n        \"\"\"\n        Load the position data\n\n        Parameters\n        ----------\n        ppm (int): pixels per metre\n        jumpmax (int): max jump in pixels between positions, more\n            than this and the position is interpolated over\n        \"\"\"\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def load_cluster_data(self, *args, **kwargs) -&gt; bool:\n        \"\"\"Load the cluster data (Kilosort/ Axona cut/ whatever else\"\"\"\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def load_settings(self, *args, **kwargs):\n        \"\"\"Loads the format specific settings file\"\"\"\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def load_ttl(self, *args, **kwargs) -&gt; bool:\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def get_spike_times(\n        self, cluster: int | list, channel: int | list, *args, **kwargs\n    ) -&gt; list | np.ndarray:\n        \"\"\"Returns the times of an individual cluster\"\"\"\n        raise NotImplementedError\n\n    def apply_filter(self, *trial_filter: TrialFilter) -&gt; np.ndarray:\n        \"\"\"Apply a mask to the data\n\n        Parameters\n        ----------\n        trial_filter (TrialFilter): A namedtuple containing the filter\n            name, start and end values\n            name (str): The name of the filter\n            start (float): The start value of the filter\n            end (float): The end value of the filter\n\n            Valid names are:\n                'dir' - the directional range to filter for\n                'speed' - min and max speed to filter for\n                'xrange' - min and max values to filter x pos values\n                'yrange' - same as xrange but for y pos\n                'time' - the times to keep / remove specified in ms\n\n            Values are pairs specifying the range of values to filter for\n            from the namedtuple TrialFilter that has fields 'start' and 'end'\n            where 'start' and 'end' are the ranges to filter for\n\n        Returns\n        -------\n        np.ndarray: An array of bools that is True where the mask is applied\n        \"\"\"\n        if len(trial_filter) == 0:\n            bool_arr = False\n            self.mask_array = False\n            self._update_filter(None)\n        else:\n            for i_filter in trial_filter:\n                self._update_filter(i_filter)\n                if i_filter is None:\n                    mask = False\n                elif i_filter.name is None:\n                    mask = False\n                else:\n                    if \"dir\" in i_filter.name and isinstance(i_filter.start, str):\n                        if len(i_filter.start) == 1:\n                            if \"w\" in i_filter.start:\n                                start = 135\n                                end = 225\n                            elif \"e\" in i_filter.start:\n                                start = 315\n                                end = 45\n                            elif \"s\" in i_filter.start:\n                                start = 225\n                                end = 315\n                            elif \"n\" in i_filter.start:\n                                start = 45\n                                end = 135\n                            else:\n                                raise ValueError(\"Invalid direction\")\n                        else:\n                            raise ValueError(\"filter must contain a key / value pair\")\n                        bool_arr = np.logical_and(\n                            self.PosCalcs.dir &gt; start, self.PosCalcs.dir &lt; end\n                        )\n                    if \"speed\" in i_filter.name:\n                        if i_filter.start &gt; i_filter.end:\n                            raise ValueError(\n                                \"First value must be less than the second one\"\n                            )\n                        else:\n                            bool_arr = np.logical_and(\n                                self.PosCalcs.speed &gt; i_filter.start,\n                                self.PosCalcs.speed &lt; i_filter.end,\n                            )\n                    elif \"dir\" in i_filter.name:\n                        if i_filter.start &lt; i_filter.end:\n                            bool_arr = np.logical_and(\n                                self.PosCalcs.dir &gt; i_filter.start,\n                                self.PosCalcs.dir &lt; i_filter.end,\n                            )\n                        else:\n                            bool_arr = np.logical_or(\n                                self.PosCalcs.dir &gt; i_filter.start,\n                                self.PosCalcs.dir &lt; i_filter.end,\n                            )\n                    elif \"xrange\" in i_filter.name:\n                        bool_arr = np.logical_and(\n                            self.PosCalcs.xy[0, :] &gt; i_filter.start,\n                            self.PosCalcs.xy[0, :] &lt; i_filter.end,\n                        )\n                    elif \"yrange\" in i_filter.name:\n                        bool_arr = np.logical_and(\n                            self.PosCalcs.xy[1, :] &gt; i_filter.start,\n                            self.PosCalcs.xy[1, :] &lt; i_filter.end,\n                        )\n                    elif \"time\" in i_filter.name:\n                        # takes the form of 'from' - 'to' times in SECONDS\n                        # such that only pos's between these ranges are KEPT\n                        from_time = int(i_filter.start * self.PosCalcs.sample_rate)\n                        to_time = int(i_filter.end * self.PosCalcs.sample_rate)\n                        bool_arr = np.zeros(shape=(1, self.PosCalcs.npos), dtype=bool)\n                        bool_arr[:, from_time:to_time] = True\n                    else:\n                        raise KeyError(\"Unrecognised key\")\n                self.mask_array = np.logical_or(self.mask_array, bool_arr)\n\n        mask = np.expand_dims(np.any(self.mask_array, axis=0), 0)\n        self.mask_array = mask\n        if self.EEGCalcs:\n            self.EEGCalcs.apply_mask(mask)\n        if self.PosCalcs:\n            self.PosCalcs.apply_mask(mask)\n        if self.RateMap:\n            self.RateMap.apply_mask(mask)\n        if self.clusterData:\n            self.clusterData.apply_mask(\n                mask,\n                xy_ts=self.PosCalcs.xyTS,\n                sample_rate=self.PosCalcs.sample_rate,\n            )\n        return mask\n\n    def initialise(self):\n        self.RateMap = RateMap(self.PosCalcs)\n        self.npos = self.PosCalcs.xy.shape[1]\n\n    def get_available_clusters_channels(self) -&gt; dict:\n        raise NotImplemented\n\n    def get_spike_times_binned_into_position(\n        self, cluster: int | list, channel: int | list\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Parameters\n        ----------\n        cluster (int | list): The cluster(s).\n        channel (int | list): The channel(s).\n\n        Returns\n        -------\n        np.ndarray - the spike times binned into the position data\n        \"\"\"\n        ts = self.get_spike_times(cluster, channel)\n        if not isinstance(ts, list):\n            ts = [ts]\n        n_clusters = 1\n        if isinstance(n_clusters, list):\n            n_clusters = len(cluster)\n        n_pos = self.PosCalcs.npos\n        binned = np.zeros((n_clusters, n_pos))\n        for i, t in enumerate(ts):\n            spk_binned = np.bincount(\n                (t * self.PosCalcs.sample_rate).astype(int), minlength=n_pos\n            )\n            if len(spk_binned) &gt; n_pos:\n                spk_binned = spk_binned[:n_pos]\n            binned[i, :] = spk_binned\n        return binned\n\n    def _get_spike_pos_idx(\n        self, cluster: int | list | None, channel: int | list, **kwargs\n    ):\n        \"\"\"\n        Returns the indices into the position data at which some cluster\n        on a given channel emitted putative spikes.\n\n        Parameters\n        ----------\n        cluster (int | list): The cluster(s). NB this can be None in which\n                case the \"spike times\" are equal to the position times, which\n                means data binned using these indices will be equivalent to\n                binning up just the position data alone.\n\n        channel (int | list): The channel identity. Ignored if cluster is None\n\n        Returns\n        -------\n        np.ndarray: The indices into the position data at which the spikes\n                occurred.\n        \"\"\"\n        pos_times = getattr(self.PosCalcs, \"xyTS\")\n        if cluster is None:\n            spk_times = getattr(self.PosCalcs, \"xyTS\")\n        elif isinstance(cluster, int):\n            spk_times = self.get_spike_times(cluster, channel)\n        elif isinstance(cluster, list) and len(cluster) == 1:\n            spk_times = self.get_spike_times(cluster[0], channel[0])\n        elif isinstance(cluster, list) and len(cluster) &gt; 1:\n            assert len(cluster) == len(\n                channel\n            ), \"Cluster and channel lists must be same length\"\n            idx = []\n            for clust, chan in zip(cluster, channel):\n                spk_times = self.get_spike_times(clust, chan)\n                _idx = np.searchsorted(pos_times, spk_times, side=\"right\") - 1\n                if np.any(_idx &gt;= self.PosCalcs.npos):\n                    _idx = np.delete(\n                        _idx, np.s_[np.argmax(_idx &gt;= self.PosCalcs.npos) :]\n                    )\n                idx.append(_idx)\n            return idx\n\n        idx = np.searchsorted(pos_times, spk_times, side=\"right\") - 1\n        if np.any(idx &gt;= self.PosCalcs.npos):\n            idx = np.delete(idx, np.s_[np.argmax(idx &gt;= self.PosCalcs.npos) :])\n\n        if kwargs.get(\"do_shuffle\", False):\n            n_shuffles = kwargs.get(\"n_shuffles\", 100)\n            random_seed = kwargs.get(\"random_seed\", None)\n            r = np.random.default_rng(random_seed)\n            time_shifts = r.integers(\n                low=30 * self.PosCalcs.sample_rate,\n                high=self.PosCalcs.npos - (30 * self.PosCalcs.sample_rate),\n                size=n_shuffles,\n            )\n            shifted_idx = []\n            for shift in time_shifts:\n                shifted_idx.append(shift_vector(idx, shift, maxlen=self.PosCalcs.npos))\n            return shifted_idx\n\n        return idx\n\n    def _get_map(\n        self,\n        cluster: int | list,\n        channel: int | list,\n        var2bin: VariableToBin.XY,\n        **kwargs,\n    ) -&gt; BinnedData:\n        \"\"\"\n        This function generates a rate map for a given cluster and channel.\n\n        Parameters\n        ----------\n        cluster (int | list): The cluster(s).\n        channel (int | list): The channel(s).\n        var2bin (VariableToBin.XY): The variable to bin. This is an enum that specifies the type of variable to bin.\n        **kwargs:\n            do_shuffle (bool): If True, the rate map will be shuffled by the default number of shuffles (100).\n                            If the n_shuffles keyword is provided, the rate map will be shuffled by that number of shuffles, and\n                            an array of shuffled rate maps will be returned e.g [100 x nx x ny].\n                            The shuffles themselves are generated by shifting the spike times by a random amount between 30s and the\n                            length of the position data minus 30s. The random amount is drawn from a uniform distribution. In order to preserve\n                            the shifts over multiple calls to this function, the option is provided to set the random seed to a fixed\n                            value using the random_seed keyword.\n                            Default is False\n            n_shuffles (int): The number of shuffles to perform. Default is 100.\n            random_seed (int): The random seed to use for the shuffles. Default is None.\n\n\n        Returns\n        -------\n        np.ndarray: The rate map as a numpy array.\n\n        \"\"\"\n        if not self.RateMap:\n            self.initialise()\n        spk_times_in_pos_samples = self._get_spike_pos_idx(cluster, channel, **kwargs)\n        npos = self._PosCalcs.npos\n        if (\n            isinstance(spk_times_in_pos_samples, list)\n            and len(spk_times_in_pos_samples) == 1\n        ):\n            spk_times_in_pos_samples = np.array(spk_times_in_pos_samples[0])\n        if isinstance(spk_times_in_pos_samples, np.ndarray):\n            spk_weights = np.bincount(spk_times_in_pos_samples, minlength=npos)\n            if len(spk_weights) &gt; npos:\n                spk_weights = np.delete(spk_weights, np.s_[npos:], 0)\n\n        elif (\n            isinstance(spk_times_in_pos_samples, list)\n            and len(spk_times_in_pos_samples) &gt; 1\n        ):  # likely the result of a shuffle arg passed to get_spike_pos_idx\n            # TODO: but not necessarily - could be multiple clusters/ channels have been passed\n            weights = []\n            for spk_idx in spk_times_in_pos_samples:\n                w = np.bincount(spk_idx, minlength=npos)\n                if len(w) &gt; npos:\n                    w = np.delete(w, np.s_[npos:], 0)\n                weights.append(w)\n            spk_weights = np.array(weights)\n\n        kwargs[\"var_type\"] = var2bin\n        rmap = self.RateMap.get_map(spk_weights, **kwargs)\n        return rmap\n\n    def get_rate_map(\n        self, cluster: int | list, channel: int | list, **kwargs\n    ) -&gt; BinnedData:\n        \"\"\"\n        Gets the rate map for the specified cluster(s) and channel.\n\n        Parameters\n        ----------\n        cluster (int | list): The cluster(s) to get the speed vs rate for.\n        channel (int | list): The channel(s) number.\n        **kwargs: Additional keyword arguments passed to _get_map\n\n        Returns\n        -------\n        BinnedData - the binned data\n        \"\"\"\n        return self._get_map(cluster, channel, VariableToBin.XY, **kwargs)\n\n    def get_hd_map(\n        self, cluster: int | list, channel: int | list, **kwargs\n    ) -&gt; BinnedData:\n        \"\"\"\n        Gets the head direction map for the specified cluster(s) and channel.\n\n        Parameters\n        ----------\n        cluster (int | list): The cluster(s) to get the speed vs rate for.\n        channel (int | list): The channel(s) number.\n        **kwargs: Additional keyword arguments passed to _get_map\n\n        Returns\n        -------\n        BinnedData - the binned data\n        \"\"\"\n\n        return self._get_map(cluster, channel, VariableToBin.DIR, **kwargs)\n\n    def get_eb_map(\n        self, cluster: int | list, channel: int | list, **kwargs\n    ) -&gt; BinnedData:\n        \"\"\"\n        Gets the edge bin map for the specified cluster(s) and channel.\n\n        Parameters\n        ----------\n        cluster (int | list): The cluster(s) to get the speed vs rate for.\n        channel (int | list): The channel(s) number.\n        **kwargs: Additional keyword arguments passed to _get_map\n\n        Returns\n        -------\n        BinnedData - the binned data\n        \"\"\"\n        return self._get_map(cluster, channel, VariableToBin.EGO_BOUNDARY, **kwargs)\n\n    def get_speed_v_rate_map(\n        self, cluster: int | list, channel: int | list, **kwargs\n    ) -&gt; BinnedData:\n        \"\"\"\n        Gets the speed vs rate for the specified cluster(s) and channel.\n\n        Parameters\n        ----------\n        cluster (int | list): The cluster(s) to get the speed vs rate for.\n        channel (int | list): The channel(s) number.\n        **kwargs: Additional keyword arguments passed to _get_map\n\n        Returns\n        -------\n        BinnedData - the binned data\n        \"\"\"\n        return self._get_map(cluster, channel, VariableToBin.SPEED, **kwargs)\n\n    def get_speed_v_hd_map(\n        self, cluster: int | list, channel: int | list, **kwargs\n    ) -&gt; BinnedData:\n        \"\"\"\n        Gets the speed vs head direction map for the specified cluster(s) and channel.\n\n        Parameters\n        ----------\n        cluster (int | list): The cluster(s) to get the speed vs head direction map for.\n        channel (int | list): The channel number.\n        **kwargs: Additional keyword arguments passed to _get_map\n        \"\"\"\n        # binsize is in cm/s and degrees\n        binsize = kwargs.get(\"binsize\", (2.5, 3))\n        return self._get_map(\n            cluster, channel, VariableToBin.SPEED_DIR, **dict(kwargs, binsize=binsize)\n        )\n\n    def get_grid_map(\n        self, cluster: int | list, channel: int | list, **kwargs\n    ) -&gt; BinnedData:\n        rmap = self.get_rate_map(cluster, channel, **kwargs)\n        kwargs = clean_kwargs(self.RateMap.autoCorr2D, kwargs)\n        sac = self.RateMap.autoCorr2D(rmap, **kwargs)\n        return sac\n\n    def get_adaptive_map(\n        self, cluster: int | list, channel: int | list, **kwargs\n    ) -&gt; BinnedData:\n        return self._get_map(\n            cluster, channel, VariableToBin.XY, map_type=MapType.ADAPTIVE, **kwargs\n        )\n\n    def get_xcorr(\n        self, cluster: int | list, channel: int | list, **kwargs\n    ) -&gt; BinnedData:\n        ts = self.get_spike_times(cluster, channel)\n        return xcorr(ts, **kwargs)\n</code></pre>"},{"location":"reference/#ephysiopy.io.recording.TrialInterface._get_map","title":"<code>_get_map(cluster, channel, var2bin, **kwargs)</code>","text":"<p>This function generates a rate map for a given cluster and channel.</p> <p>Parameters:</p> Name Type Description Default <code>cluster</code> <code>int | list</code> required <code>channel</code> <code>int | list</code> required <code>var2bin</code> <code>XY</code> required <code>**kwargs</code> <p>do_shuffle (bool): If True, the rate map will be shuffled by the default number of shuffles (100).                 If the n_shuffles keyword is provided, the rate map will be shuffled by that number of shuffles, and                 an array of shuffled rate maps will be returned e.g [100 x nx x ny].                 The shuffles themselves are generated by shifting the spike times by a random amount between 30s and the                 length of the position data minus 30s. The random amount is drawn from a uniform distribution. In order to preserve                 the shifts over multiple calls to this function, the option is provided to set the random seed to a fixed                 value using the random_seed keyword.                 Default is False n_shuffles (int): The number of shuffles to perform. Default is 100. random_seed (int): The random seed to use for the shuffles. Default is None.</p> <code>{}</code> <p>Returns:</p> Type Description <code>np.ndarray: The rate map as a numpy array.</code> Source code in <code>ephysiopy/io/recording.py</code> <pre><code>def _get_map(\n    self,\n    cluster: int | list,\n    channel: int | list,\n    var2bin: VariableToBin.XY,\n    **kwargs,\n) -&gt; BinnedData:\n    \"\"\"\n    This function generates a rate map for a given cluster and channel.\n\n    Parameters\n    ----------\n    cluster (int | list): The cluster(s).\n    channel (int | list): The channel(s).\n    var2bin (VariableToBin.XY): The variable to bin. This is an enum that specifies the type of variable to bin.\n    **kwargs:\n        do_shuffle (bool): If True, the rate map will be shuffled by the default number of shuffles (100).\n                        If the n_shuffles keyword is provided, the rate map will be shuffled by that number of shuffles, and\n                        an array of shuffled rate maps will be returned e.g [100 x nx x ny].\n                        The shuffles themselves are generated by shifting the spike times by a random amount between 30s and the\n                        length of the position data minus 30s. The random amount is drawn from a uniform distribution. In order to preserve\n                        the shifts over multiple calls to this function, the option is provided to set the random seed to a fixed\n                        value using the random_seed keyword.\n                        Default is False\n        n_shuffles (int): The number of shuffles to perform. Default is 100.\n        random_seed (int): The random seed to use for the shuffles. Default is None.\n\n\n    Returns\n    -------\n    np.ndarray: The rate map as a numpy array.\n\n    \"\"\"\n    if not self.RateMap:\n        self.initialise()\n    spk_times_in_pos_samples = self._get_spike_pos_idx(cluster, channel, **kwargs)\n    npos = self._PosCalcs.npos\n    if (\n        isinstance(spk_times_in_pos_samples, list)\n        and len(spk_times_in_pos_samples) == 1\n    ):\n        spk_times_in_pos_samples = np.array(spk_times_in_pos_samples[0])\n    if isinstance(spk_times_in_pos_samples, np.ndarray):\n        spk_weights = np.bincount(spk_times_in_pos_samples, minlength=npos)\n        if len(spk_weights) &gt; npos:\n            spk_weights = np.delete(spk_weights, np.s_[npos:], 0)\n\n    elif (\n        isinstance(spk_times_in_pos_samples, list)\n        and len(spk_times_in_pos_samples) &gt; 1\n    ):  # likely the result of a shuffle arg passed to get_spike_pos_idx\n        # TODO: but not necessarily - could be multiple clusters/ channels have been passed\n        weights = []\n        for spk_idx in spk_times_in_pos_samples:\n            w = np.bincount(spk_idx, minlength=npos)\n            if len(w) &gt; npos:\n                w = np.delete(w, np.s_[npos:], 0)\n            weights.append(w)\n        spk_weights = np.array(weights)\n\n    kwargs[\"var_type\"] = var2bin\n    rmap = self.RateMap.get_map(spk_weights, **kwargs)\n    return rmap\n</code></pre>"},{"location":"reference/#ephysiopy.io.recording.TrialInterface._get_spike_pos_idx","title":"<code>_get_spike_pos_idx(cluster, channel, **kwargs)</code>","text":"<p>Returns the indices into the position data at which some cluster on a given channel emitted putative spikes.</p> <p>Parameters:</p> Name Type Description Default <code>cluster</code> <code>int | list | None</code> <pre><code>case the \"spike times\" are equal to the position times, which\nmeans data binned using these indices will be equivalent to\nbinning up just the position data alone.\n</code></pre> required <code>channel</code> <code>int | list</code> required <p>Returns:</p> Type Description <code>np.ndarray: The indices into the position data at which the spikes</code> <p>occurred.</p> Source code in <code>ephysiopy/io/recording.py</code> <pre><code>def _get_spike_pos_idx(\n    self, cluster: int | list | None, channel: int | list, **kwargs\n):\n    \"\"\"\n    Returns the indices into the position data at which some cluster\n    on a given channel emitted putative spikes.\n\n    Parameters\n    ----------\n    cluster (int | list): The cluster(s). NB this can be None in which\n            case the \"spike times\" are equal to the position times, which\n            means data binned using these indices will be equivalent to\n            binning up just the position data alone.\n\n    channel (int | list): The channel identity. Ignored if cluster is None\n\n    Returns\n    -------\n    np.ndarray: The indices into the position data at which the spikes\n            occurred.\n    \"\"\"\n    pos_times = getattr(self.PosCalcs, \"xyTS\")\n    if cluster is None:\n        spk_times = getattr(self.PosCalcs, \"xyTS\")\n    elif isinstance(cluster, int):\n        spk_times = self.get_spike_times(cluster, channel)\n    elif isinstance(cluster, list) and len(cluster) == 1:\n        spk_times = self.get_spike_times(cluster[0], channel[0])\n    elif isinstance(cluster, list) and len(cluster) &gt; 1:\n        assert len(cluster) == len(\n            channel\n        ), \"Cluster and channel lists must be same length\"\n        idx = []\n        for clust, chan in zip(cluster, channel):\n            spk_times = self.get_spike_times(clust, chan)\n            _idx = np.searchsorted(pos_times, spk_times, side=\"right\") - 1\n            if np.any(_idx &gt;= self.PosCalcs.npos):\n                _idx = np.delete(\n                    _idx, np.s_[np.argmax(_idx &gt;= self.PosCalcs.npos) :]\n                )\n            idx.append(_idx)\n        return idx\n\n    idx = np.searchsorted(pos_times, spk_times, side=\"right\") - 1\n    if np.any(idx &gt;= self.PosCalcs.npos):\n        idx = np.delete(idx, np.s_[np.argmax(idx &gt;= self.PosCalcs.npos) :])\n\n    if kwargs.get(\"do_shuffle\", False):\n        n_shuffles = kwargs.get(\"n_shuffles\", 100)\n        random_seed = kwargs.get(\"random_seed\", None)\n        r = np.random.default_rng(random_seed)\n        time_shifts = r.integers(\n            low=30 * self.PosCalcs.sample_rate,\n            high=self.PosCalcs.npos - (30 * self.PosCalcs.sample_rate),\n            size=n_shuffles,\n        )\n        shifted_idx = []\n        for shift in time_shifts:\n            shifted_idx.append(shift_vector(idx, shift, maxlen=self.PosCalcs.npos))\n        return shifted_idx\n\n    return idx\n</code></pre>"},{"location":"reference/#ephysiopy.io.recording.TrialInterface.apply_filter","title":"<code>apply_filter(*trial_filter)</code>","text":"<p>Apply a mask to the data</p> <p>Parameters:</p> Name Type Description Default <code>trial_filter</code> <code>TrialFilter</code> <p>name, start and end values name (str): The name of the filter start (float): The start value of the filter end (float): The end value of the filter</p> <p>Valid names are:     'dir' - the directional range to filter for     'speed' - min and max speed to filter for     'xrange' - min and max values to filter x pos values     'yrange' - same as xrange but for y pos     'time' - the times to keep / remove specified in ms</p> <p>Values are pairs specifying the range of values to filter for from the namedtuple TrialFilter that has fields 'start' and 'end' where 'start' and 'end' are the ranges to filter for</p> <code>()</code> <p>Returns:</p> Type Description <code>np.ndarray: An array of bools that is True where the mask is applied</code> Source code in <code>ephysiopy/io/recording.py</code> <pre><code>def apply_filter(self, *trial_filter: TrialFilter) -&gt; np.ndarray:\n    \"\"\"Apply a mask to the data\n\n    Parameters\n    ----------\n    trial_filter (TrialFilter): A namedtuple containing the filter\n        name, start and end values\n        name (str): The name of the filter\n        start (float): The start value of the filter\n        end (float): The end value of the filter\n\n        Valid names are:\n            'dir' - the directional range to filter for\n            'speed' - min and max speed to filter for\n            'xrange' - min and max values to filter x pos values\n            'yrange' - same as xrange but for y pos\n            'time' - the times to keep / remove specified in ms\n\n        Values are pairs specifying the range of values to filter for\n        from the namedtuple TrialFilter that has fields 'start' and 'end'\n        where 'start' and 'end' are the ranges to filter for\n\n    Returns\n    -------\n    np.ndarray: An array of bools that is True where the mask is applied\n    \"\"\"\n    if len(trial_filter) == 0:\n        bool_arr = False\n        self.mask_array = False\n        self._update_filter(None)\n    else:\n        for i_filter in trial_filter:\n            self._update_filter(i_filter)\n            if i_filter is None:\n                mask = False\n            elif i_filter.name is None:\n                mask = False\n            else:\n                if \"dir\" in i_filter.name and isinstance(i_filter.start, str):\n                    if len(i_filter.start) == 1:\n                        if \"w\" in i_filter.start:\n                            start = 135\n                            end = 225\n                        elif \"e\" in i_filter.start:\n                            start = 315\n                            end = 45\n                        elif \"s\" in i_filter.start:\n                            start = 225\n                            end = 315\n                        elif \"n\" in i_filter.start:\n                            start = 45\n                            end = 135\n                        else:\n                            raise ValueError(\"Invalid direction\")\n                    else:\n                        raise ValueError(\"filter must contain a key / value pair\")\n                    bool_arr = np.logical_and(\n                        self.PosCalcs.dir &gt; start, self.PosCalcs.dir &lt; end\n                    )\n                if \"speed\" in i_filter.name:\n                    if i_filter.start &gt; i_filter.end:\n                        raise ValueError(\n                            \"First value must be less than the second one\"\n                        )\n                    else:\n                        bool_arr = np.logical_and(\n                            self.PosCalcs.speed &gt; i_filter.start,\n                            self.PosCalcs.speed &lt; i_filter.end,\n                        )\n                elif \"dir\" in i_filter.name:\n                    if i_filter.start &lt; i_filter.end:\n                        bool_arr = np.logical_and(\n                            self.PosCalcs.dir &gt; i_filter.start,\n                            self.PosCalcs.dir &lt; i_filter.end,\n                        )\n                    else:\n                        bool_arr = np.logical_or(\n                            self.PosCalcs.dir &gt; i_filter.start,\n                            self.PosCalcs.dir &lt; i_filter.end,\n                        )\n                elif \"xrange\" in i_filter.name:\n                    bool_arr = np.logical_and(\n                        self.PosCalcs.xy[0, :] &gt; i_filter.start,\n                        self.PosCalcs.xy[0, :] &lt; i_filter.end,\n                    )\n                elif \"yrange\" in i_filter.name:\n                    bool_arr = np.logical_and(\n                        self.PosCalcs.xy[1, :] &gt; i_filter.start,\n                        self.PosCalcs.xy[1, :] &lt; i_filter.end,\n                    )\n                elif \"time\" in i_filter.name:\n                    # takes the form of 'from' - 'to' times in SECONDS\n                    # such that only pos's between these ranges are KEPT\n                    from_time = int(i_filter.start * self.PosCalcs.sample_rate)\n                    to_time = int(i_filter.end * self.PosCalcs.sample_rate)\n                    bool_arr = np.zeros(shape=(1, self.PosCalcs.npos), dtype=bool)\n                    bool_arr[:, from_time:to_time] = True\n                else:\n                    raise KeyError(\"Unrecognised key\")\n            self.mask_array = np.logical_or(self.mask_array, bool_arr)\n\n    mask = np.expand_dims(np.any(self.mask_array, axis=0), 0)\n    self.mask_array = mask\n    if self.EEGCalcs:\n        self.EEGCalcs.apply_mask(mask)\n    if self.PosCalcs:\n        self.PosCalcs.apply_mask(mask)\n    if self.RateMap:\n        self.RateMap.apply_mask(mask)\n    if self.clusterData:\n        self.clusterData.apply_mask(\n            mask,\n            xy_ts=self.PosCalcs.xyTS,\n            sample_rate=self.PosCalcs.sample_rate,\n        )\n    return mask\n</code></pre>"},{"location":"reference/#ephysiopy.io.recording.TrialInterface.get_eb_map","title":"<code>get_eb_map(cluster, channel, **kwargs)</code>","text":"<p>Gets the edge bin map for the specified cluster(s) and channel.</p> <p>Parameters:</p> Name Type Description Default <code>cluster</code> <code>int | list</code> required <code>channel</code> <code>int | list</code> required <code>**kwargs</code> <code>{}</code> <p>Returns:</p> Type Description <code>BinnedData - the binned data</code> Source code in <code>ephysiopy/io/recording.py</code> <pre><code>def get_eb_map(\n    self, cluster: int | list, channel: int | list, **kwargs\n) -&gt; BinnedData:\n    \"\"\"\n    Gets the edge bin map for the specified cluster(s) and channel.\n\n    Parameters\n    ----------\n    cluster (int | list): The cluster(s) to get the speed vs rate for.\n    channel (int | list): The channel(s) number.\n    **kwargs: Additional keyword arguments passed to _get_map\n\n    Returns\n    -------\n    BinnedData - the binned data\n    \"\"\"\n    return self._get_map(cluster, channel, VariableToBin.EGO_BOUNDARY, **kwargs)\n</code></pre>"},{"location":"reference/#ephysiopy.io.recording.TrialInterface.get_hd_map","title":"<code>get_hd_map(cluster, channel, **kwargs)</code>","text":"<p>Gets the head direction map for the specified cluster(s) and channel.</p> <p>Parameters:</p> Name Type Description Default <code>cluster</code> <code>int | list</code> required <code>channel</code> <code>int | list</code> required <code>**kwargs</code> <code>{}</code> <p>Returns:</p> Type Description <code>BinnedData - the binned data</code> Source code in <code>ephysiopy/io/recording.py</code> <pre><code>def get_hd_map(\n    self, cluster: int | list, channel: int | list, **kwargs\n) -&gt; BinnedData:\n    \"\"\"\n    Gets the head direction map for the specified cluster(s) and channel.\n\n    Parameters\n    ----------\n    cluster (int | list): The cluster(s) to get the speed vs rate for.\n    channel (int | list): The channel(s) number.\n    **kwargs: Additional keyword arguments passed to _get_map\n\n    Returns\n    -------\n    BinnedData - the binned data\n    \"\"\"\n\n    return self._get_map(cluster, channel, VariableToBin.DIR, **kwargs)\n</code></pre>"},{"location":"reference/#ephysiopy.io.recording.TrialInterface.get_rate_map","title":"<code>get_rate_map(cluster, channel, **kwargs)</code>","text":"<p>Gets the rate map for the specified cluster(s) and channel.</p> <p>Parameters:</p> Name Type Description Default <code>cluster</code> <code>int | list</code> required <code>channel</code> <code>int | list</code> required <code>**kwargs</code> <code>{}</code> <p>Returns:</p> Type Description <code>BinnedData - the binned data</code> Source code in <code>ephysiopy/io/recording.py</code> <pre><code>def get_rate_map(\n    self, cluster: int | list, channel: int | list, **kwargs\n) -&gt; BinnedData:\n    \"\"\"\n    Gets the rate map for the specified cluster(s) and channel.\n\n    Parameters\n    ----------\n    cluster (int | list): The cluster(s) to get the speed vs rate for.\n    channel (int | list): The channel(s) number.\n    **kwargs: Additional keyword arguments passed to _get_map\n\n    Returns\n    -------\n    BinnedData - the binned data\n    \"\"\"\n    return self._get_map(cluster, channel, VariableToBin.XY, **kwargs)\n</code></pre>"},{"location":"reference/#ephysiopy.io.recording.TrialInterface.get_speed_v_hd_map","title":"<code>get_speed_v_hd_map(cluster, channel, **kwargs)</code>","text":"<p>Gets the speed vs head direction map for the specified cluster(s) and channel.</p> <p>Parameters:</p> Name Type Description Default <code>cluster</code> <code>int | list</code> required <code>channel</code> <code>int | list</code> required <code>**kwargs</code> <code>{}</code> Source code in <code>ephysiopy/io/recording.py</code> <pre><code>def get_speed_v_hd_map(\n    self, cluster: int | list, channel: int | list, **kwargs\n) -&gt; BinnedData:\n    \"\"\"\n    Gets the speed vs head direction map for the specified cluster(s) and channel.\n\n    Parameters\n    ----------\n    cluster (int | list): The cluster(s) to get the speed vs head direction map for.\n    channel (int | list): The channel number.\n    **kwargs: Additional keyword arguments passed to _get_map\n    \"\"\"\n    # binsize is in cm/s and degrees\n    binsize = kwargs.get(\"binsize\", (2.5, 3))\n    return self._get_map(\n        cluster, channel, VariableToBin.SPEED_DIR, **dict(kwargs, binsize=binsize)\n    )\n</code></pre>"},{"location":"reference/#ephysiopy.io.recording.TrialInterface.get_speed_v_rate_map","title":"<code>get_speed_v_rate_map(cluster, channel, **kwargs)</code>","text":"<p>Gets the speed vs rate for the specified cluster(s) and channel.</p> <p>Parameters:</p> Name Type Description Default <code>cluster</code> <code>int | list</code> required <code>channel</code> <code>int | list</code> required <code>**kwargs</code> <code>{}</code> <p>Returns:</p> Type Description <code>BinnedData - the binned data</code> Source code in <code>ephysiopy/io/recording.py</code> <pre><code>def get_speed_v_rate_map(\n    self, cluster: int | list, channel: int | list, **kwargs\n) -&gt; BinnedData:\n    \"\"\"\n    Gets the speed vs rate for the specified cluster(s) and channel.\n\n    Parameters\n    ----------\n    cluster (int | list): The cluster(s) to get the speed vs rate for.\n    channel (int | list): The channel(s) number.\n    **kwargs: Additional keyword arguments passed to _get_map\n\n    Returns\n    -------\n    BinnedData - the binned data\n    \"\"\"\n    return self._get_map(cluster, channel, VariableToBin.SPEED, **kwargs)\n</code></pre>"},{"location":"reference/#ephysiopy.io.recording.TrialInterface.get_spike_times","title":"<code>get_spike_times(cluster, channel, *args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Returns the times of an individual cluster</p> Source code in <code>ephysiopy/io/recording.py</code> <pre><code>@abc.abstractmethod\ndef get_spike_times(\n    self, cluster: int | list, channel: int | list, *args, **kwargs\n) -&gt; list | np.ndarray:\n    \"\"\"Returns the times of an individual cluster\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/#ephysiopy.io.recording.TrialInterface.get_spike_times_binned_into_position","title":"<code>get_spike_times_binned_into_position(cluster, channel)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>cluster</code> <code>int | list</code> required <code>channel</code> <code>int | list</code> required <p>Returns:</p> Type Description <code>np.ndarray - the spike times binned into the position data</code> Source code in <code>ephysiopy/io/recording.py</code> <pre><code>def get_spike_times_binned_into_position(\n    self, cluster: int | list, channel: int | list\n) -&gt; np.ndarray:\n    \"\"\"\n    Parameters\n    ----------\n    cluster (int | list): The cluster(s).\n    channel (int | list): The channel(s).\n\n    Returns\n    -------\n    np.ndarray - the spike times binned into the position data\n    \"\"\"\n    ts = self.get_spike_times(cluster, channel)\n    if not isinstance(ts, list):\n        ts = [ts]\n    n_clusters = 1\n    if isinstance(n_clusters, list):\n        n_clusters = len(cluster)\n    n_pos = self.PosCalcs.npos\n    binned = np.zeros((n_clusters, n_pos))\n    for i, t in enumerate(ts):\n        spk_binned = np.bincount(\n            (t * self.PosCalcs.sample_rate).astype(int), minlength=n_pos\n        )\n        if len(spk_binned) &gt; n_pos:\n            spk_binned = spk_binned[:n_pos]\n        binned[i, :] = spk_binned\n    return binned\n</code></pre>"},{"location":"reference/#ephysiopy.io.recording.TrialInterface.load_cluster_data","title":"<code>load_cluster_data(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Load the cluster data (Kilosort/ Axona cut/ whatever else</p> Source code in <code>ephysiopy/io/recording.py</code> <pre><code>@abc.abstractmethod\ndef load_cluster_data(self, *args, **kwargs) -&gt; bool:\n    \"\"\"Load the cluster data (Kilosort/ Axona cut/ whatever else\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/#ephysiopy.io.recording.TrialInterface.load_lfp","title":"<code>load_lfp(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Load the LFP data</p> Source code in <code>ephysiopy/io/recording.py</code> <pre><code>@abc.abstractmethod\ndef load_lfp(self, *args, **kwargs):\n    \"\"\"Load the LFP data\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/#ephysiopy.io.recording.TrialInterface.load_neural_data","title":"<code>load_neural_data(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Load the neural data</p> Source code in <code>ephysiopy/io/recording.py</code> <pre><code>@abc.abstractmethod\ndef load_neural_data(self, *args, **kwargs):\n    \"\"\"Load the neural data\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/#ephysiopy.io.recording.TrialInterface.load_pos_data","title":"<code>load_pos_data(ppm=300, jumpmax=100, *args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Load the position data</p> <p>Parameters:</p> Name Type Description Default <code>ppm</code> <code>int</code> <code>300</code> <code>jumpmax</code> <code>int</code> <p>than this and the position is interpolated over</p> <code>100</code> Source code in <code>ephysiopy/io/recording.py</code> <pre><code>@abc.abstractmethod\ndef load_pos_data(self, ppm: int = 300, jumpmax: int = 100, *args, **kwargs):\n    \"\"\"\n    Load the position data\n\n    Parameters\n    ----------\n    ppm (int): pixels per metre\n    jumpmax (int): max jump in pixels between positions, more\n        than this and the position is interpolated over\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/#ephysiopy.io.recording.TrialInterface.load_settings","title":"<code>load_settings(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Loads the format specific settings file</p> Source code in <code>ephysiopy/io/recording.py</code> <pre><code>@abc.abstractmethod\ndef load_settings(self, *args, **kwargs):\n    \"\"\"Loads the format specific settings file\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/#ephysiopy.io.recording.find_path_to_ripple_ttl","title":"<code>find_path_to_ripple_ttl(trial_root, **kwargs)</code>","text":"<p>Iterates through a directory tree and finds the path to the Ripple Detector plugin data and returns its location</p> Source code in <code>ephysiopy/io/recording.py</code> <pre><code>def find_path_to_ripple_ttl(trial_root: Path, **kwargs) -&gt; Path:\n    \"\"\"\n    Iterates through a directory tree and finds the path to the\n    Ripple Detector plugin data and returns its location\n    \"\"\"\n    exp_name = kwargs.pop(\"experiment\", \"experiment1\")\n    rec_name = kwargs.pop(\"recording\", \"recording1\")\n    ripple_match = (\n        trial_root\n        / Path(\"Record Node [0-9][0-9][0-9]\")\n        / Path(exp_name)\n        / Path(rec_name)\n        / Path(\"events\")\n        / Path(\"Ripple_Detector-[0-9][0-9][0-9].*\")\n        / Path(\"TTL\")\n    )\n    for d, c, f in os.walk(trial_root):\n        for ff in f:\n            if \".\" not in c:  # ignore hidden directories\n                if \"timestamps.npy\" in ff:\n                    if PurePath(d).match(str(ripple_match)):\n                        return Path(d)\n    return Path()\n</code></pre>"},{"location":"reference/#ephysiopy.io.recording.memmapBinaryFile","title":"<code>memmapBinaryFile(path2file, n_channels=384, **kwargs)</code>","text":"<p>Returns a numpy memmap of the int16 data in the file path2file, if present</p> Source code in <code>ephysiopy/io/recording.py</code> <pre><code>def memmapBinaryFile(path2file: str, n_channels=384, **kwargs) -&gt; np.ndarray:\n    \"\"\"\n    Returns a numpy memmap of the int16 data in the\n    file path2file, if present\n    \"\"\"\n    import os\n\n    if \"data_type\" in kwargs.keys():\n        data_type = kwargs[\"data_type\"]\n    else:\n        data_type = np.int16\n\n    if os.path.exists(path2file):\n        # make sure n_channels is int as could be str\n        n_channels = int(n_channels)\n        status = os.stat(path2file)\n        n_samples = int(status.st_size / (2.0 * n_channels))\n        mmap = np.memmap(\n            path2file, data_type, \"r\", 0, (n_channels, n_samples), order=\"F\"\n        )\n        return mmap\n    else:\n        return np.empty(0)\n</code></pre>"},{"location":"reference/#plotting-the-results","title":"Plotting the results","text":""},{"location":"reference/#ephysiopy.visualise.plotting.FigureMaker","title":"<code>FigureMaker</code>","text":"<p>               Bases: <code>object</code></p> <p>A mixin class for TrialInterface that deals solely with producing graphical output.</p> Source code in <code>ephysiopy/visualise/plotting.py</code> <pre><code>class FigureMaker(object):\n    \"\"\"\n    A mixin class for TrialInterface that deals solely with\n    producing graphical output.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initializes the FigureMaker object.\n        \"\"\"\n        self.PosCalcs = None\n\n        \"\"\"\n        Initializes the FigureMaker object with data from PosCalcs.\n        \"\"\"\n        if self.PosCalcs is not None:\n            self.RateMap = RateMap(self.PosCalcs)\n            self.npos = self.PosCalcs.xy.shape[1]\n\n    def _plot_multiple_clusters(\n        self, func, clusters: list, channel: int, **kwargs\n    ) -&gt; matplotlib.figure.Figure:\n        \"\"\"\n        Plots multiple clusters.\n\n        Args:\n            func (function): The function to apply to each cluster.\n            clusters (list): The list of clusters to plot.\n            channel (int): The channel number.\n            **kwargs: Additional keyword arguments for the function.\n        \"\"\"\n        fig = plt.figure()\n        nrows = int(np.ceil(len(clusters) / 5))\n        if \"projection\" in kwargs.keys():\n            proj = kwargs.pop(\"projection\")\n        else:\n            proj = None\n        for i, c in enumerate(clusters):\n            ax = fig.add_subplot(nrows, 5, i + 1, projection=proj)\n            ts = self.get_spike_times(c, channel)\n            func(ts, ax=ax, **kwargs)\n        return fig\n\n    @savePlot\n    @stripAxes\n    def plot_rate_map(self, cluster: int, channel: int, **kwargs) -&gt; plt.Axes:\n        \"\"\"\n        Plots the rate map for the specified cluster(s) and channel.\n\n        Args:\n            cluster (int): The cluster(s) to get the rate map for.\n            channel (int): The channel number.\n            **kwargs: Additional keyword arguments for the function.\n\n        \"\"\"\n        rmap = self.get_rate_map(cluster, channel, **kwargs)\n        vmax = np.nanmax(np.ravel(rmap.binned_data[0]))\n        ax = kwargs.pop(\"ax\", None)\n        if ax is None:\n            fig = plt.figure()\n            ax = fig.add_subplot(111)\n        kwargs = clean_kwargs(plt.pcolormesh, kwargs)\n        # TODO: doesn't deal with multiple clusters being binned\n        ax.pcolormesh(\n            rmap.bin_edges[1],\n            rmap.bin_edges[0],\n            rmap.binned_data[0],\n            cmap=jet_cmap,\n            edgecolors=\"face\",\n            vmax=vmax,\n            shading=\"auto\",\n            **kwargs,\n        )\n        ax.set_aspect(\"equal\")\n        return ax\n\n    @savePlot\n    @stripAxes\n    def plot_hd_map(self, cluster: int, channel: int, **kwargs) -&gt; plt.Axes:\n        \"\"\"\n        Gets the head direction map for the specified cluster(s) and channel.\n\n        Args:\n            cluster (int): The cluster(s) to get the head direction map\n                for.\n            channel (int): The channel number.\n            **kwargs: Additional keyword arguments for the function.\n        \"\"\"\n        rmap = self.get_hd_map(cluster, channel, **kwargs)\n        add_mrv = kwargs.pop(\"add_mrv\", False)\n        add_guides = kwargs.pop(\"add_guides\", False)\n        strip_axes = kwargs.pop(\"strip_axes\", False)\n        fill = kwargs.pop(\"fill\", False)\n        ax = kwargs.pop(\"ax\", None)\n        kwargs = clean_kwargs(plt.pcolormesh, kwargs)\n        if ax is None:\n            fig = plt.figure()\n            ax = fig.add_subplot(111, projection=\"polar\", **kwargs)\n        ax.set_theta_zero_location(\"N\")\n        # need to deal with the case where the axis is supplied but\n        # is not polar. deal with polar first\n        theta = np.deg2rad(rmap.bin_edges[0])\n        ax.clear()\n        r = (\n            rmap.binned_data[0] * self.PosCalcs.sample_rate\n        )  # in samples so * pos sample_rate\n        r = np.insert(r, -1, r[0])\n        hasData = np.any(r &gt; 0)\n        if \"polar\" in ax.name and hasData:\n            ax.plot(theta, r)\n            if fill:\n                ax.fill(theta, r, alpha=0.5)\n            ax.set_aspect(\"equal\")\n\n        if add_guides:\n            ax.set_rgrids([])\n\n        # See if we should add the mean resultant vector (mrv)\n        if add_mrv and hasData:\n            from ephysiopy.common.statscalcs import mean_resultant_vector\n\n            veclen = fc.get_mean_resultant_length(rmap.binned_data[0])\n            th = fc.get_mean_resultant_angle(rmap.binned_data[0])\n            ax.plot(\n                [0, th],\n                [0, veclen * np.max(rmap.binned_data[0]) * self.PosCalcs.sample_rate],\n                \"r\",\n            )\n        if \"polar\" in ax.name:\n            ax.set_thetagrids([0, 90, 180, 270])\n        if strip_axes:\n            return stripAxes(ax)\n        return ax\n\n    @savePlot\n    @stripAxes\n    def plot_spike_path(self, cluster=None, channel=None, **kwargs) -&gt; plt.Axes:\n        \"\"\"\n        Gets the spike path for the specified cluster(s) and channel.\n\n        Args:\n            cluster (int | None): The cluster(s) to get the spike path\n                for.\n            channel (int | None): The channel number.\n            **kwargs: Additional keyword arguments for the function.\n        \"\"\"\n        if not self.RateMap:\n            self.initialise()\n        col = kwargs.pop(\"c\", tcols.colours[1])\n        ax = kwargs.pop(\"ax\", None)\n        marker = kwargs.pop(\"marker\", \"s\")\n        if ax is None:\n            fig = plt.figure()\n            ax = fig.add_subplot(111)\n        ax.plot(\n            self.PosCalcs.xy[0, :],\n            self.PosCalcs.xy[1, :],\n            color=tcols.colours[0],\n            zorder=1,\n        )\n        ax.set_aspect(\"equal\")\n        if cluster is not None:\n            idx = self._get_spike_pos_idx(cluster, channel)\n            ax.scatter(\n                self.PosCalcs.xy[0, idx],\n                self.PosCalcs.xy[1, idx],\n                marker=marker,\n                color=col,\n                **kwargs,\n            )\n        return ax\n\n    @savePlot\n    @stripAxes\n    def plot_eb_map(self, cluster: int, channel: int, **kwargs) -&gt; plt.Axes:\n        \"\"\"\n        Gets the ego-centric boundary map for the specified cluster(s) and\n        channel.\n\n        Args:\n            cluster (int): The cluster(s) to get the ego-centric\n                boundary map for.\n            channel (int): The channel number.\n            **kwargs: Additional keyword arguments for the function.\n        \"\"\"\n        return_ratemap = kwargs.pop(\"return_ratemap\", False)\n        rmap = self.get_eb_map(cluster, channel, range=None, **kwargs)\n        ax = kwargs.pop(\"ax\", None)\n        if ax is None:\n            fig = plt.figure()\n            ax = fig.add_subplot(projection=\"polar\")\n        # sanitise kwargs before passing on to pcolormesh\n        kwargs = clean_kwargs(plt.pcolormesh, kwargs)\n        ax.pcolormesh(\n            rmap.bin_edges[1],\n            rmap.bin_edges[0],\n            rmap.binned_data[0],\n            edgecolors=\"face\",\n            shading=\"auto\",\n            **kwargs,\n        )\n        ax.set_xticks(np.arange(0, 2 * np.pi, np.pi / 4))\n        # ax.set_xticklabels(np.arange(0, 2*np.pi, np.pi/4))\n        ax.set_yticks(np.arange(0, 50, 10))\n        ax.set_yticklabels(np.arange(0, 50, 10))\n        ax.set_xlabel(\"Angle (deg)\")\n        ax.set_ylabel(\"Distance (cm)\")\n        if return_ratemap:\n            return ax, rmap\n        return ax\n\n    @savePlot\n    @stripAxes\n    def plot_eb_spikes(self, cluster: int, channel: int, **kwargs) -&gt; plt.Axes:\n        \"\"\"\n        Gets the ego-centric boundary spikes for the specified cluster(s)\n        and channel.\n\n        Args:\n            cluster (int): The cluster(s) to get the ego-centric\n                boundary spikes for.\n            channel (int): The channel number.\n            **kwargs: Additional keyword arguments for the function.\n        \"\"\"\n        if not self.RateMap:\n            self.initialise()\n        ax = kwargs.pop(\"ax\", None)\n        if ax is None:\n            fig = plt.figure()\n            ax = fig.add_subplot(111)\n        ax.set_aspect(\"equal\")\n        # Parse kwargs\n        num_dir_bins = kwargs.get(\"dir_bins\", 60)\n        rect_size = kwargs.get(\"ms\", 1)\n        add_colour_wheel = kwargs.get(\"add_colour_wheel\", False)\n        dir_colours = sns.color_palette(\"hls\", num_dir_bins)\n        # Process dirrectional data\n        idx = self._get_spike_pos_idx(cluster, channel)\n        dir_spike_fired_at = self.RateMap.dir[idx]\n        idx_of_dir_to_colour = np.floor(\n            dir_spike_fired_at / (360 / num_dir_bins)\n        ).astype(int)\n        rects = [\n            Rectangle(\n                self.RateMap.xy[:, i],\n                width=rect_size,\n                height=rect_size,\n                clip_box=ax.bbox,\n                facecolor=dir_colours[idx_of_dir_to_colour[i]],\n                rasterized=True,\n            )\n            for i in range(len(idx))\n        ]\n        ax.plot(\n            self.PosCalcs.xy[0],\n            self.PosCalcs.xy[1],\n            c=tcols.colours[0],\n            zorder=1,\n            alpha=0.3,\n        )\n        ax.add_collection(PatchCollection(rects, match_original=True))\n        if add_colour_wheel:\n            ax_col = inset_axes(\n                ax,\n                width=\"100%\",\n                height=\"100%\",\n                bbox_to_anchor=(0.75, 0.75, 0.15, 0.15),\n                axes_class=get_projection_class(\"polar\"),\n                bbox_transform=fig.transFigure,\n            )\n            ax_col.set_theta_zero_location(\"N\")\n            theta = np.linspace(0, 2 * np.pi, 1000)\n            phi = np.linspace(0, 1, 2)\n            X, Y = np.meshgrid(phi, theta)\n            norm = matplotlib.colors.Normalize(0, 2 * np.pi)\n            col_map = sns.color_palette(\"hls\", as_cmap=True)\n            ax_col.pcolormesh(theta, phi, Y.T, norm=norm, cmap=col_map)\n            ax_col.set_yticklabels([])\n            ax_col.spines[\"polar\"].set_visible(False)\n            ax_col.set_thetagrids([0, 90])\n        return ax\n\n    @savePlot\n    @stripAxes\n    def plot_sac(self, cluster: int, channel: int, **kwargs) -&gt; plt.Axes:\n        \"\"\"\n        Gets the spatial autocorrelation for the specified cluster(s) and\n        channel.\n\n        Args:\n            cluster (int): The cluster(s) to get the spatial\n                autocorrelation for.\n            channel (int): The channel number.\n            **kwargs: Additional keyword arguments for the function.\n        \"\"\"\n        sac = self.get_grid_map(cluster, channel)\n        measures = fc.grid_field_props(sac)\n        ax = kwargs.pop(\"ax\", None)\n        if ax is None:\n            fig = plt.figure()\n            ax = fig.add_subplot(111)\n        Am = sac.binned_data[0].copy()\n        Am[~measures[\"dist_to_centre\"]] = np.nan\n        Am = np.ma.masked_invalid(np.atleast_2d(Am))\n        x, y = np.meshgrid(sac.bin_edges[1].data, sac.bin_edges[0].data)\n        vmax = np.nanmax(np.ravel(sac.binned_data[0]))\n        ax.pcolormesh(\n            sac.bin_edges[1].data,\n            sac.bin_edges[0].data,\n            sac.binned_data[0],\n            cmap=grey_cmap,\n            edgecolors=\"face\",\n            vmax=vmax,\n            shading=\"auto\",\n        )\n        import copy\n\n        cmap = copy.copy(jet_cmap)\n        cmap.set_bad(\"w\", 0)\n        ax.pcolormesh(\n            sac.bin_edges[1].data,\n            sac.bin_edges[0].data,\n            Am,\n            cmap=cmap,\n            edgecolors=\"face\",\n            vmax=vmax,\n            shading=\"auto\",\n        )\n        # horizontal green line at 3 o'clock\n        _y = 0, 0\n        _x = 0, sac.bin_edges[0][-1]\n        ax.plot(_x, _y, c=\"g\")\n        mag = measures[\"scale\"] * 0.75\n        th = np.linspace(0, measures[\"orientation\"], 50)\n        from ephysiopy.common.utils import rect\n\n        [x, y] = rect(mag, th, deg=1)\n        # angle subtended by orientation\n        ax.plot(x, -y, c=\"r\", **kwargs)\n        # plot lines from centre to peaks above middle\n        for p in measures[\"closest_peak_coords\"]:\n            if p[0] &lt;= measures[\"dist_to_centre\"].shape[0] / 2:\n                ax.plot((0, p[1]), (0, p[0]), \"k\", **kwargs)\n        ax.invert_yaxis()\n        all_ax = ax.axes\n        all_ax.set_aspect(\"equal\")\n\n        return ax\n\n    @savePlot\n    def plot_speed_v_rate(self, cluster: int, channel: int, **kwargs) -&gt; plt.Axes:\n        \"\"\"\n        Gets the speed versus rate plot for the specified cluster(s) and\n        channel.\n\n        By default the distribution of speeds will be plotted as a twin\n        axis. To disable set add_speed_hist = False\n\n        Args:\n            cluster (int): The cluster(s) to get the speed versus rate\n                plot for.\n            channel (int): The channel number.\n            **kwargs: Additional keyword arguments for the function.\n        \"\"\"\n        add_speed_hist = kwargs.pop(\"add_speed_hist\", True)\n        rmap = self.get_speed_v_rate_map(cluster, channel, **kwargs)\n        # rmap is linear\n        ax = kwargs.pop(\"ax\", None)\n        if ax is None:\n            fig = plt.figure()\n            ax = fig.add_subplot(111)\n        kwargs = clean_kwargs(plt.plot, kwargs)\n        ax_colour = \"cornflowerblue\"\n        ax.plot(rmap.bin_edges[0][:-1], rmap.binned_data[0], color=ax_colour, **kwargs)\n        ax.set_xlabel(\"Speed (cm/s)\")\n        ax.set_ylabel(\"Rate (Hz)\")\n        if add_speed_hist:\n            ax.spines[\"left\"].set_color(ax_colour)\n            ax.tick_params(axis=\"y\", colors=ax_colour)\n            ax.yaxis.label.set_color(ax_colour)\n            ax2 = ax.twinx()\n            ax2_colour = \"grey\"\n            pos_weights = np.ones_like(self.PosCalcs.speed) * (\n                1 / self.PosCalcs.sample_rate\n            )\n            speed_bincounts = np.bincount(\n                np.digitize(self.PosCalcs.speed, rmap.bin_edges[0], right=True),\n                weights=pos_weights,\n            )\n            ax2.bar(\n                rmap.bin_edges[0],\n                speed_bincounts,\n                alpha=0.5,\n                width=np.mean(np.diff(rmap.bin_edges[0])),\n                ec=\"grey\",\n                fc=\"grey\",\n            )\n            ax2.set_ylabel(\"Duration (s)\")\n            ax2.spines[\"right\"].set_color(ax2_colour)\n            ax2.tick_params(axis=\"y\", colors=ax2_colour)\n            ax2.yaxis.label.set_color(ax2_colour)\n\n        return ax\n\n    # @stripAxes\n    @savePlot\n    def plot_speed_v_hd(self, cluster: int, channel: int, **kwargs) -&gt; plt.Axes:\n        \"\"\"\n        Gets the speed versus head direction plot for the specified cluster(s)\n        and channel.\n\n        Args:\n            cluster (int): The cluster(s) to get the speed versus head\n                direction plot for.\n            channel (int): The channel number.\n            **kwargs: Additional keyword arguments for the function.\n        \"\"\"\n        rmap = self.get_speed_v_hd_map(cluster, channel, **kwargs)\n        im = np.ma.MaskedArray(rmap.binned_data[0], np.isnan(rmap.binned_data[0]))\n        # mask low rates...\n        # im = np.ma.masked_where(im &lt;= 1, im)\n        # ... and where less than 0.5% of data is accounted for\n        y, x = np.meshgrid(rmap.bin_edges[0], rmap.bin_edges[1], indexing=\"ij\")\n        vmax = np.nanmax(np.ravel(im))\n        ax = kwargs.pop(\"ax\", None)\n        if ax is None:\n            fig = plt.figure()\n            ax = fig.add_subplot(111)\n        ax.pcolormesh(\n            x, y, im, cmap=jet_cmap, edgecolors=\"face\", vmax=vmax, shading=\"auto\"\n        )\n        ax.set_xticks(\n            [90, 180, 270], labels=[\"90\", \"180\", \"270\"], fontweight=\"normal\", size=6\n        )\n        ax.set_yticks(\n            [10, 20, 30, 40],\n            labels=[\"10\", \"20\", \"30\", \"40\"],\n            fontweight=\"normal\",\n            size=6,\n        )\n        ax.set_xlabel(\"Heading\", fontweight=\"normal\", size=6)\n        return ax\n\n    @savePlot\n    def plot_acorr(self, cluster: int, channel: int, **kwargs) -&gt; plt.Axes:\n        \"\"\"\n        Gets the autocorrelogram for the specified cluster(s) and channel.\n\n        Args:\n            cluster (int): The cluster(s) to get the autocorrelogram\n                for.\n            channel (int): The channel number.\n            **kwargs: Additional keyword arguments for the function.\n        \"\"\"\n\n        ts = self.get_spike_times(cluster, channel)\n        ax = self._getXCorrPlot(ts, **kwargs)\n        return ax\n\n    @savePlot\n    def plot_xcorr(\n        self, cluster_a: int, channel_a: int, cluster_b: int, channel_b: int, **kwargs\n    ) -&gt; plt.Axes:\n        \"\"\"\n        Plots the temporal cross-correlogram between cluster_a and cluster_b\n\n        Parameters\n        ----------\n        cluster_a (int) : first cluster\n        channel_a (int) : first channel\n        cluster_b (int) : second cluster\n        channel_b (int) : second channel\n\n        Returns\n        -------\n        plt.Axes : matplotlib.Axes instance\n        \"\"\"\n        if \"strip_axes\" in kwargs.keys():\n            strip_axes = kwargs.pop(\"strip_axes\")\n        else:\n            strip_axes = False\n        ax = kwargs.get(\"ax\", None)\n        if ax is None:\n            fig = plt.figure()\n            ax = fig.add_subplot(111)\n        if \"binsize\" in kwargs.keys():\n            binsize = kwargs[\"binsize\"]\n        else:\n            binsize = 0.001\n        if \"Trange\" in kwargs.keys():\n            xrange = kwargs.pop(\"Trange\")\n        else:\n            xrange = [-0.5, 0.5]\n\n        a_times = self.get_spike_times(cluster_a, channel_a)\n        b_times = self.get_spike_times(cluster_b, channel_b)\n        xcorr_binned = xcorr(a_times, b_times, Trange=xrange, binsize=binsize)\n        c = xcorr_binned.binned_data[0]\n        b = xcorr_binned.bin_edges[0]\n        ax.bar(b[:-1], c, width=binsize, color=\"k\", align=\"edge\", zorder=3)\n        ax.set_xlim(xrange)\n        ax.set_xticks((xrange[0], 0, xrange[1]))\n        ax.set_xticklabels(\"\")\n        ax.tick_params(\n            axis=\"both\", which=\"both\", left=False, right=False, bottom=False, top=False\n        )\n        ax.set_yticklabels(\"\")\n        ax.xaxis.set_ticks_position(\"bottom\")\n        if strip_axes:\n            return stripAxes(ax)\n        axtrans = transforms.blended_transform_factory(ax.transData, ax.transAxes)\n        ax.vlines(0, ymin=0, ymax=1, colors=\"lightgrey\", transform=axtrans, zorder=1)\n        return ax\n\n    @savePlot\n    def plot_raster(self, cluster: int, channel: int, **kwargs) -&gt; plt.Axes:\n        \"\"\"\n        Gets the raster plot for the specified cluster(s) and channel.\n\n        Args:\n            cluster (int | list): The cluster(s) to get the raster plot for.\n            channel (int): The channel number.\n            **kwargs: Additional keyword arguments for the function.\n        \"\"\"\n        ts = self.get_spike_times(cluster, channel)\n        ax = self._getRasterPlot(ts, cluster=cluster, **kwargs)\n        return ax\n\n    @savePlot\n    def plot_power_spectrum(self, **kwargs) -&gt; plt.Axes:\n        \"\"\"\n        Gets the power spectrum.\n\n        Args:\n            **kwargs: Additional keyword arguments for the function.\n        \"\"\"\n        p = self.EEGCalcs.calcEEGPowerSpectrum()\n        ax = self._getPowerSpectrumPlot(p[0], p[1], p[2], p[3], p[4], **kwargs)\n        return ax\n\n    @savePlot\n    def plot_theta_vs_running_speed(self, **kwargs) -&gt; QuadMesh:\n        low_theta = kwargs.pop(\"low_theta\", 6)\n        high_theta = kwargs.pop(\"high_theta\", 12)\n        low_speed = kwargs.pop(\"low_speed\", 2)\n        high_speed = kwargs.pop(\"high_speed\", 50)\n        theta_filtered_eeg = self.EEGCalcs.butterFilter(low_theta, high_theta)\n        hilbert_eeg = hilbert(theta_filtered_eeg)\n        inst_freq = (\n            self.EEGCalcs.fs / (2 * np.pi) * np.diff(np.unwrap(np.angle(hilbert_eeg)))\n        )\n        inst_freq = np.insert(inst_freq, -1, inst_freq[-1])\n        eeg_times = np.arange(0, len(self.EEGCalcs.sig)) / self.EEGCalcs.fs\n        pos_times = self.PosCalcs.xyTS\n        idx = np.searchsorted(pos_times, eeg_times)\n        idx[idx &gt;= len(pos_times)] = len(pos_times) - 1\n        eeg_speed = self.PosCalcs.speed[idx]\n        h, edges = np.histogramdd(\n            [inst_freq, eeg_speed],\n            bins=(\n                np.arange(low_theta, high_theta, 0.5),\n                np.arange(low_speed, high_speed, 2),\n            ),\n        )\n        ax = plt.pcolormesh(edges[1], edges[0], h, cmap=jet_cmap, edgecolors=\"face\")\n        return ax\n\n    @savePlot\n    def plot_clusters_theta_phase(\n        self, cluster: int, channel: int, **kwargs\n    ) -&gt; plt.Axes:\n        from ephysiopy.common.rhythmicity import LFPOscillations\n\n        ax = kwargs.pop(\"ax\", None)\n        if ax is None:\n            fig = plt.figure()\n            ax = fig.add_subplot(111, projection=\"polar\")\n        if \"polar\" not in ax.name:\n            raise ValueError(\"Need a polar axis\")\n        L = LFPOscillations(self.EEGCalcs.sig, self.EEGCalcs.fs)\n        ts = self.get_spike_times(cluster, channel)\n        phase, x, y = L.get_theta_phase(ts, **kwargs)\n        ax.hist(phase, density=True, bins=36)\n        ax.plot(x, y, linewidth=1, color=\"red\", zorder=3)\n        # add lines around the edge of the polar plot to show spiking locations\n        y_lims = ax.get_ylim()\n        line_start = y_lims[1] / 1.01\n        ax.vlines(phase, ymin=line_start, ymax=y_lims[1] * 1.15, colors=\"k\")\n        ax.set_ylim(y_lims[0], y_lims[1] * 1.15)\n        ax.set_thetagrids([])\n        ax.set_rgrids([])\n        ax.set_xticks(np.arange(0, 2 * np.pi, np.pi / 2), [\"0\", \"90\", \"180\", \"270\"])\n        return ax\n\n    @savePlot\n    @stripAxes\n    def _plotWaves(self, waves: np.ndarray, ax: matplotlib.axes, **kwargs) -&gt; plt.Axes:\n        ax.plot(waves, c=\"k\", **kwargs)\n        return ax\n\n    def _getPowerSpectrumPlot(\n        self,\n        freqs: np.ndarray,\n        power: np.ndarray,\n        sm_power: np.ndarray,\n        band_max_power: float,\n        freq_at_band_max_power: float,\n        max_freq: int = 50,\n        theta_range: tuple = [6, 12],\n        ax: plt.Axes = None,\n        **kwargs,\n    ) -&gt; plt.Axes:\n        \"\"\"\n        Plots the power spectrum. The parameters can be obtained from\n        calcEEGPowerSpectrum() in the EEGCalcsGeneric class.\n\n        Args:\n            freqs (np.array): The frequencies.\n            power (np.array): The power values.\n            sm_power (np.array): The smoothed power values.\n            band_max_power (float): The maximum power in the band.\n            freq_at_band_max_power (float): The frequency at which the maximum\n                power in the band occurs.\n            max_freq (int, optional): The maximum frequency. Defaults to 50.\n            theta_range (tuple, optional): The theta range.\n                Defaults to [6, 12].\n            ax (matplotlib.axes, optional): The axes to plot on. If None, new\n                axes are created.\n            **kwargs: Additional keyword arguments for the function.\n\n        Returns:\n            matplotlib.axes: The axes with the plot.\n        \"\"\"\n        min_freq = kwargs.pop(\"min_freq\", 0)\n        if \"strip_axes\" in kwargs.keys():\n            strip_axes = kwargs.pop(\"strip_axes\")\n        else:\n            strip_axes = False\n        # downsample frequencies and power\n        freqs = freqs[0::50]\n        power = power[0::50]\n        sm_power = sm_power[0::50]\n        if ax is None:\n            fig = plt.figure()\n            ax = fig.add_subplot(111)\n        ax.plot(freqs, power, alpha=0.5, color=[0.8627, 0.8627, 0.8627])\n        ax.plot(freqs, sm_power)\n        ax.set_xlim(min_freq, max_freq)\n        ylim = [0, np.max(sm_power[freqs &lt; max_freq])]\n        if \"ylim\" in kwargs:\n            ylim = kwargs[\"ylim\"]\n        ax.set_ylim(ylim)\n        ax.set_ylabel(\"Power\")\n        ax.set_xlabel(\"Frequency\")\n        ax.text(\n            x=theta_range[1] / 0.9,\n            y=band_max_power,\n            s=str(freq_at_band_max_power)[0:4],\n            fontsize=20,\n        )\n        from matplotlib.patches import Rectangle\n\n        r = Rectangle(\n            (theta_range[0], 0),\n            width=np.diff(theta_range)[0],\n            height=np.diff(ax.get_ylim())[0],\n            alpha=0.25,\n            color=\"r\",\n            ec=\"none\",\n        )\n        ax.add_patch(r)\n        if strip_axes:\n            return stripAxes(ax)\n        return ax\n\n    def _getXCorrPlot(\n        self, spk_times: np.array, ax: matplotlib.axes = None, **kwargs\n    ) -&gt; plt.Axes:\n        \"\"\"\n        Returns an axis containing the autocorrelogram of the spike\n        times provided over the range +/-500ms.\n\n        Args:\n            spk_times (np.array): Spike times in seconds.\n            ax (matplotlib.axes, optional): The axes to plot into. If None,\n                new axes are created.\n            **kwargs: Additional keyword arguments for the function.\n                binsize (int, optional): The size of the bins in ms. Gets\n                passed to SpikeCalcsGeneric.xcorr(). Defaults to 1.\n\n        Returns:\n            matplotlib.axes: The axes with the plot.\n        \"\"\"\n        if \"strip_axes\" in kwargs.keys():\n            strip_axes = kwargs.pop(\"strip_axes\")\n        else:\n            strip_axes = False\n        if ax is None:\n            fig = plt.figure()\n            ax = fig.add_subplot(111)\n        if \"binsize\" in kwargs.keys():\n            binsize = kwargs[\"binsize\"]\n        else:\n            binsize = 0.001\n        if \"Trange\" in kwargs.keys():\n            xrange = kwargs.pop(\"Trange\")\n        else:\n            xrange = [-0.5, 0.5]\n        ac = xcorr(spk_times, Trange=xrange, **kwargs)\n        c = ac.binned_data[0]\n        b = ac.bin_edges[0]\n        ax.bar(b[:-1], c, width=binsize, color=\"k\", align=\"edge\", zorder=3)\n        ax.set_xlim(xrange)\n        ax.set_xticks((xrange[0], 0, xrange[1]))\n        ax.set_xticklabels(\"\")\n        ax.tick_params(\n            axis=\"both\", which=\"both\", left=False, right=False, bottom=False, top=False\n        )\n        ax.set_yticklabels(\"\")\n        ax.xaxis.set_ticks_position(\"bottom\")\n        if strip_axes:\n            return stripAxes(ax)\n        axtrans = transforms.blended_transform_factory(ax.transData, ax.transAxes)\n        ax.vlines(0, ymin=0, ymax=1, colors=\"lightgrey\", transform=axtrans, zorder=1)\n        return ax\n\n    def _getRasterPlot(\n        self,\n        spk_times: np.ndarray,\n        dt=(-0.05, 0.1),\n        ax: matplotlib.axes = None,\n        cluster=0,\n        secs_per_bin: int = 0.001,\n        **kwargs,\n    ) -&gt; plt.Axes:\n        \"\"\"\n        Plots a raster plot for a specified tetrode/ cluster.\n\n        Args:\n            spk_times (np.array): The spike times in seconds\n            dt (tuple, optional): The window of time in ms to examine zeroed\n                on the event of interest i.e. the first value will probably\n                be negative as in the example. Defaults to (-50, 100).\n            prc_max (float, optional): The proportion of firing the cell has\n                to 'lose' to count as silent; a float between 0 and 1.\n                Defaults to 0.5.\n            ax (matplotlib.axes, optional): The axes to plot into.\n                If not provided a new figure is created. Defaults to None.\n            ms_per_bin (int, optional): The number of milliseconds in each bin\n                of the raster plot. Defaults to 1.\n            sample_rate (float, optional): The sample rate. Defaults to 3e4.\n            **kwargs: Additional keyword arguments for the function.\n\n        Returns:\n            matplotlib.axes: The axes with the plot.\n        \"\"\"\n        assert hasattr(self, \"ttl_data\")\n\n        if \"strip_axes\" in kwargs.keys():\n            strip_axes = kwargs.pop(\"strip_axes\")\n        else:\n            strip_axes = False\n        S = SpikeCalcsGeneric(spk_times, cluster=cluster)\n        S.event_ts = self.ttl_data[\"ttl_timestamps\"]\n        S.event_window = np.array(dt)\n        x, y = S.psth()\n        if ax is None:\n            fig = plt.figure(figsize=(4.0, 7.0))\n            axScatter = fig.add_subplot(111)\n        else:\n            axScatter = ax\n        histColor = [1 / 255.0, 1 / 255.0, 1 / 255.0]\n        axScatter.scatter(x, y, marker=\".\", s=2, rasterized=False, color=histColor)\n        divider = make_axes_locatable(axScatter)\n        axScatter.set_xticks((dt[0], 0, dt[1]))\n        axScatter.set_xticklabels((str(dt[0]), \"0\", str(dt[1])))\n        axHistx = divider.append_axes(\n            \"top\", 0.95, pad=0.2, sharex=axScatter, transform=axScatter.transAxes\n        )\n        scattTrans = transforms.blended_transform_factory(\n            axScatter.transData, axScatter.transAxes\n        )\n        stim_pwidth = self.ttl_data[\"stim_duration\"]\n        if stim_pwidth is None:\n            raise ValueError(\"stim duration is None\")\n\n        axScatter.add_patch(\n            Rectangle(\n                (0, 0),\n                width=stim_pwidth,\n                height=1,\n                transform=scattTrans,\n                color=[0, 0, 1],\n                alpha=0.3,\n            )\n        )\n        histTrans = transforms.blended_transform_factory(\n            axHistx.transData, axHistx.transAxes\n        )\n        axHistx.add_patch(\n            Rectangle(\n                (0, 0),\n                width=stim_pwidth,\n                height=1,\n                transform=histTrans,\n                color=[0, 0, 1],\n                alpha=0.3,\n            )\n        )\n        axScatter.set_ylabel(\"Laser stimulation events\", labelpad=-2.5)\n        axScatter.set_xlabel(\"Time to stimulus onset(s)\")\n        nStms = y[-1]\n        axScatter.set_ylim(0, nStms)\n        # Label only the min and max of the y-axis\n        ylabels = axScatter.get_yticklabels()\n        for i in range(1, len(ylabels) - 1):\n            ylabels[i].set_visible(False)\n        yticks = axScatter.get_yticklines()\n        for i in range(1, len(yticks) - 1):\n            yticks[i].set_visible(False)\n\n        axHistx.hist(\n            x,\n            bins=np.arange(dt[0], dt[1] + secs_per_bin, secs_per_bin),\n            color=histColor,\n            range=dt,\n            rasterized=True,\n            histtype=\"stepfilled\",\n        )\n        axHistx.set_ylabel(\"Spike count\", labelpad=-2.5)\n        plt.setp(axHistx.get_xticklabels(), visible=False)\n        # Label only the min and max of the y-axis\n        ylabels = axHistx.get_yticklabels()\n        for i in range(1, len(ylabels) - 1):\n            ylabels[i].set_visible(False)\n        yticks = axHistx.get_yticklines()\n        for i in range(1, len(yticks) - 1):\n            yticks[i].set_visible(False)\n        axHistx.set_xlim(dt)\n        axScatter.set_xlim(dt)\n        fig = plt.gcf()\n        fig.canvas.manager.set_window_title(f\"Cluster {cluster}\")\n        if strip_axes:\n            return stripAxes(axScatter)\n        return axScatter\n\n    def plotSpectrogramByDepth(\n        self,\n        nchannels: int = 384,\n        nseconds: int = 100,\n        maxFreq: int = 125,\n        channels: list = [],\n        frequencies: list = [],\n        frequencyIncrement: int = 1,\n        **kwargs,\n    ):\n        \"\"\"\n        Plots a heat map spectrogram of the LFP for each channel.\n        Line plots of power per frequency band and power on a subset of\n        channels are also displayed to the right and above the main plot.\n\n        Args:\n            nchannels (int): The number of channels on the probe.\n            nseconds (int, optional): How long in seconds from the start of\n                the trial to do the spectrogram for (for speed).\n                Default is 100.\n            maxFreq (int): The maximum frequency in Hz to plot the spectrogram\n                out to. Maximum is 1250. Default is 125.\n            channels (list): The channels to plot separately on the top plot.\n            frequencies (list): The specific frequencies to examine across\n                all channels. The mean from frequency:\n                frequency+frequencyIncrement is calculated and plotted on\n                the left hand side of the plot.\n            frequencyIncrement (int): The amount to add to each value of\n                the frequencies list above.\n            **kwargs: Additional keyword arguments for the function.\n                Valid key value pairs:\n                    \"saveas\" - save the figure to this location, needs absolute\n                    path and filename.\n\n        Notes:\n            Should also allow kwargs to specify exactly which channels\n            and / or frequency bands to do the line plots for.\n        \"\"\"\n        if not self.path2LFPdata:\n            raise TypeError(\"Not a probe recording so not plotting\")\n        import os\n\n        lfp_file = os.path.join(self.path2LFPdata, \"continuous.dat\")\n        status = os.stat(lfp_file)\n        nsamples = int(status.st_size / 2 / nchannels)\n        mmap = np.memmap(lfp_file, np.int16, \"r\", 0, (nchannels, nsamples), order=\"F\")\n        # Load the channel map NB assumes this is in the AP data\n        # location and that kilosort was run there\n        channel_map = np.squeeze(\n            np.load(os.path.join(self.path2APdata, \"channel_map.npy\"))\n        )\n        lfp_sample_rate = 2500\n        data = np.array(mmap[channel_map, 0 : nseconds * lfp_sample_rate])\n        from ephysiopy.common.ephys_generic import EEGCalcsGeneric\n\n        E = EEGCalcsGeneric(data[0, :], lfp_sample_rate)\n        E.calcEEGPowerSpectrum()\n        spec_data = np.zeros(shape=(data.shape[0], len(E.sm_power[0::50])))\n        for chan in range(data.shape[0]):\n            E = EEGCalcsGeneric(data[chan, :], lfp_sample_rate)\n            E.calcEEGPowerSpectrum()\n            spec_data[chan, :] = E.sm_power[0::50]\n\n        x, y = np.meshgrid(E.freqs[0::50], channel_map)\n        import matplotlib.colors as colors\n        from matplotlib.pyplot import cm\n        from mpl_toolkits.axes_grid1 import make_axes_locatable\n\n        _, spectoAx = plt.subplots()\n        spectoAx.pcolormesh(\n            x, y, spec_data, edgecolors=\"face\", cmap=\"bone\", norm=colors.LogNorm()\n        )\n        spectoAx.set_xlim(0, maxFreq)\n        spectoAx.set_ylim(channel_map[0], channel_map[-1])\n        spectoAx.set_xlabel(\"Frequency (Hz)\")\n        spectoAx.set_ylabel(\"Channel\")\n        divider = make_axes_locatable(spectoAx)\n        channel_spectoAx = divider.append_axes(\"top\", 1.2, pad=0.1, sharex=spectoAx)\n        meanfreq_powerAx = divider.append_axes(\"right\", 1.2, pad=0.1, sharey=spectoAx)\n        plt.setp(\n            channel_spectoAx.get_xticklabels() + meanfreq_powerAx.get_yticklabels(),\n            visible=False,\n        )\n\n        # plot mean power across some channels\n        mn_power = np.mean(spec_data, 0)\n        if not channels:\n            channels = range(1, nchannels, 60)\n        cols = iter(cm.rainbow(np.linspace(0, 1, len(channels))))\n        for chan in channels:\n            c = next(cols)\n            channel_spectoAx.plot(\n                E.freqs[0::50],\n                10 * np.log10(spec_data[chan, :] / mn_power),\n                c=c,\n                label=str(chan),\n            )\n\n        channel_spectoAx.set_ylabel(\"Channel power(dB)\")\n        channel_spectoAx.legend(\n            bbox_to_anchor=(0.0, 1.02, 1.0, 0.102),\n            loc=\"lower left\",\n            mode=\"expand\",\n            fontsize=\"x-small\",\n            ncol=4,\n        )\n\n        # plot mean frequencies across all channels\n        if not frequencyIncrement:\n            freq_inc = 6\n        else:\n            freq_inc = frequencyIncrement\n        if not frequencies:\n            lower_freqs = np.arange(1, maxFreq - freq_inc, freq_inc)\n        else:\n            lower_freqs = frequencies\n        upper_freqs = [f + freq_inc for f in lower_freqs]\n        cols = iter(cm.nipy_spectral(np.linspace(0, 1, len(upper_freqs))))\n        mn_power = np.mean(spec_data, 1)\n        for freqs in zip(lower_freqs, upper_freqs):\n            freq_mask = np.logical_and(\n                E.freqs[0::50] &gt; freqs[0], E.freqs[0::50] &lt; freqs[1]\n            )\n            mean_power = 10 * np.log10(np.mean(spec_data[:, freq_mask], 1) / mn_power)\n            c = next(cols)\n            meanfreq_powerAx.plot(\n                mean_power,\n                channel_map,\n                c=c,\n                label=str(freqs[0]) + \" - \" + str(freqs[1]),\n            )\n        meanfreq_powerAx.set_xlabel(\"Mean freq. band power(dB)\")\n        meanfreq_powerAx.legend(\n            bbox_to_anchor=(0.0, 1.02, 1.0, 0.102),\n            loc=\"lower left\",\n            mode=\"expand\",\n            fontsize=\"x-small\",\n            ncol=1,\n        )\n        if \"saveas\" in kwargs:\n            saveas = kwargs[\"saveas\"]\n            plt.savefig(saveas)\n        plt.show()\n</code></pre>"},{"location":"reference/#ephysiopy.visualise.plotting.FigureMaker.PosCalcs","title":"<code>PosCalcs = None</code>  <code>instance-attribute</code>","text":"<p>Initializes the FigureMaker object with data from PosCalcs.</p>"},{"location":"reference/#ephysiopy.visualise.plotting.FigureMaker.__init__","title":"<code>__init__()</code>","text":"<p>Initializes the FigureMaker object.</p> Source code in <code>ephysiopy/visualise/plotting.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    Initializes the FigureMaker object.\n    \"\"\"\n    self.PosCalcs = None\n\n    \"\"\"\n    Initializes the FigureMaker object with data from PosCalcs.\n    \"\"\"\n    if self.PosCalcs is not None:\n        self.RateMap = RateMap(self.PosCalcs)\n        self.npos = self.PosCalcs.xy.shape[1]\n</code></pre>"},{"location":"reference/#ephysiopy.visualise.plotting.FigureMaker._getPowerSpectrumPlot","title":"<code>_getPowerSpectrumPlot(freqs, power, sm_power, band_max_power, freq_at_band_max_power, max_freq=50, theta_range=[6, 12], ax=None, **kwargs)</code>","text":"<p>Plots the power spectrum. The parameters can be obtained from calcEEGPowerSpectrum() in the EEGCalcsGeneric class.</p> <p>Args:     freqs (np.array): The frequencies.     power (np.array): The power values.     sm_power (np.array): The smoothed power values.     band_max_power (float): The maximum power in the band.     freq_at_band_max_power (float): The frequency at which the maximum         power in the band occurs.     max_freq (int, optional): The maximum frequency. Defaults to 50.     theta_range (tuple, optional): The theta range.         Defaults to [6, 12].     ax (matplotlib.axes, optional): The axes to plot on. If None, new         axes are created.     **kwargs: Additional keyword arguments for the function.</p> <p>Returns:     matplotlib.axes: The axes with the plot.</p> Source code in <code>ephysiopy/visualise/plotting.py</code> <pre><code>def _getPowerSpectrumPlot(\n    self,\n    freqs: np.ndarray,\n    power: np.ndarray,\n    sm_power: np.ndarray,\n    band_max_power: float,\n    freq_at_band_max_power: float,\n    max_freq: int = 50,\n    theta_range: tuple = [6, 12],\n    ax: plt.Axes = None,\n    **kwargs,\n) -&gt; plt.Axes:\n    \"\"\"\n    Plots the power spectrum. The parameters can be obtained from\n    calcEEGPowerSpectrum() in the EEGCalcsGeneric class.\n\n    Args:\n        freqs (np.array): The frequencies.\n        power (np.array): The power values.\n        sm_power (np.array): The smoothed power values.\n        band_max_power (float): The maximum power in the band.\n        freq_at_band_max_power (float): The frequency at which the maximum\n            power in the band occurs.\n        max_freq (int, optional): The maximum frequency. Defaults to 50.\n        theta_range (tuple, optional): The theta range.\n            Defaults to [6, 12].\n        ax (matplotlib.axes, optional): The axes to plot on. If None, new\n            axes are created.\n        **kwargs: Additional keyword arguments for the function.\n\n    Returns:\n        matplotlib.axes: The axes with the plot.\n    \"\"\"\n    min_freq = kwargs.pop(\"min_freq\", 0)\n    if \"strip_axes\" in kwargs.keys():\n        strip_axes = kwargs.pop(\"strip_axes\")\n    else:\n        strip_axes = False\n    # downsample frequencies and power\n    freqs = freqs[0::50]\n    power = power[0::50]\n    sm_power = sm_power[0::50]\n    if ax is None:\n        fig = plt.figure()\n        ax = fig.add_subplot(111)\n    ax.plot(freqs, power, alpha=0.5, color=[0.8627, 0.8627, 0.8627])\n    ax.plot(freqs, sm_power)\n    ax.set_xlim(min_freq, max_freq)\n    ylim = [0, np.max(sm_power[freqs &lt; max_freq])]\n    if \"ylim\" in kwargs:\n        ylim = kwargs[\"ylim\"]\n    ax.set_ylim(ylim)\n    ax.set_ylabel(\"Power\")\n    ax.set_xlabel(\"Frequency\")\n    ax.text(\n        x=theta_range[1] / 0.9,\n        y=band_max_power,\n        s=str(freq_at_band_max_power)[0:4],\n        fontsize=20,\n    )\n    from matplotlib.patches import Rectangle\n\n    r = Rectangle(\n        (theta_range[0], 0),\n        width=np.diff(theta_range)[0],\n        height=np.diff(ax.get_ylim())[0],\n        alpha=0.25,\n        color=\"r\",\n        ec=\"none\",\n    )\n    ax.add_patch(r)\n    if strip_axes:\n        return stripAxes(ax)\n    return ax\n</code></pre>"},{"location":"reference/#ephysiopy.visualise.plotting.FigureMaker._getRasterPlot","title":"<code>_getRasterPlot(spk_times, dt=(-0.05, 0.1), ax=None, cluster=0, secs_per_bin=0.001, **kwargs)</code>","text":"<p>Plots a raster plot for a specified tetrode/ cluster.</p> <p>Args:     spk_times (np.array): The spike times in seconds     dt (tuple, optional): The window of time in ms to examine zeroed         on the event of interest i.e. the first value will probably         be negative as in the example. Defaults to (-50, 100).     prc_max (float, optional): The proportion of firing the cell has         to 'lose' to count as silent; a float between 0 and 1.         Defaults to 0.5.     ax (matplotlib.axes, optional): The axes to plot into.         If not provided a new figure is created. Defaults to None.     ms_per_bin (int, optional): The number of milliseconds in each bin         of the raster plot. Defaults to 1.     sample_rate (float, optional): The sample rate. Defaults to 3e4.     **kwargs: Additional keyword arguments for the function.</p> <p>Returns:     matplotlib.axes: The axes with the plot.</p> Source code in <code>ephysiopy/visualise/plotting.py</code> <pre><code>def _getRasterPlot(\n    self,\n    spk_times: np.ndarray,\n    dt=(-0.05, 0.1),\n    ax: matplotlib.axes = None,\n    cluster=0,\n    secs_per_bin: int = 0.001,\n    **kwargs,\n) -&gt; plt.Axes:\n    \"\"\"\n    Plots a raster plot for a specified tetrode/ cluster.\n\n    Args:\n        spk_times (np.array): The spike times in seconds\n        dt (tuple, optional): The window of time in ms to examine zeroed\n            on the event of interest i.e. the first value will probably\n            be negative as in the example. Defaults to (-50, 100).\n        prc_max (float, optional): The proportion of firing the cell has\n            to 'lose' to count as silent; a float between 0 and 1.\n            Defaults to 0.5.\n        ax (matplotlib.axes, optional): The axes to plot into.\n            If not provided a new figure is created. Defaults to None.\n        ms_per_bin (int, optional): The number of milliseconds in each bin\n            of the raster plot. Defaults to 1.\n        sample_rate (float, optional): The sample rate. Defaults to 3e4.\n        **kwargs: Additional keyword arguments for the function.\n\n    Returns:\n        matplotlib.axes: The axes with the plot.\n    \"\"\"\n    assert hasattr(self, \"ttl_data\")\n\n    if \"strip_axes\" in kwargs.keys():\n        strip_axes = kwargs.pop(\"strip_axes\")\n    else:\n        strip_axes = False\n    S = SpikeCalcsGeneric(spk_times, cluster=cluster)\n    S.event_ts = self.ttl_data[\"ttl_timestamps\"]\n    S.event_window = np.array(dt)\n    x, y = S.psth()\n    if ax is None:\n        fig = plt.figure(figsize=(4.0, 7.0))\n        axScatter = fig.add_subplot(111)\n    else:\n        axScatter = ax\n    histColor = [1 / 255.0, 1 / 255.0, 1 / 255.0]\n    axScatter.scatter(x, y, marker=\".\", s=2, rasterized=False, color=histColor)\n    divider = make_axes_locatable(axScatter)\n    axScatter.set_xticks((dt[0], 0, dt[1]))\n    axScatter.set_xticklabels((str(dt[0]), \"0\", str(dt[1])))\n    axHistx = divider.append_axes(\n        \"top\", 0.95, pad=0.2, sharex=axScatter, transform=axScatter.transAxes\n    )\n    scattTrans = transforms.blended_transform_factory(\n        axScatter.transData, axScatter.transAxes\n    )\n    stim_pwidth = self.ttl_data[\"stim_duration\"]\n    if stim_pwidth is None:\n        raise ValueError(\"stim duration is None\")\n\n    axScatter.add_patch(\n        Rectangle(\n            (0, 0),\n            width=stim_pwidth,\n            height=1,\n            transform=scattTrans,\n            color=[0, 0, 1],\n            alpha=0.3,\n        )\n    )\n    histTrans = transforms.blended_transform_factory(\n        axHistx.transData, axHistx.transAxes\n    )\n    axHistx.add_patch(\n        Rectangle(\n            (0, 0),\n            width=stim_pwidth,\n            height=1,\n            transform=histTrans,\n            color=[0, 0, 1],\n            alpha=0.3,\n        )\n    )\n    axScatter.set_ylabel(\"Laser stimulation events\", labelpad=-2.5)\n    axScatter.set_xlabel(\"Time to stimulus onset(s)\")\n    nStms = y[-1]\n    axScatter.set_ylim(0, nStms)\n    # Label only the min and max of the y-axis\n    ylabels = axScatter.get_yticklabels()\n    for i in range(1, len(ylabels) - 1):\n        ylabels[i].set_visible(False)\n    yticks = axScatter.get_yticklines()\n    for i in range(1, len(yticks) - 1):\n        yticks[i].set_visible(False)\n\n    axHistx.hist(\n        x,\n        bins=np.arange(dt[0], dt[1] + secs_per_bin, secs_per_bin),\n        color=histColor,\n        range=dt,\n        rasterized=True,\n        histtype=\"stepfilled\",\n    )\n    axHistx.set_ylabel(\"Spike count\", labelpad=-2.5)\n    plt.setp(axHistx.get_xticklabels(), visible=False)\n    # Label only the min and max of the y-axis\n    ylabels = axHistx.get_yticklabels()\n    for i in range(1, len(ylabels) - 1):\n        ylabels[i].set_visible(False)\n    yticks = axHistx.get_yticklines()\n    for i in range(1, len(yticks) - 1):\n        yticks[i].set_visible(False)\n    axHistx.set_xlim(dt)\n    axScatter.set_xlim(dt)\n    fig = plt.gcf()\n    fig.canvas.manager.set_window_title(f\"Cluster {cluster}\")\n    if strip_axes:\n        return stripAxes(axScatter)\n    return axScatter\n</code></pre>"},{"location":"reference/#ephysiopy.visualise.plotting.FigureMaker._getXCorrPlot","title":"<code>_getXCorrPlot(spk_times, ax=None, **kwargs)</code>","text":"<p>Returns an axis containing the autocorrelogram of the spike times provided over the range +/-500ms.</p> <p>Args:     spk_times (np.array): Spike times in seconds.     ax (matplotlib.axes, optional): The axes to plot into. If None,         new axes are created.     **kwargs: Additional keyword arguments for the function.         binsize (int, optional): The size of the bins in ms. Gets         passed to SpikeCalcsGeneric.xcorr(). Defaults to 1.</p> <p>Returns:     matplotlib.axes: The axes with the plot.</p> Source code in <code>ephysiopy/visualise/plotting.py</code> <pre><code>def _getXCorrPlot(\n    self, spk_times: np.array, ax: matplotlib.axes = None, **kwargs\n) -&gt; plt.Axes:\n    \"\"\"\n    Returns an axis containing the autocorrelogram of the spike\n    times provided over the range +/-500ms.\n\n    Args:\n        spk_times (np.array): Spike times in seconds.\n        ax (matplotlib.axes, optional): The axes to plot into. If None,\n            new axes are created.\n        **kwargs: Additional keyword arguments for the function.\n            binsize (int, optional): The size of the bins in ms. Gets\n            passed to SpikeCalcsGeneric.xcorr(). Defaults to 1.\n\n    Returns:\n        matplotlib.axes: The axes with the plot.\n    \"\"\"\n    if \"strip_axes\" in kwargs.keys():\n        strip_axes = kwargs.pop(\"strip_axes\")\n    else:\n        strip_axes = False\n    if ax is None:\n        fig = plt.figure()\n        ax = fig.add_subplot(111)\n    if \"binsize\" in kwargs.keys():\n        binsize = kwargs[\"binsize\"]\n    else:\n        binsize = 0.001\n    if \"Trange\" in kwargs.keys():\n        xrange = kwargs.pop(\"Trange\")\n    else:\n        xrange = [-0.5, 0.5]\n    ac = xcorr(spk_times, Trange=xrange, **kwargs)\n    c = ac.binned_data[0]\n    b = ac.bin_edges[0]\n    ax.bar(b[:-1], c, width=binsize, color=\"k\", align=\"edge\", zorder=3)\n    ax.set_xlim(xrange)\n    ax.set_xticks((xrange[0], 0, xrange[1]))\n    ax.set_xticklabels(\"\")\n    ax.tick_params(\n        axis=\"both\", which=\"both\", left=False, right=False, bottom=False, top=False\n    )\n    ax.set_yticklabels(\"\")\n    ax.xaxis.set_ticks_position(\"bottom\")\n    if strip_axes:\n        return stripAxes(ax)\n    axtrans = transforms.blended_transform_factory(ax.transData, ax.transAxes)\n    ax.vlines(0, ymin=0, ymax=1, colors=\"lightgrey\", transform=axtrans, zorder=1)\n    return ax\n</code></pre>"},{"location":"reference/#ephysiopy.visualise.plotting.FigureMaker._plot_multiple_clusters","title":"<code>_plot_multiple_clusters(func, clusters, channel, **kwargs)</code>","text":"<p>Plots multiple clusters.</p> <p>Args:     func (function): The function to apply to each cluster.     clusters (list): The list of clusters to plot.     channel (int): The channel number.     **kwargs: Additional keyword arguments for the function.</p> Source code in <code>ephysiopy/visualise/plotting.py</code> <pre><code>def _plot_multiple_clusters(\n    self, func, clusters: list, channel: int, **kwargs\n) -&gt; matplotlib.figure.Figure:\n    \"\"\"\n    Plots multiple clusters.\n\n    Args:\n        func (function): The function to apply to each cluster.\n        clusters (list): The list of clusters to plot.\n        channel (int): The channel number.\n        **kwargs: Additional keyword arguments for the function.\n    \"\"\"\n    fig = plt.figure()\n    nrows = int(np.ceil(len(clusters) / 5))\n    if \"projection\" in kwargs.keys():\n        proj = kwargs.pop(\"projection\")\n    else:\n        proj = None\n    for i, c in enumerate(clusters):\n        ax = fig.add_subplot(nrows, 5, i + 1, projection=proj)\n        ts = self.get_spike_times(c, channel)\n        func(ts, ax=ax, **kwargs)\n    return fig\n</code></pre>"},{"location":"reference/#ephysiopy.visualise.plotting.FigureMaker.plotSpectrogramByDepth","title":"<code>plotSpectrogramByDepth(nchannels=384, nseconds=100, maxFreq=125, channels=[], frequencies=[], frequencyIncrement=1, **kwargs)</code>","text":"<p>Plots a heat map spectrogram of the LFP for each channel. Line plots of power per frequency band and power on a subset of channels are also displayed to the right and above the main plot.</p> <p>Args:     nchannels (int): The number of channels on the probe.     nseconds (int, optional): How long in seconds from the start of         the trial to do the spectrogram for (for speed).         Default is 100.     maxFreq (int): The maximum frequency in Hz to plot the spectrogram         out to. Maximum is 1250. Default is 125.     channels (list): The channels to plot separately on the top plot.     frequencies (list): The specific frequencies to examine across         all channels. The mean from frequency:         frequency+frequencyIncrement is calculated and plotted on         the left hand side of the plot.     frequencyIncrement (int): The amount to add to each value of         the frequencies list above.     **kwargs: Additional keyword arguments for the function.         Valid key value pairs:             \"saveas\" - save the figure to this location, needs absolute             path and filename.</p> <p>Notes:     Should also allow kwargs to specify exactly which channels     and / or frequency bands to do the line plots for.</p> Source code in <code>ephysiopy/visualise/plotting.py</code> <pre><code>def plotSpectrogramByDepth(\n    self,\n    nchannels: int = 384,\n    nseconds: int = 100,\n    maxFreq: int = 125,\n    channels: list = [],\n    frequencies: list = [],\n    frequencyIncrement: int = 1,\n    **kwargs,\n):\n    \"\"\"\n    Plots a heat map spectrogram of the LFP for each channel.\n    Line plots of power per frequency band and power on a subset of\n    channels are also displayed to the right and above the main plot.\n\n    Args:\n        nchannels (int): The number of channels on the probe.\n        nseconds (int, optional): How long in seconds from the start of\n            the trial to do the spectrogram for (for speed).\n            Default is 100.\n        maxFreq (int): The maximum frequency in Hz to plot the spectrogram\n            out to. Maximum is 1250. Default is 125.\n        channels (list): The channels to plot separately on the top plot.\n        frequencies (list): The specific frequencies to examine across\n            all channels. The mean from frequency:\n            frequency+frequencyIncrement is calculated and plotted on\n            the left hand side of the plot.\n        frequencyIncrement (int): The amount to add to each value of\n            the frequencies list above.\n        **kwargs: Additional keyword arguments for the function.\n            Valid key value pairs:\n                \"saveas\" - save the figure to this location, needs absolute\n                path and filename.\n\n    Notes:\n        Should also allow kwargs to specify exactly which channels\n        and / or frequency bands to do the line plots for.\n    \"\"\"\n    if not self.path2LFPdata:\n        raise TypeError(\"Not a probe recording so not plotting\")\n    import os\n\n    lfp_file = os.path.join(self.path2LFPdata, \"continuous.dat\")\n    status = os.stat(lfp_file)\n    nsamples = int(status.st_size / 2 / nchannels)\n    mmap = np.memmap(lfp_file, np.int16, \"r\", 0, (nchannels, nsamples), order=\"F\")\n    # Load the channel map NB assumes this is in the AP data\n    # location and that kilosort was run there\n    channel_map = np.squeeze(\n        np.load(os.path.join(self.path2APdata, \"channel_map.npy\"))\n    )\n    lfp_sample_rate = 2500\n    data = np.array(mmap[channel_map, 0 : nseconds * lfp_sample_rate])\n    from ephysiopy.common.ephys_generic import EEGCalcsGeneric\n\n    E = EEGCalcsGeneric(data[0, :], lfp_sample_rate)\n    E.calcEEGPowerSpectrum()\n    spec_data = np.zeros(shape=(data.shape[0], len(E.sm_power[0::50])))\n    for chan in range(data.shape[0]):\n        E = EEGCalcsGeneric(data[chan, :], lfp_sample_rate)\n        E.calcEEGPowerSpectrum()\n        spec_data[chan, :] = E.sm_power[0::50]\n\n    x, y = np.meshgrid(E.freqs[0::50], channel_map)\n    import matplotlib.colors as colors\n    from matplotlib.pyplot import cm\n    from mpl_toolkits.axes_grid1 import make_axes_locatable\n\n    _, spectoAx = plt.subplots()\n    spectoAx.pcolormesh(\n        x, y, spec_data, edgecolors=\"face\", cmap=\"bone\", norm=colors.LogNorm()\n    )\n    spectoAx.set_xlim(0, maxFreq)\n    spectoAx.set_ylim(channel_map[0], channel_map[-1])\n    spectoAx.set_xlabel(\"Frequency (Hz)\")\n    spectoAx.set_ylabel(\"Channel\")\n    divider = make_axes_locatable(spectoAx)\n    channel_spectoAx = divider.append_axes(\"top\", 1.2, pad=0.1, sharex=spectoAx)\n    meanfreq_powerAx = divider.append_axes(\"right\", 1.2, pad=0.1, sharey=spectoAx)\n    plt.setp(\n        channel_spectoAx.get_xticklabels() + meanfreq_powerAx.get_yticklabels(),\n        visible=False,\n    )\n\n    # plot mean power across some channels\n    mn_power = np.mean(spec_data, 0)\n    if not channels:\n        channels = range(1, nchannels, 60)\n    cols = iter(cm.rainbow(np.linspace(0, 1, len(channels))))\n    for chan in channels:\n        c = next(cols)\n        channel_spectoAx.plot(\n            E.freqs[0::50],\n            10 * np.log10(spec_data[chan, :] / mn_power),\n            c=c,\n            label=str(chan),\n        )\n\n    channel_spectoAx.set_ylabel(\"Channel power(dB)\")\n    channel_spectoAx.legend(\n        bbox_to_anchor=(0.0, 1.02, 1.0, 0.102),\n        loc=\"lower left\",\n        mode=\"expand\",\n        fontsize=\"x-small\",\n        ncol=4,\n    )\n\n    # plot mean frequencies across all channels\n    if not frequencyIncrement:\n        freq_inc = 6\n    else:\n        freq_inc = frequencyIncrement\n    if not frequencies:\n        lower_freqs = np.arange(1, maxFreq - freq_inc, freq_inc)\n    else:\n        lower_freqs = frequencies\n    upper_freqs = [f + freq_inc for f in lower_freqs]\n    cols = iter(cm.nipy_spectral(np.linspace(0, 1, len(upper_freqs))))\n    mn_power = np.mean(spec_data, 1)\n    for freqs in zip(lower_freqs, upper_freqs):\n        freq_mask = np.logical_and(\n            E.freqs[0::50] &gt; freqs[0], E.freqs[0::50] &lt; freqs[1]\n        )\n        mean_power = 10 * np.log10(np.mean(spec_data[:, freq_mask], 1) / mn_power)\n        c = next(cols)\n        meanfreq_powerAx.plot(\n            mean_power,\n            channel_map,\n            c=c,\n            label=str(freqs[0]) + \" - \" + str(freqs[1]),\n        )\n    meanfreq_powerAx.set_xlabel(\"Mean freq. band power(dB)\")\n    meanfreq_powerAx.legend(\n        bbox_to_anchor=(0.0, 1.02, 1.0, 0.102),\n        loc=\"lower left\",\n        mode=\"expand\",\n        fontsize=\"x-small\",\n        ncol=1,\n    )\n    if \"saveas\" in kwargs:\n        saveas = kwargs[\"saveas\"]\n        plt.savefig(saveas)\n    plt.show()\n</code></pre>"},{"location":"reference/#ephysiopy.visualise.plotting.FigureMaker.plot_acorr","title":"<code>plot_acorr(cluster, channel, **kwargs)</code>","text":"<p>Gets the autocorrelogram for the specified cluster(s) and channel.</p> <p>Args:     cluster (int): The cluster(s) to get the autocorrelogram         for.     channel (int): The channel number.     **kwargs: Additional keyword arguments for the function.</p> Source code in <code>ephysiopy/visualise/plotting.py</code> <pre><code>@savePlot\ndef plot_acorr(self, cluster: int, channel: int, **kwargs) -&gt; plt.Axes:\n    \"\"\"\n    Gets the autocorrelogram for the specified cluster(s) and channel.\n\n    Args:\n        cluster (int): The cluster(s) to get the autocorrelogram\n            for.\n        channel (int): The channel number.\n        **kwargs: Additional keyword arguments for the function.\n    \"\"\"\n\n    ts = self.get_spike_times(cluster, channel)\n    ax = self._getXCorrPlot(ts, **kwargs)\n    return ax\n</code></pre>"},{"location":"reference/#ephysiopy.visualise.plotting.FigureMaker.plot_eb_map","title":"<code>plot_eb_map(cluster, channel, **kwargs)</code>","text":"<p>Gets the ego-centric boundary map for the specified cluster(s) and channel.</p> <p>Args:     cluster (int): The cluster(s) to get the ego-centric         boundary map for.     channel (int): The channel number.     **kwargs: Additional keyword arguments for the function.</p> Source code in <code>ephysiopy/visualise/plotting.py</code> <pre><code>@savePlot\n@stripAxes\ndef plot_eb_map(self, cluster: int, channel: int, **kwargs) -&gt; plt.Axes:\n    \"\"\"\n    Gets the ego-centric boundary map for the specified cluster(s) and\n    channel.\n\n    Args:\n        cluster (int): The cluster(s) to get the ego-centric\n            boundary map for.\n        channel (int): The channel number.\n        **kwargs: Additional keyword arguments for the function.\n    \"\"\"\n    return_ratemap = kwargs.pop(\"return_ratemap\", False)\n    rmap = self.get_eb_map(cluster, channel, range=None, **kwargs)\n    ax = kwargs.pop(\"ax\", None)\n    if ax is None:\n        fig = plt.figure()\n        ax = fig.add_subplot(projection=\"polar\")\n    # sanitise kwargs before passing on to pcolormesh\n    kwargs = clean_kwargs(plt.pcolormesh, kwargs)\n    ax.pcolormesh(\n        rmap.bin_edges[1],\n        rmap.bin_edges[0],\n        rmap.binned_data[0],\n        edgecolors=\"face\",\n        shading=\"auto\",\n        **kwargs,\n    )\n    ax.set_xticks(np.arange(0, 2 * np.pi, np.pi / 4))\n    # ax.set_xticklabels(np.arange(0, 2*np.pi, np.pi/4))\n    ax.set_yticks(np.arange(0, 50, 10))\n    ax.set_yticklabels(np.arange(0, 50, 10))\n    ax.set_xlabel(\"Angle (deg)\")\n    ax.set_ylabel(\"Distance (cm)\")\n    if return_ratemap:\n        return ax, rmap\n    return ax\n</code></pre>"},{"location":"reference/#ephysiopy.visualise.plotting.FigureMaker.plot_eb_spikes","title":"<code>plot_eb_spikes(cluster, channel, **kwargs)</code>","text":"<p>Gets the ego-centric boundary spikes for the specified cluster(s) and channel.</p> <p>Args:     cluster (int): The cluster(s) to get the ego-centric         boundary spikes for.     channel (int): The channel number.     **kwargs: Additional keyword arguments for the function.</p> Source code in <code>ephysiopy/visualise/plotting.py</code> <pre><code>@savePlot\n@stripAxes\ndef plot_eb_spikes(self, cluster: int, channel: int, **kwargs) -&gt; plt.Axes:\n    \"\"\"\n    Gets the ego-centric boundary spikes for the specified cluster(s)\n    and channel.\n\n    Args:\n        cluster (int): The cluster(s) to get the ego-centric\n            boundary spikes for.\n        channel (int): The channel number.\n        **kwargs: Additional keyword arguments for the function.\n    \"\"\"\n    if not self.RateMap:\n        self.initialise()\n    ax = kwargs.pop(\"ax\", None)\n    if ax is None:\n        fig = plt.figure()\n        ax = fig.add_subplot(111)\n    ax.set_aspect(\"equal\")\n    # Parse kwargs\n    num_dir_bins = kwargs.get(\"dir_bins\", 60)\n    rect_size = kwargs.get(\"ms\", 1)\n    add_colour_wheel = kwargs.get(\"add_colour_wheel\", False)\n    dir_colours = sns.color_palette(\"hls\", num_dir_bins)\n    # Process dirrectional data\n    idx = self._get_spike_pos_idx(cluster, channel)\n    dir_spike_fired_at = self.RateMap.dir[idx]\n    idx_of_dir_to_colour = np.floor(\n        dir_spike_fired_at / (360 / num_dir_bins)\n    ).astype(int)\n    rects = [\n        Rectangle(\n            self.RateMap.xy[:, i],\n            width=rect_size,\n            height=rect_size,\n            clip_box=ax.bbox,\n            facecolor=dir_colours[idx_of_dir_to_colour[i]],\n            rasterized=True,\n        )\n        for i in range(len(idx))\n    ]\n    ax.plot(\n        self.PosCalcs.xy[0],\n        self.PosCalcs.xy[1],\n        c=tcols.colours[0],\n        zorder=1,\n        alpha=0.3,\n    )\n    ax.add_collection(PatchCollection(rects, match_original=True))\n    if add_colour_wheel:\n        ax_col = inset_axes(\n            ax,\n            width=\"100%\",\n            height=\"100%\",\n            bbox_to_anchor=(0.75, 0.75, 0.15, 0.15),\n            axes_class=get_projection_class(\"polar\"),\n            bbox_transform=fig.transFigure,\n        )\n        ax_col.set_theta_zero_location(\"N\")\n        theta = np.linspace(0, 2 * np.pi, 1000)\n        phi = np.linspace(0, 1, 2)\n        X, Y = np.meshgrid(phi, theta)\n        norm = matplotlib.colors.Normalize(0, 2 * np.pi)\n        col_map = sns.color_palette(\"hls\", as_cmap=True)\n        ax_col.pcolormesh(theta, phi, Y.T, norm=norm, cmap=col_map)\n        ax_col.set_yticklabels([])\n        ax_col.spines[\"polar\"].set_visible(False)\n        ax_col.set_thetagrids([0, 90])\n    return ax\n</code></pre>"},{"location":"reference/#ephysiopy.visualise.plotting.FigureMaker.plot_hd_map","title":"<code>plot_hd_map(cluster, channel, **kwargs)</code>","text":"<p>Gets the head direction map for the specified cluster(s) and channel.</p> <p>Args:     cluster (int): The cluster(s) to get the head direction map         for.     channel (int): The channel number.     **kwargs: Additional keyword arguments for the function.</p> Source code in <code>ephysiopy/visualise/plotting.py</code> <pre><code>@savePlot\n@stripAxes\ndef plot_hd_map(self, cluster: int, channel: int, **kwargs) -&gt; plt.Axes:\n    \"\"\"\n    Gets the head direction map for the specified cluster(s) and channel.\n\n    Args:\n        cluster (int): The cluster(s) to get the head direction map\n            for.\n        channel (int): The channel number.\n        **kwargs: Additional keyword arguments for the function.\n    \"\"\"\n    rmap = self.get_hd_map(cluster, channel, **kwargs)\n    add_mrv = kwargs.pop(\"add_mrv\", False)\n    add_guides = kwargs.pop(\"add_guides\", False)\n    strip_axes = kwargs.pop(\"strip_axes\", False)\n    fill = kwargs.pop(\"fill\", False)\n    ax = kwargs.pop(\"ax\", None)\n    kwargs = clean_kwargs(plt.pcolormesh, kwargs)\n    if ax is None:\n        fig = plt.figure()\n        ax = fig.add_subplot(111, projection=\"polar\", **kwargs)\n    ax.set_theta_zero_location(\"N\")\n    # need to deal with the case where the axis is supplied but\n    # is not polar. deal with polar first\n    theta = np.deg2rad(rmap.bin_edges[0])\n    ax.clear()\n    r = (\n        rmap.binned_data[0] * self.PosCalcs.sample_rate\n    )  # in samples so * pos sample_rate\n    r = np.insert(r, -1, r[0])\n    hasData = np.any(r &gt; 0)\n    if \"polar\" in ax.name and hasData:\n        ax.plot(theta, r)\n        if fill:\n            ax.fill(theta, r, alpha=0.5)\n        ax.set_aspect(\"equal\")\n\n    if add_guides:\n        ax.set_rgrids([])\n\n    # See if we should add the mean resultant vector (mrv)\n    if add_mrv and hasData:\n        from ephysiopy.common.statscalcs import mean_resultant_vector\n\n        veclen = fc.get_mean_resultant_length(rmap.binned_data[0])\n        th = fc.get_mean_resultant_angle(rmap.binned_data[0])\n        ax.plot(\n            [0, th],\n            [0, veclen * np.max(rmap.binned_data[0]) * self.PosCalcs.sample_rate],\n            \"r\",\n        )\n    if \"polar\" in ax.name:\n        ax.set_thetagrids([0, 90, 180, 270])\n    if strip_axes:\n        return stripAxes(ax)\n    return ax\n</code></pre>"},{"location":"reference/#ephysiopy.visualise.plotting.FigureMaker.plot_power_spectrum","title":"<code>plot_power_spectrum(**kwargs)</code>","text":"<p>Gets the power spectrum.</p> <p>Args:     **kwargs: Additional keyword arguments for the function.</p> Source code in <code>ephysiopy/visualise/plotting.py</code> <pre><code>@savePlot\ndef plot_power_spectrum(self, **kwargs) -&gt; plt.Axes:\n    \"\"\"\n    Gets the power spectrum.\n\n    Args:\n        **kwargs: Additional keyword arguments for the function.\n    \"\"\"\n    p = self.EEGCalcs.calcEEGPowerSpectrum()\n    ax = self._getPowerSpectrumPlot(p[0], p[1], p[2], p[3], p[4], **kwargs)\n    return ax\n</code></pre>"},{"location":"reference/#ephysiopy.visualise.plotting.FigureMaker.plot_raster","title":"<code>plot_raster(cluster, channel, **kwargs)</code>","text":"<p>Gets the raster plot for the specified cluster(s) and channel.</p> <p>Args:     cluster (int | list): The cluster(s) to get the raster plot for.     channel (int): The channel number.     **kwargs: Additional keyword arguments for the function.</p> Source code in <code>ephysiopy/visualise/plotting.py</code> <pre><code>@savePlot\ndef plot_raster(self, cluster: int, channel: int, **kwargs) -&gt; plt.Axes:\n    \"\"\"\n    Gets the raster plot for the specified cluster(s) and channel.\n\n    Args:\n        cluster (int | list): The cluster(s) to get the raster plot for.\n        channel (int): The channel number.\n        **kwargs: Additional keyword arguments for the function.\n    \"\"\"\n    ts = self.get_spike_times(cluster, channel)\n    ax = self._getRasterPlot(ts, cluster=cluster, **kwargs)\n    return ax\n</code></pre>"},{"location":"reference/#ephysiopy.visualise.plotting.FigureMaker.plot_rate_map","title":"<code>plot_rate_map(cluster, channel, **kwargs)</code>","text":"<p>Plots the rate map for the specified cluster(s) and channel.</p> <p>Args:     cluster (int): The cluster(s) to get the rate map for.     channel (int): The channel number.     **kwargs: Additional keyword arguments for the function.</p> Source code in <code>ephysiopy/visualise/plotting.py</code> <pre><code>@savePlot\n@stripAxes\ndef plot_rate_map(self, cluster: int, channel: int, **kwargs) -&gt; plt.Axes:\n    \"\"\"\n    Plots the rate map for the specified cluster(s) and channel.\n\n    Args:\n        cluster (int): The cluster(s) to get the rate map for.\n        channel (int): The channel number.\n        **kwargs: Additional keyword arguments for the function.\n\n    \"\"\"\n    rmap = self.get_rate_map(cluster, channel, **kwargs)\n    vmax = np.nanmax(np.ravel(rmap.binned_data[0]))\n    ax = kwargs.pop(\"ax\", None)\n    if ax is None:\n        fig = plt.figure()\n        ax = fig.add_subplot(111)\n    kwargs = clean_kwargs(plt.pcolormesh, kwargs)\n    # TODO: doesn't deal with multiple clusters being binned\n    ax.pcolormesh(\n        rmap.bin_edges[1],\n        rmap.bin_edges[0],\n        rmap.binned_data[0],\n        cmap=jet_cmap,\n        edgecolors=\"face\",\n        vmax=vmax,\n        shading=\"auto\",\n        **kwargs,\n    )\n    ax.set_aspect(\"equal\")\n    return ax\n</code></pre>"},{"location":"reference/#ephysiopy.visualise.plotting.FigureMaker.plot_sac","title":"<code>plot_sac(cluster, channel, **kwargs)</code>","text":"<p>Gets the spatial autocorrelation for the specified cluster(s) and channel.</p> <p>Args:     cluster (int): The cluster(s) to get the spatial         autocorrelation for.     channel (int): The channel number.     **kwargs: Additional keyword arguments for the function.</p> Source code in <code>ephysiopy/visualise/plotting.py</code> <pre><code>@savePlot\n@stripAxes\ndef plot_sac(self, cluster: int, channel: int, **kwargs) -&gt; plt.Axes:\n    \"\"\"\n    Gets the spatial autocorrelation for the specified cluster(s) and\n    channel.\n\n    Args:\n        cluster (int): The cluster(s) to get the spatial\n            autocorrelation for.\n        channel (int): The channel number.\n        **kwargs: Additional keyword arguments for the function.\n    \"\"\"\n    sac = self.get_grid_map(cluster, channel)\n    measures = fc.grid_field_props(sac)\n    ax = kwargs.pop(\"ax\", None)\n    if ax is None:\n        fig = plt.figure()\n        ax = fig.add_subplot(111)\n    Am = sac.binned_data[0].copy()\n    Am[~measures[\"dist_to_centre\"]] = np.nan\n    Am = np.ma.masked_invalid(np.atleast_2d(Am))\n    x, y = np.meshgrid(sac.bin_edges[1].data, sac.bin_edges[0].data)\n    vmax = np.nanmax(np.ravel(sac.binned_data[0]))\n    ax.pcolormesh(\n        sac.bin_edges[1].data,\n        sac.bin_edges[0].data,\n        sac.binned_data[0],\n        cmap=grey_cmap,\n        edgecolors=\"face\",\n        vmax=vmax,\n        shading=\"auto\",\n    )\n    import copy\n\n    cmap = copy.copy(jet_cmap)\n    cmap.set_bad(\"w\", 0)\n    ax.pcolormesh(\n        sac.bin_edges[1].data,\n        sac.bin_edges[0].data,\n        Am,\n        cmap=cmap,\n        edgecolors=\"face\",\n        vmax=vmax,\n        shading=\"auto\",\n    )\n    # horizontal green line at 3 o'clock\n    _y = 0, 0\n    _x = 0, sac.bin_edges[0][-1]\n    ax.plot(_x, _y, c=\"g\")\n    mag = measures[\"scale\"] * 0.75\n    th = np.linspace(0, measures[\"orientation\"], 50)\n    from ephysiopy.common.utils import rect\n\n    [x, y] = rect(mag, th, deg=1)\n    # angle subtended by orientation\n    ax.plot(x, -y, c=\"r\", **kwargs)\n    # plot lines from centre to peaks above middle\n    for p in measures[\"closest_peak_coords\"]:\n        if p[0] &lt;= measures[\"dist_to_centre\"].shape[0] / 2:\n            ax.plot((0, p[1]), (0, p[0]), \"k\", **kwargs)\n    ax.invert_yaxis()\n    all_ax = ax.axes\n    all_ax.set_aspect(\"equal\")\n\n    return ax\n</code></pre>"},{"location":"reference/#ephysiopy.visualise.plotting.FigureMaker.plot_speed_v_hd","title":"<code>plot_speed_v_hd(cluster, channel, **kwargs)</code>","text":"<p>Gets the speed versus head direction plot for the specified cluster(s) and channel.</p> <p>Args:     cluster (int): The cluster(s) to get the speed versus head         direction plot for.     channel (int): The channel number.     **kwargs: Additional keyword arguments for the function.</p> Source code in <code>ephysiopy/visualise/plotting.py</code> <pre><code>@savePlot\ndef plot_speed_v_hd(self, cluster: int, channel: int, **kwargs) -&gt; plt.Axes:\n    \"\"\"\n    Gets the speed versus head direction plot for the specified cluster(s)\n    and channel.\n\n    Args:\n        cluster (int): The cluster(s) to get the speed versus head\n            direction plot for.\n        channel (int): The channel number.\n        **kwargs: Additional keyword arguments for the function.\n    \"\"\"\n    rmap = self.get_speed_v_hd_map(cluster, channel, **kwargs)\n    im = np.ma.MaskedArray(rmap.binned_data[0], np.isnan(rmap.binned_data[0]))\n    # mask low rates...\n    # im = np.ma.masked_where(im &lt;= 1, im)\n    # ... and where less than 0.5% of data is accounted for\n    y, x = np.meshgrid(rmap.bin_edges[0], rmap.bin_edges[1], indexing=\"ij\")\n    vmax = np.nanmax(np.ravel(im))\n    ax = kwargs.pop(\"ax\", None)\n    if ax is None:\n        fig = plt.figure()\n        ax = fig.add_subplot(111)\n    ax.pcolormesh(\n        x, y, im, cmap=jet_cmap, edgecolors=\"face\", vmax=vmax, shading=\"auto\"\n    )\n    ax.set_xticks(\n        [90, 180, 270], labels=[\"90\", \"180\", \"270\"], fontweight=\"normal\", size=6\n    )\n    ax.set_yticks(\n        [10, 20, 30, 40],\n        labels=[\"10\", \"20\", \"30\", \"40\"],\n        fontweight=\"normal\",\n        size=6,\n    )\n    ax.set_xlabel(\"Heading\", fontweight=\"normal\", size=6)\n    return ax\n</code></pre>"},{"location":"reference/#ephysiopy.visualise.plotting.FigureMaker.plot_speed_v_rate","title":"<code>plot_speed_v_rate(cluster, channel, **kwargs)</code>","text":"<p>Gets the speed versus rate plot for the specified cluster(s) and channel.</p> <p>By default the distribution of speeds will be plotted as a twin axis. To disable set add_speed_hist = False</p> <p>Args:     cluster (int): The cluster(s) to get the speed versus rate         plot for.     channel (int): The channel number.     **kwargs: Additional keyword arguments for the function.</p> Source code in <code>ephysiopy/visualise/plotting.py</code> <pre><code>@savePlot\ndef plot_speed_v_rate(self, cluster: int, channel: int, **kwargs) -&gt; plt.Axes:\n    \"\"\"\n    Gets the speed versus rate plot for the specified cluster(s) and\n    channel.\n\n    By default the distribution of speeds will be plotted as a twin\n    axis. To disable set add_speed_hist = False\n\n    Args:\n        cluster (int): The cluster(s) to get the speed versus rate\n            plot for.\n        channel (int): The channel number.\n        **kwargs: Additional keyword arguments for the function.\n    \"\"\"\n    add_speed_hist = kwargs.pop(\"add_speed_hist\", True)\n    rmap = self.get_speed_v_rate_map(cluster, channel, **kwargs)\n    # rmap is linear\n    ax = kwargs.pop(\"ax\", None)\n    if ax is None:\n        fig = plt.figure()\n        ax = fig.add_subplot(111)\n    kwargs = clean_kwargs(plt.plot, kwargs)\n    ax_colour = \"cornflowerblue\"\n    ax.plot(rmap.bin_edges[0][:-1], rmap.binned_data[0], color=ax_colour, **kwargs)\n    ax.set_xlabel(\"Speed (cm/s)\")\n    ax.set_ylabel(\"Rate (Hz)\")\n    if add_speed_hist:\n        ax.spines[\"left\"].set_color(ax_colour)\n        ax.tick_params(axis=\"y\", colors=ax_colour)\n        ax.yaxis.label.set_color(ax_colour)\n        ax2 = ax.twinx()\n        ax2_colour = \"grey\"\n        pos_weights = np.ones_like(self.PosCalcs.speed) * (\n            1 / self.PosCalcs.sample_rate\n        )\n        speed_bincounts = np.bincount(\n            np.digitize(self.PosCalcs.speed, rmap.bin_edges[0], right=True),\n            weights=pos_weights,\n        )\n        ax2.bar(\n            rmap.bin_edges[0],\n            speed_bincounts,\n            alpha=0.5,\n            width=np.mean(np.diff(rmap.bin_edges[0])),\n            ec=\"grey\",\n            fc=\"grey\",\n        )\n        ax2.set_ylabel(\"Duration (s)\")\n        ax2.spines[\"right\"].set_color(ax2_colour)\n        ax2.tick_params(axis=\"y\", colors=ax2_colour)\n        ax2.yaxis.label.set_color(ax2_colour)\n\n    return ax\n</code></pre>"},{"location":"reference/#ephysiopy.visualise.plotting.FigureMaker.plot_spike_path","title":"<code>plot_spike_path(cluster=None, channel=None, **kwargs)</code>","text":"<p>Gets the spike path for the specified cluster(s) and channel.</p> <p>Args:     cluster (int | None): The cluster(s) to get the spike path         for.     channel (int | None): The channel number.     **kwargs: Additional keyword arguments for the function.</p> Source code in <code>ephysiopy/visualise/plotting.py</code> <pre><code>@savePlot\n@stripAxes\ndef plot_spike_path(self, cluster=None, channel=None, **kwargs) -&gt; plt.Axes:\n    \"\"\"\n    Gets the spike path for the specified cluster(s) and channel.\n\n    Args:\n        cluster (int | None): The cluster(s) to get the spike path\n            for.\n        channel (int | None): The channel number.\n        **kwargs: Additional keyword arguments for the function.\n    \"\"\"\n    if not self.RateMap:\n        self.initialise()\n    col = kwargs.pop(\"c\", tcols.colours[1])\n    ax = kwargs.pop(\"ax\", None)\n    marker = kwargs.pop(\"marker\", \"s\")\n    if ax is None:\n        fig = plt.figure()\n        ax = fig.add_subplot(111)\n    ax.plot(\n        self.PosCalcs.xy[0, :],\n        self.PosCalcs.xy[1, :],\n        color=tcols.colours[0],\n        zorder=1,\n    )\n    ax.set_aspect(\"equal\")\n    if cluster is not None:\n        idx = self._get_spike_pos_idx(cluster, channel)\n        ax.scatter(\n            self.PosCalcs.xy[0, idx],\n            self.PosCalcs.xy[1, idx],\n            marker=marker,\n            color=col,\n            **kwargs,\n        )\n    return ax\n</code></pre>"},{"location":"reference/#ephysiopy.visualise.plotting.FigureMaker.plot_xcorr","title":"<code>plot_xcorr(cluster_a, channel_a, cluster_b, channel_b, **kwargs)</code>","text":"<p>Plots the temporal cross-correlogram between cluster_a and cluster_b</p> <p>Parameters:</p> Name Type Description Default <code>cluster_a</code> <code>int</code> required <code>channel_a</code> <code>int</code> required <code>cluster_b</code> <code>int</code> required <code>channel_b</code> <code>int</code> required <p>Returns:</p> Type Description <code>plt.Axes : matplotlib.Axes instance</code> Source code in <code>ephysiopy/visualise/plotting.py</code> <pre><code>@savePlot\ndef plot_xcorr(\n    self, cluster_a: int, channel_a: int, cluster_b: int, channel_b: int, **kwargs\n) -&gt; plt.Axes:\n    \"\"\"\n    Plots the temporal cross-correlogram between cluster_a and cluster_b\n\n    Parameters\n    ----------\n    cluster_a (int) : first cluster\n    channel_a (int) : first channel\n    cluster_b (int) : second cluster\n    channel_b (int) : second channel\n\n    Returns\n    -------\n    plt.Axes : matplotlib.Axes instance\n    \"\"\"\n    if \"strip_axes\" in kwargs.keys():\n        strip_axes = kwargs.pop(\"strip_axes\")\n    else:\n        strip_axes = False\n    ax = kwargs.get(\"ax\", None)\n    if ax is None:\n        fig = plt.figure()\n        ax = fig.add_subplot(111)\n    if \"binsize\" in kwargs.keys():\n        binsize = kwargs[\"binsize\"]\n    else:\n        binsize = 0.001\n    if \"Trange\" in kwargs.keys():\n        xrange = kwargs.pop(\"Trange\")\n    else:\n        xrange = [-0.5, 0.5]\n\n    a_times = self.get_spike_times(cluster_a, channel_a)\n    b_times = self.get_spike_times(cluster_b, channel_b)\n    xcorr_binned = xcorr(a_times, b_times, Trange=xrange, binsize=binsize)\n    c = xcorr_binned.binned_data[0]\n    b = xcorr_binned.bin_edges[0]\n    ax.bar(b[:-1], c, width=binsize, color=\"k\", align=\"edge\", zorder=3)\n    ax.set_xlim(xrange)\n    ax.set_xticks((xrange[0], 0, xrange[1]))\n    ax.set_xticklabels(\"\")\n    ax.tick_params(\n        axis=\"both\", which=\"both\", left=False, right=False, bottom=False, top=False\n    )\n    ax.set_yticklabels(\"\")\n    ax.xaxis.set_ticks_position(\"bottom\")\n    if strip_axes:\n        return stripAxes(ax)\n    axtrans = transforms.blended_transform_factory(ax.transData, ax.transAxes)\n    ax.vlines(0, ymin=0, ymax=1, colors=\"lightgrey\", transform=axtrans, zorder=1)\n    return ax\n</code></pre>"},{"location":"reference/#binning-up-data","title":"Binning up data","text":""},{"location":"reference/#ephysiopy.common.binning.RateMap","title":"<code>RateMap</code>","text":"<p>               Bases: <code>object</code></p> <p>Bins up positional data (xy, head direction etc) and produces rate maps of the relevant kind. This is a generic class meant to be independent of any particular recording format.</p> <p>Args:     xy (ndarray): The xy data as a 2 x n_samples numpy array.     hdir (ndarray): The head direction data a 1 x n_samples numpy array.     speed (ndarray): Similar to hdir.     pos_weights (ndarray): A 1 x n_samples numpy array used to weight a particular         position samples when binning data. For example, if there were 5         positions recorded and a cell spiked once in position 2 and 5 times         in position 3 and nothing anywhere else then pos_weights looks like:         [0 0 1 5 0]         In the case of binning up position this will be an array of mostly 1's         unless there are some positions you want excluded.     ppm (int, optional): Pixels per metre. Specifies how many camera pixels per metre so this,         in combination with cmsPerBin, will determine how many bins there are         in the rate map. Defaults to None.     xyInCms (bool, optional): Whether the positional data is in cms. Defaults to False.     cmsPerBin (int, optional): How many cms on a side each bin is in a rate map OR the number of         degrees per bin in the case of directional binning. Defaults to 3.     smooth_sz (int, optional): The width of the smoothing kernel for smoothing rate maps. Defaults to 5.</p> <p>Notes:     There are several instance variables you can set, see below.</p> Source code in <code>ephysiopy/common/binning.py</code> <pre><code>class RateMap(object):\n    \"\"\"\n    Bins up positional data (xy, head direction etc) and produces rate maps\n    of the relevant kind. This is a generic class meant to be independent of\n    any particular recording format.\n\n    Args:\n        xy (ndarray): The xy data as a 2 x n_samples numpy array.\n        hdir (ndarray): The head direction data a 1 x n_samples numpy array.\n        speed (ndarray): Similar to hdir.\n        pos_weights (ndarray): A 1 x n_samples numpy array used to weight a particular\n            position samples when binning data. For example, if there were 5\n            positions recorded and a cell spiked once in position 2 and 5 times\n            in position 3 and nothing anywhere else then pos_weights looks like:\n            [0 0 1 5 0]\n            In the case of binning up position this will be an array of mostly 1's\n            unless there are some positions you want excluded.\n        ppm (int, optional): Pixels per metre. Specifies how many camera pixels per metre so this,\n            in combination with cmsPerBin, will determine how many bins there are\n            in the rate map. Defaults to None.\n        xyInCms (bool, optional): Whether the positional data is in cms. Defaults to False.\n        cmsPerBin (int, optional): How many cms on a side each bin is in a rate map OR the number of\n            degrees per bin in the case of directional binning. Defaults to 3.\n        smooth_sz (int, optional): The width of the smoothing kernel for smoothing rate maps. Defaults to 5.\n\n    Notes:\n        There are several instance variables you can set, see below.\n    \"\"\"\n\n    def __init__(\n        self,\n        PosCalcs: PosCalcsGeneric,\n        pos_weights: np.ma.MaskedArray | None = None,\n        xyInCms: bool = False,\n        binsize: int = 3,\n        smooth_sz: int = 5,\n    ):\n        self.PosCalcs = PosCalcs\n        self._pos_weights = pos_weights\n        self._pos_time_splits = None\n        self._spike_weights = None\n        self._binsize = binsize\n        self._binsize2d = None\n        self._inCms = xyInCms\n        self._binedges = None  # has setter and getter - see below\n        self._x_lims = None\n        self._y_lims = None\n        self._smooth_sz = smooth_sz\n        self._smoothingType = \"gaussian\"  # 'boxcar' or 'gaussian'\n        self.whenToSmooth = \"before\"  # or 'after'\n        self._var2Bin = VariableToBin.XY\n        self._mapType = MapType.RATE\n        self._calc_bin_edges()\n\n    @property\n    def xy(self):\n        return self.PosCalcs.xy\n\n    @property\n    def dir(self):\n        return self.PosCalcs.dir\n\n    @property\n    def speed(self):\n        return self.PosCalcs.speed\n\n    @property\n    def pos_times(self):\n        return self.PosCalcs.xyTS\n\n    @property\n    def inCms(self):\n        # Whether the units are in cms or not\n        return self._inCms\n\n    @inCms.setter\n    def inCms(self, value):\n        self._inCms = value\n        # will trigger a recalculation of position vars\n        self.PosCalcs.convert2cm = value\n\n    @property\n    def ppm(self):\n        # Get the current pixels per metre (ppm)\n        return self.PosCalcs.ppm\n\n    @ppm.setter\n    def ppm(self, value):\n        # will trigger a recalculation of position vars\n        self._ppm = self.PosCalcs.ppm = value\n\n    @property\n    def var2Bin(self):\n        return self._var2Bin\n\n    @var2Bin.setter\n    def var2Bin(self, value):\n        self._var2Bin = value\n\n    @property\n    def mapType(self):\n        return self._mapType\n\n    @mapType.setter\n    def mapType(self, value):\n        self._mapType = value\n\n    @property\n    def binedges(self):\n        return self._binedges\n\n    @binedges.setter\n    def binedges(self, value):\n        self._binedges = value\n\n    @property\n    def x_lims(self):\n        return self._x_lims\n\n    @x_lims.setter\n    def x_lims(self, value):\n        self._x_lims = value\n\n    @property\n    def y_lims(self):\n        return self._y_lims\n\n    @y_lims.setter\n    def y_lims(self, value):\n        self._y_lims = value\n\n    @property\n    def pos_weights(self):\n        \"\"\"\n        The 'weights' used as an argument to np.histogram* for binning up\n        position\n        Mostly this is just an array of 1's equal to the length of the pos\n        data, but usefully can be adjusted when masking data in the trial\n        by\n        \"\"\"\n        if self._pos_weights is None:\n            self._pos_weights = np.ma.MaskedArray(np.ones(self.PosCalcs.npos))\n        return self._pos_weights\n\n    @pos_weights.setter\n    def pos_weights(self, value):\n        self._pos_weights = value\n\n    @property\n    def spike_weights(self):\n        return self._spike_weights\n\n    @spike_weights.setter\n    def spike_weights(self, value):\n        self._spike_weights = value\n\n    @property\n    def binsize(self):\n        # The number of cms per bin of the binned up map\n        return self._binsize\n\n    @binsize.setter\n    def binsize(self, value):\n        self._binsize = value\n        self._binedges = self._calc_bin_edges(value)\n\n    @property\n    def smooth_sz(self):\n        # The size of the smoothing window applied to the binned data\n        return self._smooth_sz\n\n    @smooth_sz.setter\n    def smooth_sz(self, value):\n        self._smooth_sz = value\n\n    @property\n    def smoothingType(self):\n        # The type of smoothing to do - legal values are 'boxcar' or 'gaussian'\n        return self._smoothingType\n\n    @smoothingType.setter\n    def smoothingType(self, value):\n        self._smoothingType = value\n\n    def apply_mask(self, mask):\n        # self.PosCalcs.apply_mask(mask)\n        self.pos_weights.mask = mask.data\n\n    def _getXYLimits(self):\n        \"\"\"\n        Gets the min/max of the x/y data\n        \"\"\"\n        x_lims = getattr(self, \"x_lims\", None)\n        y_lims = getattr(self, \"y_lims\", None)\n        if x_lims is None:\n            x_lims = (np.nanmin(self.xy[0]), np.nanmax(self.xy[0]))\n        if y_lims is None:\n            y_lims = (np.nanmin(self.xy[1]), np.nanmax(self.xy[1]))\n        self.x_lims = x_lims\n        self.y_lims = y_lims\n        return x_lims, y_lims\n\n    def _calc_bin_dims(self):\n        try:\n            self._binDims = [len(b) for b in self._binedges]\n        except TypeError:\n            self._binDims = len(self._binedges)\n\n    def _calc_bin_edges(self, binsize: int | tuple = 3) -&gt; tuple[np.ndarray, ...]:\n        \"\"\"\n        Aims to get the right number of bins for the variable to be binned\n\n        Args:\n            binsize (int | tuple, optional): The number of cms per bin for XY OR degrees for DIR OR cm/s for SPEED. Defaults to 3.\n\n        Returns:\n            tuple: each member an array of bin edges\n        \"\"\"\n        if self.var2Bin.value == VariableToBin.DIR.value:\n            self.binedges = np.linspace(0, 360, int(360 / binsize)).tolist()\n        elif self.var2Bin.value == VariableToBin.SPEED.value:\n            maxspeed = np.nanmax(self.speed)\n            # assume min speed = 0\n            self.binedges = np.linspace(0, maxspeed, int(maxspeed / binsize)).tolist()\n        elif self.var2Bin.value == VariableToBin.XY.value:\n            x_lims, y_lims = self._getXYLimits()\n            nxbins = int(np.ceil((x_lims[1] - x_lims[0]) / binsize))\n            nybins = int(np.ceil((y_lims[1] - y_lims[0]) / binsize))\n            _x = np.linspace(x_lims[0], x_lims[1], nxbins)\n            _y = np.linspace(y_lims[0], y_lims[1], nybins)\n            self.binedges = _y, _x\n        elif self.var2Bin.value == VariableToBin.XY_TIME.value:\n            if self._pos_time_splits is None:\n                raise ValueError(\"Need pos times to bin up XY_TIME\")\n            x_lims, y_lims = self._getXYLimits()\n            nxbins = int(np.ceil((x_lims[1] - x_lims[0]) / binsize))\n            nybins = int(np.ceil((y_lims[1] - y_lims[0]) / binsize))\n            _x = np.linspace(x_lims[0], x_lims[1], nxbins)\n            _y = np.linspace(y_lims[0], y_lims[1], nybins)\n            self.binedges = _y, _x, self.pos_time_splits\n        elif self.var2Bin.value == VariableToBin.SPEED_DIR.value:\n            maxspeed = np.nanmax(self.speed)\n            if isinstance(binsize, int):\n                self.binedges = (\n                    np.linspace(0, maxspeed, int(maxspeed / binsize)),\n                    np.linspace(0, 360, int(360 / binsize)),\n                )\n            elif isinstance(binsize, tuple):\n                self.binedges = (\n                    np.linspace(0, maxspeed, int(maxspeed / binsize[0])),\n                    np.linspace(0, 360, int(360 / binsize[1])),\n                )\n        elif self.var2Bin.value == VariableToBin.EGO_BOUNDARY.value:\n            if isinstance(binsize, (float, int)):\n                self.binedges = np.linspace(0, 50, 20), np.linspace(0, 2 * np.pi, 120)\n            elif isinstance(binsize, tuple):\n                self.binedges = np.linspace(0, 50, int(50 / binsize[0])), np.linspace(\n                    0, 2 * np.pi, int((2 * np.pi) / binsize[1])\n                )\n        self._calc_bin_dims()\n        return self.binedges\n\n    def get_map(\n        self,\n        spk_weights,\n        var_type=VariableToBin.XY,\n        map_type=MapType.RATE,\n        smoothing=True,\n        **kwargs\n    ) -&gt; BinnedData | None:\n        \"\"\"\n        Bins up the variable type var_type and returns a tuple of\n        (rmap, binnedPositionDir) or\n        (rmap, binnedPostionX, binnedPositionY)\n\n        Parameters\n        ----------\n        spk_weights (np.ndarray) - Shape equal to number of positions samples captured and consists of\n            position weights. For example, if there were 5 positions\n            recorded and a cell spiked once in position 2 and 5 times in\n            position 3 and nothing anywhere else then pos_weights looks\n            like: [0 0 1 5 0].\n            spk_weights can also be list-like where each entry in the list is a different set of\n            weights - these are enumerated through in a list comp in the ._bin_data function. In\n            this case the returned tuple will consist of a 2-tuple where the first entry is an\n            array of the ratemaps (binned_spk / binned_pos) and the second part is the binned pos data (as it's common to all\n            the spike weights)\n\n        var_type (Variable2Bin) - The variable to bin. See ephysiopy.common.utils for legal values.\n\n\n        map_type (MapType) - The kind of map returned. See ephysiopy.common.utils for legal values.\n\n        smoothing (bool, optional): Smooth the data or not. Default True.\n\n        Returns\n        -------\n        binned_data (BinnedData): An instance of BinnedData containing the binned data, the bin edges, the variable binned and\n                                  the map type. See ephysiopy.common.utils for details of the class.\n        \"\"\"\n        boundary = \"extend\"\n        pos_weights = self.pos_weights\n        if var_type.value == VariableToBin.DIR.value:\n            sample = self.dir\n            boundary = \"wrap\"\n        elif var_type.value == VariableToBin.SPEED.value:\n            sample = self.speed\n        elif var_type.value == VariableToBin.XY.value:\n            sample = self.xy\n        elif var_type.value == VariableToBin.XY_TIME.value:\n            sample = np.concatenate(\n                (np.atleast_2d(self.xy), np.atleast_2d(self.pos_times))\n            )\n        elif var_type.value == VariableToBin.SPEED_DIR.value:\n            sample = np.concatenate(\n                (np.atleast_2d(self.dir), np.atleast_2d(self.speed))\n            )\n        elif var_type.value == VariableToBin.EGO_BOUNDARY.value:\n            arena_shape = kwargs.get(\"arena_shape\", \"circle\")\n            boundary = \"wrap\"\n            binsize = kwargs.get(\"binsize\", 5)\n            if isinstance(binsize, tuple):\n                binsize = binsize[0]\n            # breakpoint()\n            ego_angles, arena_xy = self._calc_ego_angles(arena_shape, binsize)\n            ego_dists = distance.cdist(arena_xy, self.xy.T, \"euclidean\")\n            sample = np.stack((np.ravel(ego_angles.T), np.ravel(ego_dists.T)))\n            spk_weights = np.atleast_2d(spk_weights)\n            spk_weights = np.tile(spk_weights, arena_xy.shape[0])\n            pos_weights = np.tile(self.pos_weights, arena_xy.shape[0])\n            hist_range = (0, 50), (0, 2 * np.pi)\n\n            if \"range\" not in kwargs.keys():\n                kwargs[\"range\"] = hist_range\n        else:\n            raise ValueError(\"Unrecognized variable to bin.\")\n\n        assert sample is not None\n\n        self.var2Bin = var_type\n        binsize = kwargs.pop(\"binsize\", self.binsize)\n        hist_range = kwargs.pop(\"range\", None)\n\n        if hist_range is None:\n            bin_edges = self._calc_bin_edges(binsize)\n        else:\n            bin_edges = None\n        binned_pos, binned_pos_edges = self._bin_data(sample, bin_edges, pos_weights)\n        binned_pos = binned_pos / self.PosCalcs.sample_rate\n        nanIdx = binned_pos == 0\n        pos = BinnedData(var_type, MapType.POS, [binned_pos], binned_pos_edges)\n\n        if map_type.value == MapType.POS.value:  # return binned up position\n            if smoothing:\n                sm_pos = blur_image(\n                    pos,\n                    self.smooth_sz,\n                    ftype=self.smoothingType,\n                    boundary=boundary,\n                    **kwargs\n                )\n                sm_pos.set_nan_indices(nanIdx)\n                return sm_pos\n            else:\n                pos.set_nan_indices(nanIdx)\n                return pos\n\n        binned_spk, _ = self._bin_data(sample, bin_edges, spk_weights)\n        if not isinstance(binned_spk, list):\n            binned_spk = [binned_spk]\n        spk = BinnedData(var_type, MapType.SPK, binned_spk, binned_pos_edges)\n\n        if map_type.value == MapType.SPK.value:\n            if smoothing:\n                return blur_image(\n                    spk,\n                    self.smooth_sz,\n                    ftype=self.smoothingType,\n                    boundary=boundary,\n                    **kwargs\n                )\n            else:\n                return spk\n        if map_type.value == MapType.ADAPTIVE.value:\n            alpha = kwargs.pop(\"alpha\", 4)\n            # deal with a stack of binned maps\n            if binned_spk.ndim == 3:\n                smthd_rate = []\n                for i in range(binned_spk.shape[0]):\n                    smthd_rate.append(\n                        self.getAdaptiveMap(binned_pos, binned_spk[i, ...], alpha)[0]\n                    )\n            else:\n                smthd_rate, _, _ = self.getAdaptiveMap(binned_pos, binned_spk, alpha)\n            return BinnedData(var_type, map_type, smthd_rate, binned_pos_edges)\n\n        if not smoothing:\n            rmap = spk / pos\n            rmap.map_type = MapType.RATE\n            rmap.set_nan_indices(nanIdx)\n            return rmap\n\n        if \"after\" in self.whenToSmooth:\n            rmap = spk / pos\n            rmap = blur_image(\n                rmap,\n                self.smooth_sz,\n                ftype=self.smoothingType,\n                boundary=boundary,\n                **kwargs\n            )\n        else:  # default case\n            sm_pos = blur_image(\n                pos,\n                self.smooth_sz,\n                ftype=self.smoothingType,\n                boundary=boundary,\n                **kwargs\n            )\n            sm_spk = blur_image(\n                spk,\n                self.smooth_sz,\n                ftype=self.smoothingType,\n                boundary=boundary,\n                **kwargs\n            )\n            rmap = sm_spk / sm_pos\n        rmap.set_nan_indices(nanIdx)\n        return rmap\n\n    def _bin_data(self, var, bin_edges, weights) -&gt; tuple[np.ndarray, ...]:\n        \"\"\"\n        Bins data taking account of possible multi-dimensionality\n\n        Args:\n            var (array_like): The variable to bin\n            bin_edges (array_like): The edges of the data - see numpys histogramdd for more\n            weights (array_like): The weights attributed to the samples in var\n\n        Returns:\n            ndhist (2-tuple): Returns a two-tuple of the binned variable and\n                the bin edges\n\n        Notes:\n            This breaks compatability with numpys histogramdd\n            In the 2d histogram case below I swap the axes around so that x and y\n            are binned in the 'normal' format i.e. so x appears horizontally and y\n            vertically.\n            Multi-binning issue is dealt with awkwardly through checking\n            the dimensionality of the weights array.\n            'normally' this would be 1 dim but when multiple clusters are being\n            binned it will be 2 dim.\n            In that case np.apply_along_axis functionality is applied.\n            The spike weights in that case might be created like so:\n\n            &gt;&gt;&gt; spk_W = np.zeros(shape=[len(trial.nClusters), trial.npos])\n            &gt;&gt;&gt; for i, cluster in enumerate(trial.clusters):\n            &gt;&gt;&gt;\t\tx1 = trial.getClusterIdx(cluster)\n            &gt;&gt;&gt;\t\tspk_W[i, :] = np.bincount(x1, minlength=trial.npos)\n\n            This can then be fed into this fcn something like so:\n\n            &gt;&gt;&gt; rng = np.array((np.ma.min(\n                trial.POS.xy, 1).data, np.ma.max(rial.POS.xy, 1).data))\n            &gt;&gt;&gt; h = _bin_data(\n                var=trial.POS.xy, bin_edges=np.array([64, 64]),\n                weights=spk_W, rng=rng)\n\n            Returned will be a tuple containing the binned up data and\n            the bin edges for x and y (obv this will be the same for all\n            entries of h)\n        \"\"\"\n        if weights is None:\n            weights = np.ma.MaskedArray(np.ones_like(var))\n        weights = np.ma.MaskedArray(weights)\n        dims = weights.ndim\n        if dims == 1 and var.ndim == 1:\n            var = var[np.newaxis, :]\n            if bin_edges is not None:\n                bin_edges = np.atleast_2d(bin_edges)\n        elif dims &gt; 1 and var.ndim == 1:\n            var = var[np.newaxis, :]\n            if bin_edges is not None:\n                bin_edges = np.atleast_2d(bin_edges)\n        else:\n            var = np.flipud(var)\n        weights = np.atleast_2d(weights)  # needed for list comp below\n        weights = np.ma.filled(weights, 0)\n        var = np.array(var.T)\n        if bin_edges is None:\n            bin_edges = self.binedges\n        if len(bin_edges) == 1:\n            hist = bh.Histogram(\n                bh.axis.Regular(len(bin_edges[0]), bin_edges[0][0], bin_edges[0][-1])\n            )\n        else:\n            hist = bh.Histogram(\n                bh.axis.Regular(len(bin_edges[0]), bin_edges[0][0], bin_edges[0][-1]),\n                bh.axis.Regular(len(bin_edges[1]), bin_edges[1][0], bin_edges[1][-1]),\n            )\n        ndhist = []\n        for w in weights:\n            hist.fill(*var.T, weight=w)\n            ndhist.append(\n                [hist.values().copy(), [np.ravel(e) for e in hist.axes.edges]]\n            )\n            hist.reset()\n        if np.shape(weights)[0] == 1:\n            return ndhist[0]\n        else:\n            tmp = [d[0] for d in ndhist]\n            return tmp, ndhist[1]\n\n    def getAdaptiveMap(self, pos_binned, spk_binned, alpha=4):\n        \"\"\"\n        Produces a ratemap that has been adaptively binned according to the\n        algorithm described in Skaggs et al., 1996) [1]_.\n\n        Parameters\n        ----------\n        pos_binned (array_like): The binned positional data. For example that returned from get_map\n            above with mapType as 'pos'\n        spk_binned (array_like): The binned spikes\n        alpha (int, optional): A scaling parameter determing the amount of occupancy to aim at\n            in each bin. Defaults to 4. In the original paper this was set to 200.\n            This is 4 here as the pos data is binned in seconds (the original data was in pos\n            samples so this is a factor of 50 smaller than the original paper's value, given 50Hz sample rate)\n\n        Returns\n        -------\n        Returns adaptively binned spike and pos maps. Use to generate Skaggs\n        information measure\n\n        Notes\n        -----\n        Positions with high rates mean proportionately less error than those\n        with low rates, so this tries to even the playing field. This type\n        of binning should be used for calculations of spatial info\n        as with the skaggs_info method in the fieldcalcs class (see below)\n        alpha is a scaling parameter that might need tweaking for different\n        data sets.\n\n        From the paper:\n            The data [are] first binned\n            into a 64 X 64 grid of spatial locations, and then the firing rate\n            at each point in this grid was calculated by expanding a circle\n            around the point until the following criterion was met:\n                Nspks &gt; alpha / (Nocc^2 * r^2)\n            where Nspks is the number of spikes emitted in a circle of radius\n            r (in bins), Nocc is the number of occupancy samples, alpha is the\n            scaling parameter\n            The firing rate in the given bin is then calculated as:\n                sample_rate * (Nspks / Nocc)\n\n        References\n        ----------\n        .. [1] W. E. Skaggs, B. L. McNaughton, K. M. Gothard &amp; E. J. Markus\n            \"An Information-Theoretic Approach to Deciphering the Hippocampal\n            Code\"\n            Neural Information Processing Systems, 1993.\n        \"\"\"\n        #  assign output arrays\n        smthdpos = np.zeros_like(pos_binned)\n        smthdspk = np.zeros_like(spk_binned)\n        smthdrate = np.zeros_like(pos_binned)\n        idx = pos_binned == 0\n        pos_binned[idx] = np.nan\n        spk_binned[idx] = np.nan\n        visited = np.zeros_like(pos_binned)\n        visited[pos_binned &gt; 0] = 1\n        # array to check which bins have made it\n        bincheck = np.isnan(pos_binned)\n        r = 1\n        while np.any(~bincheck):\n            # create the filter kernel\n            h = disk(r)\n            h[h &gt;= np.max(h) / 3.0] = 1\n            h[h != 1] = 0\n            if h.shape &gt;= pos_binned.shape:\n                break\n            # filter the arrays using astropys convolution\n            filtpos = convolution.convolve(pos_binned, h)\n            filtspk = convolution.convolve(spk_binned, h)\n            filtvisited = convolution.convolve(visited, h)\n            # get the bins which made it through this iteration\n            truebins = alpha / (np.sqrt(filtspk) * filtpos) &lt;= r\n            truebins = np.logical_and(truebins, ~bincheck)\n            # insert values where true\n            smthdpos[truebins] = filtpos[truebins] / filtvisited[truebins]\n            smthdspk[truebins] = filtspk[truebins] / filtvisited[truebins]\n            bincheck[truebins] = True\n            r += 1\n        smthdrate = smthdspk / smthdpos\n        smthdrate[idx] = np.nan\n        smthdspk[idx] = np.nan\n        smthdpos[idx] = np.nan\n        return smthdrate, smthdspk, smthdpos\n\n    def autoCorr2D(\n        self, A: BinnedData, nodwell: np.ndarray = None, tol: float = 1e-10\n    ) -&gt; BinnedData:\n        \"\"\"\n        Performs autocorrelations on all the maps in an instance of BinnedData.\n\n        Parameters\n        ----------\n        A (BinnedData) - instance of BinnedData\n        nodwell (np.ndarray) - array with NaNs where there was no position sampled.\n        tol (float) - values below this are set to 0.\n\n        Returns\n        -------\n        BinnedData - the data in A with the maps replaced by autocorrelograms\n        \"\"\"\n        result = BinnedData(A.variable, MapType.AUTO_CORR)\n        for rmap in A.binned_data:\n            rr = self._autoCorr2D(rmap, nodwell, tol)\n            result.binned_data += [rr]\n        xlen = len(A.bin_edges[0]) - 1\n        ylen = len(A.bin_edges[1]) - 1\n        result.bin_edges = [\n            np.arange(-xlen + 1, xlen + 1),\n            np.arange(-ylen + 1, ylen + 1),\n        ]\n        return result\n\n    def _autoCorr2D(\n        self, A: np.ndarray, nodwell: np.ndarray = None, tol: float = 1e-10\n    ):\n        \"\"\"\n        Performs a spatial autocorrelation on the array A\n\n        Parameters\n        ----------\n        A (array_like): Either 2 or 3D. In the former it is simply the binned up ratemap\n            where the two dimensions correspond to x and y.\n            If 3D then the first two dimensions are x\n            and y and the third (last dimension) is 'stack' of ratemaps\n        nodwell (array_like): A boolean array corresponding the bins in the ratemap that\n            weren't visited. See Notes below.\n        tol (float, optional): Values below this are set to zero to deal with v small values\n            thrown up by the fft. Default 1e-10\n\n        Returns\n        -------\n        sac (array_like): The spatial autocorrelation in the relevant dimensionality\n\n        Notes\n        -----\n        The nodwell input can usually be generated by:\n\n        &gt;&gt;&gt; nodwell = ~np.isfinite(A)\n        \"\"\"\n        assert np.ndim(A) == 2\n        m, n = np.shape(A)\n        o = 1\n        x = np.reshape(A, (m, n, o))\n        if nodwell is None:\n            nodwell = ~np.isfinite(A)\n        nodwell = np.reshape(nodwell, (m, n, o))\n        x[nodwell] = 0\n        # [Step 1] Obtain FFTs of x, the sum of squares and bins visited\n        Fx = np.fft.fft(np.fft.fft(x, 2 * m - 1, axis=0), 2 * n - 1, axis=1)\n        FsumOfSquares_x = np.fft.fft(\n            np.fft.fft(np.power(x, 2), 2 * m - 1, axis=0), 2 * n - 1, axis=1\n        )\n        Fn = np.fft.fft(\n            np.fft.fft(np.invert(nodwell).astype(int), 2 * m - 1, axis=0),\n            2 * n - 1,\n            axis=1,\n        )\n        # [Step 2] Multiply the relevant transforms and invert to obtain the\n        # equivalent convolutions\n        rawCorr = np.fft.fftshift(\n            np.real(np.fft.ifft(np.fft.ifft(Fx * np.conj(Fx), axis=1), axis=0)),\n            axes=(0, 1),\n        )\n        sums_x = np.fft.fftshift(\n            np.real(np.fft.ifft(np.fft.ifft(np.conj(Fx) * Fn, axis=1), axis=0)),\n            axes=(0, 1),\n        )\n        sumOfSquares_x = np.fft.fftshift(\n            np.real(\n                np.fft.ifft(np.fft.ifft(Fn * np.conj(FsumOfSquares_x), axis=1), axis=0)\n            ),\n            axes=(0, 1),\n        )\n        N = np.fft.fftshift(\n            np.real(np.fft.ifft(np.fft.ifft(Fn * np.conj(Fn), axis=1), axis=0)),\n            axes=(0, 1),\n        )\n        # [Step 3] Account for rounding errors.\n        rawCorr[np.abs(rawCorr) &lt; tol] = 0\n        sums_x[np.abs(sums_x) &lt; tol] = 0\n        sumOfSquares_x[np.abs(sumOfSquares_x) &lt; tol] = 0\n        N = np.round(N)\n        N[N &lt;= 1] = np.nan\n        # [Step 4] Compute correlation matrix\n        mapStd = np.sqrt((sumOfSquares_x * N) - sums_x**2)\n        mapCovar = (rawCorr * N) - sums_x * sums_x[::-1, :, :][:, ::-1, :][:, :, :]\n\n        return np.squeeze(mapCovar / mapStd / mapStd[::-1, :, :][:, ::-1, :][:, :, :])\n\n    def crossCorr2D(\n        self,\n        A: BinnedData,\n        B: BinnedData,\n        A_nodwell: np.ndarray = None,\n        B_nodwell: np.ndarray = None,\n        tol: float = 1e-10,\n    ) -&gt; BinnedData:\n        \"\"\"\n        Performs crosscorrelations between the maps in two instances of BinnedData, A and B.\n\n        Parameters\n        ----------\n        A, B (BinnedData) - instance of BinnedData\n        A_nodwell, B_nodwell (np.ndarray) - array with NaNs where there was no position sampled.\n        tol (float) - values below this are set to 0.\n\n        Returns\n        -------\n        BinnedData - the data in A with the maps replaced by autocorrelograms\n        \"\"\"\n        result = BinnedData(A.variable, MapType.CROSS_CORR)\n        for rmap in A:\n            for rmap2 in B:\n                rr = self._crossCorr2D(rmap, rmap2, A_nodwell, B_nodwell, tol)\n                result.binned_data += [rr]\n        xlen = max(len(A.bin_edges[0]) - 1, len(B.bin_edges[0]) - 1)\n        ylen = max(len(A.bin_edges[1]) - 1, len(B.bin_edges[1]) - 1)\n        result.bin_edges = [\n            np.arange(-xlen + 1, xlen + 1),\n            np.arange(-ylen + 1, ylen + 1),\n        ]\n        return result\n\n    def _crossCorr2D(\n        self,\n        A: BinnedData,\n        B: BinnedData,\n        A_nodwell: np.ndarray = None,\n        B_nodwell: np.ndarray = None,\n        tol: float = 1e-10,\n    ):\n        \"\"\"\n        Performs crosscorrelations between the maps in two instances of BinnedData, A and B.\n\n        Parameters\n        ----------\n        A, B (BinnedData) - instance of BinnedData\n        A_nodwell, B_nodwell (np.ndarray) - array with NaNs where there was no position sampled.\n        tol (float) - values below this are set to 0.\n\n        Returns\n        -------\n        BinnedData - the data in A with the maps replaced by autocorrelograms\n        \"\"\"\n        if isinstance(A, BinnedData):\n            A = A.binned_data[0]\n        if isinstance(B, BinnedData):\n            B = B.binned_data[0]\n        if np.ndim(A) != np.ndim(B):\n            raise ValueError(\"Both arrays must have the same dimensionality\")\n        assert np.ndim(A) == 2\n        ma, na = np.shape(A)\n        mb, nb = np.shape(B)\n        oa = ob = 1\n        A = np.reshape(A, (ma, na, oa))\n        B = np.reshape(B, (mb, nb, ob))\n        if A_nodwell is None:\n            A_nodwell = ~np.isfinite(A)\n        if B_nodwell is None:\n            B_nodwell = ~np.isfinite(B)\n        A_nodwell = np.reshape(A_nodwell, (ma, na, oa))\n        B_nodwell = np.reshape(B_nodwell, (mb, nb, ob))\n        A[A_nodwell] = 0\n        B[B_nodwell] = 0\n        # [Step 1] Obtain FFTs of x, the sum of squares and bins visited\n        Fa = np.fft.fft(np.fft.fft(A, 2 * mb - 1, axis=0), 2 * nb - 1, axis=1)\n        FsumOfSquares_a = np.fft.fft(\n            np.fft.fft(np.power(A, 2), 2 * mb - 1, axis=0), 2 * nb - 1, axis=1\n        )\n        Fn_a = np.fft.fft(\n            np.fft.fft(np.invert(A_nodwell).astype(int), 2 * mb - 1, axis=0),\n            2 * nb - 1,\n            axis=1,\n        )\n        Fb = np.fft.fft(np.fft.fft(B, 2 * ma - 1, axis=0), 2 * na - 1, axis=1)\n        FsumOfSquares_b = np.fft.fft(\n            np.fft.fft(np.power(B, 2), 2 * ma - 1, axis=0), 2 * na - 1, axis=1\n        )\n        Fn_b = np.fft.fft(\n            np.fft.fft(np.invert(B_nodwell).astype(int), 2 * ma - 1, axis=0),\n            2 * na - 1,\n            axis=1,\n        )\n        # [Step 2] Multiply the relevant transforms and invert to obtain the\n        # equivalent convolutions\n        rawCorr = np.fft.fftshift(\n            np.real(np.fft.ifft(np.fft.ifft(Fa * np.conj(Fb), axis=1), axis=0))\n        )\n        sums_a = np.fft.fftshift(\n            np.real(np.fft.ifft(np.fft.ifft(Fa * np.conj(Fn_b), axis=1), axis=0))\n        )\n        sums_b = np.fft.fftshift(\n            np.real(np.fft.ifft(np.fft.ifft(Fn_a * np.conj(Fb), axis=1), axis=0))\n        )\n        sumOfSquares_a = np.fft.fftshift(\n            np.real(\n                np.fft.ifft(\n                    np.fft.ifft(FsumOfSquares_a * np.conj(Fn_b), axis=1), axis=0\n                )\n            )\n        )\n        sumOfSquares_b = np.fft.fftshift(\n            np.real(\n                np.fft.ifft(\n                    np.fft.ifft(Fn_a * np.conj(FsumOfSquares_b), axis=1), axis=0\n                )\n            )\n        )\n        N = np.fft.fftshift(\n            np.real(np.fft.ifft(np.fft.ifft(Fn_a * np.conj(Fn_b), axis=1), axis=0))\n        )\n        # [Step 3] Account for rounding errors.\n        rawCorr[np.abs(rawCorr) &lt; tol] = 0\n        sums_a[np.abs(sums_a) &lt; tol] = 0\n        sums_b[np.abs(sums_b) &lt; tol] = 0\n        sumOfSquares_a[np.abs(sumOfSquares_a) &lt; tol] = 0\n        sumOfSquares_b[np.abs(sumOfSquares_b) &lt; tol] = 0\n        N = np.round(N)\n        N[N &lt;= 1] = np.nan\n        # [Step 4] Compute correlation matrix\n        mapStd_a = np.sqrt((sumOfSquares_a * N) - sums_a**2)\n        mapStd_b = np.sqrt((sumOfSquares_b * N) - sums_b**2)\n        mapCovar = (rawCorr * N) - sums_a * sums_b\n\n        return np.squeeze(mapCovar / (mapStd_a * mapStd_b))\n\n    def tWinSAC(\n        self,\n        xy,\n        spkIdx,\n        ppm=365,\n        winSize=10,\n        pos_sample_rate=50,\n        nbins=71,\n        boxcar=5,\n        Pthresh=100,\n        downsampfreq=50,\n    ) -&gt; BinnedData:\n        \"\"\"\n        Performs a temporal windowed spatial autocorrelation.\n\n        Parameters\n        ----------\n        xy (array_like): The position data\n        spkIdx (array_like): The indices in xy where the cell fired\n        ppm (int, optional): The camera pixels per metre. Default 365\n        winSize (int, optional): The window size for the temporal search\n        pos_sample_rate (int, optional): The rate at which position was sampled. Default 50\n        nbins (int, optional): The number of bins for creating the resulting ratemap. Default 71\n        boxcar (int, optional): The size of the smoothing kernel to smooth ratemaps. Default 5\n        Pthresh (int, optional): The cut-off for values in the ratemap; values &lt; Pthresh become nans. Default 100\n        downsampfreq (int, optional): How much to downsample. Default 50\n\n        Returns\n        -------\n        H (array_like): The temporal windowed SAC\n        \"\"\"\n        # [Stage 0] Get some numbers\n        xy = xy / ppm * 100\n        n_samps = xy.shape[1]\n        n_spks = len(spkIdx)\n        winSizeBins = np.min([winSize * pos_sample_rate, n_samps])\n        # factor by which positions are downsampled\n        downsample = np.ceil(pos_sample_rate / downsampfreq)\n        Pthresh = Pthresh / downsample  # take account of downsampling\n\n        # [Stage 1] Calculate number of spikes in the window for each spikeInd\n        # (ignoring spike itself)\n        # 1a. Loop preparation\n        nSpikesInWin = np.zeros(n_spks, dtype=int)\n\n        # 1b. Keep looping until we have dealt with all spikes\n        for i, s in enumerate(spkIdx):\n            t = np.searchsorted(spkIdx, (s, s + winSizeBins))\n            nSpikesInWin[i] = len(spkIdx[t[0] : t[1]]) - 1  # ignore ith spike\n\n        # [Stage 2] Prepare for main loop\n        # 2a. Work out offset inidices to be used when storing spike data\n        off_spike = np.cumsum([nSpikesInWin])\n        off_spike = np.pad(off_spike, (1, 0), \"constant\", constant_values=(0))\n\n        # 2b. Work out number of downsampled pos bins in window and\n        # offset indices for storing data\n        nPosInWindow = np.minimum(winSizeBins, n_samps - spkIdx)\n        nDownsampInWin = np.floor((nPosInWindow - 1) / downsample) + 1\n\n        off_dwell = np.cumsum(nDownsampInWin.astype(int))\n        off_dwell = np.pad(off_dwell, (1, 0), \"constant\", constant_values=(0))\n\n        # 2c. Pre-allocate dwell and spike arrays, singles for speed\n        dwell = np.zeros((2, off_dwell[-1]), dtype=np.single) * np.nan\n        spike = np.zeros((2, off_spike[-1]), dtype=np.single) * np.nan\n\n        filled_pvals = 0\n        filled_svals = 0\n\n        for i in range(n_spks):\n            # calculate dwell displacements\n            winInd_dwell = np.arange(\n                spkIdx[i] + 1,\n                np.minimum(spkIdx[i] + winSizeBins, n_samps),\n                downsample,\n                dtype=int,\n            )\n            WL = len(winInd_dwell)\n            dwell[:, filled_pvals : filled_pvals + WL] = np.rot90(\n                np.array(np.rot90(xy[:, winInd_dwell]) - xy[:, spkIdx[i]])\n            )\n            filled_pvals = filled_pvals + WL\n            # calculate spike displacements\n            winInd_spks = (\n                i + np.nonzero(spkIdx[i + 1 : n_spks] &lt; spkIdx[i] + winSizeBins)[0]\n            )\n            WL = len(winInd_spks)\n            spike[:, filled_svals : filled_svals + WL] = np.rot90(\n                np.array(np.rot90(xy[:, spkIdx[winInd_spks]]) - xy[:, spkIdx[i]])\n            )\n            filled_svals = filled_svals + WL\n\n        dwell = np.delete(dwell, np.isnan(dwell).nonzero()[1], axis=1)\n        spike = np.delete(spike, np.isnan(spike).nonzero()[1], axis=1)\n\n        dwell = np.hstack((dwell, -dwell))\n        spike = np.hstack((spike, -spike))\n\n        dwell_min = np.min(dwell, axis=1)\n        dwell_max = np.max(dwell, axis=1)\n\n        binsize = (dwell_max[1] - dwell_min[1]) / nbins\n\n        dwell = np.round(\n            (dwell - np.ones_like(dwell) * dwell_min[:, np.newaxis]) / binsize\n        )\n        spike = np.round(\n            (spike - np.ones_like(spike) * dwell_min[:, np.newaxis]) / binsize\n        )\n\n        binsize = np.max(dwell, axis=1).astype(int)\n        binedges = np.array(((-0.5, -0.5), binsize + 0.5)).T\n        Hp, Hpe_y, Hpe_x = np.histogram2d(\n            dwell[0, :], dwell[1, :], range=binedges, bins=binsize\n        )\n        Hs, Hse_y, Hse_x = np.histogram2d(\n            spike[0, :], spike[1, :], range=binedges, bins=binsize\n        )\n\n        # reverse y,x order\n        Hp = np.swapaxes(Hp, 1, 0)\n        Hs = np.swapaxes(Hs, 1, 0)\n\n        Hp = BinnedData(VariableToBin.XY, MapType.RATE, [Hp], [Hpe_x, Hpe_y])\n        Hs = BinnedData(VariableToBin.XY, MapType.RATE, [Hs], [Hse_x, Hse_y])\n\n        # smooth the maps\n        fHp = blur_image(Hp, boxcar)\n        fHs = blur_image(Hs, boxcar)\n\n        H = fHs / fHp\n        H.set_nan_indices(Hp.binned_data[0] &lt; Pthresh)\n        return H\n\n    def _calc_ego_angles(\n        self, arena_shape: str = \"circle\", xy_binsize: float = 2.5\n    ) -&gt; tuple[np.ndarray, ...]:\n        \"\"\"\n        Calculate the angles between the segments of the arena wall\n        and the positions of the animal throughout the trial.\n\n        Parameters\n        ----------\n        arena_shape (str) - the shape of the arena, 'circle' or 'square'.\n        xy_binsize (float) - the binsize\n\n        Returns\n        -------\n        tuple of np.ndarray - the angles as well as the arena x-y coordinates.\n\n        Notes\n        -----\n        Angles are in radians.\n        \"\"\"\n        arena_width = np.ceil(\n            np.nanmean((np.nanmax(self.xy.data, 1) - np.nanmin(self.xy.data, 1)) / 2)\n        )\n        arena_width = arena_width.tolist()\n        arena_centre = Point(np.nanmin(self.xy.data, 1) + arena_width)\n\n        if \"circle\" in arena_shape:\n            arena_boundary = arena_centre.buffer(arena_width).boundary\n        elif \"square\" in arena_shape:\n            arena_boundary = arena_centre.buffer(arena_width, cap_style=3).boundary\n        arena_boundary = arena_boundary.segmentize(max_segment_length=xy_binsize)\n        arena_xy = np.array(arena_boundary.xy).T\n        animal_xy = self.xy\n        dx = np.atleast_2d(animal_xy[0]) - np.atleast_2d(arena_xy[:, 0]).T\n        dy = np.atleast_2d(animal_xy[1]) - np.atleast_2d(arena_xy[:, 1]).T\n        # make sure angles are in range [0-2PI]\n        angles = np.arctan2(dy, dx) + np.pi\n        animal_hd = np.radians(self.dir)\n        ego_angles = (angles - animal_hd) % (np.pi * 2)\n        # ego_angles in range [0-2PI]\n        # and with size arena_xy_ncoords x npos\n        return ego_angles, arena_xy\n\n    def get_disperion_map(\n        self, spk_times: np.ndarray, pos_times: np.ndarray\n    ) -&gt; BinnedData | None:\n        \"\"\"\n        Attempt to write a faster version of creating an overdispersion\n        map. A cell will sometimes fire too much or too little on a given\n        run through its receptive field. This function quantifies that.\n\n        This shows the amount of 'observed' variance in spiking around\n        the mean spiking in a bin...\n\n        Parameters\n        ----------\n        spk_times (np.ndarray) - a vector of spike times (in seconds)\n        pos_times (np.ndarray) - vector of position times (seconds)\n\n        Returns\n        -------\n        BinnedData - the overdispersion map in an instance of BinnedData\n\n        \"\"\"\n        idx = np.searchsorted(pos_times, spk_times, side=\"right\")\n        spike_weights = np.bincount(idx, minlength=len(pos_times))\n        expected_spikes = self.get_map(spike_weights, map_type=MapType.SPK)\n        # bin_edges[1] is x, bin_edges[0] is y\n        x_bins = np.digitize(self.xy[0], expected_spikes.bin_edges[1][:-1]) - 1\n        y_bins = np.digitize(self.xy[1], expected_spikes.bin_edges[0][:-1]) - 1\n        map_shape = np.shape(expected_spikes.binned_data[0])\n        pos_bins_linear_idx = np.ravel_multi_index([y_bins, x_bins], map_shape)\n        expected_spikes_xy = expected_spikes.binned_data[0][y_bins, x_bins]\n        min_rate_threshold = np.nanmax(expected_spikes.binned_data[0]) * 0.25\n        x_bins_with_firing = x_bins[spike_weights &gt; 0]\n        y_bins_with_firing = y_bins[spike_weights &gt; 0]\n        bins_with_firing_linear_idx = np.ravel_multi_index(\n            [y_bins_with_firing, x_bins_with_firing], map_shape\n        )\n\n        observed_spikes = expected_spikes\n        return observed_spikes\n</code></pre>"},{"location":"reference/#ephysiopy.common.binning.RateMap.pos_weights","title":"<code>pos_weights</code>  <code>property</code> <code>writable</code>","text":"<p>The 'weights' used as an argument to np.histogram* for binning up position Mostly this is just an array of 1's equal to the length of the pos data, but usefully can be adjusted when masking data in the trial by</p>"},{"location":"reference/#ephysiopy.common.binning.RateMap._autoCorr2D","title":"<code>_autoCorr2D(A, nodwell=None, tol=1e-10)</code>","text":"<p>Performs a spatial autocorrelation on the array A</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>ndarray</code> <p>where the two dimensions correspond to x and y. If 3D then the first two dimensions are x and y and the third (last dimension) is 'stack' of ratemaps</p> required <code>nodwell</code> <code>ndarray</code> <p>weren't visited. See Notes below.</p> <code>None</code> <code>tol</code> <code>float</code> <p>thrown up by the fft. Default 1e-10</p> <code>1e-10</code> <p>Returns:</p> Type Description <code>sac (array_like): The spatial autocorrelation in the relevant dimensionality</code> Notes <p>The nodwell input can usually be generated by:</p> <p>nodwell = ~np.isfinite(A)</p> Source code in <code>ephysiopy/common/binning.py</code> <pre><code>def _autoCorr2D(\n    self, A: np.ndarray, nodwell: np.ndarray = None, tol: float = 1e-10\n):\n    \"\"\"\n    Performs a spatial autocorrelation on the array A\n\n    Parameters\n    ----------\n    A (array_like): Either 2 or 3D. In the former it is simply the binned up ratemap\n        where the two dimensions correspond to x and y.\n        If 3D then the first two dimensions are x\n        and y and the third (last dimension) is 'stack' of ratemaps\n    nodwell (array_like): A boolean array corresponding the bins in the ratemap that\n        weren't visited. See Notes below.\n    tol (float, optional): Values below this are set to zero to deal with v small values\n        thrown up by the fft. Default 1e-10\n\n    Returns\n    -------\n    sac (array_like): The spatial autocorrelation in the relevant dimensionality\n\n    Notes\n    -----\n    The nodwell input can usually be generated by:\n\n    &gt;&gt;&gt; nodwell = ~np.isfinite(A)\n    \"\"\"\n    assert np.ndim(A) == 2\n    m, n = np.shape(A)\n    o = 1\n    x = np.reshape(A, (m, n, o))\n    if nodwell is None:\n        nodwell = ~np.isfinite(A)\n    nodwell = np.reshape(nodwell, (m, n, o))\n    x[nodwell] = 0\n    # [Step 1] Obtain FFTs of x, the sum of squares and bins visited\n    Fx = np.fft.fft(np.fft.fft(x, 2 * m - 1, axis=0), 2 * n - 1, axis=1)\n    FsumOfSquares_x = np.fft.fft(\n        np.fft.fft(np.power(x, 2), 2 * m - 1, axis=0), 2 * n - 1, axis=1\n    )\n    Fn = np.fft.fft(\n        np.fft.fft(np.invert(nodwell).astype(int), 2 * m - 1, axis=0),\n        2 * n - 1,\n        axis=1,\n    )\n    # [Step 2] Multiply the relevant transforms and invert to obtain the\n    # equivalent convolutions\n    rawCorr = np.fft.fftshift(\n        np.real(np.fft.ifft(np.fft.ifft(Fx * np.conj(Fx), axis=1), axis=0)),\n        axes=(0, 1),\n    )\n    sums_x = np.fft.fftshift(\n        np.real(np.fft.ifft(np.fft.ifft(np.conj(Fx) * Fn, axis=1), axis=0)),\n        axes=(0, 1),\n    )\n    sumOfSquares_x = np.fft.fftshift(\n        np.real(\n            np.fft.ifft(np.fft.ifft(Fn * np.conj(FsumOfSquares_x), axis=1), axis=0)\n        ),\n        axes=(0, 1),\n    )\n    N = np.fft.fftshift(\n        np.real(np.fft.ifft(np.fft.ifft(Fn * np.conj(Fn), axis=1), axis=0)),\n        axes=(0, 1),\n    )\n    # [Step 3] Account for rounding errors.\n    rawCorr[np.abs(rawCorr) &lt; tol] = 0\n    sums_x[np.abs(sums_x) &lt; tol] = 0\n    sumOfSquares_x[np.abs(sumOfSquares_x) &lt; tol] = 0\n    N = np.round(N)\n    N[N &lt;= 1] = np.nan\n    # [Step 4] Compute correlation matrix\n    mapStd = np.sqrt((sumOfSquares_x * N) - sums_x**2)\n    mapCovar = (rawCorr * N) - sums_x * sums_x[::-1, :, :][:, ::-1, :][:, :, :]\n\n    return np.squeeze(mapCovar / mapStd / mapStd[::-1, :, :][:, ::-1, :][:, :, :])\n</code></pre>"},{"location":"reference/#ephysiopy.common.binning.RateMap._bin_data","title":"<code>_bin_data(var, bin_edges, weights)</code>","text":"<p>Bins data taking account of possible multi-dimensionality</p> <p>Args:     var (array_like): The variable to bin     bin_edges (array_like): The edges of the data - see numpys histogramdd for more     weights (array_like): The weights attributed to the samples in var</p> <p>Returns:     ndhist (2-tuple): Returns a two-tuple of the binned variable and         the bin edges</p> <p>Notes:     This breaks compatability with numpys histogramdd     In the 2d histogram case below I swap the axes around so that x and y     are binned in the 'normal' format i.e. so x appears horizontally and y     vertically.     Multi-binning issue is dealt with awkwardly through checking     the dimensionality of the weights array.     'normally' this would be 1 dim but when multiple clusters are being     binned it will be 2 dim.     In that case np.apply_along_axis functionality is applied.     The spike weights in that case might be created like so:</p> <pre><code>&gt;&gt;&gt; spk_W = np.zeros(shape=[len(trial.nClusters), trial.npos])\n&gt;&gt;&gt; for i, cluster in enumerate(trial.clusters):\n&gt;&gt;&gt;         x1 = trial.getClusterIdx(cluster)\n&gt;&gt;&gt;         spk_W[i, :] = np.bincount(x1, minlength=trial.npos)\n\nThis can then be fed into this fcn something like so:\n\n&gt;&gt;&gt; rng = np.array((np.ma.min(\n    trial.POS.xy, 1).data, np.ma.max(rial.POS.xy, 1).data))\n&gt;&gt;&gt; h = _bin_data(\n    var=trial.POS.xy, bin_edges=np.array([64, 64]),\n    weights=spk_W, rng=rng)\n\nReturned will be a tuple containing the binned up data and\nthe bin edges for x and y (obv this will be the same for all\nentries of h)\n</code></pre> Source code in <code>ephysiopy/common/binning.py</code> <pre><code>def _bin_data(self, var, bin_edges, weights) -&gt; tuple[np.ndarray, ...]:\n    \"\"\"\n    Bins data taking account of possible multi-dimensionality\n\n    Args:\n        var (array_like): The variable to bin\n        bin_edges (array_like): The edges of the data - see numpys histogramdd for more\n        weights (array_like): The weights attributed to the samples in var\n\n    Returns:\n        ndhist (2-tuple): Returns a two-tuple of the binned variable and\n            the bin edges\n\n    Notes:\n        This breaks compatability with numpys histogramdd\n        In the 2d histogram case below I swap the axes around so that x and y\n        are binned in the 'normal' format i.e. so x appears horizontally and y\n        vertically.\n        Multi-binning issue is dealt with awkwardly through checking\n        the dimensionality of the weights array.\n        'normally' this would be 1 dim but when multiple clusters are being\n        binned it will be 2 dim.\n        In that case np.apply_along_axis functionality is applied.\n        The spike weights in that case might be created like so:\n\n        &gt;&gt;&gt; spk_W = np.zeros(shape=[len(trial.nClusters), trial.npos])\n        &gt;&gt;&gt; for i, cluster in enumerate(trial.clusters):\n        &gt;&gt;&gt;\t\tx1 = trial.getClusterIdx(cluster)\n        &gt;&gt;&gt;\t\tspk_W[i, :] = np.bincount(x1, minlength=trial.npos)\n\n        This can then be fed into this fcn something like so:\n\n        &gt;&gt;&gt; rng = np.array((np.ma.min(\n            trial.POS.xy, 1).data, np.ma.max(rial.POS.xy, 1).data))\n        &gt;&gt;&gt; h = _bin_data(\n            var=trial.POS.xy, bin_edges=np.array([64, 64]),\n            weights=spk_W, rng=rng)\n\n        Returned will be a tuple containing the binned up data and\n        the bin edges for x and y (obv this will be the same for all\n        entries of h)\n    \"\"\"\n    if weights is None:\n        weights = np.ma.MaskedArray(np.ones_like(var))\n    weights = np.ma.MaskedArray(weights)\n    dims = weights.ndim\n    if dims == 1 and var.ndim == 1:\n        var = var[np.newaxis, :]\n        if bin_edges is not None:\n            bin_edges = np.atleast_2d(bin_edges)\n    elif dims &gt; 1 and var.ndim == 1:\n        var = var[np.newaxis, :]\n        if bin_edges is not None:\n            bin_edges = np.atleast_2d(bin_edges)\n    else:\n        var = np.flipud(var)\n    weights = np.atleast_2d(weights)  # needed for list comp below\n    weights = np.ma.filled(weights, 0)\n    var = np.array(var.T)\n    if bin_edges is None:\n        bin_edges = self.binedges\n    if len(bin_edges) == 1:\n        hist = bh.Histogram(\n            bh.axis.Regular(len(bin_edges[0]), bin_edges[0][0], bin_edges[0][-1])\n        )\n    else:\n        hist = bh.Histogram(\n            bh.axis.Regular(len(bin_edges[0]), bin_edges[0][0], bin_edges[0][-1]),\n            bh.axis.Regular(len(bin_edges[1]), bin_edges[1][0], bin_edges[1][-1]),\n        )\n    ndhist = []\n    for w in weights:\n        hist.fill(*var.T, weight=w)\n        ndhist.append(\n            [hist.values().copy(), [np.ravel(e) for e in hist.axes.edges]]\n        )\n        hist.reset()\n    if np.shape(weights)[0] == 1:\n        return ndhist[0]\n    else:\n        tmp = [d[0] for d in ndhist]\n        return tmp, ndhist[1]\n</code></pre>"},{"location":"reference/#ephysiopy.common.binning.RateMap._calc_bin_edges","title":"<code>_calc_bin_edges(binsize=3)</code>","text":"<p>Aims to get the right number of bins for the variable to be binned</p> <p>Args:     binsize (int | tuple, optional): The number of cms per bin for XY OR degrees for DIR OR cm/s for SPEED. Defaults to 3.</p> <p>Returns:     tuple: each member an array of bin edges</p> Source code in <code>ephysiopy/common/binning.py</code> <pre><code>def _calc_bin_edges(self, binsize: int | tuple = 3) -&gt; tuple[np.ndarray, ...]:\n    \"\"\"\n    Aims to get the right number of bins for the variable to be binned\n\n    Args:\n        binsize (int | tuple, optional): The number of cms per bin for XY OR degrees for DIR OR cm/s for SPEED. Defaults to 3.\n\n    Returns:\n        tuple: each member an array of bin edges\n    \"\"\"\n    if self.var2Bin.value == VariableToBin.DIR.value:\n        self.binedges = np.linspace(0, 360, int(360 / binsize)).tolist()\n    elif self.var2Bin.value == VariableToBin.SPEED.value:\n        maxspeed = np.nanmax(self.speed)\n        # assume min speed = 0\n        self.binedges = np.linspace(0, maxspeed, int(maxspeed / binsize)).tolist()\n    elif self.var2Bin.value == VariableToBin.XY.value:\n        x_lims, y_lims = self._getXYLimits()\n        nxbins = int(np.ceil((x_lims[1] - x_lims[0]) / binsize))\n        nybins = int(np.ceil((y_lims[1] - y_lims[0]) / binsize))\n        _x = np.linspace(x_lims[0], x_lims[1], nxbins)\n        _y = np.linspace(y_lims[0], y_lims[1], nybins)\n        self.binedges = _y, _x\n    elif self.var2Bin.value == VariableToBin.XY_TIME.value:\n        if self._pos_time_splits is None:\n            raise ValueError(\"Need pos times to bin up XY_TIME\")\n        x_lims, y_lims = self._getXYLimits()\n        nxbins = int(np.ceil((x_lims[1] - x_lims[0]) / binsize))\n        nybins = int(np.ceil((y_lims[1] - y_lims[0]) / binsize))\n        _x = np.linspace(x_lims[0], x_lims[1], nxbins)\n        _y = np.linspace(y_lims[0], y_lims[1], nybins)\n        self.binedges = _y, _x, self.pos_time_splits\n    elif self.var2Bin.value == VariableToBin.SPEED_DIR.value:\n        maxspeed = np.nanmax(self.speed)\n        if isinstance(binsize, int):\n            self.binedges = (\n                np.linspace(0, maxspeed, int(maxspeed / binsize)),\n                np.linspace(0, 360, int(360 / binsize)),\n            )\n        elif isinstance(binsize, tuple):\n            self.binedges = (\n                np.linspace(0, maxspeed, int(maxspeed / binsize[0])),\n                np.linspace(0, 360, int(360 / binsize[1])),\n            )\n    elif self.var2Bin.value == VariableToBin.EGO_BOUNDARY.value:\n        if isinstance(binsize, (float, int)):\n            self.binedges = np.linspace(0, 50, 20), np.linspace(0, 2 * np.pi, 120)\n        elif isinstance(binsize, tuple):\n            self.binedges = np.linspace(0, 50, int(50 / binsize[0])), np.linspace(\n                0, 2 * np.pi, int((2 * np.pi) / binsize[1])\n            )\n    self._calc_bin_dims()\n    return self.binedges\n</code></pre>"},{"location":"reference/#ephysiopy.common.binning.RateMap._calc_ego_angles","title":"<code>_calc_ego_angles(arena_shape='circle', xy_binsize=2.5)</code>","text":"<p>Calculate the angles between the segments of the arena wall and the positions of the animal throughout the trial.</p> <p>Parameters:</p> Name Type Description Default <code>arena_shape</code> <code>str</code> <code>'circle'</code> <code>xy_binsize</code> <code>float</code> <code>2.5</code> <p>Returns:</p> Type Description <code>tuple of np.ndarray - the angles as well as the arena x-y coordinates.</code> Notes <p>Angles are in radians.</p> Source code in <code>ephysiopy/common/binning.py</code> <pre><code>def _calc_ego_angles(\n    self, arena_shape: str = \"circle\", xy_binsize: float = 2.5\n) -&gt; tuple[np.ndarray, ...]:\n    \"\"\"\n    Calculate the angles between the segments of the arena wall\n    and the positions of the animal throughout the trial.\n\n    Parameters\n    ----------\n    arena_shape (str) - the shape of the arena, 'circle' or 'square'.\n    xy_binsize (float) - the binsize\n\n    Returns\n    -------\n    tuple of np.ndarray - the angles as well as the arena x-y coordinates.\n\n    Notes\n    -----\n    Angles are in radians.\n    \"\"\"\n    arena_width = np.ceil(\n        np.nanmean((np.nanmax(self.xy.data, 1) - np.nanmin(self.xy.data, 1)) / 2)\n    )\n    arena_width = arena_width.tolist()\n    arena_centre = Point(np.nanmin(self.xy.data, 1) + arena_width)\n\n    if \"circle\" in arena_shape:\n        arena_boundary = arena_centre.buffer(arena_width).boundary\n    elif \"square\" in arena_shape:\n        arena_boundary = arena_centre.buffer(arena_width, cap_style=3).boundary\n    arena_boundary = arena_boundary.segmentize(max_segment_length=xy_binsize)\n    arena_xy = np.array(arena_boundary.xy).T\n    animal_xy = self.xy\n    dx = np.atleast_2d(animal_xy[0]) - np.atleast_2d(arena_xy[:, 0]).T\n    dy = np.atleast_2d(animal_xy[1]) - np.atleast_2d(arena_xy[:, 1]).T\n    # make sure angles are in range [0-2PI]\n    angles = np.arctan2(dy, dx) + np.pi\n    animal_hd = np.radians(self.dir)\n    ego_angles = (angles - animal_hd) % (np.pi * 2)\n    # ego_angles in range [0-2PI]\n    # and with size arena_xy_ncoords x npos\n    return ego_angles, arena_xy\n</code></pre>"},{"location":"reference/#ephysiopy.common.binning.RateMap._crossCorr2D","title":"<code>_crossCorr2D(A, B, A_nodwell=None, B_nodwell=None, tol=1e-10)</code>","text":"<p>Performs crosscorrelations between the maps in two instances of BinnedData, A and B.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>BinnedData</code> required <code>B</code> <code>BinnedData</code> required <code>A_nodwell</code> <code>ndarray</code> <code>None</code> <code>B_nodwell</code> <code>ndarray</code> <code>None</code> <code>tol</code> <code>float</code> <code>1e-10</code> <p>Returns:</p> Type Description <code>BinnedData - the data in A with the maps replaced by autocorrelograms</code> Source code in <code>ephysiopy/common/binning.py</code> <pre><code>def _crossCorr2D(\n    self,\n    A: BinnedData,\n    B: BinnedData,\n    A_nodwell: np.ndarray = None,\n    B_nodwell: np.ndarray = None,\n    tol: float = 1e-10,\n):\n    \"\"\"\n    Performs crosscorrelations between the maps in two instances of BinnedData, A and B.\n\n    Parameters\n    ----------\n    A, B (BinnedData) - instance of BinnedData\n    A_nodwell, B_nodwell (np.ndarray) - array with NaNs where there was no position sampled.\n    tol (float) - values below this are set to 0.\n\n    Returns\n    -------\n    BinnedData - the data in A with the maps replaced by autocorrelograms\n    \"\"\"\n    if isinstance(A, BinnedData):\n        A = A.binned_data[0]\n    if isinstance(B, BinnedData):\n        B = B.binned_data[0]\n    if np.ndim(A) != np.ndim(B):\n        raise ValueError(\"Both arrays must have the same dimensionality\")\n    assert np.ndim(A) == 2\n    ma, na = np.shape(A)\n    mb, nb = np.shape(B)\n    oa = ob = 1\n    A = np.reshape(A, (ma, na, oa))\n    B = np.reshape(B, (mb, nb, ob))\n    if A_nodwell is None:\n        A_nodwell = ~np.isfinite(A)\n    if B_nodwell is None:\n        B_nodwell = ~np.isfinite(B)\n    A_nodwell = np.reshape(A_nodwell, (ma, na, oa))\n    B_nodwell = np.reshape(B_nodwell, (mb, nb, ob))\n    A[A_nodwell] = 0\n    B[B_nodwell] = 0\n    # [Step 1] Obtain FFTs of x, the sum of squares and bins visited\n    Fa = np.fft.fft(np.fft.fft(A, 2 * mb - 1, axis=0), 2 * nb - 1, axis=1)\n    FsumOfSquares_a = np.fft.fft(\n        np.fft.fft(np.power(A, 2), 2 * mb - 1, axis=0), 2 * nb - 1, axis=1\n    )\n    Fn_a = np.fft.fft(\n        np.fft.fft(np.invert(A_nodwell).astype(int), 2 * mb - 1, axis=0),\n        2 * nb - 1,\n        axis=1,\n    )\n    Fb = np.fft.fft(np.fft.fft(B, 2 * ma - 1, axis=0), 2 * na - 1, axis=1)\n    FsumOfSquares_b = np.fft.fft(\n        np.fft.fft(np.power(B, 2), 2 * ma - 1, axis=0), 2 * na - 1, axis=1\n    )\n    Fn_b = np.fft.fft(\n        np.fft.fft(np.invert(B_nodwell).astype(int), 2 * ma - 1, axis=0),\n        2 * na - 1,\n        axis=1,\n    )\n    # [Step 2] Multiply the relevant transforms and invert to obtain the\n    # equivalent convolutions\n    rawCorr = np.fft.fftshift(\n        np.real(np.fft.ifft(np.fft.ifft(Fa * np.conj(Fb), axis=1), axis=0))\n    )\n    sums_a = np.fft.fftshift(\n        np.real(np.fft.ifft(np.fft.ifft(Fa * np.conj(Fn_b), axis=1), axis=0))\n    )\n    sums_b = np.fft.fftshift(\n        np.real(np.fft.ifft(np.fft.ifft(Fn_a * np.conj(Fb), axis=1), axis=0))\n    )\n    sumOfSquares_a = np.fft.fftshift(\n        np.real(\n            np.fft.ifft(\n                np.fft.ifft(FsumOfSquares_a * np.conj(Fn_b), axis=1), axis=0\n            )\n        )\n    )\n    sumOfSquares_b = np.fft.fftshift(\n        np.real(\n            np.fft.ifft(\n                np.fft.ifft(Fn_a * np.conj(FsumOfSquares_b), axis=1), axis=0\n            )\n        )\n    )\n    N = np.fft.fftshift(\n        np.real(np.fft.ifft(np.fft.ifft(Fn_a * np.conj(Fn_b), axis=1), axis=0))\n    )\n    # [Step 3] Account for rounding errors.\n    rawCorr[np.abs(rawCorr) &lt; tol] = 0\n    sums_a[np.abs(sums_a) &lt; tol] = 0\n    sums_b[np.abs(sums_b) &lt; tol] = 0\n    sumOfSquares_a[np.abs(sumOfSquares_a) &lt; tol] = 0\n    sumOfSquares_b[np.abs(sumOfSquares_b) &lt; tol] = 0\n    N = np.round(N)\n    N[N &lt;= 1] = np.nan\n    # [Step 4] Compute correlation matrix\n    mapStd_a = np.sqrt((sumOfSquares_a * N) - sums_a**2)\n    mapStd_b = np.sqrt((sumOfSquares_b * N) - sums_b**2)\n    mapCovar = (rawCorr * N) - sums_a * sums_b\n\n    return np.squeeze(mapCovar / (mapStd_a * mapStd_b))\n</code></pre>"},{"location":"reference/#ephysiopy.common.binning.RateMap._getXYLimits","title":"<code>_getXYLimits()</code>","text":"<p>Gets the min/max of the x/y data</p> Source code in <code>ephysiopy/common/binning.py</code> <pre><code>def _getXYLimits(self):\n    \"\"\"\n    Gets the min/max of the x/y data\n    \"\"\"\n    x_lims = getattr(self, \"x_lims\", None)\n    y_lims = getattr(self, \"y_lims\", None)\n    if x_lims is None:\n        x_lims = (np.nanmin(self.xy[0]), np.nanmax(self.xy[0]))\n    if y_lims is None:\n        y_lims = (np.nanmin(self.xy[1]), np.nanmax(self.xy[1]))\n    self.x_lims = x_lims\n    self.y_lims = y_lims\n    return x_lims, y_lims\n</code></pre>"},{"location":"reference/#ephysiopy.common.binning.RateMap.autoCorr2D","title":"<code>autoCorr2D(A, nodwell=None, tol=1e-10)</code>","text":"<p>Performs autocorrelations on all the maps in an instance of BinnedData.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>BinnedData</code> required <code>nodwell</code> <code>ndarray</code> <code>None</code> <code>tol</code> <code>float</code> <code>1e-10</code> <p>Returns:</p> Type Description <code>BinnedData - the data in A with the maps replaced by autocorrelograms</code> Source code in <code>ephysiopy/common/binning.py</code> <pre><code>def autoCorr2D(\n    self, A: BinnedData, nodwell: np.ndarray = None, tol: float = 1e-10\n) -&gt; BinnedData:\n    \"\"\"\n    Performs autocorrelations on all the maps in an instance of BinnedData.\n\n    Parameters\n    ----------\n    A (BinnedData) - instance of BinnedData\n    nodwell (np.ndarray) - array with NaNs where there was no position sampled.\n    tol (float) - values below this are set to 0.\n\n    Returns\n    -------\n    BinnedData - the data in A with the maps replaced by autocorrelograms\n    \"\"\"\n    result = BinnedData(A.variable, MapType.AUTO_CORR)\n    for rmap in A.binned_data:\n        rr = self._autoCorr2D(rmap, nodwell, tol)\n        result.binned_data += [rr]\n    xlen = len(A.bin_edges[0]) - 1\n    ylen = len(A.bin_edges[1]) - 1\n    result.bin_edges = [\n        np.arange(-xlen + 1, xlen + 1),\n        np.arange(-ylen + 1, ylen + 1),\n    ]\n    return result\n</code></pre>"},{"location":"reference/#ephysiopy.common.binning.RateMap.crossCorr2D","title":"<code>crossCorr2D(A, B, A_nodwell=None, B_nodwell=None, tol=1e-10)</code>","text":"<p>Performs crosscorrelations between the maps in two instances of BinnedData, A and B.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>BinnedData</code> required <code>B</code> <code>BinnedData</code> required <code>A_nodwell</code> <code>ndarray</code> <code>None</code> <code>B_nodwell</code> <code>ndarray</code> <code>None</code> <code>tol</code> <code>float</code> <code>1e-10</code> <p>Returns:</p> Type Description <code>BinnedData - the data in A with the maps replaced by autocorrelograms</code> Source code in <code>ephysiopy/common/binning.py</code> <pre><code>def crossCorr2D(\n    self,\n    A: BinnedData,\n    B: BinnedData,\n    A_nodwell: np.ndarray = None,\n    B_nodwell: np.ndarray = None,\n    tol: float = 1e-10,\n) -&gt; BinnedData:\n    \"\"\"\n    Performs crosscorrelations between the maps in two instances of BinnedData, A and B.\n\n    Parameters\n    ----------\n    A, B (BinnedData) - instance of BinnedData\n    A_nodwell, B_nodwell (np.ndarray) - array with NaNs where there was no position sampled.\n    tol (float) - values below this are set to 0.\n\n    Returns\n    -------\n    BinnedData - the data in A with the maps replaced by autocorrelograms\n    \"\"\"\n    result = BinnedData(A.variable, MapType.CROSS_CORR)\n    for rmap in A:\n        for rmap2 in B:\n            rr = self._crossCorr2D(rmap, rmap2, A_nodwell, B_nodwell, tol)\n            result.binned_data += [rr]\n    xlen = max(len(A.bin_edges[0]) - 1, len(B.bin_edges[0]) - 1)\n    ylen = max(len(A.bin_edges[1]) - 1, len(B.bin_edges[1]) - 1)\n    result.bin_edges = [\n        np.arange(-xlen + 1, xlen + 1),\n        np.arange(-ylen + 1, ylen + 1),\n    ]\n    return result\n</code></pre>"},{"location":"reference/#ephysiopy.common.binning.RateMap.getAdaptiveMap","title":"<code>getAdaptiveMap(pos_binned, spk_binned, alpha=4)</code>","text":"<p>Produces a ratemap that has been adaptively binned according to the algorithm described in Skaggs et al., 1996) [1]_.</p> <p>Parameters:</p> Name Type Description Default <code>pos_binned</code> <p>above with mapType as 'pos'</p> required <code>spk_binned</code> required <code>alpha</code> <p>in each bin. Defaults to 4. In the original paper this was set to 200. This is 4 here as the pos data is binned in seconds (the original data was in pos samples so this is a factor of 50 smaller than the original paper's value, given 50Hz sample rate)</p> <code>4</code> <p>Returns:</p> Type Description <code>Returns adaptively binned spike and pos maps. Use to generate Skaggs</code> <code>information measure</code> Notes <p>Positions with high rates mean proportionately less error than those with low rates, so this tries to even the playing field. This type of binning should be used for calculations of spatial info as with the skaggs_info method in the fieldcalcs class (see below) alpha is a scaling parameter that might need tweaking for different data sets.</p> <p>From the paper:     The data [are] first binned     into a 64 X 64 grid of spatial locations, and then the firing rate     at each point in this grid was calculated by expanding a circle     around the point until the following criterion was met:         Nspks &gt; alpha / (Nocc^2 * r^2)     where Nspks is the number of spikes emitted in a circle of radius     r (in bins), Nocc is the number of occupancy samples, alpha is the     scaling parameter     The firing rate in the given bin is then calculated as:         sample_rate * (Nspks / Nocc)</p> References <p>.. [1] W. E. Skaggs, B. L. McNaughton, K. M. Gothard &amp; E. J. Markus     \"An Information-Theoretic Approach to Deciphering the Hippocampal     Code\"     Neural Information Processing Systems, 1993.</p> Source code in <code>ephysiopy/common/binning.py</code> <pre><code>def getAdaptiveMap(self, pos_binned, spk_binned, alpha=4):\n    \"\"\"\n    Produces a ratemap that has been adaptively binned according to the\n    algorithm described in Skaggs et al., 1996) [1]_.\n\n    Parameters\n    ----------\n    pos_binned (array_like): The binned positional data. For example that returned from get_map\n        above with mapType as 'pos'\n    spk_binned (array_like): The binned spikes\n    alpha (int, optional): A scaling parameter determing the amount of occupancy to aim at\n        in each bin. Defaults to 4. In the original paper this was set to 200.\n        This is 4 here as the pos data is binned in seconds (the original data was in pos\n        samples so this is a factor of 50 smaller than the original paper's value, given 50Hz sample rate)\n\n    Returns\n    -------\n    Returns adaptively binned spike and pos maps. Use to generate Skaggs\n    information measure\n\n    Notes\n    -----\n    Positions with high rates mean proportionately less error than those\n    with low rates, so this tries to even the playing field. This type\n    of binning should be used for calculations of spatial info\n    as with the skaggs_info method in the fieldcalcs class (see below)\n    alpha is a scaling parameter that might need tweaking for different\n    data sets.\n\n    From the paper:\n        The data [are] first binned\n        into a 64 X 64 grid of spatial locations, and then the firing rate\n        at each point in this grid was calculated by expanding a circle\n        around the point until the following criterion was met:\n            Nspks &gt; alpha / (Nocc^2 * r^2)\n        where Nspks is the number of spikes emitted in a circle of radius\n        r (in bins), Nocc is the number of occupancy samples, alpha is the\n        scaling parameter\n        The firing rate in the given bin is then calculated as:\n            sample_rate * (Nspks / Nocc)\n\n    References\n    ----------\n    .. [1] W. E. Skaggs, B. L. McNaughton, K. M. Gothard &amp; E. J. Markus\n        \"An Information-Theoretic Approach to Deciphering the Hippocampal\n        Code\"\n        Neural Information Processing Systems, 1993.\n    \"\"\"\n    #  assign output arrays\n    smthdpos = np.zeros_like(pos_binned)\n    smthdspk = np.zeros_like(spk_binned)\n    smthdrate = np.zeros_like(pos_binned)\n    idx = pos_binned == 0\n    pos_binned[idx] = np.nan\n    spk_binned[idx] = np.nan\n    visited = np.zeros_like(pos_binned)\n    visited[pos_binned &gt; 0] = 1\n    # array to check which bins have made it\n    bincheck = np.isnan(pos_binned)\n    r = 1\n    while np.any(~bincheck):\n        # create the filter kernel\n        h = disk(r)\n        h[h &gt;= np.max(h) / 3.0] = 1\n        h[h != 1] = 0\n        if h.shape &gt;= pos_binned.shape:\n            break\n        # filter the arrays using astropys convolution\n        filtpos = convolution.convolve(pos_binned, h)\n        filtspk = convolution.convolve(spk_binned, h)\n        filtvisited = convolution.convolve(visited, h)\n        # get the bins which made it through this iteration\n        truebins = alpha / (np.sqrt(filtspk) * filtpos) &lt;= r\n        truebins = np.logical_and(truebins, ~bincheck)\n        # insert values where true\n        smthdpos[truebins] = filtpos[truebins] / filtvisited[truebins]\n        smthdspk[truebins] = filtspk[truebins] / filtvisited[truebins]\n        bincheck[truebins] = True\n        r += 1\n    smthdrate = smthdspk / smthdpos\n    smthdrate[idx] = np.nan\n    smthdspk[idx] = np.nan\n    smthdpos[idx] = np.nan\n    return smthdrate, smthdspk, smthdpos\n</code></pre>"},{"location":"reference/#ephysiopy.common.binning.RateMap.get_disperion_map","title":"<code>get_disperion_map(spk_times, pos_times)</code>","text":"<p>Attempt to write a faster version of creating an overdispersion map. A cell will sometimes fire too much or too little on a given run through its receptive field. This function quantifies that.</p> <p>This shows the amount of 'observed' variance in spiking around the mean spiking in a bin...</p> <p>Parameters:</p> Name Type Description Default <code>spk_times</code> <code>ndarray</code> required <code>pos_times</code> <code>ndarray</code> required <p>Returns:</p> Type Description <code>BinnedData - the overdispersion map in an instance of BinnedData</code> Source code in <code>ephysiopy/common/binning.py</code> <pre><code>def get_disperion_map(\n    self, spk_times: np.ndarray, pos_times: np.ndarray\n) -&gt; BinnedData | None:\n    \"\"\"\n    Attempt to write a faster version of creating an overdispersion\n    map. A cell will sometimes fire too much or too little on a given\n    run through its receptive field. This function quantifies that.\n\n    This shows the amount of 'observed' variance in spiking around\n    the mean spiking in a bin...\n\n    Parameters\n    ----------\n    spk_times (np.ndarray) - a vector of spike times (in seconds)\n    pos_times (np.ndarray) - vector of position times (seconds)\n\n    Returns\n    -------\n    BinnedData - the overdispersion map in an instance of BinnedData\n\n    \"\"\"\n    idx = np.searchsorted(pos_times, spk_times, side=\"right\")\n    spike_weights = np.bincount(idx, minlength=len(pos_times))\n    expected_spikes = self.get_map(spike_weights, map_type=MapType.SPK)\n    # bin_edges[1] is x, bin_edges[0] is y\n    x_bins = np.digitize(self.xy[0], expected_spikes.bin_edges[1][:-1]) - 1\n    y_bins = np.digitize(self.xy[1], expected_spikes.bin_edges[0][:-1]) - 1\n    map_shape = np.shape(expected_spikes.binned_data[0])\n    pos_bins_linear_idx = np.ravel_multi_index([y_bins, x_bins], map_shape)\n    expected_spikes_xy = expected_spikes.binned_data[0][y_bins, x_bins]\n    min_rate_threshold = np.nanmax(expected_spikes.binned_data[0]) * 0.25\n    x_bins_with_firing = x_bins[spike_weights &gt; 0]\n    y_bins_with_firing = y_bins[spike_weights &gt; 0]\n    bins_with_firing_linear_idx = np.ravel_multi_index(\n        [y_bins_with_firing, x_bins_with_firing], map_shape\n    )\n\n    observed_spikes = expected_spikes\n    return observed_spikes\n</code></pre>"},{"location":"reference/#ephysiopy.common.binning.RateMap.get_map","title":"<code>get_map(spk_weights, var_type=VariableToBin.XY, map_type=MapType.RATE, smoothing=True, **kwargs)</code>","text":"<p>Bins up the variable type var_type and returns a tuple of (rmap, binnedPositionDir) or (rmap, binnedPostionX, binnedPositionY)</p> <p>Parameters:</p> Name Type Description Default <code>spk_weights</code> <p>position weights. For example, if there were 5 positions recorded and a cell spiked once in position 2 and 5 times in position 3 and nothing anywhere else then pos_weights looks like: [0 0 1 5 0]. spk_weights can also be list-like where each entry in the list is a different set of weights - these are enumerated through in a list comp in the ._bin_data function. In this case the returned tuple will consist of a 2-tuple where the first entry is an array of the ratemaps (binned_spk / binned_pos) and the second part is the binned pos data (as it's common to all the spike weights)</p> required <code>var_type</code> <code>XY</code> <code>map_type</code> <code>RATE</code> <code>smoothing</code> <code>True</code> <p>Returns:</p> Type Description <code>binned_data (BinnedData): An instance of BinnedData containing the binned data, the bin edges, the variable binned and</code> <p>the map type. See ephysiopy.common.utils for details of the class.</p> Source code in <code>ephysiopy/common/binning.py</code> <pre><code>def get_map(\n    self,\n    spk_weights,\n    var_type=VariableToBin.XY,\n    map_type=MapType.RATE,\n    smoothing=True,\n    **kwargs\n) -&gt; BinnedData | None:\n    \"\"\"\n    Bins up the variable type var_type and returns a tuple of\n    (rmap, binnedPositionDir) or\n    (rmap, binnedPostionX, binnedPositionY)\n\n    Parameters\n    ----------\n    spk_weights (np.ndarray) - Shape equal to number of positions samples captured and consists of\n        position weights. For example, if there were 5 positions\n        recorded and a cell spiked once in position 2 and 5 times in\n        position 3 and nothing anywhere else then pos_weights looks\n        like: [0 0 1 5 0].\n        spk_weights can also be list-like where each entry in the list is a different set of\n        weights - these are enumerated through in a list comp in the ._bin_data function. In\n        this case the returned tuple will consist of a 2-tuple where the first entry is an\n        array of the ratemaps (binned_spk / binned_pos) and the second part is the binned pos data (as it's common to all\n        the spike weights)\n\n    var_type (Variable2Bin) - The variable to bin. See ephysiopy.common.utils for legal values.\n\n\n    map_type (MapType) - The kind of map returned. See ephysiopy.common.utils for legal values.\n\n    smoothing (bool, optional): Smooth the data or not. Default True.\n\n    Returns\n    -------\n    binned_data (BinnedData): An instance of BinnedData containing the binned data, the bin edges, the variable binned and\n                              the map type. See ephysiopy.common.utils for details of the class.\n    \"\"\"\n    boundary = \"extend\"\n    pos_weights = self.pos_weights\n    if var_type.value == VariableToBin.DIR.value:\n        sample = self.dir\n        boundary = \"wrap\"\n    elif var_type.value == VariableToBin.SPEED.value:\n        sample = self.speed\n    elif var_type.value == VariableToBin.XY.value:\n        sample = self.xy\n    elif var_type.value == VariableToBin.XY_TIME.value:\n        sample = np.concatenate(\n            (np.atleast_2d(self.xy), np.atleast_2d(self.pos_times))\n        )\n    elif var_type.value == VariableToBin.SPEED_DIR.value:\n        sample = np.concatenate(\n            (np.atleast_2d(self.dir), np.atleast_2d(self.speed))\n        )\n    elif var_type.value == VariableToBin.EGO_BOUNDARY.value:\n        arena_shape = kwargs.get(\"arena_shape\", \"circle\")\n        boundary = \"wrap\"\n        binsize = kwargs.get(\"binsize\", 5)\n        if isinstance(binsize, tuple):\n            binsize = binsize[0]\n        # breakpoint()\n        ego_angles, arena_xy = self._calc_ego_angles(arena_shape, binsize)\n        ego_dists = distance.cdist(arena_xy, self.xy.T, \"euclidean\")\n        sample = np.stack((np.ravel(ego_angles.T), np.ravel(ego_dists.T)))\n        spk_weights = np.atleast_2d(spk_weights)\n        spk_weights = np.tile(spk_weights, arena_xy.shape[0])\n        pos_weights = np.tile(self.pos_weights, arena_xy.shape[0])\n        hist_range = (0, 50), (0, 2 * np.pi)\n\n        if \"range\" not in kwargs.keys():\n            kwargs[\"range\"] = hist_range\n    else:\n        raise ValueError(\"Unrecognized variable to bin.\")\n\n    assert sample is not None\n\n    self.var2Bin = var_type\n    binsize = kwargs.pop(\"binsize\", self.binsize)\n    hist_range = kwargs.pop(\"range\", None)\n\n    if hist_range is None:\n        bin_edges = self._calc_bin_edges(binsize)\n    else:\n        bin_edges = None\n    binned_pos, binned_pos_edges = self._bin_data(sample, bin_edges, pos_weights)\n    binned_pos = binned_pos / self.PosCalcs.sample_rate\n    nanIdx = binned_pos == 0\n    pos = BinnedData(var_type, MapType.POS, [binned_pos], binned_pos_edges)\n\n    if map_type.value == MapType.POS.value:  # return binned up position\n        if smoothing:\n            sm_pos = blur_image(\n                pos,\n                self.smooth_sz,\n                ftype=self.smoothingType,\n                boundary=boundary,\n                **kwargs\n            )\n            sm_pos.set_nan_indices(nanIdx)\n            return sm_pos\n        else:\n            pos.set_nan_indices(nanIdx)\n            return pos\n\n    binned_spk, _ = self._bin_data(sample, bin_edges, spk_weights)\n    if not isinstance(binned_spk, list):\n        binned_spk = [binned_spk]\n    spk = BinnedData(var_type, MapType.SPK, binned_spk, binned_pos_edges)\n\n    if map_type.value == MapType.SPK.value:\n        if smoothing:\n            return blur_image(\n                spk,\n                self.smooth_sz,\n                ftype=self.smoothingType,\n                boundary=boundary,\n                **kwargs\n            )\n        else:\n            return spk\n    if map_type.value == MapType.ADAPTIVE.value:\n        alpha = kwargs.pop(\"alpha\", 4)\n        # deal with a stack of binned maps\n        if binned_spk.ndim == 3:\n            smthd_rate = []\n            for i in range(binned_spk.shape[0]):\n                smthd_rate.append(\n                    self.getAdaptiveMap(binned_pos, binned_spk[i, ...], alpha)[0]\n                )\n        else:\n            smthd_rate, _, _ = self.getAdaptiveMap(binned_pos, binned_spk, alpha)\n        return BinnedData(var_type, map_type, smthd_rate, binned_pos_edges)\n\n    if not smoothing:\n        rmap = spk / pos\n        rmap.map_type = MapType.RATE\n        rmap.set_nan_indices(nanIdx)\n        return rmap\n\n    if \"after\" in self.whenToSmooth:\n        rmap = spk / pos\n        rmap = blur_image(\n            rmap,\n            self.smooth_sz,\n            ftype=self.smoothingType,\n            boundary=boundary,\n            **kwargs\n        )\n    else:  # default case\n        sm_pos = blur_image(\n            pos,\n            self.smooth_sz,\n            ftype=self.smoothingType,\n            boundary=boundary,\n            **kwargs\n        )\n        sm_spk = blur_image(\n            spk,\n            self.smooth_sz,\n            ftype=self.smoothingType,\n            boundary=boundary,\n            **kwargs\n        )\n        rmap = sm_spk / sm_pos\n    rmap.set_nan_indices(nanIdx)\n    return rmap\n</code></pre>"},{"location":"reference/#ephysiopy.common.binning.RateMap.tWinSAC","title":"<code>tWinSAC(xy, spkIdx, ppm=365, winSize=10, pos_sample_rate=50, nbins=71, boxcar=5, Pthresh=100, downsampfreq=50)</code>","text":"<p>Performs a temporal windowed spatial autocorrelation.</p> <p>Parameters:</p> Name Type Description Default <code>xy</code> required <code>spkIdx</code> required <code>ppm</code> <code>365</code> <code>winSize</code> <code>10</code> <code>pos_sample_rate</code> <code>50</code> <code>nbins</code> <code>71</code> <code>boxcar</code> <code>5</code> <code>Pthresh</code> <code>100</code> <code>downsampfreq</code> <code>50</code> <p>Returns:</p> Type Description <code>H (array_like): The temporal windowed SAC</code> Source code in <code>ephysiopy/common/binning.py</code> <pre><code>def tWinSAC(\n    self,\n    xy,\n    spkIdx,\n    ppm=365,\n    winSize=10,\n    pos_sample_rate=50,\n    nbins=71,\n    boxcar=5,\n    Pthresh=100,\n    downsampfreq=50,\n) -&gt; BinnedData:\n    \"\"\"\n    Performs a temporal windowed spatial autocorrelation.\n\n    Parameters\n    ----------\n    xy (array_like): The position data\n    spkIdx (array_like): The indices in xy where the cell fired\n    ppm (int, optional): The camera pixels per metre. Default 365\n    winSize (int, optional): The window size for the temporal search\n    pos_sample_rate (int, optional): The rate at which position was sampled. Default 50\n    nbins (int, optional): The number of bins for creating the resulting ratemap. Default 71\n    boxcar (int, optional): The size of the smoothing kernel to smooth ratemaps. Default 5\n    Pthresh (int, optional): The cut-off for values in the ratemap; values &lt; Pthresh become nans. Default 100\n    downsampfreq (int, optional): How much to downsample. Default 50\n\n    Returns\n    -------\n    H (array_like): The temporal windowed SAC\n    \"\"\"\n    # [Stage 0] Get some numbers\n    xy = xy / ppm * 100\n    n_samps = xy.shape[1]\n    n_spks = len(spkIdx)\n    winSizeBins = np.min([winSize * pos_sample_rate, n_samps])\n    # factor by which positions are downsampled\n    downsample = np.ceil(pos_sample_rate / downsampfreq)\n    Pthresh = Pthresh / downsample  # take account of downsampling\n\n    # [Stage 1] Calculate number of spikes in the window for each spikeInd\n    # (ignoring spike itself)\n    # 1a. Loop preparation\n    nSpikesInWin = np.zeros(n_spks, dtype=int)\n\n    # 1b. Keep looping until we have dealt with all spikes\n    for i, s in enumerate(spkIdx):\n        t = np.searchsorted(spkIdx, (s, s + winSizeBins))\n        nSpikesInWin[i] = len(spkIdx[t[0] : t[1]]) - 1  # ignore ith spike\n\n    # [Stage 2] Prepare for main loop\n    # 2a. Work out offset inidices to be used when storing spike data\n    off_spike = np.cumsum([nSpikesInWin])\n    off_spike = np.pad(off_spike, (1, 0), \"constant\", constant_values=(0))\n\n    # 2b. Work out number of downsampled pos bins in window and\n    # offset indices for storing data\n    nPosInWindow = np.minimum(winSizeBins, n_samps - spkIdx)\n    nDownsampInWin = np.floor((nPosInWindow - 1) / downsample) + 1\n\n    off_dwell = np.cumsum(nDownsampInWin.astype(int))\n    off_dwell = np.pad(off_dwell, (1, 0), \"constant\", constant_values=(0))\n\n    # 2c. Pre-allocate dwell and spike arrays, singles for speed\n    dwell = np.zeros((2, off_dwell[-1]), dtype=np.single) * np.nan\n    spike = np.zeros((2, off_spike[-1]), dtype=np.single) * np.nan\n\n    filled_pvals = 0\n    filled_svals = 0\n\n    for i in range(n_spks):\n        # calculate dwell displacements\n        winInd_dwell = np.arange(\n            spkIdx[i] + 1,\n            np.minimum(spkIdx[i] + winSizeBins, n_samps),\n            downsample,\n            dtype=int,\n        )\n        WL = len(winInd_dwell)\n        dwell[:, filled_pvals : filled_pvals + WL] = np.rot90(\n            np.array(np.rot90(xy[:, winInd_dwell]) - xy[:, spkIdx[i]])\n        )\n        filled_pvals = filled_pvals + WL\n        # calculate spike displacements\n        winInd_spks = (\n            i + np.nonzero(spkIdx[i + 1 : n_spks] &lt; spkIdx[i] + winSizeBins)[0]\n        )\n        WL = len(winInd_spks)\n        spike[:, filled_svals : filled_svals + WL] = np.rot90(\n            np.array(np.rot90(xy[:, spkIdx[winInd_spks]]) - xy[:, spkIdx[i]])\n        )\n        filled_svals = filled_svals + WL\n\n    dwell = np.delete(dwell, np.isnan(dwell).nonzero()[1], axis=1)\n    spike = np.delete(spike, np.isnan(spike).nonzero()[1], axis=1)\n\n    dwell = np.hstack((dwell, -dwell))\n    spike = np.hstack((spike, -spike))\n\n    dwell_min = np.min(dwell, axis=1)\n    dwell_max = np.max(dwell, axis=1)\n\n    binsize = (dwell_max[1] - dwell_min[1]) / nbins\n\n    dwell = np.round(\n        (dwell - np.ones_like(dwell) * dwell_min[:, np.newaxis]) / binsize\n    )\n    spike = np.round(\n        (spike - np.ones_like(spike) * dwell_min[:, np.newaxis]) / binsize\n    )\n\n    binsize = np.max(dwell, axis=1).astype(int)\n    binedges = np.array(((-0.5, -0.5), binsize + 0.5)).T\n    Hp, Hpe_y, Hpe_x = np.histogram2d(\n        dwell[0, :], dwell[1, :], range=binedges, bins=binsize\n    )\n    Hs, Hse_y, Hse_x = np.histogram2d(\n        spike[0, :], spike[1, :], range=binedges, bins=binsize\n    )\n\n    # reverse y,x order\n    Hp = np.swapaxes(Hp, 1, 0)\n    Hs = np.swapaxes(Hs, 1, 0)\n\n    Hp = BinnedData(VariableToBin.XY, MapType.RATE, [Hp], [Hpe_x, Hpe_y])\n    Hs = BinnedData(VariableToBin.XY, MapType.RATE, [Hs], [Hse_x, Hse_y])\n\n    # smooth the maps\n    fHp = blur_image(Hp, boxcar)\n    fHs = blur_image(Hs, boxcar)\n\n    H = fHs / fHp\n    H.set_nan_indices(Hp.binned_data[0] &lt; Pthresh)\n    return H\n</code></pre>"},{"location":"reference/#field-calculations","title":"Field calculations","text":""},{"location":"reference/#ephysiopy.common.fieldcalcs.PROP_VALS","title":"<code>PROP_VALS = set(PROPS.values())</code>  <code>module-attribute</code>","text":"<p>A custom class for dealing with segments of an LFP signal and how they relate to specific runs (see RunProps below) through a  receptive field (see FieldProps below)</p>"},{"location":"reference/#ephysiopy.common.fieldcalcs.FieldProps","title":"<code>FieldProps</code>","text":"<p>               Bases: <code>RegionProperties</code></p> Source code in <code>ephysiopy/common/fieldcalcs.py</code> <pre><code>class FieldProps(RegionProperties):\n    def __init__(\n        self,\n        slice,\n        label,\n        label_image,\n        binned_data,\n        cache,\n        *,\n        extra_properties,\n        spacing,\n        offset,\n        index=0,\n    ):\n        intensity_image = binned_data.binned_data[index]\n        super().__init__(\n            slice,\n            label,\n            label_image,\n            intensity_image,\n            cache_active=cache,\n            spacing=spacing,\n            extra_properties=extra_properties,\n            offset=offset,\n        )\n        self.binned_data = binned_data\n        self._runs = []\n\n    @property\n    def runs(self):\n        return self._runs\n\n    @property\n    def run_slices(self):\n        return [r._slice for r in self._runs]\n\n    @runs.setter\n    def runs(self, r):\n        self._runs = r\n        print(f\"Field {self.label} has {len(r)} potential runs\")\n\n    @property\n    def run_labels(self):\n        return np.array([r.label for r in self.runs])\n\n    def __getattr__(self, attr):\n        if self._intensity_image is None and attr in _require_intensity_image:\n            raise AttributeError(\n                f\"Attribute '{attr}' unavailable when `intensity_image` \"\n                f\"has not been specified.\"\n            )\n        if attr in self._extra_properties:\n            func = self._extra_properties[attr]\n            n_args = _infer_number_of_required_args(func)\n            # determine whether func requires intensity image\n            if n_args == 2:\n                if self._intensity_image is not None:\n                    if self._multichannel:\n                        multichannel_list = [\n                            func(self.image, self.image_intensity[..., i])\n                            for i in range(self.image_intensity.shape[-1])\n                        ]\n                        return np.stack(multichannel_list, axis=-1)\n                    else:\n                        return func(self.image, self.image_intensity)\n                else:\n                    raise AttributeError(\n                        f\"intensity image required to calculate {attr}\"\n                    )\n            elif n_args == 3:\n                if self._intensity_image is not None:\n                    return func(\n                        self.image,\n                        self.image_intensity,\n                        self.xy_coords,\n                    )\n                else:\n                    raise AttributeError(\n                        f\"intensity image required to calculate {attr}\"\n                    )\n            elif n_args == 1:\n                return func(self.image)\n            else:\n                raise AttributeError(\n                    f\"Custom regionprop function's number of arguments must \"\n                    f\"be 1, 2 or 3 but {attr} takes {n_args} arguments.\"\n                )\n        elif attr in PROPS and attr.lower() == attr:\n            if (\n                self._intensity_image is None\n                and PROPS[attr] in _require_intensity_image\n            ):\n                raise AttributeError(\n                    f\"Attribute '{attr}' unavailable when `intensity_image` \"\n                    f\"has not been specified.\"\n                )\n            # retrieve deprecated property (excluding old CamelCase ones)\n            return getattr(self, PROPS[attr])\n        else:\n            raise AttributeError(f\"'{type(self)}' object has no attribute '{attr}'\")\n\n    def __str__(self):\n        \"\"\"\n\n        Override the string representation printed to the console\n        \"\"\"\n        return f\"Field {self.label} has {len(self.runs)} runs\"\n\n    # The maximum index of the intensity image for the region\n    @property\n    def max_index(self) -&gt; np.ndarray:\n        return np.array(\n            np.unravel_index(\n                np.nanargmax(self.image_intensity, axis=None),\n                self.image_intensity.shape,\n            )\n        )\n\n    @property\n    def num_runs(self) -&gt; int:\n        return len(self.runs)\n\n    @property\n    def cumulative_time(self) -&gt; np.ndarray:\n        return np.concatenate([r.cumulative_time for r in self.runs])\n\n    @property\n    def runs_speed(self) -&gt; np.ndarray:\n        return np.concatenate([r._speed for r in self.runs])\n\n    @property\n    def runs_observed_spikes(self) -&gt; np.ndarray:\n        return np.concatenate([r.observed_spikes for r in self.runs])\n\n    def runs_expected_spikes(\n        self, expected_rate_at_pos: np.ndarray, sample_rate: int = 50\n    ) -&gt; np.ndarray:\n        return np.concatenate(\n            [r.expected_spikes(expected_rate_at_pos, sample_rate) for r in self.runs]\n        )\n\n    @property\n    def spike_position_index(self):\n        return np.concatenate([r.spike_position_index for r in self.runs])\n\n    # The x-y coordinate at the field peak\n    @property\n    def xy_at_peak(self) -&gt; np.ndarray:\n        mi = self.max_index\n        x_max = self.binned_data.bin_edges[1][mi[1] + self.slice[1].start]\n        y_max = self.binned_data.bin_edges[0][mi[0] + self.slice[0].start]\n        return np.array([x_max, y_max])\n\n    @property\n    def xy_coords(self) -&gt; np.ndarray:\n        return np.concatenate([r.xy.T for r in self.runs]).T\n\n    # The x-y coordinates zeroed with respect to the peak\n    @property\n    def xy_relative_to_peak(self) -&gt; np.ndarray:\n        return (self.xy_coords.T - self.xy_at_peak).T\n\n    # The angle each x-y coordinate makes to the field peak\n    @property\n    def xy_angle_to_peak(self) -&gt; np.ndarray:\n        xy_to_peak = self.xy_relative_to_peak\n        return np.arctan2(xy_to_peak[1], xy_to_peak[0])\n\n    # The distance of each x-y coordinate to the field peak\n    @property\n    def xy_dist_to_peak(self) -&gt; np.ndarray:\n        xy_to_peak = self.xy_relative_to_peak\n        return np.hypot(xy_to_peak[0], xy_to_peak[1])\n\n    # The perimeter of the masked region as an array of bool\n    @property\n    def bw_perim(self) -&gt; np.ndarray:\n        return bwperim(self.image)\n\n    @property\n    def perimeter_coords(self) -&gt; tuple:\n        return np.nonzero(self.bw_perim)\n\n    @property\n    def global_perimeter_coords(self) -&gt; np.ndarray:\n        perim_xy = self.perimeter_coords\n        x = self.binned_data.bin_edges[1][perim_xy[1] + self.slice[1].start]\n        y = self.binned_data.bin_edges[0][perim_xy[0] + self.slice[0].start]\n        return np.array([x, y])\n\n    def perimeter_minus_field_max(self) -&gt; np.ndarray:\n        mi = self.max_index\n        perimeter_coords = self.perimeter_coords\n        return np.array([perimeter_coords[0] - mi[0], perimeter_coords[1] - mi[1]])\n\n    # The angle each point on the perimeter makes to the field peak\n    @property\n    def perimeter_angle_from_peak(self) -&gt; np.ndarray:\n        perimeter_minus_field_max = self.perimeter_minus_field_max()\n        return np.arctan2(perimeter_minus_field_max[0], perimeter_minus_field_max[1])\n\n    # The distance of each point on the perimeter to the field peak\n    @property\n    def perimeter_dist_from_peak(self) -&gt; np.ndarray:\n        perimeter_minus_field_max = self.perimeter_minus_field_max()\n        return np.hypot(perimeter_minus_field_max[0], perimeter_minus_field_max[1])\n\n    @property\n    def bin_coords(self) -&gt; np.ndarray:\n        bin_edges = self.binned_data.bin_edges\n        return np.array(\n            [bin_edges[1][self.coords[:, 1]], bin_edges[0][self.coords[:, 0]]]\n        )\n\n    @property\n    def phi(self) -&gt; np.ndarray:\n        \"\"\"\n        Calculate the angular distance between the mean direction of each run and\n        each position samples direction to the field centre\n        \"\"\"\n        return np.concatenate([r.phi for r in self.runs]).T\n\n    @property\n    def rho(self) -&gt; np.ndarray:\n        return np.concatenate([r.rho for r in self.runs])\n\n    @property\n    def pos_xy(self) -&gt; np.ndarray:\n        return np.concatenate([r.pos_xy.T for r in self.runs]).T\n\n    @property\n    def pos_phi(self) -&gt; np.ndarray:\n        \"\"\"\n        Calculate the angular distance between the mean direction of each run and\n        each position samples direction to the field centre\n        \"\"\"\n        return np.concatenate([r.pos_phi for r in self.runs]).T\n\n    @property\n    def pos_r(self) -&gt; np.ndarray:\n        \"\"\"\n        Calculate the ratio of the distance from the field peak to the position sample\n        and the distance from the field peak to the point on the perimeter that is most\n        colinear with the position sample\n\n        NB The values just before being returned can be &gt;= 1 so these are capped to 1\n        \"\"\"\n        return np.concatenate([r.pos_r for r in self.runs])\n\n    @property\n    def r_and_phi_to_x_and_y(self) -&gt; np.ndarray:\n        return np.vstack(pol2cart(self.pos_r, self.pos_phi))\n\n    def r_per_run(self) -&gt; np.ndarray:\n        perimeter_coords = self.perimeter_coords\n        return np.concatenate(\n            [\n                run.r(\n                    self.xy_at_peak,\n                    self.perimeter_angle_from_peak,\n                    perimeter_coords,\n                    self.max_index,\n                )\n                for run in self.runs\n            ]\n        )\n\n    @property\n    def current_direction(self) -&gt; np.ndarray:\n        return np.concatenate([r.current_direction for r in self.runs])\n\n    @property\n    def cumulative_distance(self) -&gt; np.ndarray:\n        return np.concatenate([r.cumulative_distance for r in self.runs])\n\n    @property\n    def projected_direction(self) -&gt; np.ndarray:\n        \"\"\"\n        direction projected onto the mean run direction is just the x-coord\n        when cartesian x and y is converted to from polar rho and phi\n        \"\"\"\n        return np.concatenate([r.pos_xy[0] for r in self.runs])\n\n    def overdispersion(\n        self, spike_train: np.ndarray, sample_rate: int = 50\n    ) -&gt; np.ndarray:\n        return np.array([r.overdispersion(spike_train, sample_rate) for r in self.runs])\n\n    # Over-ride the next intensity_* functions so they use the\n    # nan versions\n    @property\n    def intensity_max(self) -&gt; float:\n        vals = self.image_intensity[self.image]\n        return np.nanmax(vals, axis=0).astype(np.float64, copy=False)\n\n    @property\n    def intensity_mean(self) -&gt; float:\n        return np.nanmean(self.image_intensity[self.image], axis=0)\n\n    @property\n    def intensity_min(self) -&gt; float:\n        vals = self.image_intensity[self.image]\n        return np.nanmin(vals, axis=0).astype(np.float64, copy=False)\n\n    @property\n    def intensity_std(self) -&gt; float:\n        vals = self.image_intensity[self.image]\n        return np.nanstd(vals, axis=0)\n\n    def smooth_runs(self, k: float, spatial_lp_cut: int, sample_rate: int):\n        \"\"\"\n        Smooth in x and y in preparation for converting the smoothed cartesian\n        coordinates to polar ones\n\n        Parameters\n        ----------\n        k (float) - smoothing constant for the instantaneous firing rate\n        spatial_lp_cut (int) - spatial lowpass cut off\n        sample_rate (int) - position sample rate in Hz\n        \"\"\"\n        [r.smooth_xy(k, spatial_lp_cut, sample_rate) for r in self.runs]\n</code></pre>"},{"location":"reference/#ephysiopy.common.fieldcalcs.FieldProps.phi","title":"<code>phi</code>  <code>property</code>","text":"<p>Calculate the angular distance between the mean direction of each run and each position samples direction to the field centre</p>"},{"location":"reference/#ephysiopy.common.fieldcalcs.FieldProps.pos_phi","title":"<code>pos_phi</code>  <code>property</code>","text":"<p>Calculate the angular distance between the mean direction of each run and each position samples direction to the field centre</p>"},{"location":"reference/#ephysiopy.common.fieldcalcs.FieldProps.pos_r","title":"<code>pos_r</code>  <code>property</code>","text":"<p>Calculate the ratio of the distance from the field peak to the position sample and the distance from the field peak to the point on the perimeter that is most colinear with the position sample</p> <p>NB The values just before being returned can be &gt;= 1 so these are capped to 1</p>"},{"location":"reference/#ephysiopy.common.fieldcalcs.FieldProps.projected_direction","title":"<code>projected_direction</code>  <code>property</code>","text":"<p>direction projected onto the mean run direction is just the x-coord when cartesian x and y is converted to from polar rho and phi</p>"},{"location":"reference/#ephysiopy.common.fieldcalcs.FieldProps.__str__","title":"<code>__str__()</code>","text":"<p>Override the string representation printed to the console</p> Source code in <code>ephysiopy/common/fieldcalcs.py</code> <pre><code>def __str__(self):\n    \"\"\"\n\n    Override the string representation printed to the console\n    \"\"\"\n    return f\"Field {self.label} has {len(self.runs)} runs\"\n</code></pre>"},{"location":"reference/#ephysiopy.common.fieldcalcs.FieldProps.smooth_runs","title":"<code>smooth_runs(k, spatial_lp_cut, sample_rate)</code>","text":"<p>Smooth in x and y in preparation for converting the smoothed cartesian coordinates to polar ones</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>float</code> required <code>spatial_lp_cut</code> <code>int</code> required <code>sample_rate</code> <code>int</code> required Source code in <code>ephysiopy/common/fieldcalcs.py</code> <pre><code>def smooth_runs(self, k: float, spatial_lp_cut: int, sample_rate: int):\n    \"\"\"\n    Smooth in x and y in preparation for converting the smoothed cartesian\n    coordinates to polar ones\n\n    Parameters\n    ----------\n    k (float) - smoothing constant for the instantaneous firing rate\n    spatial_lp_cut (int) - spatial lowpass cut off\n    sample_rate (int) - position sample rate in Hz\n    \"\"\"\n    [r.smooth_xy(k, spatial_lp_cut, sample_rate) for r in self.runs]\n</code></pre>"},{"location":"reference/#ephysiopy.common.fieldcalcs.RunProps","title":"<code>RunProps</code>","text":"<p>               Bases: <code>object</code></p> Source code in <code>ephysiopy/common/fieldcalcs.py</code> <pre><code>class RunProps(object):\n    def __init__(\n        self,\n        label,\n        slice,\n        xy_coords,\n        spike_count,\n        speed,\n        peak_xy,\n        max_index,\n        perimeter_coords,\n    ):\n        assert xy_coords.shape[1] == len(spike_count)\n        self.label = label\n        self._xy_coords = xy_coords\n        self._slice = slice\n        self._spike_count = spike_count\n        self._speed = speed\n        self._peak_xy = peak_xy\n        self._max_index = max_index\n        self._perimeter_coords = perimeter_coords\n        self.xy_is_smoothed = False\n\n    def __str__(self):\n        return f\"id: {self.label}: {np.sum(self._spike_count)} spikes\"\n\n    @property\n    def xy(self):\n        return self._xy_coords\n\n    @xy.setter\n    def xy(self, val):\n        self._xy_coords = val\n\n    @property\n    def hdir(self):\n        d = np.arctan2(np.diff(self.xy[1]), np.diff(self.xy[0]))\n        d = np.append(d, d[-1])\n        return np.rad2deg(d)\n\n    @property\n    def min_speed(self):\n        return np.nanmin(self._speed)\n\n    def __len__(self):\n        return self._slice.stop - self._slice.start\n\n    @property\n    def cumulative_time(self) -&gt; np.ndarray:\n        return np.arange(len(self))\n\n    @property\n    def duration(self):\n        return self._slice.stop - self._slice.start\n\n    @property\n    def n_spikes(self):\n        return np.nansum(self._spike_count)\n\n    @property\n    def run_start(self):\n        return self._slice.start\n\n    @property\n    def run_stop(self):\n        return self._slice.stop\n\n    @property\n    def mean_direction(self):\n        return stats.circmean(np.deg2rad(self.hdir))\n\n    @property\n    def current_direction(self):\n        return self.rho * np.cos(np.deg2rad(self.hdir) - self.phi)\n\n    @property\n    def cumulative_distance(self):\n        d = np.sqrt(np.abs(np.diff(np.power(self.rho, 2))))\n        d = np.insert(d, 0, 0)\n        return np.cumsum(d)\n\n    @property\n    def spike_position_index(self):\n        return np.take(\n            range(self._slice.start, self._slice.stop), repeat_ind(self._spike_count)\n        )\n\n    @property\n    def observed_spikes(self):\n        return self._spike_count\n\n    def expected_spikes(\n        self, expected_rate_at_pos: np.ndarray, sample_rate: int = 50\n    ) -&gt; np.ndarray:\n        return expected_rate_at_pos[self._slice] / sample_rate\n\n    def overdispersion(self, spike_train: np.ndarray, sample_rate: int = 50) -&gt; float:\n        obs_spikes = np.sum(self._spike_count)\n        expt_spikes = np.sum(spike_train[self._slice]) / sample_rate\n        Z = np.nan\n        if obs_spikes &gt;= expt_spikes:\n            Z = (obs_spikes - expt_spikes - 0.5) / np.sqrt(expt_spikes)\n        else:\n            Z = (obs_spikes - expt_spikes + 0.5) / np.sqrt(expt_spikes)\n        return Z\n\n    def smooth_xy(self, k: float, spatial_lp_cut: int, sample_rate: int):\n        \"\"\"\n        Smooth in x and y in preparation for converting the smoothed cartesian\n        coordinates to polar ones\n\n        Parameters\n        ----------\n        k (float) - smoothing constant for the instantaneous firing rate\n        spatial_lp_cut (int) - spatial lowpass cut off\n        sample_rate (int) - position sample rate in Hz\n        \"\"\"\n        f_len = np.floor((self.run_stop - self.run_start) * k) + 1\n        h = signal.firwin(\n            int(f_len),\n            fs=sample_rate,\n            cutoff=spatial_lp_cut / sample_rate * 2,\n            window=\"blackman\",\n        )\n        padlen = 2 * len(h)\n        if padlen == self.xy.shape[1]:\n            padlen = padlen - 1\n        self.xy = signal.filtfilt(h, [1], self.xy, padlen=padlen, axis=1)\n        self.xy_is_smoothed = True\n\n    @property\n    def xy_angle_to_peak(self):\n        xy_to_peak = (self.xy.T - self._peak_xy).T\n        return np.arctan2(xy_to_peak[1], xy_to_peak[0])\n\n    @property\n    def xy_dist_to_peak(self):\n        xy_to_peak = (self.xy.T - self._peak_xy).T\n        return np.hypot(xy_to_peak[0], xy_to_peak[1])\n\n    @property\n    def xy_dist_to_peak_normed(self):\n        x_y = self.r_and_phi_to_x_and_y\n        return np.hypot(x_y[0], x_y[1])\n\n    def perimeter_minus_field_max(self):\n        mi = self._max_index\n        perimeter_coords = self._perimeter_coords\n        return (\n            perimeter_coords[0] - mi[0],\n            perimeter_coords[1] - mi[1],\n        )\n\n    def perimeter_angle_from_peak(self):\n        perimeter_minus_field_max = self.perimeter_minus_field_max()\n        return np.arctan2(perimeter_minus_field_max[0], perimeter_minus_field_max[1])\n\n    @property\n    def pos_xy(self):\n        pos_x, pos_y = pol2cart(self.pos_r, self.pos_phi)\n        return np.vstack([pos_x, pos_y])\n\n    @property\n    def pos_r(self):\n        angle_df = circ_abs(\n            self.perimeter_angle_from_peak()[:, np.newaxis]\n            - self.xy_angle_to_peak[np.newaxis, :]\n        )\n        perimeter_idx = np.argmin(angle_df, 0)\n        tmp = (\n            self._perimeter_coords[1][perimeter_idx] - self._max_index[1],\n            self._perimeter_coords[0][perimeter_idx] - self._max_index[0],\n        )\n\n        perimeter_dist_to_peak = np.hypot(tmp[0], tmp[1])\n        r = self.xy_dist_to_peak / perimeter_dist_to_peak\n        capped_vals = r &gt;= 1\n        r[capped_vals] = 1\n        return r\n\n    # calculate the angular distance between the runs main direction and the\n    # pos's direction to the peak centre\n    @property\n    def pos_phi(self):\n        return self.xy_angle_to_peak - self.mean_direction\n\n    @property\n    def rho(self):\n        rho, _ = cart2pol(self.pos_xy[0], self.pos_xy[1])\n        return rho\n\n    @property\n    def phi(self):\n        _, phi = cart2pol(self.pos_xy[0], self.pos_xy[1])\n        return phi\n\n    @property\n    def r_and_phi_to_x_and_y(self):\n        return np.vstack(pol2cart(self.r, self.phi))\n\n    \"\"\"\n    Define a measure of tortuosity to see how direct the run was\n    from field entry to exit. It's jsut the ratio of the distance between\n    a straight line joining the entry-exit points and the actual distance\n    of the run\n    \"\"\"\n\n    @property\n    def tortuosity(self):\n        direct_line_distance = np.hypot(\n            self.xy[0, 0] - self.xy[0, -1], self.xy[1, 0] - self.xy[1, -1]\n        )\n        xy_df = np.diff(self.xy)\n        traversed_distance = np.sum(np.hypot(xy_df[0], xy_df[1]))\n        return direct_line_distance / traversed_distance\n</code></pre>"},{"location":"reference/#ephysiopy.common.fieldcalcs.RunProps.smooth_xy","title":"<code>smooth_xy(k, spatial_lp_cut, sample_rate)</code>","text":"<p>Smooth in x and y in preparation for converting the smoothed cartesian coordinates to polar ones</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>float</code> required <code>spatial_lp_cut</code> <code>int</code> required <code>sample_rate</code> <code>int</code> required Source code in <code>ephysiopy/common/fieldcalcs.py</code> <pre><code>def smooth_xy(self, k: float, spatial_lp_cut: int, sample_rate: int):\n    \"\"\"\n    Smooth in x and y in preparation for converting the smoothed cartesian\n    coordinates to polar ones\n\n    Parameters\n    ----------\n    k (float) - smoothing constant for the instantaneous firing rate\n    spatial_lp_cut (int) - spatial lowpass cut off\n    sample_rate (int) - position sample rate in Hz\n    \"\"\"\n    f_len = np.floor((self.run_stop - self.run_start) * k) + 1\n    h = signal.firwin(\n        int(f_len),\n        fs=sample_rate,\n        cutoff=spatial_lp_cut / sample_rate * 2,\n        window=\"blackman\",\n    )\n    padlen = 2 * len(h)\n    if padlen == self.xy.shape[1]:\n        padlen = padlen - 1\n    self.xy = signal.filtfilt(h, [1], self.xy, padlen=padlen, axis=1)\n    self.xy_is_smoothed = True\n</code></pre>"},{"location":"reference/#ephysiopy.common.fieldcalcs._get_field_labels","title":"<code>_get_field_labels(A, **kwargs)</code>","text":"<p>Returns a labeled version of A after finding the peaks in A and finding the watershed basins from the markers found from those peaks. Used in field_props() and grid_field_props()</p> <p>Args:     A (np.ndarray): The array to process     min_distance (float, optional): The distance in bins between fields to     separate the regions of the image     clear_border (bool, optional): Input to skimage.feature.peak_local_max.     The number of         pixels to ignore at the edge of the image</p> Source code in <code>ephysiopy/common/fieldcalcs.py</code> <pre><code>def _get_field_labels(A: np.ndarray, **kwargs) -&gt; tuple:\n    \"\"\"\n    Returns a labeled version of A after finding the peaks\n    in A and finding the watershed basins from the markers\n    found from those peaks. Used in field_props() and\n    grid_field_props()\n\n    Args:\n        A (np.ndarray): The array to process\n        min_distance (float, optional): The distance in bins between fields to\n        separate the regions of the image\n        clear_border (bool, optional): Input to skimage.feature.peak_local_max.\n        The number of\n            pixels to ignore at the edge of the image\n    \"\"\"\n    clear_border = True\n    if \"clear_border\" in kwargs:\n        clear_border = kwargs.pop(\"clear_border\")\n\n    min_distance = 1\n    if \"min_distance\" in kwargs:\n        min_distance = kwargs.pop(\"min_distance\")\n\n    A[~np.isfinite(A)] = -1\n    A[A &lt; 0] = -1\n    Ac_r = skimage.exposure.rescale_intensity(\n        A, in_range=\"image\", out_range=(0, 1000)\n    ).astype(np.int32)\n    peak_coords = skimage.feature.peak_local_max(\n        Ac_r, min_distance=min_distance, exclude_border=clear_border\n    )\n    peaksMask = np.zeros_like(A, dtype=bool)\n    peaksMask[tuple(peak_coords.T)] = True\n    peaksLabel, _ = ndi.label(peaksMask)\n    ws = watershed(image=-1 * A, markers=peaksLabel)\n    return peak_coords, ws\n</code></pre>"},{"location":"reference/#ephysiopy.common.fieldcalcs.border_score","title":"<code>border_score(A, B=None, shape='square', fieldThresh=0.3, circumPrc=0.2, binSize=3.0, minArea=200)</code>","text":"<p>Calculates a border score totally dis-similar to that calculated in Solstad et al (2008)</p> <p>Args:     A (array_like): Should be the ratemap     B (array_like): This should be a boolean mask where True (1)         is equivalent to the presence of a border and False (0)         is equivalent to 'open space'. Naievely this will be the         edges of the ratemap but could be used to take account of         boundary insertions/ creations to check tuning to multiple         environmental boundaries. Default None: when the mask is         None then a mask is created that has 1's at the edges of the         ratemap i.e. it is assumed that occupancy = environmental         shape     shape (str): description of environment shape. Currently         only 'square' or 'circle' accepted. Used to calculate the         proportion of the environmental boundaries to examine for         firing     fieldThresh (float): Between 0 and 1 this is the percentage         amount of the maximum firing rate         to remove from the ratemap (i.e. to remove noise)     smthKernSig (float): the sigma value used in smoothing the ratemap         (again!) with a gaussian kernel     circumPrc (float): The percentage amount of the circumference         of the environment that the field needs to be to count         as long enough to make it through     binSize (float): bin size in cm     minArea (float): min area for a field to be considered     debug (bool): If True then some plots and text will be output</p> <p>Returns:     float: the border score</p> <p>Notes:     If the cell is a border cell (BVC) then we know that it should     fire at a fixed distance from a given boundary (possibly more     than one). In essence this algorithm estimates the amount of     variance in this distance i.e. if the cell is a border cell this     number should be small. This is achieved by first doing a bunch of     morphological operations to isolate individual fields in the     ratemap (similar to the code used in phasePrecession.py - see     the partitionFields method therein). These partitioned fields are then     thinned out (using skimage's skeletonize) to a single pixel     wide field which will lie more or less in the middle of the     (highly smoothed) sub-field. It is the variance in distance from the     nearest boundary along this pseudo-iso-line that is the boundary     measure</p> <pre><code>Other things to note are that the pixel-wide field has to have some\nminimum length. In the case of a circular environment this is set to\n20% of the circumference; in the case of a square environment markers\nthis is at least half the length of the longest side\n</code></pre> Source code in <code>ephysiopy/common/fieldcalcs.py</code> <pre><code>def border_score(\n    A,\n    B=None,\n    shape=\"square\",\n    fieldThresh=0.3,\n    circumPrc=0.2,\n    binSize=3.0,\n    minArea=200,\n):\n    \"\"\"\n\n    Calculates a border score totally dis-similar to that calculated in\n    Solstad et al (2008)\n\n    Args:\n        A (array_like): Should be the ratemap\n        B (array_like): This should be a boolean mask where True (1)\n            is equivalent to the presence of a border and False (0)\n            is equivalent to 'open space'. Naievely this will be the\n            edges of the ratemap but could be used to take account of\n            boundary insertions/ creations to check tuning to multiple\n            environmental boundaries. Default None: when the mask is\n            None then a mask is created that has 1's at the edges of the\n            ratemap i.e. it is assumed that occupancy = environmental\n            shape\n        shape (str): description of environment shape. Currently\n            only 'square' or 'circle' accepted. Used to calculate the\n            proportion of the environmental boundaries to examine for\n            firing\n        fieldThresh (float): Between 0 and 1 this is the percentage\n            amount of the maximum firing rate\n            to remove from the ratemap (i.e. to remove noise)\n        smthKernSig (float): the sigma value used in smoothing the ratemap\n            (again!) with a gaussian kernel\n        circumPrc (float): The percentage amount of the circumference\n            of the environment that the field needs to be to count\n            as long enough to make it through\n        binSize (float): bin size in cm\n        minArea (float): min area for a field to be considered\n        debug (bool): If True then some plots and text will be output\n\n    Returns:\n        float: the border score\n\n    Notes:\n        If the cell is a border cell (BVC) then we know that it should\n        fire at a fixed distance from a given boundary (possibly more\n        than one). In essence this algorithm estimates the amount of\n        variance in this distance i.e. if the cell is a border cell this\n        number should be small. This is achieved by first doing a bunch of\n        morphological operations to isolate individual fields in the\n        ratemap (similar to the code used in phasePrecession.py - see\n        the partitionFields method therein). These partitioned fields are then\n        thinned out (using skimage's skeletonize) to a single pixel\n        wide field which will lie more or less in the middle of the\n        (highly smoothed) sub-field. It is the variance in distance from the\n        nearest boundary along this pseudo-iso-line that is the boundary\n        measure\n\n        Other things to note are that the pixel-wide field has to have some\n        minimum length. In the case of a circular environment this is set to\n        20% of the circumference; in the case of a square environment markers\n        this is at least half the length of the longest side\n    \"\"\"\n    # need to know borders of the environment so we can see if a field\n    # touches the edges, and the perimeter length of the environment\n    # deal with square or circles differently\n    borderMask = np.zeros_like(A)\n    A_rows, A_cols = np.shape(A)\n    if \"circle\" in shape:\n        radius = np.max(np.array(np.shape(A))) / 2.0\n        dist_mask = skimage.morphology.disk(radius)\n        if np.shape(dist_mask) &gt; np.shape(A):\n            dist_mask = dist_mask[1 : A_rows + 1, 1 : A_cols + 1]\n        tmp = np.zeros([A_rows + 2, A_cols + 2])\n        tmp[1:-1, 1:-1] = dist_mask\n        dists = ndi.distance_transform_bf(tmp)\n        dists = dists[1:-1, 1:-1]\n        borderMask = np.logical_xor(dists &lt;= 0, dists &lt; 2)\n        # open up the border mask a little\n        borderMask = skimage.morphology.binary_dilation(\n            borderMask, skimage.morphology.disk(1)\n        )\n    elif \"square\" in shape:\n        borderMask[0:3, :] = 1\n        borderMask[-3:, :] = 1\n        borderMask[:, 0:3] = 1\n        borderMask[:, -3:] = 1\n        tmp = np.zeros([A_rows + 2, A_cols + 2])\n        dist_mask = np.ones_like(A)\n        tmp[1:-1, 1:-1] = dist_mask\n        dists = ndi.distance_transform_bf(tmp)\n        # remove edges to make same shape as input ratemap\n        dists = dists[1:-1, 1:-1]\n    A[~np.isfinite(A)] = 0\n    # get some morphological info about the fields in the ratemap\n    # start image processing:\n    # get some markers\n    # NB I've tried a variety of techniques to optimise this part and the\n    # best seems to be the local adaptive thresholding technique which)\n    # smooths locally with a gaussian - see the skimage docs for more\n    idx = A &gt;= np.nanmax(np.ravel(A)) * fieldThresh\n    A_thresh = np.zeros_like(A)\n    A_thresh[idx] = A[idx]\n\n    # label these markers so each blob has a unique id\n    labels, nFields = ndi.label(A_thresh)\n    # remove small objects\n    min_size = int(minArea / binSize) - 1\n    skimage.morphology.remove_small_objects(labels, min_size=min_size, connectivity=2)\n    labels = skimage.segmentation.relabel_sequential(labels)[0]\n    nFields = np.nanmax(labels)\n    if nFields == 0:\n        return np.nan\n\n    # Iterate over the labelled parts of the array labels calculating\n    # how much of the total circumference of the environment edge it\n    # covers\n\n    fieldAngularCoverage = np.zeros([1, nFields]) * np.nan\n    fractionOfPixelsOnBorder = np.zeros([1, nFields]) * np.nan\n    fieldsToKeep = np.zeros_like(A).astype(bool)\n    for i in range(1, nFields + 1):\n        fieldMask = np.logical_and(labels == i, borderMask)\n\n        # check the angle subtended by the fieldMask\n        if np.nansum(fieldMask.astype(int)) &gt; 0:\n            s = skimage.measure.regionprops(\n                fieldMask.astype(int), intensity_image=A_thresh\n            )[0]\n            x = s.coords[:, 0] - (A_cols / 2.0)\n            y = s.coords[:, 1] - (A_rows / 2.0)\n            subtended_angle = np.rad2deg(np.ptp(np.arctan2(x, y)))\n            if subtended_angle &gt; (360 * circumPrc):\n                pixelsOnBorder = np.count_nonzero(fieldMask) / float(\n                    np.count_nonzero(labels == i)\n                )\n                fractionOfPixelsOnBorder[:, i - 1] = pixelsOnBorder\n                if pixelsOnBorder &gt; 0.5:\n                    fieldAngularCoverage[0, i - 1] = subtended_angle\n\n            fieldsToKeep = np.logical_or(fieldsToKeep, labels == i)\n\n    # Check the fields are big enough to qualify (minArea)\n    # returning nan if not\n    def fn(val):\n        return np.count_nonzero(val)\n\n    field_sizes = ndi.labeled_comprehension(\n        A, labels, range(1, nFields + 1), fn, float, 0\n    )\n    field_sizes /= binSize\n    if not np.any(field_sizes) &gt; (minArea / binSize):\n        warnings.warn(\n            f\"No fields bigger than the minimum size of {minArea/binSize} (minArea/binSize) could be found\"\n        )\n        return np.nan\n\n    fieldAngularCoverage = fieldAngularCoverage / 360.0\n    rateInField = A[fieldsToKeep]\n    # normalize firing rate in the field to sum to 1\n    rateInField = rateInField / np.nansum(rateInField)\n    dist2WallInField = dists[fieldsToKeep]\n    Dm = np.dot(dist2WallInField, rateInField)\n    if \"circle\" in shape:\n        Dm = Dm / radius\n    elif \"square\" in shape:\n        Dm = Dm / (np.nanmax(np.shape(A)) / 2.0)\n    borderScore = (fractionOfPixelsOnBorder - Dm) / (fractionOfPixelsOnBorder + Dm)\n    return np.nanmax(borderScore)\n</code></pre>"},{"location":"reference/#ephysiopy.common.fieldcalcs.calc_angs","title":"<code>calc_angs(points)</code>","text":"<p>Calculates the angles for all triangles in a delaunay tesselation of the peak points in the ratemap</p> Source code in <code>ephysiopy/common/fieldcalcs.py</code> <pre><code>def calc_angs(points):\n    \"\"\"\n    Calculates the angles for all triangles in a delaunay tesselation of\n    the peak points in the ratemap\n    \"\"\"\n\n    # calculate the lengths of the sides of the triangles\n    tri = spatial.Delaunay(points)\n    angs = []\n    for s in tri.simplices:\n        A = tri.points[s[1]] - tri.points[s[0]]\n        B = tri.points[s[2]] - tri.points[s[1]]\n        C = tri.points[s[0]] - tri.points[s[2]]\n        for e1, e2 in ((A, -B), (B, -C), (C, -A)):\n            num = np.dot(e1, e2)\n            denom = np.linalg.norm(e1) * np.linalg.norm(e2)\n            angs.append(np.arccos(num / denom) * 180 / np.pi)\n    return np.array(angs).T\n</code></pre>"},{"location":"reference/#ephysiopy.common.fieldcalcs.coherence","title":"<code>coherence(smthd_rate, unsmthd_rate)</code>","text":"<p>calculates coherence of receptive field via correlation of smoothed and unsmoothed ratemaps</p> Source code in <code>ephysiopy/common/fieldcalcs.py</code> <pre><code>def coherence(smthd_rate, unsmthd_rate):\n    \"\"\"calculates coherence of receptive field via correlation of smoothed\n    and unsmoothed ratemaps\n    \"\"\"\n    smthd = smthd_rate.ravel()\n    unsmthd = unsmthd_rate.ravel()\n    si = ~np.isnan(smthd)\n    ui = ~np.isnan(unsmthd)\n    idx = ~(~si | ~ui)\n    coherence = np.corrcoef(unsmthd[idx], smthd[idx])\n    return coherence[1, 0]\n</code></pre>"},{"location":"reference/#ephysiopy.common.fieldcalcs.deform_SAC","title":"<code>deform_SAC(A, circleXY=None, ellipseXY=None)</code>","text":"<p>Deforms a SAC that is non-circular to be more circular</p> <p>Basically a blatant attempt to improve grid scores, possibly introduced in a paper by Matt Nolan...</p> <p>Args:     A (array_like): The SAC     circleXY (array_like, optional): The xy coordinates defining a circle.     Default None.     ellipseXY (array_like, optional): The xy coordinates defining an     ellipse. Default None.</p> <p>Returns:     deformed_sac (array_like): The SAC deformed to be more circular</p> <p>See Also:     ephysiopy.common.ephys_generic.FieldCalcs.grid_field_props     skimage.transform.AffineTransform     skimage.transform.warp     skimage.exposure.rescale_intensity</p> Source code in <code>ephysiopy/common/fieldcalcs.py</code> <pre><code>def deform_SAC(A, circleXY=None, ellipseXY=None):\n    \"\"\"\n    Deforms a SAC that is non-circular to be more circular\n\n    Basically a blatant attempt to improve grid scores, possibly\n    introduced in a paper by Matt Nolan...\n\n    Args:\n        A (array_like): The SAC\n        circleXY (array_like, optional): The xy coordinates defining a circle.\n        Default None.\n        ellipseXY (array_like, optional): The xy coordinates defining an\n        ellipse. Default None.\n\n    Returns:\n        deformed_sac (array_like): The SAC deformed to be more circular\n\n    See Also:\n        ephysiopy.common.ephys_generic.FieldCalcs.grid_field_props\n        skimage.transform.AffineTransform\n        skimage.transform.warp\n        skimage.exposure.rescale_intensity\n    \"\"\"\n    if circleXY is None or ellipseXY is None:\n        SAC_stats = grid_field_props(A)\n        circleXY = SAC_stats[\"circleXY\"]\n        ellipseXY = SAC_stats[\"ellipseXY\"]\n        # The ellipse detection stuff might have failed, if so\n        # return the original SAC\n        if circleXY is None:\n            warnings.warn(\"Ellipse detection failed. Returning original SAC\")\n            return A\n\n    tform = skimage.transform.AffineTransform()\n    tform.estimate(ellipseXY, circleXY)\n\n    \"\"\"\n    the transformation algorithms used here crop values &lt; 0 to 0. Need to\n    rescale the SAC values before doing the deformation and then rescale\n    again so the values assume the same range as in the unadulterated SAC\n    \"\"\"\n    A[np.isnan(A)] = 0\n    SACmin = np.nanmin(A.flatten())\n    SACmax = np.nanmax(A.flatten())  # should be 1 if autocorr\n    AA = A + 1\n    deformedSAC = skimage.transform.warp(\n        AA / np.nanmax(AA.flatten()), inverse_map=tform.inverse, cval=0\n    )\n    return skimage.exposure.rescale_intensity(deformedSAC, out_range=(SACmin, SACmax))\n</code></pre>"},{"location":"reference/#ephysiopy.common.fieldcalcs.field_lims","title":"<code>field_lims(A)</code>","text":"<p>Returns a labelled matrix of the ratemap A. Uses anything greater than the half peak rate to select as a field. Data is heavily smoothed.</p> <p>Args:     A (BinnedData): A BinnedData instance containing the ratemap</p> <p>Returns:     label (np.array): The labelled ratemap</p> Source code in <code>ephysiopy/common/fieldcalcs.py</code> <pre><code>def field_lims(A):\n    \"\"\"\n    Returns a labelled matrix of the ratemap A.\n    Uses anything greater than the half peak rate to select as a field.\n    Data is heavily smoothed.\n\n    Args:\n        A (BinnedData): A BinnedData instance containing the ratemap\n\n    Returns:\n        label (np.array): The labelled ratemap\n    \"\"\"\n    Ac = A.binned_data[0]\n    nan_idx = np.isnan(Ac)\n    Ac[nan_idx] = 0\n    h = int(np.max(Ac.shape) / 2)\n    sm_rmap = blur_image(A, h, ftype=\"gaussian\").binned_data[0]\n    thresh = np.max(sm_rmap.ravel()) * 0.2  # select area &gt; 20% of peak\n    distance = ndi.distance_transform_edt(sm_rmap &gt; thresh)\n    peak_idx = skimage.feature.peak_local_max(\n        distance, exclude_border=False, labels=sm_rmap &gt; thresh\n    )\n    mask = np.zeros_like(distance, dtype=bool)\n    mask[tuple(peak_idx.T)] = True\n    label = ndi.label(mask)[0]\n    w = watershed(image=-distance, markers=label, mask=sm_rmap &gt; thresh)\n    label = ndi.label(w)[0]\n    return label\n</code></pre>"},{"location":"reference/#ephysiopy.common.fieldcalcs.field_props","title":"<code>field_props(A, min_dist=5, neighbours=2, prc=50, plot=False, ax=None, tri=False, verbose=True, **kwargs)</code>","text":"<p>Returns a dictionary of properties of the field(s) in a ratemap A</p> <p>Args:     A (array_like): a ratemap (but could be any image)     min_dist (float): the separation (in bins) between fields for measures         such as field distance to make sense. Used to         partition the image into separate fields in the call to         feature.peak_local_max     neighbours (int): the number of fields to consider as neighbours to         any given field. Defaults to 2     prc (float): percent of fields to consider     ax (matplotlib.Axes): user supplied axis. If None a new figure window     is created     tri (bool): whether to do Delaunay triangulation between fields         and add to plot     verbose (bool): dumps the properties to the console     plot (bool): whether to plot some output - currently consists of the         ratemap A, the fields of which are outline in a black         contour. Default False</p> <p>Returns:     result (dict): The properties of the field(s) in the input ratemap A</p> Source code in <code>ephysiopy/common/fieldcalcs.py</code> <pre><code>def field_props(\n    A,\n    min_dist=5,\n    neighbours=2,\n    prc=50,\n    plot=False,\n    ax=None,\n    tri=False,\n    verbose=True,\n    **kwargs,\n):\n    \"\"\"\n    Returns a dictionary of properties of the field(s) in a ratemap A\n\n    Args:\n        A (array_like): a ratemap (but could be any image)\n        min_dist (float): the separation (in bins) between fields for measures\n            such as field distance to make sense. Used to\n            partition the image into separate fields in the call to\n            feature.peak_local_max\n        neighbours (int): the number of fields to consider as neighbours to\n            any given field. Defaults to 2\n        prc (float): percent of fields to consider\n        ax (matplotlib.Axes): user supplied axis. If None a new figure window\n        is created\n        tri (bool): whether to do Delaunay triangulation between fields\n            and add to plot\n        verbose (bool): dumps the properties to the console\n        plot (bool): whether to plot some output - currently consists of the\n            ratemap A, the fields of which are outline in a black\n            contour. Default False\n\n    Returns:\n        result (dict): The properties of the field(s) in the input ratemap A\n    \"\"\"\n\n    from skimage.measure import find_contours\n    from sklearn.neighbors import NearestNeighbors\n\n    nan_idx = np.isnan(A)\n    Ac = A.copy()\n    Ac[np.isnan(A)] = 0\n    # smooth Ac more to remove local irregularities\n    n = ny = 5\n    x, y = np.mgrid[-n : n + 1, -ny : ny + 1]\n    g = np.exp(-(x**2 / float(n) + y**2 / float(ny)))\n    g = g / g.sum()\n    Ac = signal.convolve(Ac, g, mode=\"same\")\n\n    peak_idx, field_labels = _get_field_labels(Ac, **kwargs)\n\n    nFields = np.max(field_labels)\n    if neighbours &gt; nFields:\n        print(\n            \"neighbours value of {0} &gt; the {1} peaks found\".format(neighbours, nFields)\n        )\n        print(\"Reducing neighbours to number of peaks found\")\n        neighbours = nFields\n    sub_field_mask = np.zeros((nFields, Ac.shape[0], Ac.shape[1]))\n    sub_field_props = skimage.measure.regionprops(field_labels, intensity_image=Ac)\n    sub_field_centroids = []\n    sub_field_size = []\n\n    for sub_field in sub_field_props:\n        tmp = np.zeros(Ac.shape).astype(bool)\n        tmp[sub_field.coords[:, 0], sub_field.coords[:, 1]] = True\n        tmp2 = Ac &gt; sub_field.max_intensity * (prc / float(100))\n        sub_field_mask[sub_field.label - 1, :, :] = np.logical_and(tmp2, tmp)\n        sub_field_centroids.append(sub_field.centroid)\n        sub_field_size.append(sub_field.area)  # in bins\n    sub_field_mask = np.sum(sub_field_mask, 0)\n    contours = skimage.measure.find_contours(sub_field_mask, 0.5)\n    # find the nearest neighbors to the peaks of each sub-field\n    nbrs = NearestNeighbors(n_neighbors=neighbours, algorithm=\"ball_tree\").fit(peak_idx)\n    distances, _ = nbrs.kneighbors(peak_idx)\n    mean_field_distance = np.mean(distances[:, 1:neighbours])\n\n    nValid_bins = np.sum(~nan_idx)\n    # calculate the amount of out of field firing\n    A_non_field = np.zeros_like(A) * np.nan\n    A_non_field[~sub_field_mask.astype(bool)] = A[~sub_field_mask.astype(bool)]\n    A_non_field[nan_idx] = np.nan\n    out_of_field_firing_prc = (\n        np.count_nonzero(A_non_field &gt; 0) / float(nValid_bins)\n    ) * 100\n    Ac[np.isnan(A)] = np.nan\n    # get some stats about the field ellipticity\n    ellipse_ratio = np.nan\n    _, central_field, _ = limit_to_one(A, prc=50)\n\n    contour_coords = find_contours(central_field, 0.5)\n    from skimage.measure import EllipseModel\n\n    E = EllipseModel()\n    E.estimate(contour_coords[0])\n    ellipse_axes = E.params[2:4]\n    ellipse_ratio = np.min(ellipse_axes) / np.max(ellipse_axes)\n\n    \"\"\" using the peak_idx values calculate the angles of the triangles that\n    make up a delaunay tesselation of the space if the calc_angles arg is\n    in kwargs\n    \"\"\"\n    if \"calc_angs\" in kwargs.keys():\n        angs = calc_angs(peak_idx)\n    else:\n        angs = None\n\n    props = {\n        \"Ac\": Ac,\n        \"Peak_rate\": np.nanmax(A),\n        \"Mean_rate\": np.nanmean(A),\n        \"Field_size\": np.mean(sub_field_size),\n        \"Pct_bins_with_firing\": (np.sum(sub_field_mask) / nValid_bins) * 100,\n        \"Out_of_field_firing_prc\": out_of_field_firing_prc,\n        \"Dist_between_fields\": mean_field_distance,\n        \"Num_fields\": float(nFields),\n        \"Sub_field_mask\": sub_field_mask,\n        \"Smoothed_map\": Ac,\n        \"field_labels\": field_labels,\n        \"Peak_idx\": peak_idx,\n        \"angles\": angs,\n        \"contours\": contours,\n        \"ellipse_ratio\": ellipse_ratio,\n    }\n\n    if verbose:\n        print(\n            \"\\nPercentage of bins with firing: {:.2%}\".format(\n                np.sum(sub_field_mask) / nValid_bins\n            )\n        )\n        print(\n            \"Percentage out of field firing: {:.2%}\".format(\n                np.count_nonzero(A_non_field &gt; 0) / float(nValid_bins)\n            )\n        )\n        print(f\"Peak firing rate: {np.nanmax(A)} Hz\")\n        print(f\"Mean firing rate: {np.nanmean(A)} Hz\")\n        print(f\"Number of fields: {nFields}\")\n        print(f\"Mean field size: {np.mean(sub_field_size)} cm\")\n        print(f\"Mean inter-peak distance between fields: {mean_field_distance} cm\")\n    return props\n</code></pre>"},{"location":"reference/#ephysiopy.common.fieldcalcs.fieldprops","title":"<code>fieldprops(label_image, binned_data, xy, spikes_per_pos, cache=True, *, extra_properties=None, spacing=None, offset=None, **kwargs)</code>","text":"<p>Measure properties of labeled image regions.</p> <p>Parameters:</p> Name Type Description Default <code>label_image</code> <code>(M, N[, P]) ndarray</code> <p>Labeled input image. Labels with value 0 are ignored.</p> <p>.. versionchanged:: 0.14.1     Previously, <code>label_image</code> was processed by <code>numpy.squeeze</code> and     so any number of singleton dimensions was allowed. This resulted in     inconsistent handling of images with singleton dimensions. To     recover the old behaviour, use     <code>regionprops(np.squeeze(label_image), ...)</code>.</p> required <code>xy</code> <code>(2 x N) ndarray</code> <p>The x-y coordinates for all runs through the field corresponding to a particular label</p> required <code>binned_data</code> <code>BinnedData instance from ephysiopy.common.utils</code> required <code>cache</code> <code>bool</code> <p>Determine whether to cache calculated properties. The computation is much faster for cached properties, whereas the memory consumption increases.</p> <code>True</code> <code>extra_properties</code> <code>Iterable of callables</code> <p>Add extra property computation functions that are not included with skimage. The name of the property is derived from the function name, the dtype is inferred by calling the function on a small sample. If the name of an extra property clashes with the name of an existing property the extra property will not be visible and a UserWarning is issued. A property computation function must take a region mask as its first argument. If the property requires an intensity image, it must accept the intensity image as the second argument.</p> <code>None</code> <code>spacing</code> <p>The pixel spacing along each axis of the image.</p> <code>None</code> <code>offset</code> <code>array-like of int, shape `(label_image.ndim,)`</code> <p>Coordinates of the origin (\"top-left\" corner) of the label image. Normally this is ([0, ]0, 0), but it might be different if one wants to obtain regionprops of subvolumes within a larger volume.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>properties</code> <code>list of RegionProperties</code> <p>Each item describes one labeled region, and can be accessed using the attributes listed below.</p> Notes <p>The following properties can be accessed as attributes or keys:</p> <p>area : float     Area of the region i.e. number of pixels of the region scaled by pixel-area. area_bbox : float     Area of the bounding box i.e. number of pixels of bounding box scaled by pixel-area. area_convex : float     Area of the convex hull image, which is the smallest convex     polygon that encloses the region. area_filled : float     Area of the region with all the holes filled in. axis_major_length : float     The length of the major axis of the ellipse that has the same     normalized second central moments as the region. axis_minor_length : float     The length of the minor axis of the ellipse that has the same     normalized second central moments as the region. bbox : tuple     Bounding box <code>(min_row, min_col, max_row, max_col)</code>.     Pixels belonging to the bounding box are in the half-open interval     <code>[min_row; max_row)</code> and <code>[min_col; max_col)</code>. centroid : array     Centroid coordinate tuple <code>(row, col)</code>. centroid_local : array     Centroid coordinate tuple <code>(row, col)</code>, relative to region bounding     box. centroid_weighted : array     Centroid coordinate tuple <code>(row, col)</code> weighted with intensity     image. centroid_weighted_local : array     Centroid coordinate tuple <code>(row, col)</code>, relative to region bounding     box, weighted with intensity image. coords_scaled : (K, 2) ndarray     Coordinate list <code>(row, col)</code> of the region scaled by <code>spacing</code>. coords : (K, 2) ndarray     Coordinate list <code>(row, col)</code> of the region. eccentricity : float     Eccentricity of the ellipse that has the same second-moments as the     region. The eccentricity is the ratio of the focal distance     (distance between focal points) over the major axis length.     The value is in the interval [0, 1).     When it is 0, the ellipse becomes a circle. equivalent_diameter_area : float     The diameter of a circle with the same area as the region. euler_number : int     Euler characteristic of the set of non-zero pixels.     Computed as number of connected components subtracted by number of     holes (input.ndim connectivity). In 3D, number of connected     components plus number of holes subtracted by number of tunnels. extent : float     Ratio of pixels in the region to pixels in the total bounding box.     Computed as <code>area / (rows * cols)</code> feret_diameter_max : float     Maximum Feret's diameter computed as the longest distance between     points around a region's convex hull contour as determined by     <code>find_contours</code>. [5]_ image : (H, J) ndarray     Sliced binary region image which has the same size as bounding box. image_convex : (H, J) ndarray     Binary convex hull image which has the same size as bounding box. image_filled : (H, J) ndarray     Binary region image with filled holes which has the same size as     bounding box. image_intensity : ndarray     Image inside region bounding box. inertia_tensor : ndarray     Inertia tensor of the region for the rotation around its mass. inertia_tensor_eigvals : tuple     The eigenvalues of the inertia tensor in decreasing order. intensity_max : float     Value with the greatest intensity in the region. intensity_mean : float     Value with the mean intensity in the region. intensity_min : float     Value with the least intensity in the region. intensity_std : float     Standard deviation of the intensity in the region. label : int     The label in the labeled input image. moments : (3, 3) ndarray     Spatial moments up to 3rd order::</p> <pre><code>    m_ij = sum{ array(row, col) * row^i * col^j }\n\nwhere the sum is over the `row`, `col` coordinates of the region.\n</code></pre> <p>moments_central : (3, 3) ndarray     Central moments (translation invariant) up to 3rd order::</p> <pre><code>    mu_ij = sum{ array(row, col) * (row - row_c)^i * (col - col_c)^j }\n\nwhere the sum is over the `row`, `col` coordinates of the region,\nand `row_c` and `col_c` are the coordinates of the region's centroid.\n</code></pre> <p>moments_hu : tuple     Hu moments (translation, scale and rotation invariant). moments_normalized : (3, 3) ndarray     Normalized moments (translation and scale invariant) up to 3rd order::</p> <pre><code>    nu_ij = mu_ij / m_00^[(i+j)/2 + 1]\n\nwhere `m_00` is the zeroth spatial moment.\n</code></pre> <p>moments_weighted : (3, 3) ndarray     Spatial moments of intensity image up to 3rd order::</p> <pre><code>    wm_ij = sum{ array(row, col) * row^i * col^j }\n\nwhere the sum is over the `row`, `col` coordinates of the region.\n</code></pre> <p>moments_weighted_central : (3, 3) ndarray     Central moments (translation invariant) of intensity image up to     3rd order::</p> <pre><code>    wmu_ij = sum{ array(row, col) * (row - row_c)^i * (col - col_c)^j }\n\nwhere the sum is over the `row`, `col` coordinates of the region,\nand `row_c` and `col_c` are the coordinates of the region's weighted\ncentroid.\n</code></pre> <p>moments_weighted_hu : tuple     Hu moments (translation, scale and rotation invariant) of intensity     image. moments_weighted_normalized : (3, 3) ndarray     Normalized moments (translation and scale invariant) of intensity     image up to 3rd order::</p> <pre><code>    wnu_ij = wmu_ij / wm_00^[(i+j)/2 + 1]\n\nwhere ``wm_00`` is the zeroth spatial moment (intensity-weighted area).\n</code></pre> <p>num_pixels : int     Number of foreground pixels. orientation : float     Angle between the 0th axis (rows) and the major     axis of the ellipse that has the same second moments as the region,     ranging from <code>-pi/2</code> to <code>pi/2</code> counter-clockwise. perimeter : float     Perimeter of object which approximates the contour as a line     through the centers of border pixels using a 4-connectivity. perimeter_crofton : float     Perimeter of object approximated by the Crofton formula in 4     directions. slice : tuple of slices     A slice to extract the object from the source image. solidity : float     Ratio of pixels in the region to pixels of the convex hull image.</p> <p>Each region also supports iteration, so that you can do::</p> <p>for prop in region:       print(prop, region[prop])</p> See Also <p>label</p> References <p>.. [1] Wilhelm Burger, Mark Burge. Principles of Digital Image Processing:        Core Algorithms. Springer-Verlag, London, 2009. .. [2] B. J\u00e4hne. Digital Image Processing. Springer-Verlag,        Berlin-Heidelberg, 6. edition, 2005. .. [3] T. H. Reiss. Recognizing Planar Objects Using Invariant Image        Features, from Lecture notes in computer science, p. 676. Springer,        Berlin, 1993. .. [4] https://en.wikipedia.org/wiki/Image_moment .. [5] W. Pabst, E. Gregorov\u00e1. Characterization of particles and particle        systems, pp. 27-28. ICT Prague, 2007.        https://old.vscht.cz/sil/keramika/Characterization_of_particles/CPPS%20_English%20version_.pdf</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from skimage import data, util\n&gt;&gt;&gt; from skimage.measure import label, regionprops\n&gt;&gt;&gt; img = util.img_as_ubyte(data.coins()) &gt; 110\n&gt;&gt;&gt; label_img = label(img, connectivity=img.ndim)\n&gt;&gt;&gt; props = regionprops(label_img)\n&gt;&gt;&gt; # centroid of first labeled object\n&gt;&gt;&gt; props[0].centroid\n(22.72987986048314, 81.91228523446583)\n&gt;&gt;&gt; # centroid of first labeled object\n&gt;&gt;&gt; props[0]['centroid']\n(22.72987986048314, 81.91228523446583)\n</code></pre> <p>Add custom measurements by passing functions as <code>extra_properties</code></p> <pre><code>&gt;&gt;&gt; from skimage import data, util\n&gt;&gt;&gt; from skimage.measure import label, regionprops\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; img = util.img_as_ubyte(data.coins()) &gt; 110\n&gt;&gt;&gt; label_img = label(img, connectivity=img.ndim)\n&gt;&gt;&gt; def pixelcount(regionmask):\n...     return np.sum(regionmask)\n&gt;&gt;&gt; props = regionprops(label_img, extra_properties=(pixelcount,))\n&gt;&gt;&gt; props[0].pixelcount\n7741\n&gt;&gt;&gt; props[1]['pixelcount']\n42\n</code></pre> Source code in <code>ephysiopy/common/fieldcalcs.py</code> <pre><code>def fieldprops(\n    label_image,\n    binned_data,\n    xy,\n    spikes_per_pos,\n    cache=True,\n    *,\n    extra_properties=None,\n    spacing=None,\n    offset=None,\n    **kwargs,\n):\n    r\"\"\"Measure properties of labeled image regions.\n\n    Parameters\n    ----------\n    label_image : (M, N[, P]) ndarray\n        Labeled input image. Labels with value 0 are ignored.\n\n        .. versionchanged:: 0.14.1\n            Previously, ``label_image`` was processed by ``numpy.squeeze`` and\n            so any number of singleton dimensions was allowed. This resulted in\n            inconsistent handling of images with singleton dimensions. To\n            recover the old behaviour, use\n            ``regionprops(np.squeeze(label_image), ...)``.\n    xy : (2 x N) ndarray\n        The x-y coordinates for all runs through the field corresponding to\n        a particular label\n    binned_data : BinnedData instance from ephysiopy.common.utils\n    cache : bool, optional\n        Determine whether to cache calculated properties. The computation is\n        much faster for cached properties, whereas the memory consumption\n        increases.\n    extra_properties : Iterable of callables\n        Add extra property computation functions that are not included with\n        skimage. The name of the property is derived from the function name,\n        the dtype is inferred by calling the function on a small sample.\n        If the name of an extra property clashes with the name of an existing\n        property the extra property will not be visible and a UserWarning is\n        issued. A property computation function must take a region mask as its\n        first argument. If the property requires an intensity image, it must\n        accept the intensity image as the second argument.\n    spacing: tuple of float, shape (ndim,)\n        The pixel spacing along each axis of the image.\n    offset : array-like of int, shape `(label_image.ndim,)`, optional\n        Coordinates of the origin (\"top-left\" corner) of the label image.\n        Normally this is ([0, ]0, 0), but it might be different if one wants\n        to obtain regionprops of subvolumes within a larger volume.\n\n    Returns\n    -------\n    properties : list of RegionProperties\n        Each item describes one labeled region, and can be accessed using the\n        attributes listed below.\n\n    Notes\n    -----\n    The following properties can be accessed as attributes or keys:\n\n    **area** : float\n        Area of the region i.e. number of pixels of the region scaled by pixel-area.\n    **area_bbox** : float\n        Area of the bounding box i.e. number of pixels of bounding box scaled by pixel-area.\n    **area_convex** : float\n        Area of the convex hull image, which is the smallest convex\n        polygon that encloses the region.\n    **area_filled** : float\n        Area of the region with all the holes filled in.\n    **axis_major_length** : float\n        The length of the major axis of the ellipse that has the same\n        normalized second central moments as the region.\n    **axis_minor_length** : float\n        The length of the minor axis of the ellipse that has the same\n        normalized second central moments as the region.\n    **bbox** : tuple\n        Bounding box ``(min_row, min_col, max_row, max_col)``.\n        Pixels belonging to the bounding box are in the half-open interval\n        ``[min_row; max_row)`` and ``[min_col; max_col)``.\n    **centroid** : array\n        Centroid coordinate tuple ``(row, col)``.\n    **centroid_local** : array\n        Centroid coordinate tuple ``(row, col)``, relative to region bounding\n        box.\n    **centroid_weighted** : array\n        Centroid coordinate tuple ``(row, col)`` weighted with intensity\n        image.\n    **centroid_weighted_local** : array\n        Centroid coordinate tuple ``(row, col)``, relative to region bounding\n        box, weighted with intensity image.\n    **coords_scaled** : (K, 2) ndarray\n        Coordinate list ``(row, col)`` of the region scaled by ``spacing``.\n    **coords** : (K, 2) ndarray\n        Coordinate list ``(row, col)`` of the region.\n    **eccentricity** : float\n        Eccentricity of the ellipse that has the same second-moments as the\n        region. The eccentricity is the ratio of the focal distance\n        (distance between focal points) over the major axis length.\n        The value is in the interval [0, 1).\n        When it is 0, the ellipse becomes a circle.\n    **equivalent_diameter_area** : float\n        The diameter of a circle with the same area as the region.\n    **euler_number** : int\n        Euler characteristic of the set of non-zero pixels.\n        Computed as number of connected components subtracted by number of\n        holes (input.ndim connectivity). In 3D, number of connected\n        components plus number of holes subtracted by number of tunnels.\n    **extent** : float\n        Ratio of pixels in the region to pixels in the total bounding box.\n        Computed as ``area / (rows * cols)``\n    **feret_diameter_max** : float\n        Maximum Feret's diameter computed as the longest distance between\n        points around a region's convex hull contour as determined by\n        ``find_contours``. [5]_\n    **image** : (H, J) ndarray\n        Sliced binary region image which has the same size as bounding box.\n    **image_convex** : (H, J) ndarray\n        Binary convex hull image which has the same size as bounding box.\n    **image_filled** : (H, J) ndarray\n        Binary region image with filled holes which has the same size as\n        bounding box.\n    **image_intensity** : ndarray\n        Image inside region bounding box.\n    **inertia_tensor** : ndarray\n        Inertia tensor of the region for the rotation around its mass.\n    **inertia_tensor_eigvals** : tuple\n        The eigenvalues of the inertia tensor in decreasing order.\n    **intensity_max** : float\n        Value with the greatest intensity in the region.\n    **intensity_mean** : float\n        Value with the mean intensity in the region.\n    **intensity_min** : float\n        Value with the least intensity in the region.\n    **intensity_std** : float\n        Standard deviation of the intensity in the region.\n    **label** : int\n        The label in the labeled input image.\n    **moments** : (3, 3) ndarray\n        Spatial moments up to 3rd order::\n\n            m_ij = sum{ array(row, col) * row^i * col^j }\n\n        where the sum is over the `row`, `col` coordinates of the region.\n    **moments_central** : (3, 3) ndarray\n        Central moments (translation invariant) up to 3rd order::\n\n            mu_ij = sum{ array(row, col) * (row - row_c)^i * (col - col_c)^j }\n\n        where the sum is over the `row`, `col` coordinates of the region,\n        and `row_c` and `col_c` are the coordinates of the region's centroid.\n    **moments_hu** : tuple\n        Hu moments (translation, scale and rotation invariant).\n    **moments_normalized** : (3, 3) ndarray\n        Normalized moments (translation and scale invariant) up to 3rd order::\n\n            nu_ij = mu_ij / m_00^[(i+j)/2 + 1]\n\n        where `m_00` is the zeroth spatial moment.\n    **moments_weighted** : (3, 3) ndarray\n        Spatial moments of intensity image up to 3rd order::\n\n            wm_ij = sum{ array(row, col) * row^i * col^j }\n\n        where the sum is over the `row`, `col` coordinates of the region.\n    **moments_weighted_central** : (3, 3) ndarray\n        Central moments (translation invariant) of intensity image up to\n        3rd order::\n\n            wmu_ij = sum{ array(row, col) * (row - row_c)^i * (col - col_c)^j }\n\n        where the sum is over the `row`, `col` coordinates of the region,\n        and `row_c` and `col_c` are the coordinates of the region's weighted\n        centroid.\n    **moments_weighted_hu** : tuple\n        Hu moments (translation, scale and rotation invariant) of intensity\n        image.\n    **moments_weighted_normalized** : (3, 3) ndarray\n        Normalized moments (translation and scale invariant) of intensity\n        image up to 3rd order::\n\n            wnu_ij = wmu_ij / wm_00^[(i+j)/2 + 1]\n\n        where ``wm_00`` is the zeroth spatial moment (intensity-weighted area).\n    **num_pixels** : int\n        Number of foreground pixels.\n    **orientation** : float\n        Angle between the 0th axis (rows) and the major\n        axis of the ellipse that has the same second moments as the region,\n        ranging from `-pi/2` to `pi/2` counter-clockwise.\n    **perimeter** : float\n        Perimeter of object which approximates the contour as a line\n        through the centers of border pixels using a 4-connectivity.\n    **perimeter_crofton** : float\n        Perimeter of object approximated by the Crofton formula in 4\n        directions.\n    **slice** : tuple of slices\n        A slice to extract the object from the source image.\n    **solidity** : float\n        Ratio of pixels in the region to pixels of the convex hull image.\n\n    Each region also supports iteration, so that you can do::\n\n      for prop in region:\n          print(prop, region[prop])\n\n    See Also\n    --------\n    label\n\n    References\n    ----------\n    .. [1] Wilhelm Burger, Mark Burge. Principles of Digital Image Processing:\n           Core Algorithms. Springer-Verlag, London, 2009.\n    .. [2] B. J\u00e4hne. Digital Image Processing. Springer-Verlag,\n           Berlin-Heidelberg, 6. edition, 2005.\n    .. [3] T. H. Reiss. Recognizing Planar Objects Using Invariant Image\n           Features, from Lecture notes in computer science, p. 676. Springer,\n           Berlin, 1993.\n    .. [4] https://en.wikipedia.org/wiki/Image_moment\n    .. [5] W. Pabst, E. Gregorov\u00e1. Characterization of particles and particle\n           systems, pp. 27-28. ICT Prague, 2007.\n           https://old.vscht.cz/sil/keramika/Characterization_of_particles/CPPS%20_English%20version_.pdf\n\n    Examples\n    --------\n    &gt;&gt;&gt; from skimage import data, util\n    &gt;&gt;&gt; from skimage.measure import label, regionprops\n    &gt;&gt;&gt; img = util.img_as_ubyte(data.coins()) &gt; 110\n    &gt;&gt;&gt; label_img = label(img, connectivity=img.ndim)\n    &gt;&gt;&gt; props = regionprops(label_img)\n    &gt;&gt;&gt; # centroid of first labeled object\n    &gt;&gt;&gt; props[0].centroid\n    (22.72987986048314, 81.91228523446583)\n    &gt;&gt;&gt; # centroid of first labeled object\n    &gt;&gt;&gt; props[0]['centroid']\n    (22.72987986048314, 81.91228523446583)\n\n    Add custom measurements by passing functions as ``extra_properties``\n\n    &gt;&gt;&gt; from skimage import data, util\n    &gt;&gt;&gt; from skimage.measure import label, regionprops\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; img = util.img_as_ubyte(data.coins()) &gt; 110\n    &gt;&gt;&gt; label_img = label(img, connectivity=img.ndim)\n    &gt;&gt;&gt; def pixelcount(regionmask):\n    ...     return np.sum(regionmask)\n    &gt;&gt;&gt; props = regionprops(label_img, extra_properties=(pixelcount,))\n    &gt;&gt;&gt; props[0].pixelcount\n    7741\n    &gt;&gt;&gt; props[1]['pixelcount']\n    42\n\n    \"\"\"\n\n    assert label_image.shape == binned_data.binned_data[0].shape\n    if spikes_per_pos is not None:\n        assert len(spikes_per_pos) == xy.shape[1]\n\n    if label_image.ndim not in (2, 3):\n        raise TypeError(\"Only 2-D and 3-D images supported.\")\n\n    if not np.issubdtype(label_image.dtype, np.integer):\n        if np.issubdtype(label_image.dtype, bool):\n            raise TypeError(\n                \"Non-integer image types are ambiguous: \"\n                \"use skimage.measure.label to label the connected \"\n                \"components of label_image, \"\n                \"or label_image.astype(np.uint8) to interpret \"\n                \"the True values as a single label.\"\n            )\n        else:\n            raise TypeError(\"Non-integer label_image types are ambiguous\")\n\n    if offset is None:\n        offset_arr = np.zeros((label_image.ndim,), dtype=int)\n    else:\n        offset_arr = np.asarray(offset)\n        if offset_arr.ndim != 1 or offset_arr.size != label_image.ndim:\n            raise ValueError(\n                \"Offset should be an array-like of integers \"\n                \"of shape (label_image.ndim,); \"\n                f\"{offset} was provided.\"\n            )\n\n    pos_sample_rate = kwargs.get(\"pos_sample_rate\", 50)\n    ye, xe = binned_data.bin_edges\n    x_bins = np.digitize(xy[0], xe[:-1])\n    y_bins = np.digitize(xy[1], ye[:-1])\n    xy_field_label = label_image[y_bins - 1, x_bins - 1]\n    labelled_runs = labelContigNonZeroRuns(xy_field_label)\n    run_starts = getLabelStarts(labelled_runs)\n    run_stops = getLabelEnds(labelled_runs)\n\n    # calculate the speed for possibly filtering runs later\n    speed = None\n    if xy is not None:\n        speed = np.ma.MaskedArray(\n            np.abs(np.ma.ediff1d(np.hypot(xy[0], xy[1])) * pos_sample_rate)\n        )\n        speed = np.append(speed, speed[-1])\n\n    regions = []\n\n    run_id = 0\n\n    objects = ndi.find_objects(label_image)\n\n    for i, sl in enumerate(objects):\n        if sl is None:\n            continue\n\n        label = i + 1\n\n        # get the runs through this field and filter for min run length\n        run_index = np.unique(labelled_runs[xy_field_label == label])\n        run_slices = [\n            slice(run_starts[ri - 1], run_stops[ri - 1] + 1)\n            for ri in run_index\n            if (run_stops[ri - 1] - run_starts[ri - 1]) &gt; 2\n        ]\n        props = FieldProps(\n            sl,\n            label,\n            label_image,\n            binned_data,\n            cache=cache,\n            spacing=spacing,\n            extra_properties=extra_properties,\n            offset=offset_arr,\n        )\n        # extract a few metrics for instantiating the RunProps objects...\n        peak_xy = props.xy_at_peak\n        max_index = props.max_index\n        perimeter_coords = props.perimeter_coords\n        runs = []\n        for rs in run_slices:\n            r = RunProps(\n                run_id,\n                rs,\n                xy[:, rs],\n                spikes_per_pos[rs],\n                speed[rs],\n                peak_xy,\n                max_index,\n                perimeter_coords,\n            )\n            run_id += 1\n            runs.append(r)\n        # ... and add the list of runs to the FieldProps instance\n        props.runs = runs\n\n        regions.append(props)\n\n    return regions\n</code></pre>"},{"location":"reference/#ephysiopy.common.fieldcalcs.get_circular_regions","title":"<code>get_circular_regions(A, **kwargs)</code>","text":"<p>Returns a list of images which are expanding circular regions centred on the middle of the image out to the image edge. Used for calculating the grid score of each image to find the one with the max grid score. Based on some Moser paper I can't recall.</p> <p>Args:     A (np.ndarray): The SAC</p> <p>Keyword Args:     min_radius (int): The smallest radius circle to start with</p> Source code in <code>ephysiopy/common/fieldcalcs.py</code> <pre><code>def get_circular_regions(A: np.ndarray, **kwargs) -&gt; list:\n    \"\"\"\n    Returns a list of images which are expanding circular\n    regions centred on the middle of the image out to the\n    image edge. Used for calculating the grid score of each\n    image to find the one with the max grid score. Based on\n    some Moser paper I can't recall.\n\n    Args:\n        A (np.ndarray): The SAC\n\n    Keyword Args:\n        min_radius (int): The smallest radius circle to start with\n    \"\"\"\n    from skimage.measure import CircleModel, grid_points_in_poly\n\n    min_radius = 5\n    if \"min_radius\" in kwargs.keys():\n        min_radius = kwargs[\"min_radius\"]\n\n    centre = tuple([d // 2 for d in np.shape(A)])\n    max_radius = min(tuple(np.subtract(np.shape(A), centre)))\n    t = np.linspace(0, 2 * np.pi, 51)\n    circle = CircleModel()\n\n    result = []\n    for radius in range(min_radius, max_radius):\n        circle.params = [*centre, radius]\n        xy = circle.predict_xy(t)\n        mask = grid_points_in_poly(np.shape(A), xy)\n        im = A.copy()\n        im[~mask] = np.nan\n        result.append(im)\n    return result\n</code></pre>"},{"location":"reference/#ephysiopy.common.fieldcalcs.get_deformed_sac_gridscore","title":"<code>get_deformed_sac_gridscore(A)</code>","text":"<p>Deforms a non-circular SAC into a circular SAC (circular meaning the ellipse drawn around the edges of the 6 nearest peaks to the SAC centre) and returns get_basic_griscore() calculated on the deformed (or re-formed?!) SAC</p> Source code in <code>ephysiopy/common/fieldcalcs.py</code> <pre><code>def get_deformed_sac_gridscore(A: np.ndarray):\n    \"\"\"\n    Deforms a non-circular SAC into a circular SAC (circular meaning\n    the ellipse drawn around the edges of the 6 nearest peaks to the\n    SAC centre) and returns get_basic_griscore() calculated on the\n    deformed (or re-formed?!) SAC\n    \"\"\"\n    deformed_SAC = deform_SAC(A)\n    return gridness(deformed_SAC)\n</code></pre>"},{"location":"reference/#ephysiopy.common.fieldcalcs.get_expanding_circle_gridscore","title":"<code>get_expanding_circle_gridscore(A, **kwargs)</code>","text":"<p>Calculates the gridscore for each circular sub-region of image A where the circles are centred on the image centre and expanded to the edge of the image. The maximum of the get_basic_gridscore() for each of these circular sub-regions is returned as the gridscore</p> Source code in <code>ephysiopy/common/fieldcalcs.py</code> <pre><code>def get_expanding_circle_gridscore(A: np.ndarray, **kwargs):\n    \"\"\"\n    Calculates the gridscore for each circular sub-region of image A\n    where the circles are centred on the image centre and expanded to\n    the edge of the image. The maximum of the get_basic_gridscore() for\n    each of these circular sub-regions is returned as the gridscore\n    \"\"\"\n\n    images = get_circular_regions(A, **kwargs)\n    gridscores = [gridness(im) for im in images]\n    return max(gridscores)\n</code></pre>"},{"location":"reference/#ephysiopy.common.fieldcalcs.get_mean_resultant","title":"<code>get_mean_resultant(ego_boundary_map)</code>","text":"<p>Calculates the mean resultant vector of a boundary map in egocentric coordinates</p> <p>See Hinman et al., 2019 for more details</p> <p>Args:     ego_boundary_map (np.ndarray): The egocentric boundary map</p> <p>Returns:     float: The mean resultant vector of the egocentric boundary map</p> Source code in <code>ephysiopy/common/fieldcalcs.py</code> <pre><code>def get_mean_resultant(ego_boundary_map: np.ndarray) -&gt; np.complex128 | float:\n    \"\"\"\n    Calculates the mean resultant vector of a boundary map in egocentric coordinates\n\n    See Hinman et al., 2019 for more details\n\n    Args:\n        ego_boundary_map (np.ndarray): The egocentric boundary map\n\n    Returns:\n        float: The mean resultant vector of the egocentric boundary map\n    \"\"\"\n    if np.nansum(ego_boundary_map) == 0:\n        return np.nan\n    m, n = ego_boundary_map.shape\n    angles = np.linspace(0, 2 * np.pi, n)\n    MR = np.nansum(np.nansum(ego_boundary_map, 0) * np.power(np.e, angles * 1j)) / (\n        n * m\n    )\n    return MR\n</code></pre>"},{"location":"reference/#ephysiopy.common.fieldcalcs.get_thigmotaxis_score","title":"<code>get_thigmotaxis_score(xy, shape='circle')</code>","text":"<p>Returns a score which is the ratio of the time spent in the inner portion of an environment to the time spent in the outer portion. The portions are allocated so that they have equal area.</p> <p>Args:     xy (np.ndarray): The xy coordinates of the animal's position. 2 x nsamples     shape (str): The shape of the environment. Legal values are 'circle'     and 'square'. Default 'circle'</p> <p>Returns: thigmoxtaxis_score (float): Values closer to 1 indicate the animal spent more time in the inner portion of the environment. Values closer to -1 indicates the animal spent more time in the outer portion of the environment. A value of 0 indicates the animal spent equal time in both portions of the environment.</p> Source code in <code>ephysiopy/common/fieldcalcs.py</code> <pre><code>def get_thigmotaxis_score(xy: np.ndarray, shape: str = \"circle\") -&gt; float:\n    \"\"\"\n    Returns a score which is the ratio of the time spent in the inner\n    portion of an environment to the time spent in the outer portion.\n    The portions are allocated so that they have equal area.\n\n    Args:\n        xy (np.ndarray): The xy coordinates of the animal's position. 2 x nsamples\n        shape (str): The shape of the environment. Legal values are 'circle'\n        and 'square'. Default 'circle'\n\n    Returns:\n    thigmoxtaxis_score (float): Values closer to 1 indicate the\n    animal spent more time in the inner portion of the environment. Values closer to -1\n    indicates the animal spent more time in the outer portion of the environment.\n    A value of 0 indicates the animal spent equal time in both portions of the\n    environment.\n    \"\"\"\n    # centre the coords to get the max distance from the centre\n    xc, yc = np.min(xy, -1) + np.ptp(xy, -1) / 2\n    xy = xy - np.array([[xc], [yc]])\n    n_pos = np.shape(xy)[1]\n    inner_mask = np.zeros((n_pos), dtype=bool)\n    if shape == \"circle\":\n        outer_radius = np.max(np.hypot(xy[0], xy[1]))\n        inner_radius = outer_radius / np.sqrt(2)\n        inner_mask = np.less(np.hypot(xy[0], xy[1]), inner_radius, out=inner_mask)\n    elif shape == \"square\":\n        width, height = np.ptp(xy, -1)\n        inner_width = width / np.sqrt(2)\n        inner_height = height / np.sqrt(2)\n        x_gap = (width - inner_width) / 2\n        y_gap = (height - inner_height) / 2\n        x_mask = (xy[0] &gt; np.min(xy[0]) + x_gap) &amp; (xy[0] &lt; np.max(xy[0]) - x_gap)\n        y_mask = (xy[1] &gt; np.min(xy[1]) + y_gap) &amp; (xy[1] &lt; np.max(xy[1]) - y_gap)\n        inner_mask = np.logical_and(x_mask, y_mask, out=inner_mask)\n    return (np.count_nonzero(inner_mask) - np.count_nonzero(~inner_mask)) / n_pos\n</code></pre>"},{"location":"reference/#ephysiopy.common.fieldcalcs.global_threshold","title":"<code>global_threshold(A, prc=50, min_dist=5)</code>","text":"<p>Globally thresholds a ratemap and counts number of fields found</p> Source code in <code>ephysiopy/common/fieldcalcs.py</code> <pre><code>def global_threshold(A, prc=50, min_dist=5):\n    \"\"\"\n    Globally thresholds a ratemap and counts number of fields found\n    \"\"\"\n    Ac = A.copy()\n    Ac[np.isnan(A)] = 0\n    n = ny = 5\n    x, y = np.mgrid[-n : n + 1, -ny : ny + 1]\n    g = np.exp(-(x**2 / float(n) + y**2 / float(ny)))\n    g = g / g.sum()\n    Ac = signal.convolve(Ac, g, mode=\"same\")\n    maxRate = np.nanmax(np.ravel(Ac))\n    Ac[Ac &lt; maxRate * (prc / float(100))] = 0\n    Ac_r = skimage.exposure.rescale_intensity(\n        Ac, in_range=\"image\", out_range=(0, 1000)\n    ).astype(np.int32)\n    peak_idx = skimage.feature.peak_local_max(\n        Ac_r, min_distance=min_dist, exclude_border=False\n    )\n    peak_mask = np.zeros_like(Ac, dtype=bool)\n    peak_mask[tuple(peak_idx.T)] = True\n    peak_labels = skimage.measure.label(peak_mask, connectivity=2)\n    field_labels = watershed(image=Ac * -1, markers=peak_labels)\n    nFields = np.max(field_labels)\n    return nFields\n</code></pre>"},{"location":"reference/#ephysiopy.common.fieldcalcs.grid_field_props","title":"<code>grid_field_props(A, maxima='centroid', allProps=True, **kwargs)</code>","text":"<p>Extracts various measures from a spatial autocorrelogram</p> <p>Args:     A: BinnedData object containing the spatial autocorrelogram (SAC) in         A.binned_data[0]     maxima (str, optional): The method used to detect the peaks in the SAC.         Legal values are 'single' and 'centroid'. Default 'centroid'     allProps (bool, optional): Whether to return a dictionary that     contains the attempt to fit an ellipse around the edges of the     central size peaks. See below         Default True</p> <p>Returns:     props (dict): A dictionary containing measures of the SAC.     Keys include:         * gridness score         * scale         * orientation         * coordinates of the peaks (nominally 6) closest to SAC centre         * a binary mask around the extent of the 6 central fields         * values of the rotation procedure used to calculate gridness         * ellipse axes and angle (if allProps is True and the it worked)</p> <p>Notes:     The output from this method can be used as input to the show() method     of this class.     When it is the plot produced will display a lot more informative.     The coordinate system internally used is centred on the image centre.</p> <p>See Also:     ephysiopy.common.binning.autoCorr2D()</p> Source code in <code>ephysiopy/common/fieldcalcs.py</code> <pre><code>def grid_field_props(A: BinnedData, maxima=\"centroid\", allProps=True, **kwargs):\n    \"\"\"\n    Extracts various measures from a spatial autocorrelogram\n\n    Args:\n        A: BinnedData object containing the spatial autocorrelogram (SAC) in\n            A.binned_data[0]\n        maxima (str, optional): The method used to detect the peaks in the SAC.\n            Legal values are 'single' and 'centroid'. Default 'centroid'\n        allProps (bool, optional): Whether to return a dictionary that\n        contains the attempt to fit an ellipse around the edges of the\n        central size peaks. See below\n            Default True\n\n    Returns:\n        props (dict): A dictionary containing measures of the SAC.\n        Keys include:\n            * gridness score\n            * scale\n            * orientation\n            * coordinates of the peaks (nominally 6) closest to SAC centre\n            * a binary mask around the extent of the 6 central fields\n            * values of the rotation procedure used to calculate gridness\n            * ellipse axes and angle (if allProps is True and the it worked)\n\n    Notes:\n        The output from this method can be used as input to the show() method\n        of this class.\n        When it is the plot produced will display a lot more informative.\n        The coordinate system internally used is centred on the image centre.\n\n    See Also:\n        ephysiopy.common.binning.autoCorr2D()\n    \"\"\"\n    \"\"\"\n    Assign the output dictionary now as we want to return immediately if\n    the input is bad\n    \"\"\"\n    dictKeys = (\n        \"gridscore\",\n        \"scale\",\n        \"orientation\",\n        \"closest_peak_coords\",\n        \"dist_to_centre\",\n        \"ellipse_axes\",\n        \"ellipse_angle\",\n        \"ellipseXY\",\n        \"circleXY\",\n        \"rotationArr\",\n        \"rotationCorrVals\",\n    )\n\n    outDict = dict.fromkeys(dictKeys, np.nan)\n\n    A_tmp = A.binned_data[0].copy()\n\n    if np.all(np.isnan(A_tmp)):\n        warnings.warn(\"No data in SAC - returning nans in measures dict\")\n        outDict[\"dist_to_centre\"] = np.atleast_2d(np.array([0, 0]))\n        outDict[\"scale\"] = 0\n        outDict[\"closest_peak_coords\"] = np.atleast_2d(np.array([0, 0]))\n        return outDict\n\n    A_tmp[~np.isfinite(A_tmp)] = -1\n    A_tmp[A_tmp &lt;= 0] = -1\n    A_sz = np.array(np.shape(A_tmp))\n    # [STAGE 1] find peaks &amp; identify 7 closest to centre\n    min_distance = np.ceil(np.min(A_sz / 2) / 8.0).astype(int)\n    min_distance = kwargs.get(\"min_distance\", min_distance)\n\n    _, _, field_labels, _ = partitionFields(\n        A, field_threshold_percent=10, field_rate_threshold=0.001\n    )\n    # peak_idx, field_labels = _get_field_labels(A_tmp, neighbours=7, **kwargs)\n    # a fcn for the labeled_comprehension function that returns\n    # linear indices in A where the values in A for each label are\n    # greater than half the max in that labeled region\n\n    def fn(val, pos):\n        return pos[val &gt; (np.max(val) / 2)]\n\n    nLbls = np.max(field_labels)\n    indices = ndi.labeled_comprehension(\n        A_tmp, field_labels, np.arange(0, nLbls), fn, np.ndarray, 0, True\n    )\n    # turn linear indices into coordinates\n    coords = [np.unravel_index(i, A_sz) for i in indices]\n    half_peak_labels = np.zeros(shape=A_sz)\n    for peak_id, coord in enumerate(coords):\n        xc, yc = coord\n        half_peak_labels[xc, yc] = peak_id\n\n    # Get some statistics about the labeled regions\n    lbl_range = np.arange(0, nLbls)\n    peak_coords = ndi.maximum_position(A.binned_data[0], half_peak_labels, lbl_range)\n    peak_coords = np.array(peak_coords)\n    # Now convert the peak_coords to the image centre coordinate system\n    x_peaks, y_peaks = peak_coords.T\n    x_peaks_ij = A.bin_edges[0][x_peaks]\n    y_peaks_ij = A.bin_edges[1][y_peaks]\n    peak_coords = np.array([x_peaks_ij, y_peaks_ij]).T\n    # Get some distance and morphology measures\n    peak_dist_to_centre = np.hypot(peak_coords[:, 0], peak_coords[:, 1])\n    closest_peak_idx = np.argsort(peak_dist_to_centre)\n    central_peak_label = closest_peak_idx[0]\n    closest_peak_idx = closest_peak_idx[1 : np.min((7, len(closest_peak_idx) - 1))]\n    # closest_peak_idx should now the indices of the labeled 6 peaks\n    # surrounding the central peak at the image centre\n    scale = np.median(peak_dist_to_centre[closest_peak_idx])\n    orientation = np.nan\n    orientation = grid_orientation(peak_coords, closest_peak_idx)\n\n    xv, yv = np.meshgrid(A.bin_edges[0], A.bin_edges[1], indexing=\"ij\")\n    xv = xv[:-1, :-1]  # remove last row and column\n    yv = yv[:-1:, :-1]  # remove last row and column\n    dist_to_centre = np.hypot(xv, yv)\n    # get the max distance of the half-peak width labeled fields\n    # from the centre of the image\n    max_dist_from_centre = 0\n    for peak_id, _coords in enumerate(coords):\n        if peak_id in closest_peak_idx:\n            xc, yc = _coords\n            if np.any(xc) and np.any(yc):\n                xc = A.bin_edges[0][xc]\n                yc = A.bin_edges[1][yc]\n                d = np.max(np.hypot(xc, yc))\n                if d &gt; max_dist_from_centre:\n                    max_dist_from_centre = d\n\n    # Set the outer bits and the central region of the SAC to nans\n    # getting ready for the correlation procedure\n    dist_to_centre[np.abs(dist_to_centre) &gt; max_dist_from_centre] = 0\n    dist_to_centre[half_peak_labels == central_peak_label] = 0\n    dist_to_centre[dist_to_centre != 0] = 1\n    dist_to_centre = dist_to_centre.astype(bool)\n    sac_middle = A.binned_data[0].copy()\n    sac_middle[~dist_to_centre] = np.nan\n\n    if \"step\" in kwargs.keys():\n        step = kwargs.pop(\"step\")\n    else:\n        step = 30\n    try:\n        gridscore, rotationCorrVals, rotationArr = gridness(sac_middle, step=step)\n    except Exception:\n        gridscore, rotationCorrVals, rotationArr = np.nan, np.nan, np.nan\n\n    if allProps:\n        # attempt to fit an ellipse around the outer edges of the nearest\n        # peaks to the centre of the SAC. First find the outer edges for\n        # the closest peaks using a ndimages labeled_comprehension\n        try:\n\n            def fn2(val, pos):\n                xc, yc = np.unravel_index(pos, A_sz)\n                xc = xc - np.floor(A_sz[0] / 2)\n                yc = yc - np.floor(A_sz[1] / 2)\n                idx = np.argmax(np.hypot(xc, yc))\n                return xc[idx], yc[idx]\n\n            ellipse_coords = ndi.labeled_comprehension(\n                A.binned_data[0],\n                half_peak_labels,\n                closest_peak_idx,\n                fn2,\n                tuple,\n                0,\n                True,\n            )\n\n            ellipse_fit_coords = np.array([(x, y) for x, y in ellipse_coords])\n            from skimage.measure import EllipseModel\n\n            E = EllipseModel()\n            E.estimate(ellipse_fit_coords)\n            im_centre = E.params[0:2]\n            ellipse_axes = E.params[2:4]\n            ellipse_angle = E.params[-1]\n            ellipseXY = E.predict_xy(np.linspace(0, 2 * np.pi, 50), E.params)\n\n            # get the min containing circle given the eliipse minor axis\n            from skimage.measure import CircleModel\n\n            _params = [im_centre, np.min(ellipse_axes)]\n            circleXY = CircleModel().predict_xy(\n                np.linspace(0, 2 * np.pi, 50), params=_params\n            )\n        except (TypeError, ValueError):  # non-iterable x and y\n            ellipse_axes = None\n            ellipse_angle = (None, None)\n            ellipseXY = None\n            circleXY = None\n\n    # collect all the following keywords into a dict for output\n    closest_peak_coords = np.array(peak_coords)[closest_peak_idx]\n\n    # Assign values to the output dictionary created at the start\n    for thiskey in outDict.keys():\n        outDict[thiskey] = locals()[thiskey]\n        # neat trick: locals is a dict holding all locally scoped variables\n    return outDict\n</code></pre>"},{"location":"reference/#ephysiopy.common.fieldcalcs.grid_orientation","title":"<code>grid_orientation(peakCoords, closestPeakIdx)</code>","text":"<p>Calculates the orientation angle of a grid field.</p> <p>The orientation angle is the angle of the first peak working counter-clockwise from 3 o'clock</p> <p>Args:     peakCoords (array_like): The peak coordinates as pairs of xy     closestPeakIdx (array_like): A 1D array of the indices in peakCoords     of the peaks closest to the centre of the SAC</p> <p>Returns:     peak_orientation (float): The first value in an array of the angles of     the peaks in the SAC working counter-clockwise from a line     extending from the middle of the SAC to 3 o'clock.</p> Source code in <code>ephysiopy/common/fieldcalcs.py</code> <pre><code>def grid_orientation(peakCoords, closestPeakIdx):\n    \"\"\"\n    Calculates the orientation angle of a grid field.\n\n    The orientation angle is the angle of the first peak working\n    counter-clockwise from 3 o'clock\n\n    Args:\n        peakCoords (array_like): The peak coordinates as pairs of xy\n        closestPeakIdx (array_like): A 1D array of the indices in peakCoords\n        of the peaks closest to the centre of the SAC\n\n    Returns:\n        peak_orientation (float): The first value in an array of the angles of\n        the peaks in the SAC working counter-clockwise from a line\n        extending from the middle of the SAC to 3 o'clock.\n    \"\"\"\n    if len(peakCoords) &lt; 3 or closestPeakIdx.size == 0:\n        return np.nan\n    else:\n        from ephysiopy.common.utils import polar\n\n        peaks = peakCoords[closestPeakIdx]\n        theta = polar(peaks[:, 1], -peaks[:, 0], deg=1)[1]\n        return np.sort(theta.compress(theta &gt;= 0))[0]\n</code></pre>"},{"location":"reference/#ephysiopy.common.fieldcalcs.gridness","title":"<code>gridness(image, step=30)</code>","text":"<p>Calculates the gridness score in a grid cell SAC.</p> <p>Briefly, the data in <code>image</code> is rotated in <code>step</code> amounts and each rotated array is correlated with the original. The maximum of the values at 30, 90 and 150 degrees is the subtracted from the minimum of the values at 60, 120 and 180 degrees to give the grid score.</p> <p>Args:     image (array_like): The spatial autocorrelogram     step (int, optional): The amount to rotate the SAC in each step of the     rotational correlation procedure</p> <p>Returns:     gridmeasures (3-tuple): The gridscore, the correlation values at each     <code>step</code> and the rotational array</p> <p>Notes:     The correlation performed is a Pearsons R. Some rescaling of the     values in <code>image</code> is performed following rotation.</p> <p>See Also:     skimage.transform.rotate : for how the rotation of <code>image</code> is done     skimage.exposure.rescale_intensity : for the resscaling following     rotation</p> Source code in <code>ephysiopy/common/fieldcalcs.py</code> <pre><code>def gridness(image, step=30):\n    \"\"\"\n    Calculates the gridness score in a grid cell SAC.\n\n    Briefly, the data in `image` is rotated in `step` amounts and\n    each rotated array is correlated with the original.\n    The maximum of the values at 30, 90 and 150 degrees\n    is the subtracted from the minimum of the values at 60, 120\n    and 180 degrees to give the grid score.\n\n    Args:\n        image (array_like): The spatial autocorrelogram\n        step (int, optional): The amount to rotate the SAC in each step of the\n        rotational correlation procedure\n\n    Returns:\n        gridmeasures (3-tuple): The gridscore, the correlation values at each\n        `step` and the rotational array\n\n    Notes:\n        The correlation performed is a Pearsons R. Some rescaling of the\n        values in `image` is performed following rotation.\n\n    See Also:\n        skimage.transform.rotate : for how the rotation of `image` is done\n        skimage.exposure.rescale_intensity : for the resscaling following\n        rotation\n    \"\"\"\n    # TODO: add options in here for whether the full range of correlations\n    # are wanted or whether a reduced set is wanted (i.e. at the 30-tuples)\n    from collections import OrderedDict\n\n    rotationalCorrVals = OrderedDict.fromkeys(np.arange(0, 181, step), np.nan)\n    rotationArr = np.zeros(len(rotationalCorrVals)) * np.nan\n    # autoCorrMiddle needs to be rescaled or the image rotation falls down\n    # as values are cropped to lie between 0 and 1.0\n    in_range = (np.nanmin(image), np.nanmax(image))\n    out_range = (0, 1)\n    import skimage\n\n    autoCorrMiddleRescaled = skimage.exposure.rescale_intensity(\n        image, in_range=in_range, out_range=out_range\n    )\n    origNanIdx = np.isnan(autoCorrMiddleRescaled.ravel())\n    gridscore = np.nan\n    try:\n        for idx, angle in enumerate(rotationalCorrVals.keys()):\n            rotatedA = skimage.transform.rotate(\n                autoCorrMiddleRescaled, angle=angle, cval=np.nan, order=3\n            )\n            # ignore nans\n            rotatedNanIdx = np.isnan(rotatedA.ravel())\n            allNans = np.logical_or(origNanIdx, rotatedNanIdx)\n            # get the correlation between the original and rotated images\n            rotationalCorrVals[angle] = stats.pearsonr(\n                autoCorrMiddleRescaled.ravel()[~allNans], rotatedA.ravel()[~allNans]\n            )[0]\n            rotationArr[idx] = rotationalCorrVals[angle]\n    except Exception:\n        return gridscore, rotationalCorrVals, rotationArr\n    gridscore = np.min((rotationalCorrVals[60], rotationalCorrVals[120])) - np.max(\n        (rotationalCorrVals[150], rotationalCorrVals[30], rotationalCorrVals[90])\n    )\n    return gridscore, rotationalCorrVals, rotationArr\n</code></pre>"},{"location":"reference/#ephysiopy.common.fieldcalcs.kl_spatial_sparsity","title":"<code>kl_spatial_sparsity(pos_map)</code>","text":"<p>Calculates a measure of spatial sampling of an arena by comparing the given spatial sampling to a uniform one using kl divergence</p> <p>Data in pos_map should be unsmoothed (not checked) and the MapType should be POS (checked)</p> Source code in <code>ephysiopy/common/fieldcalcs.py</code> <pre><code>def kl_spatial_sparsity(pos_map: BinnedData) -&gt; float:\n    \"\"\"\n    Calculates a measure of spatial sampling of an arena by comparing the\n    given spatial sampling to a uniform one using kl divergence\n\n    Data in pos_map should be unsmoothed (not checked) and the MapType should\n    be POS (checked)\n    \"\"\"\n    assert pos_map.map_type == MapType.POS\n    return kldiv_dir(np.ravel(pos_map.binned_data[0]))\n</code></pre>"},{"location":"reference/#ephysiopy.common.fieldcalcs.kldiv","title":"<code>kldiv(X, pvect1, pvect2, variant='')</code>","text":"<p>Calculates the Kullback-Leibler or Jensen-Shannon divergence between two distributions.</p> <p>Args:     X (array_like): Vector of M variable values     P1 (array_like): Length-M vector of probabilities representing     distribution 1     P2 (array_like): Length-M vector of probabilities representing     distribution 2     sym (str, optional): If 'sym', returns a symmetric variant of the         Kullback-Leibler divergence, given by [KL(P1,P2)+KL(P2,P1)]/2     js (str, optional): If 'js', returns the Jensen-Shannon divergence,     given by         [KL(P1,Q)+KL(P2,Q)]/2, where Q = (P1+P2)/2</p> <p>Returns:     float: The Kullback-Leibler divergence or Jensen-Shannon divergence</p> <p>Notes:     The Kullback-Leibler divergence is given by:</p> <pre><code>.. math:: KL(P1(x),P2(x)) = sum_[P1(x).log(P1(x)/P2(x))]\n\nIf X contains duplicate values, there will be an warning message,\nand these values will be treated as distinct values.  (I.e., the\nactual values do not enter into the computation, but the probabilities\nfor the two duplicate values will be considered as probabilities\ncorresponding to two unique values.).\nThe elements of probability vectors P1 and P2 must\neach sum to 1 +/- .00001.\n\nThis function is taken from one on the Mathworks file exchange\n</code></pre> <p>See Also:     Cover, T.M. and J.A. Thomas. \"Elements of Information Theory,\" Wiley,     1991.</p> <pre><code>https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence\n</code></pre> Source code in <code>ephysiopy/common/fieldcalcs.py</code> <pre><code>def kldiv(\n    X: np.ndarray, pvect1: np.ndarray, pvect2: np.ndarray, variant: str = \"\"\n) -&gt; float:\n    \"\"\"\n    Calculates the Kullback-Leibler or Jensen-Shannon divergence between\n    two distributions.\n\n    Args:\n        X (array_like): Vector of M variable values\n        P1 (array_like): Length-M vector of probabilities representing\n        distribution 1\n        P2 (array_like): Length-M vector of probabilities representing\n        distribution 2\n        sym (str, optional): If 'sym', returns a symmetric variant of the\n            Kullback-Leibler divergence, given by [KL(P1,P2)+KL(P2,P1)]/2\n        js (str, optional): If 'js', returns the Jensen-Shannon divergence,\n        given by\n            [KL(P1,Q)+KL(P2,Q)]/2, where Q = (P1+P2)/2\n\n    Returns:\n        float: The Kullback-Leibler divergence or Jensen-Shannon divergence\n\n    Notes:\n        The Kullback-Leibler divergence is given by:\n\n        .. math:: KL(P1(x),P2(x)) = sum_[P1(x).log(P1(x)/P2(x))]\n\n        If X contains duplicate values, there will be an warning message,\n        and these values will be treated as distinct values.  (I.e., the\n        actual values do not enter into the computation, but the probabilities\n        for the two duplicate values will be considered as probabilities\n        corresponding to two unique values.).\n        The elements of probability vectors P1 and P2 must\n        each sum to 1 +/- .00001.\n\n        This function is taken from one on the Mathworks file exchange\n\n    See Also:\n        Cover, T.M. and J.A. Thomas. \"Elements of Information Theory,\" Wiley,\n        1991.\n\n        https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence\n    \"\"\"\n\n    if len(np.unique(X)) != len(np.sort(X)):\n        warnings.warn(\n            \"X contains duplicate values. Treated as distinct values.\", UserWarning\n        )\n    if (\n        not np.equal(np.shape(X), np.shape(pvect1)).all()\n        or not np.equal(np.shape(X), np.shape(pvect2)).all()\n    ):\n        raise ValueError(\"Inputs are not the same size\")\n    if (np.abs(np.sum(pvect1) - 1) &gt; 0.00001) or (np.abs(np.sum(pvect2) - 1) &gt; 0.00001):\n        print(f\"Probabilities sum to {np.abs(np.sum(pvect1))} for pvect1\")\n        print(f\"Probabilities sum to {np.abs(np.sum(pvect2))} for pvect2\")\n        warnings.warn(\"Probabilities don\" \"t sum to 1.\", UserWarning)\n    if variant:\n        if variant == \"js\":\n            logqvect = np.log2((pvect2 + pvect1) / 2)\n            KL = 0.5 * (\n                np.nansum(pvect1 * (np.log2(pvect1) - logqvect))\n                + np.sum(pvect2 * (np.log2(pvect2) - logqvect))\n            )\n            return float(KL)\n        elif variant == \"sym\":\n            KL1 = np.nansum(pvect1 * (np.log2(pvect1) - np.log2(pvect2)))\n            KL2 = np.nansum(pvect2 * (np.log2(pvect2) - np.log2(pvect1)))\n            KL = (KL1 + KL2) / 2\n            return float(KL)\n        else:\n            warnings.warn(\"Last argument not recognised\", UserWarning)\n    KL = np.nansum(pvect1 * (np.log2(pvect1) - np.log2(pvect2)))\n    return float(KL)\n</code></pre>"},{"location":"reference/#ephysiopy.common.fieldcalcs.kldiv_dir","title":"<code>kldiv_dir(polarPlot)</code>","text":"<p>Returns a kl divergence for directional firing: measure of directionality. Calculates kl diveregence between a smoothed ratemap (probably should be smoothed otherwise information theoretic measures don't 'care' about position of bins relative to one another) and a pure circular distribution. The larger the divergence the more tendancy the cell has to fire when the animal faces a specific direction.</p> <p>Args:     polarPlot (1D-array): The binned and smoothed directional ratemap</p> <p>Returns:     klDivergence (float): The divergence from circular of the 1D-array     from a uniform circular distribution</p> Source code in <code>ephysiopy/common/fieldcalcs.py</code> <pre><code>def kldiv_dir(polarPlot: np.ndarray) -&gt; float:\n    \"\"\"\n    Returns a kl divergence for directional firing: measure of directionality.\n    Calculates kl diveregence between a smoothed ratemap (probably should be\n    smoothed otherwise information theoretic measures\n    don't 'care' about position of bins relative to one another) and a\n    pure circular distribution.\n    The larger the divergence the more tendancy the cell has to fire when the\n    animal faces a specific direction.\n\n    Args:\n        polarPlot (1D-array): The binned and smoothed directional ratemap\n\n    Returns:\n        klDivergence (float): The divergence from circular of the 1D-array\n        from a uniform circular distribution\n    \"\"\"\n\n    __inc = 0.00001\n    polarPlot = np.atleast_2d(polarPlot)\n    polarPlot[np.isnan(polarPlot)] = __inc\n    polarPlot[polarPlot == 0] = __inc\n    normdPolar = polarPlot / float(np.nansum(polarPlot))\n    nDirBins = polarPlot.shape[1]\n    compCirc = np.ones_like(polarPlot) / float(nDirBins)\n    X = np.arange(0, nDirBins)\n    kldivergence = kldiv(np.atleast_2d(X), normdPolar, compCirc)\n    return kldivergence\n</code></pre>"},{"location":"reference/#ephysiopy.common.fieldcalcs.limit_to_one","title":"<code>limit_to_one(A, prc=50, min_dist=5)</code>","text":"<p>Processes a multi-peaked ratemap (ie grid cell) and returns a matrix where the multi-peaked ratemap consist of a single peaked field that is a) not connected to the border and b) close to the middle of the ratemap</p> Source code in <code>ephysiopy/common/fieldcalcs.py</code> <pre><code>def limit_to_one(A, prc=50, min_dist=5):\n    \"\"\"\n    Processes a multi-peaked ratemap (ie grid cell) and returns a matrix\n    where the multi-peaked ratemap consist of a single peaked field that is\n    a) not connected to the border and b) close to the middle of the\n    ratemap\n    \"\"\"\n    Ac = A.copy()\n    Ac[np.isnan(A)] = 0\n    # smooth Ac more to remove local irregularities\n    n = ny = 5\n    x, y = np.mgrid[-n : n + 1, -ny : ny + 1]\n    g = np.exp(-(x**2 / float(n) + y**2 / float(ny)))\n    g = g / g.sum()\n    Ac = signal.convolve(Ac, g, mode=\"same\")\n    # remove really small values\n    Ac[Ac &lt; 1e-10] = 0\n    Ac_r = skimage.exposure.rescale_intensity(\n        Ac, in_range=\"image\", out_range=(0, 1000)\n    ).astype(np.int32)\n    peak_idx = skimage.feature.peak_local_max(\n        Ac_r, min_distance=min_dist, exclude_border=False\n    )\n    peak_mask = np.zeros_like(Ac, dtype=bool)\n    peak_mask[tuple(peak_idx.T)] = True\n    peak_labels = skimage.measure.label(peak_mask, connectivity=2)\n    field_labels = watershed(image=Ac * -1, markers=peak_labels)\n    nFields = np.max(field_labels)\n    sub_field_mask = np.zeros((nFields, Ac.shape[0], Ac.shape[1]))\n    labelled_sub_field_mask = np.zeros_like(sub_field_mask)\n    sub_field_props = skimage.measure.regionprops(field_labels, intensity_image=Ac)\n    sub_field_centroids = []\n    sub_field_size = []\n\n    for sub_field in sub_field_props:\n        tmp = np.zeros(Ac.shape).astype(bool)\n        tmp[sub_field.coords[:, 0], sub_field.coords[:, 1]] = True\n        tmp2 = Ac &gt; sub_field.max_intensity * (prc / float(100))\n        sub_field_mask[sub_field.label - 1, :, :] = np.logical_and(tmp2, tmp)\n        labelled_sub_field_mask[sub_field.label - 1, np.logical_and(tmp2, tmp)] = (\n            sub_field.label\n        )\n        sub_field_centroids.append(sub_field.centroid)\n        sub_field_size.append(sub_field.area)  # in bins\n    sub_field_mask = np.sum(sub_field_mask, 0)\n    middle = np.round(np.array(A.shape) / 2)\n    normd_dists = sub_field_centroids - middle\n    field_dists_from_middle = np.hypot(normd_dists[:, 0], normd_dists[:, 1])\n    central_field_idx = np.argmin(field_dists_from_middle)\n    central_field = np.squeeze(labelled_sub_field_mask[central_field_idx, :, :])\n    # collapse the labelled mask down to an 2d array\n    labelled_sub_field_mask = np.sum(labelled_sub_field_mask, 0)\n    # clear the border\n    cleared_mask = skimage.segmentation.clear_border(central_field)\n    # check we've still got stuff in the matrix or fail\n    if ~np.any(cleared_mask):\n        print(\"No fields were detected away from edges so nothing returned\")\n        return None, None, None\n    else:\n        central_field_props = sub_field_props[central_field_idx]\n    return central_field_props, central_field, central_field_idx\n</code></pre>"},{"location":"reference/#ephysiopy.common.fieldcalcs.local_threshold","title":"<code>local_threshold(A, prc=50, min_dist=5)</code>","text":"<p>Locally thresholds a ratemap to take only the surrounding prc amount around any local peak</p> Source code in <code>ephysiopy/common/fieldcalcs.py</code> <pre><code>def local_threshold(A, prc=50, min_dist=5):\n    \"\"\"\n    Locally thresholds a ratemap to take only the surrounding prc amount\n    around any local peak\n    \"\"\"\n    Ac = A.copy()\n    nanidx = np.isnan(Ac)\n    Ac[nanidx] = 0\n    # smooth Ac more to remove local irregularities\n    n = ny = 5\n    x, y = np.mgrid[-n : n + 1, -ny : ny + 1]\n    g = np.exp(-(x**2 / float(n) + y**2 / float(ny)))\n    g = g / g.sum()\n    Ac = signal.convolve(Ac, g, mode=\"same\")\n    # rescale the image going in to peak_local_max and cast to\n    # integer dtype as there is an invert operation internally\n    # that only works in int or bool dtypes\n    Ac_r = skimage.exposure.rescale_intensity(\n        Ac, in_range=\"image\", out_range=(0, 1000)\n    ).astype(np.int32)\n    peak_idx = skimage.feature.peak_local_max(\n        Ac_r, min_distance=min_dist, exclude_border=False\n    )\n    peak_mask = np.zeros_like(Ac, dtype=bool)\n    peak_mask[tuple(peak_idx.T)] = True\n    peak_labels = skimage.measure.label(peak_mask, connectivity=2)\n    field_labels = watershed(image=Ac * -1, markers=peak_labels)\n    nFields = np.max(field_labels)\n    sub_field_mask = np.zeros((nFields, Ac.shape[0], Ac.shape[1]))\n    sub_field_props = skimage.measure.regionprops(field_labels, intensity_image=Ac)\n    sub_field_centroids = []\n    sub_field_size = []\n\n    for sub_field in sub_field_props:\n        tmp = np.zeros(Ac.shape).astype(bool)\n        tmp[sub_field.coords[:, 0], sub_field.coords[:, 1]] = True\n        tmp2 = Ac &gt; sub_field.max_intensity * (prc / float(100))\n        sub_field_mask[sub_field.label - 1, :, :] = np.logical_and(tmp2, tmp)\n        sub_field_centroids.append(sub_field.centroid)\n        sub_field_size.append(sub_field.area)  # in bins\n    sub_field_mask = np.sum(sub_field_mask, 0)\n    A_out = np.zeros_like(A)\n    A_out[sub_field_mask.astype(bool)] = A[sub_field_mask.astype(bool)]\n    A_out[nanidx] = np.nan\n    return A_out\n</code></pre>"},{"location":"reference/#ephysiopy.common.fieldcalcs.partitionFields","title":"<code>partitionFields(binned_data, field_threshold_percent=50, field_rate_threshold=0.5, area_threshold=0.01)</code>","text":"<p>Partitions fields.</p> <p>Partitions spikes into fields by finding the watersheds around the peaks of a super-smoothed ratemap</p> <p>Parameters:</p> Name Type Description Default <code>binned_data</code> <code>BinnedData</code> required <code>field_threshold_percent</code> <code>int</code> <pre><code>                        of the maximum firing rate in the field\n</code></pre> <code>50</code> <code>field_rate_threshold</code> <code>float</code> <code>0.5</code> <code>area_threshold</code> <pre><code>                 environment size. Default of 0.01 says a field has to be at\n                 least 1% of the size of the environment i.e.\n                 binned_area_width * binned_area_height to be counted as a field\n</code></pre> <code>0.01</code> <p>Returns:</p> Type Description <code>tuple[np.ndarray] - including:</code> <p>peaksXY (array_like): The xy coordinates of the peak rates in each field peaksRate (array_like): The peak rates in peaksXY labels (numpy.ndarray): An array of the labels corresponding to each field (starting  1) rmap (numpy.ndarray): The ratemap of the tetrode / cluster</p> Source code in <code>ephysiopy/common/fieldcalcs.py</code> <pre><code>def partitionFields(\n    binned_data: BinnedData,\n    field_threshold_percent: int = 50,\n    field_rate_threshold: float = 0.5,\n    area_threshold=0.01,\n) -&gt; tuple[np.ndarray, ...]:\n    \"\"\"\n    Partitions fields.\n\n    Partitions spikes into fields by finding the watersheds around the\n    peaks of a super-smoothed ratemap\n\n    Parameters\n    ----------\n    binned_data (BinnedData) - an instance of ephysiopy.common.utils.BinnedData\n    field_threshold_percent (int) - removes pixels in a field that fall below this percent\n                                    of the maximum firing rate in the field\n    field_rate_threshold (float) - anything below this firing rate in Hz threshold is set to 0\n    area_threshold (float) - defines the minimum field size as a proportion of the\n                             environment size. Default of 0.01 says a field has to be at\n                             least 1% of the size of the environment i.e.\n                             binned_area_width * binned_area_height to be counted as a field\n\n    Returns\n    -------\n    tuple[np.ndarray] - including:\n        peaksXY (array_like): The xy coordinates of the peak rates in\n        each field\n        peaksRate (array_like): The peak rates in peaksXY\n        labels (numpy.ndarray): An array of the labels corresponding to\n        each field (starting  1)\n        rmap (numpy.ndarray): The ratemap of the tetrode / cluster\n    \"\"\"\n    ye, xe = binned_data.bin_edges\n    rmap = binned_data[0]\n    # start image processing:\n    # Usually the binned_data has a large number of bins which potentially\n    # leaves lots of \"holes\" in the ratemap (nans) as there will be lots of\n    # positions that aren't sampled. Get over this by preserving areas outside\n    # the sampled area as nans whilst filling in the nans that live within the\n    # receptive fields\n    rmap_filled = infill_ratemap(rmap.binned_data)\n    # get the labels\n    # binarise the ratemap so that anything above field_rate_threshold is set to 1\n    # and anything below to 0\n    rmap_to_label = copy.copy(rmap_filled.data)\n    rmap_to_label[np.isnan(rmap_filled)] = 0\n    rmap_to_label[rmap_to_label &gt; field_rate_threshold] = 1\n    rmap_to_label[rmap_to_label &lt; field_rate_threshold] = 0\n    labels = skimage.measure.label(rmap_to_label, background=0)\n    # labels is now a labelled int array from 0 to however many fields have\n    # been detected\n    # Get the coordinates of the peak firing rate within each firing field...\n    fieldId, _ = np.unique(labels, return_index=True)\n    fieldId = fieldId[1::]\n    peakCoords = np.array(\n        ndi.maximum_position(rmap_filled, labels=labels, index=fieldId)\n    ).astype(int)\n    # ... and convert these to actual x-y coordinates wrt to the position data\n    peaksXY = np.vstack((xe[peakCoords[:, 1]], ye[peakCoords[:, 0]]))\n\n    # TODO: this labeled_comprehension is not working too well for fields that\n    # have a fairly uniform firing rate distribution across them (using np.nanmax\n    # in the function fn defined for use in the labeled_comprehension)\n    # or those that have nicely gaussian shaped fields (which was using np.nanmedian)\n    # find the peak rate at each of the centre of the detected fields to\n    # subsequently threshold the field at some fraction of the peak value\n    # use a labeled_comprehension to do this\n\n    def fn(val, pos):\n        return pos[val &lt; (np.nanmax(val) * (field_threshold_percent / 100))]\n\n    #\n    indices = ndi.labeled_comprehension(\n        rmap_filled, labels, None, fn, np.ndarray, 0, True\n    )\n\n    # breakpoint()\n    labels[np.unravel_index(indices=indices, shape=labels.shape)] = 0\n    min_field_size = np.ceil(np.prod(labels.shape) * area_threshold).astype(int)\n    # breakpoint()\n    labels = skimage.morphology.remove_small_objects(\n        labels, min_size=min_field_size, connectivity=2\n    )\n    # relable the fields\n    labels = skimage.segmentation.relabel_sequential(labels)[0]\n\n    # re-calculate the peakCoords array as we may have removed some\n    # objects\n    fieldId, _ = np.unique(labels, return_index=True)\n    fieldId = fieldId[1::]\n    # breakpoint()\n    peakCoords = np.array(\n        ndi.maximum_position(rmap_filled, labels=labels, index=fieldId)\n    ).astype(int)\n    peaksXY = np.vstack((xe[peakCoords[:, 1]], ye[peakCoords[:, 0]]))\n    peakRates = rmap_filled[peakCoords[:, 0], peakCoords[:, 1]]\n    peakLabels = labels[peakCoords[:, 0], peakCoords[:, 1]]\n    peaksXY = peaksXY[:, peakLabels - 1]\n    peaksRate = peakRates[peakLabels - 1]\n    return peaksXY, peaksRate, labels, rmap_filled\n</code></pre>"},{"location":"reference/#ephysiopy.common.fieldcalcs.skaggs_info","title":"<code>skaggs_info(ratemap, dwelltimes, **kwargs)</code>","text":"<p>Calculates Skaggs information measure</p> <p>Args:     ratemap (array_like): The binned up ratemap     dwelltimes (array_like): Must be same size as ratemap</p> <p>Returns:     bits_per_spike (float): Skaggs information score</p> <p>Notes:     THIS DATA SHOULD UNDERGO ADAPTIVE BINNING     See getAdaptiveMap() in binning class</p> <pre><code>Returns Skaggs et al's estimate of spatial information\nin bits per spike:\n\n.. math:: I = sum_{x} p(x).r(x).log(r(x)/r)\n</code></pre> Source code in <code>ephysiopy/common/fieldcalcs.py</code> <pre><code>def skaggs_info(ratemap, dwelltimes, **kwargs):\n    \"\"\"\n    Calculates Skaggs information measure\n\n    Args:\n        ratemap (array_like): The binned up ratemap\n        dwelltimes (array_like): Must be same size as ratemap\n\n    Returns:\n        bits_per_spike (float): Skaggs information score\n\n    Notes:\n        THIS DATA SHOULD UNDERGO ADAPTIVE BINNING\n        See getAdaptiveMap() in binning class\n\n        Returns Skaggs et al's estimate of spatial information\n        in bits per spike:\n\n        .. math:: I = sum_{x} p(x).r(x).log(r(x)/r)\n    \"\"\"\n    sample_rate = kwargs.get(\"sample_rate\", 50)\n\n    dwelltimes = dwelltimes / sample_rate  # assumed sample rate of 50Hz\n    if ratemap.ndim &gt; 1:\n        ratemap = np.reshape(ratemap, (np.prod(np.shape(ratemap)), 1))\n        dwelltimes = np.reshape(dwelltimes, (np.prod(np.shape(dwelltimes)), 1))\n    duration = np.nansum(dwelltimes)\n    meanrate = np.nansum(ratemap * dwelltimes) / duration\n    if meanrate &lt;= 0.0:\n        bits_per_spike = np.nan\n        return bits_per_spike\n    p_x = dwelltimes / duration\n    p_r = ratemap / meanrate\n    dum = p_x * ratemap\n    ind = np.nonzero(dum)[0]\n    bits_per_spike = np.nansum(dum[ind] * np.log2(p_r[ind]))\n    bits_per_spike = bits_per_spike / meanrate\n    return bits_per_spike\n</code></pre>"},{"location":"reference/#ephysiopy.common.fieldcalcs.spatial_sparsity","title":"<code>spatial_sparsity(rate_map, pos_map)</code>","text":"<p>Calculates the spatial sparsity of a rate map as defined by Markus et al (1994)</p> <p>For example, a sparsity score of 0.10 indicates that the cell fired on 10% of the maze surface</p> <p>Args:     rate_map (np.ndarray): The rate map     pos_map (np.ndarray): The occupancy map</p> <p>Returns:     float: The spatial sparsity of the rate map</p> <p>References:     Markus, E.J., Barnes, C.A., McNaughton, B.L., Gladden, V.L. &amp;     Skaggs, W.E. Spatial information content and reliability of     hippocampal CA1 neurons: effects of visual input. Hippocampus     4, 410\u2013421 (1994).</p> Source code in <code>ephysiopy/common/fieldcalcs.py</code> <pre><code>def spatial_sparsity(rate_map: np.ndarray, pos_map: np.ndarray) -&gt; float:\n    \"\"\"\n    Calculates the spatial sparsity of a rate map as defined by\n    Markus et al (1994)\n\n    For example, a sparsity score of 0.10 indicates that the cell fired on\n    10% of the maze surface\n\n    Args:\n        rate_map (np.ndarray): The rate map\n        pos_map (np.ndarray): The occupancy map\n\n    Returns:\n        float: The spatial sparsity of the rate map\n\n    References:\n        Markus, E.J., Barnes, C.A., McNaughton, B.L., Gladden, V.L. &amp;\n        Skaggs, W.E. Spatial information content and reliability of\n        hippocampal CA1 neurons: effects of visual input. Hippocampus\n        4, 410\u2013421 (1994).\n\n    \"\"\"\n    p_i = pos_map / np.nansum(pos_map)\n    sparsity = np.nansum(p_i * rate_map) ** 2 / np.nansum(p_i * rate_map**2)\n    return sparsity\n</code></pre>"},{"location":"reference/#phase-coding","title":"Phase coding","text":""},{"location":"reference/#ephysiopy.common.phasecoding.phasePrecession2D","title":"<code>phasePrecession2D</code>","text":"<p>               Bases: <code>object</code></p> <p>Performs phase precession analysis for single unit data</p> <p>Mostly a total rip-off of code written by Ali Jeewajee for his paper on 2D phase precession in place and grid cells [1]_</p> <p>.. [1] Jeewajee A, Barry C, Douchamps V, Manson D, Lever C, Burgess N.     Theta phase precession of grid and place cell firing in open     environments.     Philos Trans R Soc Lond B Biol Sci. 2013 Dec 23;369(1635):20120532.     doi: 10.1098/rstb.2012.0532.</p> <p>Args:     lfp_sig (np.array): The LFP signal against which cells might precess...     lfp_fs (int): The sampling frequency of the LFP signal     xy (np.array): The position data as 2 x num_position_samples     spike_ts (np.array): The times in samples at which the cell fired     pos_ts (np.array): The times in samples at which position was captured     pp_config (dict): Contains parameters for running the analysis.         See phase_precession_config dict in ephysiopy.common.eegcalcs</p> Source code in <code>ephysiopy/common/phasecoding.py</code> <pre><code>class phasePrecession2D(object):\n    \"\"\"\n    Performs phase precession analysis for single unit data\n\n    Mostly a total rip-off of code written by Ali Jeewajee for his paper on\n    2D phase precession in place and grid cells [1]_\n\n    .. [1] Jeewajee A, Barry C, Douchamps V, Manson D, Lever C, Burgess N.\n        Theta phase precession of grid and place cell firing in open\n        environments.\n        Philos Trans R Soc Lond B Biol Sci. 2013 Dec 23;369(1635):20120532.\n        doi: 10.1098/rstb.2012.0532.\n\n    Args:\n        lfp_sig (np.array): The LFP signal against which cells might precess...\n        lfp_fs (int): The sampling frequency of the LFP signal\n        xy (np.array): The position data as 2 x num_position_samples\n        spike_ts (np.array): The times in samples at which the cell fired\n        pos_ts (np.array): The times in samples at which position was captured\n        pp_config (dict): Contains parameters for running the analysis.\n            See phase_precession_config dict in ephysiopy.common.eegcalcs\n    \"\"\"\n\n    def __init__(\n        self,\n        lfp_sig: np.ndarray,\n        lfp_fs: int,\n        xy: np.ndarray,\n        spike_ts: np.ndarray,\n        pos_ts: np.ndarray,\n        pp_config: dict = phase_precession_config,\n        regressors=None,\n    ):\n        # Set up the parameters\n        # this sets a bunch of member attributes from the pp_config dict\n        self.orig_xy = xy\n        self.update_config(pp_config)\n        self._pos_ts = pos_ts\n\n        self.update_regressors(regressors)\n\n        self.regressor = 1000\n        self.alpha = 0.05\n        self.hyp = 0\n        self.conf = True\n\n        # Process the EEG data a bit...\n        self.eeg = lfp_sig\n        L = LFPOscillations(lfp_sig, lfp_fs)\n        self.min_theta = pp_config[\"min_theta\"]\n        self.max_theta = pp_config[\"max_theta\"]\n        filt_sig, phase, _, _, _ = L.getFreqPhase(\n            lfp_sig, [self.min_theta, self.max_theta], 2\n        )\n        self.filteredEEG = filt_sig\n        self.phase = phase\n        self.phaseAdj = np.ma.MaskedArray\n\n        self.update_position(self.ppm, cm=self.convert_xy_2_cm)\n        self.update_rate_map()\n\n        spk_times_in_pos_samples = self.getSpikePosIndices(spike_ts)\n        spk_weights = np.bincount(spk_times_in_pos_samples, minlength=len(self.pos_ts))\n        self.spike_times_in_pos_samples = spk_times_in_pos_samples\n        self.spk_weights = spk_weights\n\n        self.spike_ts = spike_ts\n\n    @property\n    def pos_ts(self):\n        return self._pos_ts\n\n    @pos_ts.setter\n    def pos_ts(self, value):\n        self._pos_ts = value\n\n    @property\n    def spike_eeg_idx(self):\n        return (self.spike_ts * self.lfp_sample_rate).astype(int)\n\n    @property\n    def spike_pos_idx(self):\n        return (self.spike_ts * self.pos_sample_rate).astype(int)\n\n    def update_regressors(self, reg_keys: list):\n        \"\"\"\n        Create a dict to hold the stats values for\n        each regressor\n        Default regressors are:\n            \"spk_numWithinRun\",\n            \"pos_exptdRate_cum\",\n            \"pos_instFR\",\n            \"pos_timeInRun\",\n            \"pos_d_cum\",\n            \"pos_d_meanDir\",\n            \"pos_d_currentdir\",\n            \"spk_thetaBatchLabelInRun\"\n\n        NB: The regressors have differing sizes of 'values' depending on the\n        type of the regressor:\n        spk_* - integer values of the spike number within a run or the theta batch\n                in a run, so has a length equal to the number of spikes collected\n        pos_* - a bincount of some type so equal to the number of position samples\n                collected\n        eeg_* - only one at present, the instantaneous firing rate binned into the\n                number of eeg samples so equal to that in length\n        \"\"\"\n        if reg_keys is None:\n            reg_keys = all_regressors\n        else:\n            assert all([regressor in all_regressors for regressor in reg_keys])\n\n        # Create a dict to hold the stats values for\n        # each regressor\n        stats_dict = {\n            \"values\": np.ma.MaskedArray,\n            \"pha\": np.ma.MaskedArray,\n            \"slope\": float,\n            \"intercept\": float,\n            \"cor\": float,\n            \"p\": float,\n            \"cor_boot\": float,\n            \"p_shuffled\": float,\n            \"ci\": float,\n            \"reg\": float,\n        }\n        self.regressors = {}\n        self.regressors = defaultdict(lambda: stats_dict.copy(), self.regressors)\n        [self.regressors[regressor] for regressor in reg_keys]\n        # each of the regressors in regressor_keys is a key with a value\n        # of stats_dict\n\n    def update_regressor_values(self, key: str, values):\n        # Check whether values is a masked array and if not make it one\n        self.regressors[key][\"values\"] = values\n\n    def update_regressor_mask(self, key: str, indices):\n        # Mask entries in the 'values' and 'pha' arrays of the relevant regressor\n        breakpoint()\n        self.regressors[key][\"values\"].mask[indices] = False\n\n    def get_regressors(self):\n        return self.regressors.keys()\n\n    def get_regressor(self, key):\n        return self.regressors[key]\n\n    def update_config(self, pp_config):\n        [\n            setattr(self, attribute, pp_config[attribute])\n            for attribute in pp_config.keys()\n        ]\n\n    def update_position(self, ppm: float, cm: bool):\n        P = PosCalcsGeneric(\n            self.orig_xy[0, :],\n            self.orig_xy[1, :],\n            ppm=ppm,\n            convert2cm=cm,\n        )\n        P.postprocesspos(tracker_params={\"AxonaBadValue\": 1023})\n        # ... do the ratemap creation here once\n        self.PosData = P\n\n    def update_rate_map(self):\n        R = RateMap(self.PosData, xyInCms=self.convert_xy_2_cm)\n        R.binsize = self.bins_per_cm\n        R.smooth_sz = self.field_smoothing_kernel_len\n        R.ppm = self.ppm\n        self.RateMap = R  # this will be used a fair bit below\n\n    def getSpikePosIndices(self, spk_times: np.ndarray):\n        pos_times = getattr(self, \"pos_ts\")\n        idx = np.searchsorted(pos_times, spk_times)\n        idx[idx == len(pos_times)] = idx[idx == len(pos_times)] - 1\n        return idx\n\n    def performRegression(self, **kwargs):\n        \"\"\"\n        Wrapper function for doing the actual regression which has multiple\n        stages.\n\n        Specifically here we partition fields into sub-fields, get a bunch of\n        information about the position, spiking and theta data and then\n        do the actual regression.\n\n        Args:\n            tetrode (int): The tetrode to examine\n            cluster (int): The cluster to examine\n            laserEvents (array_like, optional): The on times for laser events\n            if present. Default is None\n        Valid keyword args:\n            plot (bool): whether to plot the results of field partitions, the regression(s)\n                etc\n        See Also:\n            ephysiopy.common.eegcalcs.phasePrecession.partitionFields()\n            ephysiopy.common.eegcalcs.phasePrecession.getPosProps()\n            ephysiopy.common.eegcalcs.phasePrecession.getThetaProps()\n            ephysiopy.common.eegcalcs.phasePrecession.getSpikeProps()\n            ephysiopy.common.eegcalcs.phasePrecession._ppRegress()\n        \"\"\"\n        do_plot = kwargs.get(\"plot\", False)\n\n        # Partition fields - comes from ephysiopy.common.fieldca\n        binned_data = self.RateMap.get_map(self.spk_weights)\n        _, _, labels, _ = partitionFields(\n            binned_data,\n            self.field_threshold_percent,\n            self.field_threshold,\n            self.area_threshold,\n        )\n\n        # split into runs\n        field_properties = self.getPosProps(labels)\n        # self.posdict = posD\n        # self.rundict = runD\n\n        # get theta cycles, amplitudes, phase etc\n        self.getThetaProps(field_properties)\n        # TODO: next: getSpikeProps(field_properties)\n        # the fields that are set within getSpikeProps() I think can be added\n        # to the individual runs within the FieldProps instance\n        # spkCount is just spikes binned wrt eeg timebase\n        #\n\n        # get the indices of spikes for various metrics such as\n        # theta cycle, run etc\n        spkD = self.getSpikeProps(field_properties)\n        # self.spkdict = spkD\n        # at this point the 'values' and 'pha' arrays in the regressors dict are all\n        # npos elements long and are masked arrays. keep as masked arrays and just modify the\n        # masks instead of truncating the data\n        # Do the regressions\n        self._ppRegress(spkD)\n\n        # Plot the results if asked\n        if do_plot:\n            n_regressors = len(self.get_regressors())\n            n_rows = np.ceil(n_regressors / 2).astype(int)\n            if n_regressors == 1:\n                fig, ax = plt.subplots(1, 1, figsize=(3, 5))\n            else:\n                fig, ax = plt.subplots(2, n_rows, figsize=(10, 10))\n            fig.canvas.manager.set_window_title(\"Regression results\")\n            if isinstance(ax, list | np.ndarray):\n                ax = flatten_list(ax)\n            if n_regressors == 1:\n                ax = [ax]\n            for ra in zip(self.get_regressors(), ax):\n                self.plotRegressor(ra[0], ra[1])\n\n    def getPosProps(\n        self,\n        labels: np.ndarray,\n    ) -&gt; list:\n        \"\"\"\n        Uses the output of partitionFields and returns vectors the same\n        length as pos.\n\n        Args:\n            tetrode, cluster (int): The tetrode / cluster to examine\n            peaksXY (array_like): The x-y coords of the peaks in the ratemap\n            laserEvents (array_like): The position indices of on events\n            (laser on)\n\n        Returns:\n            pos_dict, run_dict (dict): Contains a whole bunch of information\n            for the whole trial and also on a run-by-run basis (run_dict).\n            See the end of this function for all the key / value pairs.\n        \"\"\"\n        spikeTS = self.spike_ts  # in seconds\n        xy = self.RateMap.xy\n        xydir = self.RateMap.dir\n        # spd = self.RateMap.speed\n        spkPosInd = self.spike_pos_idx\n        nPos = xy.shape[1]\n        spkPosInd[spkPosInd &gt; nPos] = nPos - 1\n        xydir = np.squeeze(xydir)\n\n        binned_data = self.RateMap.get_map(self.spk_weights)\n        ye, xe = binned_data.bin_edges\n        rmap = binned_data.binned_data[0]\n        # The large number of bins combined with the super-smoothed ratemap\n        # will lead to fields labelled with lots of small holes in. Fill those\n        # gaps in here and calculate the perimeter of the fields based on that\n        # labelled image\n        labels, _ = ndimage.label(ndimage.binary_fill_holes(labels))\n\n        rmap_zeros = rmap.copy()\n        rmap_zeros[np.isnan(rmap)] = 0\n        xBins = np.digitize(xy[0], xe[:-1])\n        yBins = np.digitize(xy[1], ye[:-1])\n\n        observed_spikes_in_time = np.bincount(\n            (spikeTS * self.pos_sample_rate).astype(int), minlength=nPos\n        )\n\n        field_props = fieldprops(\n            labels,\n            binned_data,\n            xy,\n            observed_spikes_in_time,\n            sample_rate=self.pos_sample_rate,\n        )\n        print(\n            f\"Filtering runs for min duration {self.minimum_allowed_run_duration}, speed {self.minimum_allowed_run_speed} and min spikes {self.min_spikes}\"\n        )\n        field_props = filter_runs(\n            field_props,\n            self.minimum_allowed_run_duration,\n            self.minimum_allowed_run_speed,\n            min_spikes=1,\n        )\n        # Smooth the runs before calculating other metrics\n        [\n            f.smooth_runs(\n                self.ifr_smoothing_constant,\n                self.spatial_lowpass_cutoff,\n                self.pos_sample_rate,\n            )\n            for f in field_props\n        ]\n        if \"pos_d_currentdir\" in self.regressors.keys():\n            d_currentdir = np.ones(shape=self.PosData.npos) * np.nan\n            for f in field_props:\n                for r in f.runs:\n                    d_currentdir[r._slice] = r.current_direction\n\n            self.update_regressor_values(\"pos_d_currentdir\", d_currentdir)\n\n        # calculate the cumulative distance travelled on each run\n        # only goes from 0-1\n        if \"pos_d_cum\" in self.regressors.keys():\n            d_cumulative = np.ones(shape=self.PosData.npos) * np.nan\n            for f in field_props:\n                for r in f.runs:\n                    d_cumulative[r._slice] = r.cumulative_distance\n\n            self.update_regressor_values(\"pos_d_cum\", d_cumulative)\n\n        # calculate cumulative sum of the expected normalised firing rate\n        # only goes from 0-1\n        # NB I'm not sure why this is called expected rate as there is nothing\n        # about firing rate in this just the accumulation of rho\n        if \"pos_exptdRate_cum\" in self.regressors.keys():\n            exptd_rate_all = np.ones(shape=self.PosData.npos) * np.nan\n            rmap_infilled = infill_ratemap(rmap)\n            exptd_rate = rmap_infilled[yBins - 1, xBins - 1]\n            # setting the sample rate to 1 here will result in firing rate being returned\n            # not expected spike count\n            for f in field_props:\n                for r in f.runs:\n                    exptd_rate_all[r._slice] = r.expected_spikes(exptd_rate, 1)\n\n            self.update_regressor_values(\"pos_exptdRate_cum\", exptd_rate_all)\n\n        # direction projected onto the run mean direction is just the x coord\n        # good - remembering that xy_new is rho,phi\n        # this might be wrong - need to check i'm grabbing the right value\n        # from FieldProps... could be rho\n        if \"pos_d_meanDir\" in self.regressors.keys():\n            d_meandir = np.ones(shape=self.PosData.npos) * np.nan\n            for f in field_props:\n                for r in f.runs:\n                    d_meandir[r._slice] = r.pos_r\n\n            self.update_regressor_values(\"pos_d_meanDir\", d_meandir)\n\n        # smooth binned spikes to get an instantaneous firing rate\n        # set up the smoothing kernel\n        # all up at 1.0\n        if \"pos_instFR\" in self.regressors.keys():\n            kernLenInBins = np.round(self.ifr_kernel_len * self.bins_per_second)\n            kernSig = self.ifr_kernel_sigma * self.bins_per_second\n            regressor = signal.windows.gaussian(kernLenInBins, kernSig)\n            # apply the smoothing kernel over the binned observed spikes\n            ifr = signal.convolve(observed_spikes_in_time, regressor, mode=\"same\")\n            inst_firing_rate = np.zeros_like(ifr)\n            for field in field_props:\n                for i_slice in field.run_slices:\n                    inst_firing_rate[i_slice] = ifr[i_slice]\n            self.update_regressor_values(\"pos_instFR\", inst_firing_rate)\n\n        # find time spent within run\n        # only goes from 0-1\n        if \"pos_timeInRun\" in self.regressors.keys():\n            time_in_run = np.ones(shape=self.PosData.npos) * np.nan\n            for f in field_props:\n                for r in f.runs:\n                    time_in_run[r._slice] = r.cumulative_time / self.pos_sample_rate\n\n            self.update_regressor_values(\"pos_timeInRun\", time_in_run)\n\n        # mnSpd = np.concatenate([f.runs_speed for f in field_props])\n        # centralPeripheral = np.concatenate([f.pos_xy[1] for f in field_props])\n\n        return field_props\n\n    def getThetaProps(self, field_props: list[FieldProps]) -&gt; None:\n        \"\"\"\n        Processes the LFP data and inserts into each run within each field\n        a segment of LFP data that has had its phase and amplitude extracted\n        as well as some other metadata\n        \"\"\"\n        spikeTS = self.spike_ts\n        phase = np.ma.MaskedArray(self.phase, mask=True)\n        filteredEEG = self.filteredEEG\n        oldAmplt = filteredEEG.copy()\n        # get indices of spikes into eeg\n        spkEEGIdx = self.spike_eeg_idx\n        spkCount = np.bincount(spkEEGIdx, minlength=len(phase))\n        spkPhase = phase.copy()\n        # unmask the valid entries\n        spkPhase.mask[spkEEGIdx] = False\n        minSpikingPhase = getPhaseOfMinSpiking(spkPhase)\n        # force phase to lie between 0 and 2PI\n        phaseAdj = fixAngle(\n            phase - minSpikingPhase * (np.pi / 180) + self.allowed_min_spike_phase\n        )\n        isNegFreq = np.diff(np.unwrap(phaseAdj)) &lt; 0\n        isNegFreq = np.append(isNegFreq, isNegFreq[-1])\n        # get start of theta cycles as points where diff &gt; pi\n        phaseDf = np.diff(phaseAdj)\n        cycleStarts = phaseDf[1::] &lt; -np.pi\n        cycleStarts = np.append(cycleStarts, True)\n        cycleStarts = np.insert(cycleStarts, 0, True)\n        cycleStarts[isNegFreq] = False\n        cycleLabel = np.cumsum(cycleStarts)\n\n        # caculate power and find low power cycles\n        power = np.power(filteredEEG, 2)\n        cycleTotValidPow = np.bincount(\n            cycleLabel[~isNegFreq], weights=power[~isNegFreq]\n        )\n        cycleValidBinCount = np.bincount(cycleLabel[~isNegFreq])\n        cycleValidMnPow = cycleTotValidPow / cycleValidBinCount\n        powRejectThresh = np.percentile(\n            cycleValidMnPow, self.min_power_percent_threshold\n        )\n        cycleHasBadPow = cycleValidMnPow &lt; powRejectThresh\n\n        # find cycles too long or too short\n        allowed_theta_len = (\n            np.floor((1.0 / self.max_theta) * self.lfp_sample_rate).astype(int),\n            np.ceil((1.0 / self.min_theta) * self.lfp_sample_rate).astype(int),\n        )\n        cycleTotBinCount = np.bincount(cycleLabel)\n        cycleHasBadLen = np.logical_or(\n            cycleTotBinCount &lt; allowed_theta_len[0],\n            cycleTotBinCount &gt; allowed_theta_len[1],\n        )\n\n        # remove data calculated as 'bad'\n        isBadCycle = np.logical_or(cycleHasBadLen, cycleHasBadPow)\n        isInBadCycle = isBadCycle[cycleLabel]\n        isBad = np.logical_or(isInBadCycle, isNegFreq)\n        breakpoint()\n        # TODO: above phaseAdj is being created as a Masked array with all mask values as True...\n        phaseAdj = np.ma.MaskedArray(phaseAdj, mask=np.invert(isBad))\n        self.phaseAdj = phaseAdj\n        ampAdj = np.ma.MaskedArray(filteredEEG, mask=np.invert(isBad))\n        cycleLabel = np.ma.MaskedArray(cycleLabel, mask=np.invert(isBad))\n        self.cycleLabel = cycleLabel\n        spkCount = np.ma.MaskedArray(spkCount, mask=np.invert(isBad))\n        # Extract all the relevant values from the arrays above and\n        # add to each run\n        lfp_to_pos_ratio = self.lfp_sample_rate / self.pos_sample_rate\n        for field in field_props:\n            for run in field.runs:\n                lfp_slice = slice(\n                    int(run._slice.start * lfp_to_pos_ratio),\n                    int(run._slice.stop * lfp_to_pos_ratio),\n                )\n                spike_times = spikeTS[\n                    np.logical_and(\n                        spikeTS &gt; lfp_slice.start / self.lfp_sample_rate,\n                        spikeTS &lt; lfp_slice.stop / self.lfp_sample_rate,\n                    )\n                ]\n                lfp_segment = LFPSegment(\n                    field.label,\n                    run.label,\n                    lfp_slice,\n                    spike_times,\n                    self.eeg[lfp_slice],\n                    self.filteredEEG[lfp_slice],\n                    phaseAdj[lfp_slice],\n                    ampAdj[lfp_slice],\n                    self.lfp_sample_rate,\n                    [self.min_theta, self.max_theta],\n                )\n                run.lfp_data = lfp_segment\n\n    def getSpikeProps(self, field_props: list):\n        # TODO: the regressor values here need updating so they are the same length\n        # as the number of positions and masked in the correct places to maintain\n        # consistency with the regressors added in the getPosProps method\n        spikeTS = self.spike_ts\n        spkPosIdx = self.spike_pos_idx\n        spkEEGIdx = self.spike_eeg_idx\n\n        durations = flatten_list([[r.duration for r in f.runs] for f in field_props])\n        durations = np.array(durations) / self.pos_sample_rate\n        spk_counts = flatten_list([[r.n_spikes for r in f.runs] for f in field_props])\n        spk_counts = np.array(spk_counts)\n        run_firing_rates = spk_counts / durations\n\n        runLabel = np.zeros(shape=self.PosData.npos, dtype=int)\n        for field in field_props:\n            for run in field.runs:\n                runLabel[run._slice] = run.label\n\n        spkRunLabel = runLabel[spkPosIdx]\n        # NB: Unlike the old way of doing this the run labels won't be\n        # in ascending order as the runs are extracted from the fields themselves\n        # which are basically labelled using skimage.measure.label (see partitionFields)\n\n        cycleLabel = self.cycleLabel\n        thetaCycleLabel = cycleLabel[spkEEGIdx]\n\n        firstInTheta = thetaCycleLabel[-1:] != thetaCycleLabel[1::]\n        firstInTheta = np.insert(firstInTheta, 0, True)\n        # lastInTheta = firstInTheta[1::]\n        numWithinRun = labelledCumSum(np.ones_like(spkRunLabel), spkRunLabel)\n        thetaBatchLabelInRun = labelledCumSum(firstInTheta.astype(float), spkRunLabel)\n\n        # NB this is NOT the firing rate in pos bins but rather spikes/second\n        rateInPosBins = run_firing_rates\n        # update the regressor dict from __init__ with relevant values\n        # all up at 1.0\n        if \"spk_numWithinRun\" in self.regressors.keys():\n            self.update_regressor_values(\"spk_numWithinRun\", numWithinRun)\n        # all up at 1.0\n        if \"spk_thetaBatchLabelInRun\" in self.regressors.keys():\n            self.update_regressor_values(\n                \"spk_thetaBatchLabelInRun\", thetaBatchLabelInRun\n            )\n        spkKeys = (\n            \"spikeTS\",\n            \"spkPosIdx\",\n            \"spkEEGIdx\",\n            \"spkRunLabel\",\n            \"thetaCycleLabel\",\n            \"firstInTheta\",\n            # \"lastInTheta\",\n            \"numWithinRun\",\n            \"thetaBatchLabelInRun\",\n            \"spk_counts\",\n            \"run_firing_rates\",\n        )\n        spkDict = dict.fromkeys(spkKeys, np.nan)\n        for thiskey in spkDict.keys():\n            spkDict[thiskey] = locals()[thiskey]\n        print(f\"Total spikes available for this cluster: {len(spikeTS)}\")\n        print(f\"Total spikes used for analysis: {np.sum(spk_counts)}\")\n        return spkDict\n\n    def _ppRegress(self, spkDict, whichSpk=\"first\"):\n\n        phase = self.phaseAdj\n        newSpkRunLabel = spkDict[\"spkRunLabel\"].copy()\n        # TODO: need code to deal with splitting the data based on a group of\n        # variables\n        spkUsed = newSpkRunLabel &gt; 0\n        # Calling compressed() method on spkUsed gives a boolean mask with length equal to\n        # the number of spikes emitted by the cluster where True is a valid spike (ie it was\n        # emitted when in a receptive field detected by the getPosProps() method above)\n        # firstInTheta (and presumably lastInTheta) need to be the same length as\n        # the number of pos samples - currently it's just the length of some smaller\n        # subset of the length of the number of spikes\n        if \"first\" in whichSpk:\n            spkUsed[~spkDict[\"firstInTheta\"]] = False\n        elif \"last\" in whichSpk:\n            if len(spkDict[\"lastInTheta\"]) &lt; len(spkDict[\"spkRunLabel\"]):\n                spkDict[\"lastInTheta\"] = np.insert(spkDict[\"lastInTheta\"], -1, False)\n            spkUsed[~spkDict[\"lastInTheta\"]] = False\n        spkPosIdxUsed = spkDict[\"spkPosIdx\"].astype(int)\n        # copy self.regressors and update with spk/ pos of interest\n        regressors = self.regressors.copy()\n        # the length of the 'values' of the regressors is dependent on the variable\n        # so 'pos_' regressors are vectors as long as the number of position samples\n        # and 'spk_' regressors are as long as the number of spikes for the current\n        # cluster. These different length vectors need to be dealt with differently..\n        for regressor in regressors.keys():\n            # breakpoint()\n            # self.update_regressor_mask(regressor, spkPosIdxUsed)\n            if regressor.startswith(\"spk_\"):\n                self.update_regressor_values(\n                    regressor, regressors[regressor][\"values\"][spkUsed]\n                )\n            elif regressor.startswith(\"pos_\"):\n                self.update_regressor_values(\n                    regressor, regressors[regressor][\"values\"][spkPosIdxUsed[spkUsed]]\n                )\n        # breakpoint()\n        phase = phase[spkDict[\"spkEEGIdx\"][spkUsed]]\n        phase = phase.astype(np.double)\n        if \"mean\" in whichSpk:\n            goodPhase = ~np.isnan(phase)\n            cycleLabels = spkDict[\"thetaCycleLabel\"][spkUsed]\n            sz = np.max(cycleLabels)\n            cycleComplexPhase = np.squeeze(np.zeros(sz, dtype=np.complex))\n            np.add.at(\n                cycleComplexPhase,\n                cycleLabels[goodPhase] - 1,\n                np.exp(1j * phase[goodPhase]),\n            )\n            phase = np.angle(cycleComplexPhase)\n            spkCountPerCycle = np.bincount(cycleLabels[goodPhase], minlength=sz)\n            for regressor in regressors.keys():\n                regressors[regressor][\"values\"] = (\n                    np.bincount(\n                        cycleLabels[goodPhase],\n                        weights=regressors[regressor][\"values\"][goodPhase],\n                        minlength=sz,\n                    )\n                    / spkCountPerCycle\n                )\n\n        goodPhase = ~np.isnan(phase)\n        for regressor in regressors.keys():\n            print(f\"Doing regression: {regressor}\")\n            goodRegressor = ~np.isnan(regressors[regressor][\"values\"])\n            if np.any(goodRegressor):\n                # breakpoint()\n                reg = regressors[regressor][\"values\"][\n                    np.logical_and(goodRegressor, goodPhase)\n                ]\n                pha = phase[np.logical_and(goodRegressor, goodPhase)]\n                # TODO: all the pha values are masked. make sure only the relevant ones are!\n                regressors[regressor][\"slope\"], regressors[regressor][\"intercept\"] = (\n                    circRegress(reg, pha)\n                )\n                regressors[regressor][\"pha\"] = pha\n                mnx = np.mean(reg)\n                reg = reg - mnx\n                mxx = np.max(np.abs(reg)) + np.spacing(1)\n                reg = reg / mxx\n                # problem regressors = instFR, pos_d_cum\n                # breakpoint()\n                theta = np.mod(np.abs(regressors[regressor][\"slope\"]) * reg, 2 * np.pi)\n                rho, p, rho_boot, p_shuff, ci = circCircCorrTLinear(\n                    theta, pha, self.regressor, self.alpha, self.hyp, self.conf\n                )\n                regressors[regressor][\"reg\"] = reg\n                regressors[regressor][\"cor\"] = rho\n                regressors[regressor][\"p\"] = p\n                regressors[regressor][\"cor_boot\"] = rho_boot\n                regressors[regressor][\"p_shuffled\"] = p_shuff\n                regressors[regressor][\"ci\"] = ci\n\n        self.reg_phase = phase\n        return regressors\n\n    def plotRegressor(self, regressor: str, ax=None):\n        assert regressor in self.regressors.keys()\n        if ax is None:\n            fig = plt.figure(figsize=(3, 5))\n            ax = fig.add_subplot(111)\n        else:\n            ax = ax\n        vals = self.regressors[regressor][\"values\"]\n        pha = self.reg_phase\n        slope = self.regressors[regressor][\"slope\"]\n        intercept = self.regressors[regressor][\"intercept\"]\n        mm = (0, -4 * np.pi, -2 * np.pi, 2 * np.pi, 4 * np.pi)\n        for m in mm:\n            ax.plot((-1, 1), (-slope + intercept + m, slope + intercept + m), \"r\", lw=3)\n            ax.plot(vals, pha + m, \"regressor.\")\n        ax.set_xlim(-1, 1)\n        xtick_locs = np.linspace(-1, 1, 3)\n        ax.set_xticks(xtick_locs, list(map(str, xtick_locs)))\n        ax.set_yticks(sorted(mm), [\"-4\u03c0\", \"-2\u03c0\", \"0\", \"2\u03c0\", \"4\u03c0\"])\n        ax.set_ylim(-2 * np.pi, 4 * np.pi)\n        title_str = f\"{regressor} vs phase: slope = {slope:.2f}, \\nintercept = {intercept:.2f}, p_shuffled = {self.regressors[regressor]['p_shuffled']:.2f}\"\n        ax.set_title(title_str, fontsize=subaxis_title_fontsize)\n        ax.set_ylabel(\"Phase\", fontsize=subaxis_title_fontsize)\n        ax.set_xlabel(\"Normalised position\", fontsize=subaxis_title_fontsize)\n        return ax\n\n    def plotPPRegression(self, regressorDict, regressor2plot=\"pos_d_cum\", ax=None):\n\n        t = self.getLFPPhaseValsForSpikeTS()\n        x = self.RateMap.xy[0, self.spike_times_in_pos_samples]\n        from ephysiopy.common import fieldcalcs\n\n        rmap = self.RateMap.get_map(self.spk_weights)\n        xe, ye = rmap.bin_edges\n        label = fieldcalcs.field_lims(rmap)\n        rmap = rmap.binned_data[0].T\n        xInField = xe[label.nonzero()[1]]\n        mask = np.logical_and(x &gt; np.min(xInField), x &lt; np.max(xInField))\n        x = x[mask]\n        t = t[mask]\n        # keep x between -1 and +1\n        mnx = np.mean(x)\n        xn = x - mnx\n        mxx = np.max(np.abs(xn))\n        x = xn / mxx\n        # keep tn between 0 and 2pi\n        t = np.remainder(t, 2 * np.pi)\n        slope, intercept = circRegress(x, t)\n        rho, p, rho_boot, p_shuff, ci = circCircCorrTLinear(x, t)\n        plt.figure()\n        if ax is None:\n            fig = plt.figure()\n            ax = fig.add_subplot(111)\n        else:\n            ax = ax\n        ax.plot(x, t, \".\", color=\"regressor\")\n        ax.plot(x, t + 2 * np.pi, \".\", color=\"regressor\")\n        mm = (0, -2 * np.pi, 2 * np.pi, 4 * np.pi)\n        for m in mm:\n            ax.plot((-1, 1), (-slope + intercept + m, slope + intercept + m), \"r\", lw=3)\n        ax.set_xlim((-1, 1))\n        ax.set_ylim((-np.pi, 3 * np.pi))\n        return {\n            \"slope\": slope,\n            \"intercept\": intercept,\n            \"rho\": rho,\n            \"p\": p,\n            \"rho_boot\": rho_boot,\n            \"p_shuff\": p_shuff,\n            \"ci\": ci,\n        }\n\n    def getLFPPhaseValsForSpikeTS(self):\n        ts = self.spike_times_in_pos_samples * (\n            self.lfp_sample_rate / self.pos_sample_rate\n        )\n        ts_idx = np.array(np.floor(ts), dtype=int)\n        return self.phase[ts_idx]\n</code></pre>"},{"location":"reference/#ephysiopy.common.phasecoding.phasePrecession2D.getPosProps","title":"<code>getPosProps(labels)</code>","text":"<p>Uses the output of partitionFields and returns vectors the same length as pos.</p> <p>Args:     tetrode, cluster (int): The tetrode / cluster to examine     peaksXY (array_like): The x-y coords of the peaks in the ratemap     laserEvents (array_like): The position indices of on events     (laser on)</p> <p>Returns:     pos_dict, run_dict (dict): Contains a whole bunch of information     for the whole trial and also on a run-by-run basis (run_dict).     See the end of this function for all the key / value pairs.</p> Source code in <code>ephysiopy/common/phasecoding.py</code> <pre><code>def getPosProps(\n    self,\n    labels: np.ndarray,\n) -&gt; list:\n    \"\"\"\n    Uses the output of partitionFields and returns vectors the same\n    length as pos.\n\n    Args:\n        tetrode, cluster (int): The tetrode / cluster to examine\n        peaksXY (array_like): The x-y coords of the peaks in the ratemap\n        laserEvents (array_like): The position indices of on events\n        (laser on)\n\n    Returns:\n        pos_dict, run_dict (dict): Contains a whole bunch of information\n        for the whole trial and also on a run-by-run basis (run_dict).\n        See the end of this function for all the key / value pairs.\n    \"\"\"\n    spikeTS = self.spike_ts  # in seconds\n    xy = self.RateMap.xy\n    xydir = self.RateMap.dir\n    # spd = self.RateMap.speed\n    spkPosInd = self.spike_pos_idx\n    nPos = xy.shape[1]\n    spkPosInd[spkPosInd &gt; nPos] = nPos - 1\n    xydir = np.squeeze(xydir)\n\n    binned_data = self.RateMap.get_map(self.spk_weights)\n    ye, xe = binned_data.bin_edges\n    rmap = binned_data.binned_data[0]\n    # The large number of bins combined with the super-smoothed ratemap\n    # will lead to fields labelled with lots of small holes in. Fill those\n    # gaps in here and calculate the perimeter of the fields based on that\n    # labelled image\n    labels, _ = ndimage.label(ndimage.binary_fill_holes(labels))\n\n    rmap_zeros = rmap.copy()\n    rmap_zeros[np.isnan(rmap)] = 0\n    xBins = np.digitize(xy[0], xe[:-1])\n    yBins = np.digitize(xy[1], ye[:-1])\n\n    observed_spikes_in_time = np.bincount(\n        (spikeTS * self.pos_sample_rate).astype(int), minlength=nPos\n    )\n\n    field_props = fieldprops(\n        labels,\n        binned_data,\n        xy,\n        observed_spikes_in_time,\n        sample_rate=self.pos_sample_rate,\n    )\n    print(\n        f\"Filtering runs for min duration {self.minimum_allowed_run_duration}, speed {self.minimum_allowed_run_speed} and min spikes {self.min_spikes}\"\n    )\n    field_props = filter_runs(\n        field_props,\n        self.minimum_allowed_run_duration,\n        self.minimum_allowed_run_speed,\n        min_spikes=1,\n    )\n    # Smooth the runs before calculating other metrics\n    [\n        f.smooth_runs(\n            self.ifr_smoothing_constant,\n            self.spatial_lowpass_cutoff,\n            self.pos_sample_rate,\n        )\n        for f in field_props\n    ]\n    if \"pos_d_currentdir\" in self.regressors.keys():\n        d_currentdir = np.ones(shape=self.PosData.npos) * np.nan\n        for f in field_props:\n            for r in f.runs:\n                d_currentdir[r._slice] = r.current_direction\n\n        self.update_regressor_values(\"pos_d_currentdir\", d_currentdir)\n\n    # calculate the cumulative distance travelled on each run\n    # only goes from 0-1\n    if \"pos_d_cum\" in self.regressors.keys():\n        d_cumulative = np.ones(shape=self.PosData.npos) * np.nan\n        for f in field_props:\n            for r in f.runs:\n                d_cumulative[r._slice] = r.cumulative_distance\n\n        self.update_regressor_values(\"pos_d_cum\", d_cumulative)\n\n    # calculate cumulative sum of the expected normalised firing rate\n    # only goes from 0-1\n    # NB I'm not sure why this is called expected rate as there is nothing\n    # about firing rate in this just the accumulation of rho\n    if \"pos_exptdRate_cum\" in self.regressors.keys():\n        exptd_rate_all = np.ones(shape=self.PosData.npos) * np.nan\n        rmap_infilled = infill_ratemap(rmap)\n        exptd_rate = rmap_infilled[yBins - 1, xBins - 1]\n        # setting the sample rate to 1 here will result in firing rate being returned\n        # not expected spike count\n        for f in field_props:\n            for r in f.runs:\n                exptd_rate_all[r._slice] = r.expected_spikes(exptd_rate, 1)\n\n        self.update_regressor_values(\"pos_exptdRate_cum\", exptd_rate_all)\n\n    # direction projected onto the run mean direction is just the x coord\n    # good - remembering that xy_new is rho,phi\n    # this might be wrong - need to check i'm grabbing the right value\n    # from FieldProps... could be rho\n    if \"pos_d_meanDir\" in self.regressors.keys():\n        d_meandir = np.ones(shape=self.PosData.npos) * np.nan\n        for f in field_props:\n            for r in f.runs:\n                d_meandir[r._slice] = r.pos_r\n\n        self.update_regressor_values(\"pos_d_meanDir\", d_meandir)\n\n    # smooth binned spikes to get an instantaneous firing rate\n    # set up the smoothing kernel\n    # all up at 1.0\n    if \"pos_instFR\" in self.regressors.keys():\n        kernLenInBins = np.round(self.ifr_kernel_len * self.bins_per_second)\n        kernSig = self.ifr_kernel_sigma * self.bins_per_second\n        regressor = signal.windows.gaussian(kernLenInBins, kernSig)\n        # apply the smoothing kernel over the binned observed spikes\n        ifr = signal.convolve(observed_spikes_in_time, regressor, mode=\"same\")\n        inst_firing_rate = np.zeros_like(ifr)\n        for field in field_props:\n            for i_slice in field.run_slices:\n                inst_firing_rate[i_slice] = ifr[i_slice]\n        self.update_regressor_values(\"pos_instFR\", inst_firing_rate)\n\n    # find time spent within run\n    # only goes from 0-1\n    if \"pos_timeInRun\" in self.regressors.keys():\n        time_in_run = np.ones(shape=self.PosData.npos) * np.nan\n        for f in field_props:\n            for r in f.runs:\n                time_in_run[r._slice] = r.cumulative_time / self.pos_sample_rate\n\n        self.update_regressor_values(\"pos_timeInRun\", time_in_run)\n\n    # mnSpd = np.concatenate([f.runs_speed for f in field_props])\n    # centralPeripheral = np.concatenate([f.pos_xy[1] for f in field_props])\n\n    return field_props\n</code></pre>"},{"location":"reference/#ephysiopy.common.phasecoding.phasePrecession2D.getThetaProps","title":"<code>getThetaProps(field_props)</code>","text":"<p>Processes the LFP data and inserts into each run within each field a segment of LFP data that has had its phase and amplitude extracted as well as some other metadata</p> Source code in <code>ephysiopy/common/phasecoding.py</code> <pre><code>def getThetaProps(self, field_props: list[FieldProps]) -&gt; None:\n    \"\"\"\n    Processes the LFP data and inserts into each run within each field\n    a segment of LFP data that has had its phase and amplitude extracted\n    as well as some other metadata\n    \"\"\"\n    spikeTS = self.spike_ts\n    phase = np.ma.MaskedArray(self.phase, mask=True)\n    filteredEEG = self.filteredEEG\n    oldAmplt = filteredEEG.copy()\n    # get indices of spikes into eeg\n    spkEEGIdx = self.spike_eeg_idx\n    spkCount = np.bincount(spkEEGIdx, minlength=len(phase))\n    spkPhase = phase.copy()\n    # unmask the valid entries\n    spkPhase.mask[spkEEGIdx] = False\n    minSpikingPhase = getPhaseOfMinSpiking(spkPhase)\n    # force phase to lie between 0 and 2PI\n    phaseAdj = fixAngle(\n        phase - minSpikingPhase * (np.pi / 180) + self.allowed_min_spike_phase\n    )\n    isNegFreq = np.diff(np.unwrap(phaseAdj)) &lt; 0\n    isNegFreq = np.append(isNegFreq, isNegFreq[-1])\n    # get start of theta cycles as points where diff &gt; pi\n    phaseDf = np.diff(phaseAdj)\n    cycleStarts = phaseDf[1::] &lt; -np.pi\n    cycleStarts = np.append(cycleStarts, True)\n    cycleStarts = np.insert(cycleStarts, 0, True)\n    cycleStarts[isNegFreq] = False\n    cycleLabel = np.cumsum(cycleStarts)\n\n    # caculate power and find low power cycles\n    power = np.power(filteredEEG, 2)\n    cycleTotValidPow = np.bincount(\n        cycleLabel[~isNegFreq], weights=power[~isNegFreq]\n    )\n    cycleValidBinCount = np.bincount(cycleLabel[~isNegFreq])\n    cycleValidMnPow = cycleTotValidPow / cycleValidBinCount\n    powRejectThresh = np.percentile(\n        cycleValidMnPow, self.min_power_percent_threshold\n    )\n    cycleHasBadPow = cycleValidMnPow &lt; powRejectThresh\n\n    # find cycles too long or too short\n    allowed_theta_len = (\n        np.floor((1.0 / self.max_theta) * self.lfp_sample_rate).astype(int),\n        np.ceil((1.0 / self.min_theta) * self.lfp_sample_rate).astype(int),\n    )\n    cycleTotBinCount = np.bincount(cycleLabel)\n    cycleHasBadLen = np.logical_or(\n        cycleTotBinCount &lt; allowed_theta_len[0],\n        cycleTotBinCount &gt; allowed_theta_len[1],\n    )\n\n    # remove data calculated as 'bad'\n    isBadCycle = np.logical_or(cycleHasBadLen, cycleHasBadPow)\n    isInBadCycle = isBadCycle[cycleLabel]\n    isBad = np.logical_or(isInBadCycle, isNegFreq)\n    breakpoint()\n    # TODO: above phaseAdj is being created as a Masked array with all mask values as True...\n    phaseAdj = np.ma.MaskedArray(phaseAdj, mask=np.invert(isBad))\n    self.phaseAdj = phaseAdj\n    ampAdj = np.ma.MaskedArray(filteredEEG, mask=np.invert(isBad))\n    cycleLabel = np.ma.MaskedArray(cycleLabel, mask=np.invert(isBad))\n    self.cycleLabel = cycleLabel\n    spkCount = np.ma.MaskedArray(spkCount, mask=np.invert(isBad))\n    # Extract all the relevant values from the arrays above and\n    # add to each run\n    lfp_to_pos_ratio = self.lfp_sample_rate / self.pos_sample_rate\n    for field in field_props:\n        for run in field.runs:\n            lfp_slice = slice(\n                int(run._slice.start * lfp_to_pos_ratio),\n                int(run._slice.stop * lfp_to_pos_ratio),\n            )\n            spike_times = spikeTS[\n                np.logical_and(\n                    spikeTS &gt; lfp_slice.start / self.lfp_sample_rate,\n                    spikeTS &lt; lfp_slice.stop / self.lfp_sample_rate,\n                )\n            ]\n            lfp_segment = LFPSegment(\n                field.label,\n                run.label,\n                lfp_slice,\n                spike_times,\n                self.eeg[lfp_slice],\n                self.filteredEEG[lfp_slice],\n                phaseAdj[lfp_slice],\n                ampAdj[lfp_slice],\n                self.lfp_sample_rate,\n                [self.min_theta, self.max_theta],\n            )\n            run.lfp_data = lfp_segment\n</code></pre>"},{"location":"reference/#ephysiopy.common.phasecoding.phasePrecession2D.performRegression","title":"<code>performRegression(**kwargs)</code>","text":"<p>Wrapper function for doing the actual regression which has multiple stages.</p> <p>Specifically here we partition fields into sub-fields, get a bunch of information about the position, spiking and theta data and then do the actual regression.</p> <p>Args:     tetrode (int): The tetrode to examine     cluster (int): The cluster to examine     laserEvents (array_like, optional): The on times for laser events     if present. Default is None Valid keyword args:     plot (bool): whether to plot the results of field partitions, the regression(s)         etc See Also:     ephysiopy.common.eegcalcs.phasePrecession.partitionFields()     ephysiopy.common.eegcalcs.phasePrecession.getPosProps()     ephysiopy.common.eegcalcs.phasePrecession.getThetaProps()     ephysiopy.common.eegcalcs.phasePrecession.getSpikeProps()     ephysiopy.common.eegcalcs.phasePrecession._ppRegress()</p> Source code in <code>ephysiopy/common/phasecoding.py</code> <pre><code>def performRegression(self, **kwargs):\n    \"\"\"\n    Wrapper function for doing the actual regression which has multiple\n    stages.\n\n    Specifically here we partition fields into sub-fields, get a bunch of\n    information about the position, spiking and theta data and then\n    do the actual regression.\n\n    Args:\n        tetrode (int): The tetrode to examine\n        cluster (int): The cluster to examine\n        laserEvents (array_like, optional): The on times for laser events\n        if present. Default is None\n    Valid keyword args:\n        plot (bool): whether to plot the results of field partitions, the regression(s)\n            etc\n    See Also:\n        ephysiopy.common.eegcalcs.phasePrecession.partitionFields()\n        ephysiopy.common.eegcalcs.phasePrecession.getPosProps()\n        ephysiopy.common.eegcalcs.phasePrecession.getThetaProps()\n        ephysiopy.common.eegcalcs.phasePrecession.getSpikeProps()\n        ephysiopy.common.eegcalcs.phasePrecession._ppRegress()\n    \"\"\"\n    do_plot = kwargs.get(\"plot\", False)\n\n    # Partition fields - comes from ephysiopy.common.fieldca\n    binned_data = self.RateMap.get_map(self.spk_weights)\n    _, _, labels, _ = partitionFields(\n        binned_data,\n        self.field_threshold_percent,\n        self.field_threshold,\n        self.area_threshold,\n    )\n\n    # split into runs\n    field_properties = self.getPosProps(labels)\n    # self.posdict = posD\n    # self.rundict = runD\n\n    # get theta cycles, amplitudes, phase etc\n    self.getThetaProps(field_properties)\n    # TODO: next: getSpikeProps(field_properties)\n    # the fields that are set within getSpikeProps() I think can be added\n    # to the individual runs within the FieldProps instance\n    # spkCount is just spikes binned wrt eeg timebase\n    #\n\n    # get the indices of spikes for various metrics such as\n    # theta cycle, run etc\n    spkD = self.getSpikeProps(field_properties)\n    # self.spkdict = spkD\n    # at this point the 'values' and 'pha' arrays in the regressors dict are all\n    # npos elements long and are masked arrays. keep as masked arrays and just modify the\n    # masks instead of truncating the data\n    # Do the regressions\n    self._ppRegress(spkD)\n\n    # Plot the results if asked\n    if do_plot:\n        n_regressors = len(self.get_regressors())\n        n_rows = np.ceil(n_regressors / 2).astype(int)\n        if n_regressors == 1:\n            fig, ax = plt.subplots(1, 1, figsize=(3, 5))\n        else:\n            fig, ax = plt.subplots(2, n_rows, figsize=(10, 10))\n        fig.canvas.manager.set_window_title(\"Regression results\")\n        if isinstance(ax, list | np.ndarray):\n            ax = flatten_list(ax)\n        if n_regressors == 1:\n            ax = [ax]\n        for ra in zip(self.get_regressors(), ax):\n            self.plotRegressor(ra[0], ra[1])\n</code></pre>"},{"location":"reference/#ephysiopy.common.phasecoding.phasePrecession2D.update_regressors","title":"<code>update_regressors(reg_keys)</code>","text":"<p>Create a dict to hold the stats values for each regressor Default regressors are:     \"spk_numWithinRun\",     \"pos_exptdRate_cum\",     \"pos_instFR\",     \"pos_timeInRun\",     \"pos_d_cum\",     \"pos_d_meanDir\",     \"pos_d_currentdir\",     \"spk_thetaBatchLabelInRun\"</p> <p>NB: The regressors have differing sizes of 'values' depending on the type of the regressor: spk_ - integer values of the spike number within a run or the theta batch         in a run, so has a length equal to the number of spikes collected pos_ - a bincount of some type so equal to the number of position samples         collected eeg_* - only one at present, the instantaneous firing rate binned into the         number of eeg samples so equal to that in length</p> Source code in <code>ephysiopy/common/phasecoding.py</code> <pre><code>def update_regressors(self, reg_keys: list):\n    \"\"\"\n    Create a dict to hold the stats values for\n    each regressor\n    Default regressors are:\n        \"spk_numWithinRun\",\n        \"pos_exptdRate_cum\",\n        \"pos_instFR\",\n        \"pos_timeInRun\",\n        \"pos_d_cum\",\n        \"pos_d_meanDir\",\n        \"pos_d_currentdir\",\n        \"spk_thetaBatchLabelInRun\"\n\n    NB: The regressors have differing sizes of 'values' depending on the\n    type of the regressor:\n    spk_* - integer values of the spike number within a run or the theta batch\n            in a run, so has a length equal to the number of spikes collected\n    pos_* - a bincount of some type so equal to the number of position samples\n            collected\n    eeg_* - only one at present, the instantaneous firing rate binned into the\n            number of eeg samples so equal to that in length\n    \"\"\"\n    if reg_keys is None:\n        reg_keys = all_regressors\n    else:\n        assert all([regressor in all_regressors for regressor in reg_keys])\n\n    # Create a dict to hold the stats values for\n    # each regressor\n    stats_dict = {\n        \"values\": np.ma.MaskedArray,\n        \"pha\": np.ma.MaskedArray,\n        \"slope\": float,\n        \"intercept\": float,\n        \"cor\": float,\n        \"p\": float,\n        \"cor_boot\": float,\n        \"p_shuffled\": float,\n        \"ci\": float,\n        \"reg\": float,\n    }\n    self.regressors = {}\n    self.regressors = defaultdict(lambda: stats_dict.copy(), self.regressors)\n    [self.regressors[regressor] for regressor in reg_keys]\n</code></pre>"},{"location":"reference/#ephysiopy.common.phasecoding.ccc","title":"<code>ccc(t, p)</code>","text":"<p>Calculates correlation between two random circular variables</p> Source code in <code>ephysiopy/common/phasecoding.py</code> <pre><code>def ccc(t, p):\n    \"\"\"\n    Calculates correlation between two random circular variables\n    \"\"\"\n    n = len(t)\n    A = np.sum(np.cos(t) * np.cos(p))\n    B = np.sum(np.sin(t) * np.sin(p))\n    C = np.sum(np.cos(t) * np.sin(p))\n    D = np.sum(np.sin(t) * np.cos(p))\n    E = np.sum(np.cos(2 * t))\n    F = np.sum(np.sin(2 * t))\n    G = np.sum(np.cos(2 * p))\n    H = np.sum(np.sin(2 * p))\n    rho = 4 * (A * B - C * D) / np.sqrt((n**2 - E**2 - F**2) * (n**2 - G**2 - H**2))\n    return rho\n</code></pre>"},{"location":"reference/#ephysiopy.common.phasecoding.ccc_jack","title":"<code>ccc_jack(t, p)</code>","text":"<p>Function used to calculate jackknife estimates of correlation</p> Source code in <code>ephysiopy/common/phasecoding.py</code> <pre><code>def ccc_jack(t, p):\n    \"\"\"\n    Function used to calculate jackknife estimates of correlation\n    \"\"\"\n    n = len(t) - 1\n    A = np.cos(t) * np.cos(p)\n    A = np.sum(A) - A\n    B = np.sin(t) * np.sin(p)\n    B = np.sum(B) - B\n    C = np.cos(t) * np.sin(p)\n    C = np.sum(C) - C\n    D = np.sin(t) * np.cos(p)\n    D = np.sum(D) - D\n    E = np.cos(2 * t)\n    E = np.sum(E) - E\n    F = np.sin(2 * t)\n    F = np.sum(F) - F\n    G = np.cos(2 * p)\n    G = np.sum(G) - G\n    H = np.sin(2 * p)\n    H = np.sum(H) - H\n    rho = 4 * (A * B - C * D) / np.sqrt((n**2 - E**2 - F**2) * (n**2 - G**2 - H**2))\n    return rho\n</code></pre>"},{"location":"reference/#ephysiopy.common.phasecoding.circCircCorrTLinear","title":"<code>circCircCorrTLinear(theta, phi, regressor=1000, alpha=0.05, hyp=0, conf=True)</code>","text":"<p>An almost direct copy from AJs Matlab fcn to perform correlation between 2 circular random variables.</p> <p>Returns the correlation value (rho), p-value, bootstrapped correlation values, shuffled p values and correlation values.</p> <p>Args:     theta, phi (array_like): mx1 array containing circular data (radians)         whose correlation is to be measured     regressor (int, optional): number of permutations to use to calculate p-value         from randomisation and bootstrap estimation of confidence         intervals.         Leave empty to calculate p-value analytically (NB confidence         intervals will not be calculated). Default is 1000.     alpha (float, optional): hypothesis test level e.g. 0.05, 0.01 etc.         Default is 0.05.     hyp (int, optional): hypothesis to test; -1/ 0 / 1 (-ve correlated /         correlated in either direction / positively correlated).         Default is 0.     conf (bool, optional): True or False to calculate confidence intervals         via jackknife or bootstrap. Default is True.</p> <p>References:     Fisher (1993), Statistical Analysis of Circular Data,         Cambridge University Press, ISBN: 0 521 56890 0</p> Source code in <code>ephysiopy/common/phasecoding.py</code> <pre><code>def circCircCorrTLinear(theta, phi, regressor=1000, alpha=0.05, hyp=0, conf=True):\n    \"\"\"\n    An almost direct copy from AJs Matlab fcn to perform correlation\n    between 2 circular random variables.\n\n    Returns the correlation value (rho), p-value, bootstrapped correlation\n    values, shuffled p values and correlation values.\n\n    Args:\n        theta, phi (array_like): mx1 array containing circular data (radians)\n            whose correlation is to be measured\n        regressor (int, optional): number of permutations to use to calculate p-value\n            from randomisation and bootstrap estimation of confidence\n            intervals.\n            Leave empty to calculate p-value analytically (NB confidence\n            intervals will not be calculated). Default is 1000.\n        alpha (float, optional): hypothesis test level e.g. 0.05, 0.01 etc.\n            Default is 0.05.\n        hyp (int, optional): hypothesis to test; -1/ 0 / 1 (-ve correlated /\n            correlated in either direction / positively correlated).\n            Default is 0.\n        conf (bool, optional): True or False to calculate confidence intervals\n            via jackknife or bootstrap. Default is True.\n\n    References:\n        Fisher (1993), Statistical Analysis of Circular Data,\n            Cambridge University Press, ISBN: 0 521 56890 0\n    \"\"\"\n    theta = theta.ravel()\n    phi = phi.ravel()\n\n    if not len(theta) == len(phi):\n        print(\"theta and phi not same length - try again!\")\n        raise ValueError()\n\n    # estimate correlation\n    rho = ccc(theta, phi)\n    n = len(theta)\n\n    # derive p-values\n    if regressor:\n        p_shuff = shuffledPVal(theta, phi, rho, regressor, hyp)\n        p = np.nan\n\n    # estimtate ci's for correlation\n    if n &gt;= 25 and conf:\n        # obtain jackknife estimates of rho and its ci's\n        rho_jack = ccc_jack(theta, phi)\n        rho_jack = n * rho - (n - 1) * rho_jack\n        rho_boot = np.mean(rho_jack)\n        rho_jack_std = np.std(rho_jack)\n        ci = (\n            rho_boot - (1 / np.sqrt(n)) * rho_jack_std * norm.ppf(alpha / 2, (0, 1))[0],\n            rho_boot + (1 / np.sqrt(n)) * rho_jack_std * norm.ppf(alpha / 2, (0, 1))[0],\n        )\n    elif conf and regressor and n &lt; 25 and n &gt; 4:\n        from sklearn.utils import resample\n\n        # set up the bootstrapping parameters\n        boot_samples = []\n        for i in range(regressor):\n            theta_sample = resample(theta, replace=True)\n            phi_sample = resample(phi, replace=True)\n            boot_samples.append(\n                ccc(\n                    theta_sample[np.isfinite(theta_sample)],\n                    phi_sample[np.isfinite(phi_sample)],\n                )\n            )\n        rho_boot = np.nanmean(boot_samples)\n        # confidence intervals\n        p = ((1.0 - alpha) / 2.0) * 100\n        lower = np.nanmax(0.0, np.nanpercentile(boot_samples, p))\n        p = (alpha + ((1.0 - alpha) / 2.0)) * 100\n        upper = np.nanmin(1.0, np.nanpercentile(boot_samples, p))\n\n        ci = (lower, upper)\n    else:\n        rho_boot = np.nan\n        ci = np.nan\n\n    return rho, p, rho_boot, p_shuff, ci\n</code></pre>"},{"location":"reference/#ephysiopy.common.phasecoding.circRegress","title":"<code>circRegress(x, t)</code>","text":"<p>Finds approximation to circular-linear regression for phase precession.</p> <p>Args:     x (list): n-by-1 list of in-field positions (linear variable)     t (list): n-by-1 list of phases, in degrees (converted to radians)</p> <p>Note:     Neither x nor t can contain NaNs, must be paired (of equal length).</p> Source code in <code>ephysiopy/common/phasecoding.py</code> <pre><code>def circRegress(x, t):\n    \"\"\"\n    Finds approximation to circular-linear regression for phase precession.\n\n    Args:\n        x (list): n-by-1 list of in-field positions (linear variable)\n        t (list): n-by-1 list of phases, in degrees (converted to radians)\n\n    Note:\n        Neither x nor t can contain NaNs, must be paired (of equal length).\n    \"\"\"\n    # transform the linear co-variate to the range -1 to 1\n    if not np.any(x) or not np.any(t):\n        return x, t\n    mnx = np.mean(x)\n    xn = x - mnx\n    mxx = np.max(np.fabs(xn))\n    xn = xn / mxx\n    # keep tn between 0 and 2pi\n    tn = np.remainder(t, 2 * np.pi)\n    # constrain max slope to give at most 720 degrees of phase precession\n    # over the field\n    max_slope = (2 * np.pi) / (np.max(xn) - np.min(xn))\n\n    # perform slope optimisation and find intercept\n    def _cost(m, x, t):\n        return -np.abs(np.sum(np.exp(1j * (t - m * x)))) / len(t - m * x)\n\n    slope = optimize.fminbound(_cost, -1 * max_slope, max_slope, args=(xn, tn))\n    intercept = np.arctan2(\n        np.sum(np.sin(tn - slope * xn)), np.sum(np.cos(tn - slope * xn))\n    )\n    intercept = intercept + ((0 - slope) * (mnx / mxx))\n    slope = slope / mxx\n    return slope, intercept\n</code></pre>"},{"location":"reference/#ephysiopy.common.phasecoding.plot_spikes_in_runs_per_field","title":"<code>plot_spikes_in_runs_per_field(field_label, run_starts, run_ends, spikes_in_time, ttls_in_time=None, **kwargs)</code>","text":"<p>Debug plotting to show spikes per run per field found in the ratemap as a raster plot</p> <p>Args: field_label (np.ndarray): The field labels for each position bin     a vector run_start_stop_idx (np.ndarray): The start and stop indices of each run     has shape (n_runs, 2) spikes_in_time (np.ndarray): The number of spikes in each position bin     a vector</p> <p>kwargs: separate_plots (bool): If True then each field will be plotted in a separate figure</p> <p>single_axes (bool): If True will plot all the runs/ spikes in a single axis with fields delimited by horizontal lines</p> <p>Returns: fig, axes (tuple): The figure and axes objects</p> Source code in <code>ephysiopy/common/phasecoding.py</code> <pre><code>def plot_spikes_in_runs_per_field(\n    field_label: np.ndarray,\n    run_starts: np.ndarray,\n    run_ends: np.ndarray,\n    spikes_in_time: np.ndarray,\n    ttls_in_time: np.ndarray | None = None,\n    **kwargs,\n):\n    \"\"\"\n    Debug plotting to show spikes per run per field found in the ratemap\n    as a raster plot\n\n    Args:\n    field_label (np.ndarray): The field labels for each position bin\n        a vector\n    run_start_stop_idx (np.ndarray): The start and stop indices of each run\n        has shape (n_runs, 2)\n    spikes_in_time (np.ndarray): The number of spikes in each position bin\n        a vector\n\n    kwargs:\n    separate_plots (bool): If True then each field will be plotted in a\n    separate figure\n\n    single_axes (bool): If True will plot all the runs/ spikes in a single\n    axis with fields delimited by horizontal lines\n\n    Returns:\n    fig, axes (tuple): The figure and axes objects\n    \"\"\"\n    spikes_in_time = np.ravel(spikes_in_time)\n    if ttls_in_time:\n        assert len(spikes_in_time) == len(ttls_in_time)\n    run_start_stop_idx = np.array([run_starts, run_ends]).T\n    run_field_id = field_label[run_start_stop_idx[:, 0]]\n    runs_per_field = np.histogram(run_field_id, bins=range(1, max(run_field_id) + 2))[0]\n    max_run_len = np.max(run_start_stop_idx[:, 1] - run_start_stop_idx[:, 0])\n    all_slices = np.array([slice(r[0], r[1]) for r in run_start_stop_idx])\n    # create the figure window first then do the iteration through fields etc\n    master_raster_arr = []\n    # a grey colour for the background i.e. how long the run was\n    grey = np.array([0.8627, 0.8627, 0.8627, 1])\n    # iterate through each field then pull out the\n    max_spikes = np.nanmax(spikes_in_time).astype(int) + 1\n    orig_cmap = matplotlib.colormaps[\"spring\"].resampled(max_spikes)\n    cmap = orig_cmap(np.linspace(0, 1, max_spikes))\n    cmap[0, :] = grey\n    newcmap = ListedColormap(cmap)\n    # some lists to hold the outputs\n    # spike count for each run through the field\n    master_raster_arr = []\n    # list for count of total number of spikes per field\n    spikes_per_run = []\n    # counts of ttl puleses emitted during each run\n    if ttls_in_time is not None:\n        ttls_per_field = []\n    # collect all the per field spiking, ttls etc first then plot\n    # in a separate iteration\n    for i, field_id in enumerate(np.unique(run_field_id)):\n        # create a temporary array to hold the raster for this fields runs\n        raster_arr = np.zeros(shape=(runs_per_field[i], max_run_len)) * np.nan\n        ttl_arr = np.zeros(shape=(runs_per_field[i], max_run_len)) * np.nan\n        # get the indices into the time binned spikes of the runs\n        i_field_slices = all_slices[run_field_id == field_id]\n        # breakpoint()\n        for j, s in enumerate(i_field_slices):\n            i_run_len = s.stop - s.start\n            raster_arr[j, 0:i_run_len] = spikes_in_time[s]\n            if ttls_in_time is not None:\n                ttl_arr[j, 0:i_run_len] = ttls_in_time[s]\n        spikes_per_run.append(int(np.nansum(raster_arr)))\n        if ttls_in_time:\n            ttls_per_field.append(ttl_arr)\n        master_raster_arr.append(raster_arr)\n\n    if \"separate_plots\" in kwargs.keys():\n        for i, field_id in enumerate(np.unique(run_field_id)):\n            _, ax = plt.subplots(1, 1)\n            ax.imshow(master_raster_arr[i], cmap=newcmap, aspect=\"auto\")\n            ax.axes.get_xaxis().set_ticks([])\n            ax.axes.get_yaxis().set_ticks([])\n            ax.set_ylabel(f\"Field {field_id}\")\n    elif \"single_axes\" in kwargs.keys():\n        # deal with master_raster_arr here\n        _, ax = plt.subplots(1, 1)\n        if ttls_in_time:\n            ttls = np.array(flatten_list(ttls_per_field))\n            ax.imshow(ttls, cmap=matplotlib.colormaps[\"bone\"])\n        spiking_arr = np.array(flatten_list(master_raster_arr))\n        ax.imshow(spiking_arr, cmap=newcmap, alpha=0.6)\n        ax.axes.get_xaxis().set_ticks([])\n        ax.axes.get_yaxis().set_ticks([])\n        ax.hlines(np.cumsum(runs_per_field)[:-1], 0, max_run_len, \"regressor\")\n        ax.set_xlim(0, max_run_len)\n        ytick_locs = np.insert(np.cumsum(runs_per_field), 0, 0)\n        ytick_locs = np.diff(ytick_locs) // 2 + ytick_locs[:-1]\n        ax.set_yticks(ytick_locs, list(map(str, np.unique(run_field_id))))\n        ax.set_ylabel(\"Field ID\", rotation=90, labelpad=10)\n        ax.set_xlabel(\"Time (s)\")\n        ax.set_xticks([0, max_run_len], [\"0\", f\"{(max_run_len)/50:.2f}\"])\n        axes2 = ax.twinx()\n        axes2.set_yticks(ytick_locs, list(map(str, spikes_per_run)))\n        axes2.set_ylim(ax.get_ylim())\n        axes2.set_ylabel(\"Spikes per field\", rotation=270, labelpad=10)\n</code></pre>"},{"location":"reference/#ephysiopy.common.phasecoding.shuffledPVal","title":"<code>shuffledPVal(theta, phi, rho, regressor, hyp)</code>","text":"<p>Calculates shuffled p-values for correlation</p> Source code in <code>ephysiopy/common/phasecoding.py</code> <pre><code>def shuffledPVal(theta, phi, rho, regressor, hyp):\n    \"\"\"\n    Calculates shuffled p-values for correlation\n    \"\"\"\n    n = len(theta)\n    idx = np.zeros((n, regressor))\n    for i in range(regressor):\n        idx[:, i] = np.random.permutation(np.arange(n))\n\n    thetaPerms = theta[idx.astype(int)]\n\n    A = np.dot(np.cos(phi), np.cos(thetaPerms))\n    B = np.dot(np.sin(phi), np.sin(thetaPerms))\n    C = np.dot(np.sin(phi), np.cos(thetaPerms))\n    D = np.dot(np.cos(phi), np.sin(thetaPerms))\n    E = np.sum(np.cos(2 * theta))\n    F = np.sum(np.sin(2 * theta))\n    G = np.sum(np.cos(2 * phi))\n    H = np.sum(np.sin(2 * phi))\n\n    rho_sim = 4 * (A * B - C * D) / np.sqrt((n**2 - E**2 - F**2) * (n**2 - G**2 - H**2))\n\n    if hyp == 1:\n        p_shuff = np.sum(rho_sim &gt;= rho) / float(regressor)\n    elif hyp == -1:\n        p_shuff = np.sum(rho_sim &lt;= rho) / float(regressor)\n    elif hyp == 0:\n        p_shuff = np.sum(np.fabs(rho_sim) &gt; np.fabs(rho)) / float(regressor)\n    else:\n        p_shuff = np.nan\n\n    return p_shuff\n</code></pre>"},{"location":"reference/#rhymicity","title":"Rhymicity","text":""},{"location":"reference/#ephysiopy.common.rhythmicity.CosineDirectionalTuning","title":"<code>CosineDirectionalTuning</code>","text":"<p>               Bases: <code>object</code></p> <p>Produces output to do with Welday et al (2011) like analysis of rhythmic firing a la oscialltory interference model</p> Source code in <code>ephysiopy/common/rhythmicity.py</code> <pre><code>class CosineDirectionalTuning(object):\n    \"\"\"\n    Produces output to do with Welday et al (2011) like analysis\n    of rhythmic firing a la oscialltory interference model\n    \"\"\"\n\n    def __init__(\n        self,\n        spike_times: np.ndarray,\n        pos_times: np.ndarray,\n        spk_clusters: np.ndarray,\n        x: np.ndarray,\n        y: np.ndarray,\n        tracker_params={},\n    ):\n        \"\"\"\n        Args:\n            spike_times (1d np.array): Spike times\n            pos_times (1d np.array): Position times\n            spk_clusters (1d np.array): Spike clusters\n            x and y (1d np.array): Position coordinates\n            tracker_params (dict): From the PosTracker as created in\n                                    OESettings.Settings.parse\n\n        Note:\n            All timestamps should be given in sub-millisecond accurate seconds\n            and pos_xy in cms\n        \"\"\"\n        self.spike_times = spike_times\n        self.pos_times = pos_times\n        self.spk_clusters = spk_clusters\n        \"\"\"\n        There can be more spikes than pos samples in terms of sampling as the\n        open-ephys buffer probably needs to finish writing and the camera has\n        already stopped, so cut of any cluster indices and spike times\n        that exceed the length of the pos indices\n        \"\"\"\n        idx_to_keep = self.spike_times &lt; self.pos_times[-1]\n        self.spike_times = self.spike_times[idx_to_keep]\n        self.spk_clusters = self.spk_clusters[idx_to_keep]\n        self._pos_sample_rate = 30\n        self._spk_sample_rate = 3e4\n        self._pos_samples_for_spike = None\n        self._min_runlength = 0.4  # in seconds\n        self.posCalcs = PosCalcsGeneric(\n            x, y, 230, cm=True, jumpmax=100, tracker_params=tracker_params\n        )\n        self.spikeCalcs = SpikeCalcsGeneric(spike_times, spk_clusters[0])\n        self.spikeCalcs.spk_clusters = spk_clusters\n        self.posCalcs.postprocesspos(tracker_params)\n        xy = self.posCalcs.xy\n        hdir = self.posCalcs.dir\n        self.posCalcs.calcSpeed(xy)\n        self._xy = xy\n        self._hdir = hdir\n        self._speed = self.posCalcs.speed\n        # TEMPORARY FOR POWER SPECTRUM STUFF\n        self.smthKernelWidth = 2\n        self.smthKernelSigma = 0.1875\n        self.sn2Width = 2\n        self.thetaRange = [7, 11]\n        self.xmax = 11\n\n    @property\n    def spk_sample_rate(self):\n        return self._spk_sample_rate\n\n    @spk_sample_rate.setter\n    def spk_sample_rate(self, value):\n        self._spk_sample_rate = value\n\n    @property\n    def pos_sample_rate(self):\n        return self._pos_sample_rate\n\n    @pos_sample_rate.setter\n    def pos_sample_rate(self, value):\n        self._pos_sample_rate = value\n\n    @property\n    def min_runlength(self):\n        return self._min_runlength\n\n    @min_runlength.setter\n    def min_runlength(self, value):\n        self._min_runlength = value\n\n    @property\n    def xy(self):\n        return self._xy\n\n    @xy.setter\n    def xy(self, value):\n        self._xy = value\n\n    @property\n    def hdir(self):\n        return self._hdir\n\n    @hdir.setter\n    def hdir(self, value):\n        self._hdir = value\n\n    @property\n    def speed(self):\n        return self._speed\n\n    @speed.setter\n    def speed(self, value):\n        self._speed = value\n\n    @property\n    def pos_samples_for_spike(self):\n        return self._pos_samples_for_spike\n\n    @pos_samples_for_spike.setter\n    def pos_samples_for_spike(self, value):\n        self._pos_samples_for_spike = value\n\n    def _rolling_window(self, a: np.array, window: int):\n        \"\"\"\n        Totally nabbed from SO:\n        https://stackoverflow.com/questions/6811183/rolling-window-for-1d-arrays-in-numpy\n        \"\"\"\n        shape = a.shape[:-1] + (a.shape[-1] - window + 1, window)\n        strides = a.strides + (a.strides[-1],)\n        return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n\n    def getPosIndices(self):\n        self.pos_samples_for_spike = np.floor(\n            self.spike_times * self.pos_sample_rate\n        ).astype(int)\n\n    def getClusterPosIndices(self, clust: int) -&gt; np.array:\n        if self.pos_samples_for_spike is None:\n            self.getPosIndices()\n        clust_pos_idx = self.pos_samples_for_spike[self.spk_clusters == clust]\n        clust_pos_idx[clust_pos_idx &gt;= len(self.pos_times)] = len(self.pos_times) - 1\n        return clust_pos_idx\n\n    def getClusterSpikeTimes(self, cluster: int):\n        ts = self.spike_times[self.spk_clusters == cluster]\n        if self.pos_samples_for_spike is None:\n            self.getPosIndices()\n        return ts\n\n    def getDirectionalBinPerPosition(self, binwidth: int):\n        \"\"\"\n        Direction is in degrees as that what is created by me in some of the\n        other bits of this package.\n\n        Args:\n            binwidth (int): The bin width in degrees\n\n        Returns:\n            A digitization of which directional bin each pos sample belongs to\n        \"\"\"\n\n        bins = np.arange(0, 360, binwidth)\n        return np.digitize(self.hdir, bins)\n\n    def getDirectionalBinForCluster(self, cluster: int):\n        b = self.getDirectionalBinPerPosition(45)\n        cluster_pos = self.getClusterPosIndices(cluster)\n        # idx_to_keep = cluster_pos &lt; len(self.pos_times)\n        # cluster_pos = cluster_pos[idx_to_keep]\n        return b[cluster_pos]\n\n    def getRunsOfMinLength(self):\n        \"\"\"\n        Identifies runs of at least self.min_runlength seconds long,\n        which at 30Hz pos sampling rate equals 12 samples, and\n        returns the start and end indices at which\n        the run was occurred and the directional bin that run belongs to\n\n        Returns:\n            np.array: The start and end indices into pos samples of the run\n                      and the directional bin to which it belongs\n        \"\"\"\n\n        b = self.getDirectionalBinPerPosition(45)\n        # nabbed from SO\n        from itertools import groupby\n\n        grouped_runs = [(k, sum(1 for i in g)) for k, g in groupby(b)]\n        grouped_runs = np.array(grouped_runs)\n        run_start_indices = np.cumsum(grouped_runs[:, 1]) - grouped_runs[:, 1]\n        min_len_in_samples = int(self.pos_sample_rate * self.min_runlength)\n        min_len_runs_mask = grouped_runs[:, 1] &gt;= min_len_in_samples\n        ret = np.array(\n            [run_start_indices[min_len_runs_mask], grouped_runs[min_len_runs_mask, 1]]\n        ).T\n        # ret contains run length as last column\n        ret = np.insert(ret, 1, np.sum(ret, 1), 1)\n        ret = np.insert(ret, 2, grouped_runs[min_len_runs_mask, 0], 1)\n        return ret[:, 0:3]\n\n    def speedFilterRuns(self, runs: np.array, minspeed=5.0):\n        \"\"\"\n        Given the runs identified in getRunsOfMinLength, filter for speed\n        and return runs that meet the min speed criteria.\n\n        The function goes over the runs with a moving window of length equal\n        to self.min_runlength in samples and sees if any of those segments\n        meet the speed criteria and splits them out into separate runs if true.\n\n        NB For now this means the same spikes might get included in the\n        autocorrelation procedure later as the\n        moving window will use overlapping periods - can be modified later.\n\n        Args:\n            runs (3 x nRuns np.array): Generated from getRunsOfMinLength\n            minspeed (float): Min running speed in cm/s for an epoch (minimum\n                              epoch length defined previously\n                              in getRunsOfMinLength as minlength, usually 0.4s)\n\n        Returns:\n            3 x nRuns np.array: A modified version of the \"runs\" input variable\n        \"\"\"\n        minlength_in_samples = int(self.pos_sample_rate * self.min_runlength)\n        run_list = runs.tolist()\n        all_speed = np.array(self.speed)\n        for start_idx, end_idx, dir_bin in run_list:\n            this_runs_speed = all_speed[start_idx:end_idx]\n            this_runs_runs = self._rolling_window(this_runs_speed, minlength_in_samples)\n            run_mask = np.all(this_runs_runs &gt; minspeed, 1)\n            if np.any(run_mask):\n                print(\"got one\")\n\n    \"\"\"\n    def testing(self, cluster: int):\n        ts = self.getClusterSpikeTimes(cluster)\n        pos_idx = self.getClusterPosIndices(cluster)\n\n        dir_bins = self.getDirectionalBinPerPosition(45)\n        cluster_dir_bins = dir_bins[pos_idx.astype(int)]\n\n        from scipy.signal import periodogram, boxcar, filtfilt\n\n        acorrs = []\n        max_freqs = []\n        max_idx = []\n        isis = []\n\n        acorr_range = np.array([-500, 500])\n        for i in range(1, 9):\n            this_bin_indices = cluster_dir_bins == i\n            this_ts = ts[this_bin_indices]  # in seconds still so * 1000 for ms\n            y = self.spikeCalcs.xcorr(this_ts*1000, Trange=acorr_range)\n            isis.append(y)\n            corr, acorr_bins = np.histogram(\n                y[y != 0], bins=501, range=acorr_range)\n            freqs, power = periodogram(corr, fs=200, return_onesided=True)\n            # Smooth the power over +/- 1Hz\n            b = boxcar(3)\n            h = filtfilt(b, 3, power)\n            # Square the amplitude first\n            sqd_amp = h ** 2\n            # Then find the mean power in the +/-1Hz band either side of that\n            theta_band_max_idx = np.nonzero(\n                sqd_amp == np.max(\n                    sqd_amp[np.logical_and(freqs &gt; 6, freqs &lt; 11)]))[0][0]\n            max_freq = freqs[theta_band_max_idx]\n            acorrs.append(corr)\n            max_freqs.append(max_freq)\n            max_idx.append(theta_band_max_idx)\n        return isis, acorrs, max_freqs, max_idx, acorr_bins\n\n    def plotXCorrsByDirection(self, cluster: int):\n        acorr_range = np.array([-500, 500])\n        # plot_range = np.array([-400,400])\n        nbins = 501\n        isis, acorrs, max_freqs, max_idx, acorr_bins = self.testing(cluster)\n        bin_labels = np.arange(0, 360, 45)\n        fig, axs = plt.subplots(8)\n        pts = []\n        for i, a in enumerate(isis):\n            axs[i].hist(\n                a[a != 0], bins=nbins, range=acorr_range,\n                color='k', histtype='stepfilled')\n            # find the max of the first positive peak\n            corr, _ = np.histogram(a[a != 0], bins=nbins, range=acorr_range)\n            axs[i].set_xlim(acorr_range)\n            axs[i].set_ylabel(str(bin_labels[i]))\n            axs[i].set_yticklabels('')\n            if i &lt; 7:\n                axs[i].set_xticklabels('')\n            axs[i].spines['right'].set_visible(False)\n            axs[i].spines['top'].set_visible(False)\n            axs[i].spines['left'].set_visible(False)\n        plt.show()\n        return pts\n    \"\"\"\n\n    def intrinsic_freq_autoCorr(\n        self,\n        spkTimes=None,\n        posMask=None,\n        maxFreq=25,\n        acBinSize=0.002,\n        acWindow=0.5,\n        plot=True,\n        **kwargs,\n    ):\n        \"\"\"\n        This is taken and adapted from ephysiopy.common.eegcalcs.EEGCalcs\n\n        Args:\n            spkTimes (np.array): Times in seconds of the cells firing\n            posMask (np.array): Boolean array corresponding to the length of\n                                spkTimes where True is stuff to keep\n            maxFreq (float): The maximum frequency to do the power spectrum\n                                out to\n            acBinSize (float): The bin size of the autocorrelogram in seconds\n            acWindow (float): The range of the autocorr in seconds\n\n        Note:\n            Make sure all times are in seconds\n        \"\"\"\n        acBinsPerPos = 1.0 / self.pos_sample_rate / acBinSize\n        acWindowSizeBins = np.round(acWindow / acBinSize)\n        binCentres = np.arange(0.5, len(posMask) * acBinsPerPos) * acBinSize\n        spkTrHist, _ = np.histogram(spkTimes, bins=binCentres)\n\n        # split the single histogram into individual chunks\n        splitIdx = np.nonzero(np.diff(posMask.astype(int)))[0] + 1\n        splitMask = np.split(posMask, splitIdx)\n        splitSpkHist = np.split(spkTrHist, (splitIdx * acBinsPerPos).astype(int))\n        histChunks = []\n        for i in range(len(splitSpkHist)):\n            if np.all(splitMask[i]):\n                if np.sum(splitSpkHist[i]) &gt; 2:\n                    if len(splitSpkHist[i]) &gt; int(acWindowSizeBins) * 2:\n                        histChunks.append(splitSpkHist[i])\n        autoCorrGrid = np.zeros((int(acWindowSizeBins) + 1, len(histChunks)))\n        chunkLens = []\n        from scipy import signal\n\n        print(f\"num chunks = {len(histChunks)}\")\n        for i in range(len(histChunks)):\n            lenThisChunk = len(histChunks[i])\n            chunkLens.append(lenThisChunk)\n            tmp = np.zeros(lenThisChunk * 2)\n            tmp[lenThisChunk // 2 : lenThisChunk // 2 + lenThisChunk] = histChunks[i]\n            tmp2 = signal.fftconvolve(\n                tmp, histChunks[i][::-1], mode=\"valid\"\n            )  # the autocorrelation\n            autoCorrGrid[:, i] = (\n                tmp2[lenThisChunk // 2 : lenThisChunk // 2 + int(acWindowSizeBins) + 1]\n                / acBinsPerPos\n            )\n\n        totalLen = np.sum(chunkLens)\n        autoCorrSum = np.nansum(autoCorrGrid, 1) / totalLen\n        meanNormdAc = autoCorrSum[1::] - np.nanmean(autoCorrSum[1::])\n        # return meanNormdAc\n        out = self.power_spectrum(\n            eeg=meanNormdAc,\n            binWidthSecs=acBinSize,\n            maxFreq=maxFreq,\n            pad2pow=16,\n            **kwargs,\n        )\n        out.update({\"meanNormdAc\": meanNormdAc})\n        if plot:\n            fig = plt.gcf()\n            ax = fig.gca()\n            xlim = ax.get_xlim()\n            ylim = ax.get_ylim()\n            ax.imshow(\n                autoCorrGrid,\n                extent=[\n                    maxFreq * 0.6,\n                    maxFreq,\n                    np.max(out[\"Power\"]) * 0.6,\n                    ax.get_ylim()[1],\n                ],\n            )\n            ax.set_ylim(ylim)\n            ax.set_xlim(xlim)\n        return out\n\n    def power_spectrum(\n        self,\n        eeg,\n        plot=True,\n        binWidthSecs=None,\n        maxFreq=25,\n        pad2pow=None,\n        ymax=None,\n        **kwargs,\n    ):\n        \"\"\"\n        Method used by eeg_power_spectra and intrinsic_freq_autoCorr\n        Signal in must be mean normalised already\n        \"\"\"\n\n        # Get raw power spectrum\n        nqLim = 1 / binWidthSecs / 2.0\n        origLen = len(eeg)\n        # if pad2pow is None:\n        # \tfftLen = int(np.power(2, self._nextpow2(origLen)))\n        # else:\n        fftLen = int(np.power(2, pad2pow))\n        fftHalfLen = int(fftLen / float(2) + 1)\n\n        fftRes = np.fft.fft(eeg, fftLen)\n        # get power density from fft and discard second half of spectrum\n        _power = np.power(np.abs(fftRes), 2) / origLen\n        power = np.delete(_power, np.s_[fftHalfLen::])\n        power[1:-2] = power[1:-2] * 2\n\n        # calculate freqs and crop spectrum to requested range\n        freqs = nqLim * np.linspace(0, 1, fftHalfLen)\n        freqs = freqs[freqs &lt;= maxFreq].T\n        power = power[0 : len(freqs)]\n\n        # smooth spectrum using gaussian kernel\n        binsPerHz = (fftHalfLen - 1) / nqLim\n        kernelLen = np.round(self.smthKernelWidth * binsPerHz)\n        kernelSig = self.smthKernelSigma * binsPerHz\n        from scipy import signal\n\n        k = signal.windows.gaussian(kernelLen, kernelSig) / (kernelLen / 2 / 2)\n        power_sm = signal.fftconvolve(power, k[::-1], mode=\"same\")\n\n        # calculate some metrics\n        # find max in theta band\n        spectrumMaskBand = np.logical_and(\n            freqs &gt; self.thetaRange[0], freqs &lt; self.thetaRange[1]\n        )\n        bandMaxPower = np.max(power_sm[spectrumMaskBand])\n        maxBinInBand = np.argmax(power_sm[spectrumMaskBand])\n        bandFreqs = freqs[spectrumMaskBand]\n        freqAtBandMaxPower = bandFreqs[maxBinInBand]\n        # self.maxBinInBand = maxBinInBand\n        # self.freqAtBandMaxPower = freqAtBandMaxPower\n        # self.bandMaxPower = bandMaxPower\n\n        # find power in small window around peak and divide by power in rest\n        # of spectrum to get snr\n        spectrumMaskPeak = np.logical_and(\n            freqs &gt; freqAtBandMaxPower - self.sn2Width / 2,\n            freqs &lt; freqAtBandMaxPower + self.sn2Width / 2,\n        )\n        s2n = np.nanmean(power_sm[spectrumMaskPeak]) / np.nanmean(\n            power_sm[~spectrumMaskPeak]\n        )\n        self.freqs = freqs\n        self.power_sm = power_sm\n        self.spectrumMaskPeak = spectrumMaskPeak\n        if plot:\n            fig = plt.figure()\n            ax = fig.add_subplot(111)\n            if ymax is None:\n                ymax = np.min([2 * np.max(power), np.max(power_sm)])\n                if ymax == 0:\n                    ymax = 1\n            ax.plot(freqs, power, c=[0.9, 0.9, 0.9])\n            # ax.hold(True)\n            ax.plot(freqs, power_sm, \"k\", lw=2)\n            ax.axvline(self.thetaRange[0], c=\"b\", ls=\"--\")\n            ax.axvline(self.thetaRange[1], c=\"b\", ls=\"--\")\n            _, stemlines, _ = ax.stem([freqAtBandMaxPower], [bandMaxPower], linefmt=\"r\")\n            # plt.setp(stemlines, 'linewidth', 2)\n            ax.fill_between(\n                freqs,\n                0,\n                power_sm,\n                where=spectrumMaskPeak,\n                color=\"r\",\n                alpha=0.25,\n                zorder=25,\n            )\n            # ax.set_ylim(0, ymax)\n            # ax.set_xlim(0, self.xmax)\n            ax.set_xlabel(\"Frequency (Hz)\")\n            ax.set_ylabel(\"Power density (W/Hz)\")\n        out_dict = {\n            \"maxFreq\": freqAtBandMaxPower,\n            \"Power\": power_sm,\n            \"Freqs\": freqs,\n            \"s2n\": s2n,\n            \"Power_raw\": power,\n            \"k\": k,\n            \"kernelLen\": kernelLen,\n            \"kernelSig\": kernelSig,\n            \"binsPerHz\": binsPerHz,\n            \"kernelLen\": kernelLen,\n        }\n        return out_dict\n</code></pre>"},{"location":"reference/#ephysiopy.common.rhythmicity.CosineDirectionalTuning.spk_clusters","title":"<code>spk_clusters = self.spk_clusters[idx_to_keep]</code>  <code>instance-attribute</code>","text":"<p>There can be more spikes than pos samples in terms of sampling as the open-ephys buffer probably needs to finish writing and the camera has already stopped, so cut of any cluster indices and spike times that exceed the length of the pos indices</p>"},{"location":"reference/#ephysiopy.common.rhythmicity.CosineDirectionalTuning.__init__","title":"<code>__init__(spike_times, pos_times, spk_clusters, x, y, tracker_params={})</code>","text":"<p>Args:     spike_times (1d np.array): Spike times     pos_times (1d np.array): Position times     spk_clusters (1d np.array): Spike clusters     x and y (1d np.array): Position coordinates     tracker_params (dict): From the PosTracker as created in                             OESettings.Settings.parse</p> <p>Note:     All timestamps should be given in sub-millisecond accurate seconds     and pos_xy in cms</p> Source code in <code>ephysiopy/common/rhythmicity.py</code> <pre><code>def __init__(\n    self,\n    spike_times: np.ndarray,\n    pos_times: np.ndarray,\n    spk_clusters: np.ndarray,\n    x: np.ndarray,\n    y: np.ndarray,\n    tracker_params={},\n):\n    \"\"\"\n    Args:\n        spike_times (1d np.array): Spike times\n        pos_times (1d np.array): Position times\n        spk_clusters (1d np.array): Spike clusters\n        x and y (1d np.array): Position coordinates\n        tracker_params (dict): From the PosTracker as created in\n                                OESettings.Settings.parse\n\n    Note:\n        All timestamps should be given in sub-millisecond accurate seconds\n        and pos_xy in cms\n    \"\"\"\n    self.spike_times = spike_times\n    self.pos_times = pos_times\n    self.spk_clusters = spk_clusters\n    \"\"\"\n    There can be more spikes than pos samples in terms of sampling as the\n    open-ephys buffer probably needs to finish writing and the camera has\n    already stopped, so cut of any cluster indices and spike times\n    that exceed the length of the pos indices\n    \"\"\"\n    idx_to_keep = self.spike_times &lt; self.pos_times[-1]\n    self.spike_times = self.spike_times[idx_to_keep]\n    self.spk_clusters = self.spk_clusters[idx_to_keep]\n    self._pos_sample_rate = 30\n    self._spk_sample_rate = 3e4\n    self._pos_samples_for_spike = None\n    self._min_runlength = 0.4  # in seconds\n    self.posCalcs = PosCalcsGeneric(\n        x, y, 230, cm=True, jumpmax=100, tracker_params=tracker_params\n    )\n    self.spikeCalcs = SpikeCalcsGeneric(spike_times, spk_clusters[0])\n    self.spikeCalcs.spk_clusters = spk_clusters\n    self.posCalcs.postprocesspos(tracker_params)\n    xy = self.posCalcs.xy\n    hdir = self.posCalcs.dir\n    self.posCalcs.calcSpeed(xy)\n    self._xy = xy\n    self._hdir = hdir\n    self._speed = self.posCalcs.speed\n    # TEMPORARY FOR POWER SPECTRUM STUFF\n    self.smthKernelWidth = 2\n    self.smthKernelSigma = 0.1875\n    self.sn2Width = 2\n    self.thetaRange = [7, 11]\n    self.xmax = 11\n</code></pre>"},{"location":"reference/#ephysiopy.common.rhythmicity.CosineDirectionalTuning._rolling_window","title":"<code>_rolling_window(a, window)</code>","text":"<p>Totally nabbed from SO: https://stackoverflow.com/questions/6811183/rolling-window-for-1d-arrays-in-numpy</p> Source code in <code>ephysiopy/common/rhythmicity.py</code> <pre><code>def _rolling_window(self, a: np.array, window: int):\n    \"\"\"\n    Totally nabbed from SO:\n    https://stackoverflow.com/questions/6811183/rolling-window-for-1d-arrays-in-numpy\n    \"\"\"\n    shape = a.shape[:-1] + (a.shape[-1] - window + 1, window)\n    strides = a.strides + (a.strides[-1],)\n    return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n</code></pre>"},{"location":"reference/#ephysiopy.common.rhythmicity.CosineDirectionalTuning.getDirectionalBinPerPosition","title":"<code>getDirectionalBinPerPosition(binwidth)</code>","text":"<p>Direction is in degrees as that what is created by me in some of the other bits of this package.</p> <p>Args:     binwidth (int): The bin width in degrees</p> <p>Returns:     A digitization of which directional bin each pos sample belongs to</p> Source code in <code>ephysiopy/common/rhythmicity.py</code> <pre><code>def getDirectionalBinPerPosition(self, binwidth: int):\n    \"\"\"\n    Direction is in degrees as that what is created by me in some of the\n    other bits of this package.\n\n    Args:\n        binwidth (int): The bin width in degrees\n\n    Returns:\n        A digitization of which directional bin each pos sample belongs to\n    \"\"\"\n\n    bins = np.arange(0, 360, binwidth)\n    return np.digitize(self.hdir, bins)\n</code></pre>"},{"location":"reference/#ephysiopy.common.rhythmicity.CosineDirectionalTuning.getRunsOfMinLength","title":"<code>getRunsOfMinLength()</code>","text":"<p>Identifies runs of at least self.min_runlength seconds long, which at 30Hz pos sampling rate equals 12 samples, and returns the start and end indices at which the run was occurred and the directional bin that run belongs to</p> <p>Returns:     np.array: The start and end indices into pos samples of the run               and the directional bin to which it belongs</p> Source code in <code>ephysiopy/common/rhythmicity.py</code> <pre><code>def getRunsOfMinLength(self):\n    \"\"\"\n    Identifies runs of at least self.min_runlength seconds long,\n    which at 30Hz pos sampling rate equals 12 samples, and\n    returns the start and end indices at which\n    the run was occurred and the directional bin that run belongs to\n\n    Returns:\n        np.array: The start and end indices into pos samples of the run\n                  and the directional bin to which it belongs\n    \"\"\"\n\n    b = self.getDirectionalBinPerPosition(45)\n    # nabbed from SO\n    from itertools import groupby\n\n    grouped_runs = [(k, sum(1 for i in g)) for k, g in groupby(b)]\n    grouped_runs = np.array(grouped_runs)\n    run_start_indices = np.cumsum(grouped_runs[:, 1]) - grouped_runs[:, 1]\n    min_len_in_samples = int(self.pos_sample_rate * self.min_runlength)\n    min_len_runs_mask = grouped_runs[:, 1] &gt;= min_len_in_samples\n    ret = np.array(\n        [run_start_indices[min_len_runs_mask], grouped_runs[min_len_runs_mask, 1]]\n    ).T\n    # ret contains run length as last column\n    ret = np.insert(ret, 1, np.sum(ret, 1), 1)\n    ret = np.insert(ret, 2, grouped_runs[min_len_runs_mask, 0], 1)\n    return ret[:, 0:3]\n</code></pre>"},{"location":"reference/#ephysiopy.common.rhythmicity.CosineDirectionalTuning.intrinsic_freq_autoCorr","title":"<code>intrinsic_freq_autoCorr(spkTimes=None, posMask=None, maxFreq=25, acBinSize=0.002, acWindow=0.5, plot=True, **kwargs)</code>","text":"<p>This is taken and adapted from ephysiopy.common.eegcalcs.EEGCalcs</p> <p>Args:     spkTimes (np.array): Times in seconds of the cells firing     posMask (np.array): Boolean array corresponding to the length of                         spkTimes where True is stuff to keep     maxFreq (float): The maximum frequency to do the power spectrum                         out to     acBinSize (float): The bin size of the autocorrelogram in seconds     acWindow (float): The range of the autocorr in seconds</p> <p>Note:     Make sure all times are in seconds</p> Source code in <code>ephysiopy/common/rhythmicity.py</code> <pre><code>def intrinsic_freq_autoCorr(\n    self,\n    spkTimes=None,\n    posMask=None,\n    maxFreq=25,\n    acBinSize=0.002,\n    acWindow=0.5,\n    plot=True,\n    **kwargs,\n):\n    \"\"\"\n    This is taken and adapted from ephysiopy.common.eegcalcs.EEGCalcs\n\n    Args:\n        spkTimes (np.array): Times in seconds of the cells firing\n        posMask (np.array): Boolean array corresponding to the length of\n                            spkTimes where True is stuff to keep\n        maxFreq (float): The maximum frequency to do the power spectrum\n                            out to\n        acBinSize (float): The bin size of the autocorrelogram in seconds\n        acWindow (float): The range of the autocorr in seconds\n\n    Note:\n        Make sure all times are in seconds\n    \"\"\"\n    acBinsPerPos = 1.0 / self.pos_sample_rate / acBinSize\n    acWindowSizeBins = np.round(acWindow / acBinSize)\n    binCentres = np.arange(0.5, len(posMask) * acBinsPerPos) * acBinSize\n    spkTrHist, _ = np.histogram(spkTimes, bins=binCentres)\n\n    # split the single histogram into individual chunks\n    splitIdx = np.nonzero(np.diff(posMask.astype(int)))[0] + 1\n    splitMask = np.split(posMask, splitIdx)\n    splitSpkHist = np.split(spkTrHist, (splitIdx * acBinsPerPos).astype(int))\n    histChunks = []\n    for i in range(len(splitSpkHist)):\n        if np.all(splitMask[i]):\n            if np.sum(splitSpkHist[i]) &gt; 2:\n                if len(splitSpkHist[i]) &gt; int(acWindowSizeBins) * 2:\n                    histChunks.append(splitSpkHist[i])\n    autoCorrGrid = np.zeros((int(acWindowSizeBins) + 1, len(histChunks)))\n    chunkLens = []\n    from scipy import signal\n\n    print(f\"num chunks = {len(histChunks)}\")\n    for i in range(len(histChunks)):\n        lenThisChunk = len(histChunks[i])\n        chunkLens.append(lenThisChunk)\n        tmp = np.zeros(lenThisChunk * 2)\n        tmp[lenThisChunk // 2 : lenThisChunk // 2 + lenThisChunk] = histChunks[i]\n        tmp2 = signal.fftconvolve(\n            tmp, histChunks[i][::-1], mode=\"valid\"\n        )  # the autocorrelation\n        autoCorrGrid[:, i] = (\n            tmp2[lenThisChunk // 2 : lenThisChunk // 2 + int(acWindowSizeBins) + 1]\n            / acBinsPerPos\n        )\n\n    totalLen = np.sum(chunkLens)\n    autoCorrSum = np.nansum(autoCorrGrid, 1) / totalLen\n    meanNormdAc = autoCorrSum[1::] - np.nanmean(autoCorrSum[1::])\n    # return meanNormdAc\n    out = self.power_spectrum(\n        eeg=meanNormdAc,\n        binWidthSecs=acBinSize,\n        maxFreq=maxFreq,\n        pad2pow=16,\n        **kwargs,\n    )\n    out.update({\"meanNormdAc\": meanNormdAc})\n    if plot:\n        fig = plt.gcf()\n        ax = fig.gca()\n        xlim = ax.get_xlim()\n        ylim = ax.get_ylim()\n        ax.imshow(\n            autoCorrGrid,\n            extent=[\n                maxFreq * 0.6,\n                maxFreq,\n                np.max(out[\"Power\"]) * 0.6,\n                ax.get_ylim()[1],\n            ],\n        )\n        ax.set_ylim(ylim)\n        ax.set_xlim(xlim)\n    return out\n</code></pre>"},{"location":"reference/#ephysiopy.common.rhythmicity.CosineDirectionalTuning.power_spectrum","title":"<code>power_spectrum(eeg, plot=True, binWidthSecs=None, maxFreq=25, pad2pow=None, ymax=None, **kwargs)</code>","text":"<p>Method used by eeg_power_spectra and intrinsic_freq_autoCorr Signal in must be mean normalised already</p> Source code in <code>ephysiopy/common/rhythmicity.py</code> <pre><code>def power_spectrum(\n    self,\n    eeg,\n    plot=True,\n    binWidthSecs=None,\n    maxFreq=25,\n    pad2pow=None,\n    ymax=None,\n    **kwargs,\n):\n    \"\"\"\n    Method used by eeg_power_spectra and intrinsic_freq_autoCorr\n    Signal in must be mean normalised already\n    \"\"\"\n\n    # Get raw power spectrum\n    nqLim = 1 / binWidthSecs / 2.0\n    origLen = len(eeg)\n    # if pad2pow is None:\n    # \tfftLen = int(np.power(2, self._nextpow2(origLen)))\n    # else:\n    fftLen = int(np.power(2, pad2pow))\n    fftHalfLen = int(fftLen / float(2) + 1)\n\n    fftRes = np.fft.fft(eeg, fftLen)\n    # get power density from fft and discard second half of spectrum\n    _power = np.power(np.abs(fftRes), 2) / origLen\n    power = np.delete(_power, np.s_[fftHalfLen::])\n    power[1:-2] = power[1:-2] * 2\n\n    # calculate freqs and crop spectrum to requested range\n    freqs = nqLim * np.linspace(0, 1, fftHalfLen)\n    freqs = freqs[freqs &lt;= maxFreq].T\n    power = power[0 : len(freqs)]\n\n    # smooth spectrum using gaussian kernel\n    binsPerHz = (fftHalfLen - 1) / nqLim\n    kernelLen = np.round(self.smthKernelWidth * binsPerHz)\n    kernelSig = self.smthKernelSigma * binsPerHz\n    from scipy import signal\n\n    k = signal.windows.gaussian(kernelLen, kernelSig) / (kernelLen / 2 / 2)\n    power_sm = signal.fftconvolve(power, k[::-1], mode=\"same\")\n\n    # calculate some metrics\n    # find max in theta band\n    spectrumMaskBand = np.logical_and(\n        freqs &gt; self.thetaRange[0], freqs &lt; self.thetaRange[1]\n    )\n    bandMaxPower = np.max(power_sm[spectrumMaskBand])\n    maxBinInBand = np.argmax(power_sm[spectrumMaskBand])\n    bandFreqs = freqs[spectrumMaskBand]\n    freqAtBandMaxPower = bandFreqs[maxBinInBand]\n    # self.maxBinInBand = maxBinInBand\n    # self.freqAtBandMaxPower = freqAtBandMaxPower\n    # self.bandMaxPower = bandMaxPower\n\n    # find power in small window around peak and divide by power in rest\n    # of spectrum to get snr\n    spectrumMaskPeak = np.logical_and(\n        freqs &gt; freqAtBandMaxPower - self.sn2Width / 2,\n        freqs &lt; freqAtBandMaxPower + self.sn2Width / 2,\n    )\n    s2n = np.nanmean(power_sm[spectrumMaskPeak]) / np.nanmean(\n        power_sm[~spectrumMaskPeak]\n    )\n    self.freqs = freqs\n    self.power_sm = power_sm\n    self.spectrumMaskPeak = spectrumMaskPeak\n    if plot:\n        fig = plt.figure()\n        ax = fig.add_subplot(111)\n        if ymax is None:\n            ymax = np.min([2 * np.max(power), np.max(power_sm)])\n            if ymax == 0:\n                ymax = 1\n        ax.plot(freqs, power, c=[0.9, 0.9, 0.9])\n        # ax.hold(True)\n        ax.plot(freqs, power_sm, \"k\", lw=2)\n        ax.axvline(self.thetaRange[0], c=\"b\", ls=\"--\")\n        ax.axvline(self.thetaRange[1], c=\"b\", ls=\"--\")\n        _, stemlines, _ = ax.stem([freqAtBandMaxPower], [bandMaxPower], linefmt=\"r\")\n        # plt.setp(stemlines, 'linewidth', 2)\n        ax.fill_between(\n            freqs,\n            0,\n            power_sm,\n            where=spectrumMaskPeak,\n            color=\"r\",\n            alpha=0.25,\n            zorder=25,\n        )\n        # ax.set_ylim(0, ymax)\n        # ax.set_xlim(0, self.xmax)\n        ax.set_xlabel(\"Frequency (Hz)\")\n        ax.set_ylabel(\"Power density (W/Hz)\")\n    out_dict = {\n        \"maxFreq\": freqAtBandMaxPower,\n        \"Power\": power_sm,\n        \"Freqs\": freqs,\n        \"s2n\": s2n,\n        \"Power_raw\": power,\n        \"k\": k,\n        \"kernelLen\": kernelLen,\n        \"kernelSig\": kernelSig,\n        \"binsPerHz\": binsPerHz,\n        \"kernelLen\": kernelLen,\n    }\n    return out_dict\n</code></pre>"},{"location":"reference/#ephysiopy.common.rhythmicity.CosineDirectionalTuning.speedFilterRuns","title":"<code>speedFilterRuns(runs, minspeed=5.0)</code>","text":"<p>Given the runs identified in getRunsOfMinLength, filter for speed and return runs that meet the min speed criteria.</p> <p>The function goes over the runs with a moving window of length equal to self.min_runlength in samples and sees if any of those segments meet the speed criteria and splits them out into separate runs if true.</p> <p>NB For now this means the same spikes might get included in the autocorrelation procedure later as the moving window will use overlapping periods - can be modified later.</p> <p>Args:     runs (3 x nRuns np.array): Generated from getRunsOfMinLength     minspeed (float): Min running speed in cm/s for an epoch (minimum                       epoch length defined previously                       in getRunsOfMinLength as minlength, usually 0.4s)</p> <p>Returns:     3 x nRuns np.array: A modified version of the \"runs\" input variable</p> Source code in <code>ephysiopy/common/rhythmicity.py</code> <pre><code>def speedFilterRuns(self, runs: np.array, minspeed=5.0):\n    \"\"\"\n    Given the runs identified in getRunsOfMinLength, filter for speed\n    and return runs that meet the min speed criteria.\n\n    The function goes over the runs with a moving window of length equal\n    to self.min_runlength in samples and sees if any of those segments\n    meet the speed criteria and splits them out into separate runs if true.\n\n    NB For now this means the same spikes might get included in the\n    autocorrelation procedure later as the\n    moving window will use overlapping periods - can be modified later.\n\n    Args:\n        runs (3 x nRuns np.array): Generated from getRunsOfMinLength\n        minspeed (float): Min running speed in cm/s for an epoch (minimum\n                          epoch length defined previously\n                          in getRunsOfMinLength as minlength, usually 0.4s)\n\n    Returns:\n        3 x nRuns np.array: A modified version of the \"runs\" input variable\n    \"\"\"\n    minlength_in_samples = int(self.pos_sample_rate * self.min_runlength)\n    run_list = runs.tolist()\n    all_speed = np.array(self.speed)\n    for start_idx, end_idx, dir_bin in run_list:\n        this_runs_speed = all_speed[start_idx:end_idx]\n        this_runs_runs = self._rolling_window(this_runs_speed, minlength_in_samples)\n        run_mask = np.all(this_runs_runs &gt; minspeed, 1)\n        if np.any(run_mask):\n            print(\"got one\")\n</code></pre>"},{"location":"reference/#ephysiopy.common.rhythmicity.LFPOscillations","title":"<code>LFPOscillations</code>","text":"<p>               Bases: <code>object</code></p> <p>Does stuff with the LFP such as looking at nested oscillations (theta/ gamma coupling), the modulation index of such phenomena, filtering out certain frequencies in the LFP, getting the instantaneous phase and amplitude and so on</p> Source code in <code>ephysiopy/common/rhythmicity.py</code> <pre><code>class LFPOscillations(object):\n    \"\"\"\n    Does stuff with the LFP such as looking at nested oscillations\n    (theta/ gamma coupling), the modulation index of such phenomena,\n    filtering out certain frequencies in the LFP, getting the instantaneous\n    phase and amplitude and so on\n\n    \"\"\"\n\n    def __init__(self, sig, fs, **kwargs):\n        self.sig = sig\n        self.fs = fs\n\n    def getFreqPhase(self, sig, band2filter: list, ford=3):\n        \"\"\"\n        Uses the Hilbert transform to calculate the instantaneous phase and\n        amplitude of the time series in sig.\n\n        Args:\n            sig (np.array): The signal to be analysed\n            ford (int): The order for the Butterworth filter\n            band2filter (list): The two frequencies to be filtered for\n        \"\"\"\n        if sig is None:\n            sig = self.sig\n        band2filter = np.array(band2filter, dtype=float)\n\n        b, a = signal.butter(ford, band2filter / (self.fs / 2), btype=\"bandpass\")\n\n        filt_sig = signal.filtfilt(b, a, sig, padtype=\"odd\")\n        hilbert_sig = signal.hilbert(filt_sig)\n        phase = np.angle(hilbert_sig)\n        amplitude = np.abs(hilbert_sig)\n        inst_freq = self.fs / (2 * np.pi) * np.diff(np.unwrap(phase))\n        inst_freq = np.insert(inst_freq, -1, inst_freq[-1])\n        amplitude_filtered = signal.filtfilt(b, a, amplitude, padtype=\"odd\")\n        return filt_sig, phase, amplitude, amplitude_filtered, inst_freq\n\n    def modulationindex(\n        self,\n        sig=None,\n        nbins=20,\n        forder=2,\n        thetaband=[4, 8],\n        gammaband=[30, 80],\n        plot=True,\n    ):\n        \"\"\"\n        Calculates the modulation index of theta and gamma oscillations.\n        Specifically this is the circular correlation between the phase of\n        theta and the power of theta.\n\n        Args:\n            sig (np.array): The LFP signal\n            nbins (int): The number of bins in the circular range 0 to 2*pi\n            forder (int): The order of the butterworth filter\n            thetaband (list): The lower/upper bands of the theta freq range\n            gammaband (list): The lower/upper bands of the gamma freq range\n            plot (bool): Show some pics or not\n        \"\"\"\n        if sig is None:\n            sig = self.sig\n        sig = sig - np.ma.mean(sig)\n        if np.ma.is_masked(sig):\n            sig = np.ma.compressed(sig)\n        _, lowphase, _, _, _ = self.getFreqPhase(sig, thetaband, forder)\n        _, _, highamp, _, _ = self.getFreqPhase(sig, gammaband, forder)\n        inc = 2 * np.pi / nbins\n        a = np.arange(-np.pi + inc / 2, np.pi, inc)\n        dt = np.array([-inc / 2, inc / 2])\n        pbins = a[:, np.newaxis] + dt[np.newaxis, :]\n        amp = np.zeros((nbins))\n        phaselen = np.arange(len(lowphase))\n        for i in range(nbins):\n            pts = np.nonzero(\n                (lowphase &gt;= pbins[i, 0]) * (lowphase &lt; pbins[i, 1]) * phaselen\n            )\n            amp[i] = np.mean(highamp[pts])\n        amp = amp / np.sum(amp)\n        from ephysiopy.common.statscalcs import circ_r\n\n        mi = circ_r(pbins[:, 1], amp)\n        if plot:\n            fig = plt.figure()\n            ax = fig.add_subplot(111, polar=True)\n            w = np.pi / (nbins / 2)\n            ax.bar(pbins[:, 1], amp, width=w)\n            ax.set_title(\"Modulation index={0:.5f}\".format(mi))\n        return mi\n\n    def plv(\n        self,\n        sig=None,\n        forder=2,\n        thetaband=[4, 8],\n        gammaband=[30, 80],\n        plot=True,\n        **kwargs,\n    ):\n        \"\"\"\n        Computes the phase-amplitude coupling (PAC) of nested oscillations.\n        More specifically this is the phase-locking value (PLV) between two\n        nested oscillations in EEG data, in this case theta (default 4-8Hz)\n        and gamma (defaults to 30-80Hz). A PLV of unity indicates perfect phase\n        locking (here PAC) and a value of zero indicates no locking (no PAC)\n\n        Args:\n            eeg (numpy array): The eeg data itself. This is a 1-d array which\n            can be masked or not\n            forder (int): The order of the filter(s) applied to the eeg data\n            thetaband, gammaband (list/array): The range of values to bandpass\n            filter for for the theta and gamma ranges\n            plot (bool, optional): Whether to plot the resulting binned up\n            polar plot which shows the amplitude of the gamma oscillation\n            found at different phases of the theta oscillation.\n            Default is True.\n\n        Returns:\n            plv (float): The value of the phase-amplitude coupling\n        \"\"\"\n\n        if sig is None:\n            sig = self.sig\n        sig = sig - np.ma.mean(sig)\n        if np.ma.is_masked(sig):\n            sig = np.ma.compressed(sig)\n\n        _, lowphase, _, _, _ = self.getFreqPhase(sig, thetaband, forder)\n        _, _, _, highamp_f, _ = self.getFreqPhase(sig, gammaband, forder)\n\n        highampphase = np.angle(signal.hilbert(highamp_f))\n        phasedf = highampphase - lowphase\n        phasedf = np.exp(1j * phasedf)\n        phasedf = np.angle(phasedf)\n        from ephysiopy.common.statscalcs import circ_r\n\n        plv = circ_r(phasedf)\n        th = np.linspace(0.0, 2 * np.pi, 20, endpoint=False)\n        h, _ = np.histogram(phasedf, bins=20)\n        h = h / float(len(phasedf))\n\n        if plot:\n            fig = plt.figure()\n            ax = fig.add_subplot(111, polar=True)\n            w = np.pi / 10\n            ax.bar(th, h, width=w, bottom=0.0)\n        return plv, th, h\n\n    def filterForLaser(self, sig=None, width=0.125, dip=15.0, stimFreq=6.66):\n        \"\"\"\n        Attempts to filter out frequencies from optogenetic experiments where\n        the frequency of laser stimulation was at 6.66Hz.\n\n        Note:\n            This method needs tweaking for each trial as the power in the signal\n            is variable across trials / animals etc. A potential improvement could be using mean\n            power or a similar metric.\n        \"\"\"\n        from scipy.signal import filtfilt, firwin, kaiserord\n\n        nyq = self.fs / 2.0\n        width = width / nyq\n        dip = dip\n        N, beta = kaiserord(dip, width)\n        print(\"N: {0}\\nbeta: {1}\".format(N, beta))\n        upper = np.ceil(nyq / stimFreq)\n        c = np.arange(stimFreq, upper * stimFreq, stimFreq)\n        dt = np.array([-0.125, 0.125])\n        cutoff_hz = dt[:, np.newaxis] + c[np.newaxis, :]\n        cutoff_hz = cutoff_hz.ravel()\n        cutoff_hz = np.append(cutoff_hz, nyq - 1)\n        cutoff_hz.sort()\n        cutoff_hz_nyq = cutoff_hz / nyq\n        taps = firwin(N, cutoff_hz_nyq, window=(\"kaiser\", beta))\n        if sig is None:\n            sig = self.sig\n        fx = filtfilt(taps, [1.0], sig)\n        return fx\n\n    def theta_running(\n        self, pos_data: PosCalcsGeneric, lfp_data: EEGCalcsGeneric, **kwargs\n    ) -&gt; tuple[np.ma.MaskedArray, ...]:\n        \"\"\"\n        Returns metrics to do with the theta frequency/ power and running speed/ acceleration\n\n\n        \"\"\"\n        low_theta = kwargs.pop(\"low_theta\", 6)\n        high_theta = kwargs.pop(\"high_theta\", 12)\n        low_speed = kwargs.pop(\"low_speed\", 2)\n        high_speed = kwargs.pop(\"high_speed\", 35)\n        nbins = kwargs.pop(\"nbins\", 13)\n        _, _, _, _, inst_freq = self.getFreqPhase(\n            lfp_data.sig, band2filter=[low_theta, high_theta]\n        )\n        # interpolate speed to match the frequency of the LFP data\n        eeg_time = np.linspace(\n            0, lfp_data.sig.shape[0] / lfp_data.fs, len(lfp_data.sig)\n        )\n        pos_time = np.linspace(0, pos_data.duration, pos_data.npos)\n        interpolated_speed = np.interp(eeg_time, pos_time, pos_data.speed)\n        h, e = np.histogramdd(\n            [inst_freq, interpolated_speed],\n            bins=(\n                np.linspace(low_theta, high_theta, nbins),\n                np.linspace(low_speed, high_speed, nbins),\n            ),\n        )\n        # overlay the mean points for each speed bin\n        spd_bins = np.linspace(low_speed, high_speed, nbins)\n\n        def __freq_calc__(fn: Callable) -&gt; list:\n            return [\n                fn(\n                    inst_freq[\n                        np.logical_and(interpolated_speed &gt; s1, interpolated_speed &lt; s2)\n                    ]\n                )\n                for s1, s2 in zip(spd_bins[:-1], spd_bins[1:])\n            ]\n\n        mean_freqs = __freq_calc__(np.mean)\n        counts = [\n            np.count_nonzero(np.logical_and(pos_data.speed &gt;= s1, pos_data.speed &lt; s2))\n            for s1, s2 in zip(spd_bins[:-1], spd_bins[1:])\n        ]\n        std_freqs = __freq_calc__(np.std) / np.sqrt(counts)\n\n        plt.pcolormesh(\n            e[1],\n            e[0],\n            h,\n            cmap=matplotlib.colormaps[\"bone_r\"],\n            norm=matplotlib.colors.LogNorm(),\n        )\n        plt.colorbar()\n        plt.errorbar(x=spd_bins[1:] - 2, y=mean_freqs, yerr=std_freqs, fmt=\"r.\")\n        ax = plt.gca()\n        ax.set_ylim((low_theta, high_theta))\n        ax.set_ylabel(\"Frequency (Hz)\")\n        ax.set_xlabel(\"Running speed (cm/s)\")\n        # mask the speed and lfp vectors so we can return these based\n        # on the low/high bounds of speed &amp; theta for doing correlations/\n        # stats later\n        speed_masked = np.ma.masked_outside(interpolated_speed, low_speed, high_speed)\n        theta_masked = np.ma.masked_outside(inst_freq, low_theta, high_theta)\n        # extract both masks, combine and re-apply\n        mask = np.logical_or(speed_masked.mask, theta_masked.mask)\n        speed_masked.mask = mask\n        theta_masked.mask = mask\n        # do the linear regression to add to the plot\n        # alternative argument here says we expect the correlation to be positive\n        res = linregress(\n            speed_masked.compressed(), theta_masked.compressed(), alternative=\"greater\"\n        )\n        ax.plot(spd_bins[1:] - 2, res.intercept + res.slope * (spd_bins[1:] - 2), \"r--\")\n        ax.set_title(\n            f\"r = {res.rvalue:.2f}, p = {res.pvalue:.2f}, intercept = {res.intercept:.2f}\"\n        )\n        return speed_masked, theta_masked\n\n    def get_theta_phase(self, cluster_times: np.ndarray, **kwargs):\n        \"\"\"\n        Calculates the phase of theta at which a cluster emitted spikes\n        and returns a fit to a vonmises distribution\n\n        Parameters\n        ----------\n        cluster_times (np.ndarray) - the times the cluster emitted spikes in\n                                     seconds\n\n        Notes\n        -----\n        kwargs can include:\n            low_theta (int) - low end for bandpass filter\n            high_theta (int) - high end for bandpass filter\n\n        \"\"\"\n        low_theta = kwargs.pop(\"low_theta\", 6)\n        high_theta = kwargs.pop(\"high_theta\", 12)\n        _, phase, _, _, _ = self.getFreqPhase(self.sig, [low_theta, high_theta])\n        # get indices into the phase vector\n        phase_idx = np.array(cluster_times * self.fs, dtype=int)\n        # It's possible that there are indices higher than the length of\n        # the phase vector so lets set them to the last index\n        bad_idx = np.nonzero(phase_idx &gt; len(phase))[0]\n        phase_idx[bad_idx] = len(phase) - 1\n        # get some stats for fitting to a vonmises\n        kappa, loc, _ = stats.vonmises.fit(phase[phase_idx])\n        x = np.linspace(-np.pi, np.pi, num=501)\n        y = np.exp(kappa * np.cos(x - loc)) / (2 * np.pi * i0(kappa))\n        return phase[phase_idx], x, y\n\n    def spike_xy_phase_plot(\n        self,\n        cluster: int,\n        pos_data: PosCalcsGeneric,\n        phy_data: TemplateModel,\n        lfp_data: EEGCalcsGeneric,\n    ) -&gt; plt.Axes:\n        \"\"\"\n        Produces a plot of the phase of theta at which each spike was\n        emitted. Each spike is plotted according to the x-y location the\n        animal was in when it was fired and the colour of the marker\n        corresponds to the phase of theta at which it fired.\n        \"\"\"\n        _, phase, _, _, _ = self.getFreqPhase(lfp_data.sig, [6, 12])\n        cluster_times = phy_data.spike_times[\n            phy_data.spike_clusters == cluster\n        ]  # in seconds\n        # get indices into the phase vector\n        phase_idx = np.array(cluster_times * self.fs, dtype=int)\n        # It's possible that there are indices higher than the length of\n        # the phase vector so lets set them to the last index\n        bad_idx = np.nonzero(phase_idx &gt; len(phase))[0]\n        phase_idx[bad_idx] = len(phase) - 1\n        # get indices into the position data\n        pos_idx = np.array(cluster_times * pos_data.sample_rate, dtype=int)\n        bad_idx = np.nonzero(pos_idx &gt;= len(pos_data.xyTS))[0]\n        pos_idx[bad_idx] = len(pos_data.xyTS) - 1\n        # add PI to phases to remove negativity\n        # cluster_phases = phase[phase_idx]\n        # TODO: create the colour map for phase and plot\n        spike_xy = pos_data.xy[:, pos_idx]\n        spike_phase = phase[phase_idx]\n        cmap = matplotlib.colormaps[\"hsv\"]\n        fig, ax = plt.subplots()\n        ax.plot(pos_data.xy[0], pos_data.xy[1], color=\"lightgrey\", zorder=0)\n        ax.scatter(spike_xy[0], spike_xy[1], c=spike_phase, cmap=cmap, zorder=1)\n        return ax\n</code></pre>"},{"location":"reference/#ephysiopy.common.rhythmicity.LFPOscillations.filterForLaser","title":"<code>filterForLaser(sig=None, width=0.125, dip=15.0, stimFreq=6.66)</code>","text":"<p>Attempts to filter out frequencies from optogenetic experiments where the frequency of laser stimulation was at 6.66Hz.</p> <p>Note:     This method needs tweaking for each trial as the power in the signal     is variable across trials / animals etc. A potential improvement could be using mean     power or a similar metric.</p> Source code in <code>ephysiopy/common/rhythmicity.py</code> <pre><code>def filterForLaser(self, sig=None, width=0.125, dip=15.0, stimFreq=6.66):\n    \"\"\"\n    Attempts to filter out frequencies from optogenetic experiments where\n    the frequency of laser stimulation was at 6.66Hz.\n\n    Note:\n        This method needs tweaking for each trial as the power in the signal\n        is variable across trials / animals etc. A potential improvement could be using mean\n        power or a similar metric.\n    \"\"\"\n    from scipy.signal import filtfilt, firwin, kaiserord\n\n    nyq = self.fs / 2.0\n    width = width / nyq\n    dip = dip\n    N, beta = kaiserord(dip, width)\n    print(\"N: {0}\\nbeta: {1}\".format(N, beta))\n    upper = np.ceil(nyq / stimFreq)\n    c = np.arange(stimFreq, upper * stimFreq, stimFreq)\n    dt = np.array([-0.125, 0.125])\n    cutoff_hz = dt[:, np.newaxis] + c[np.newaxis, :]\n    cutoff_hz = cutoff_hz.ravel()\n    cutoff_hz = np.append(cutoff_hz, nyq - 1)\n    cutoff_hz.sort()\n    cutoff_hz_nyq = cutoff_hz / nyq\n    taps = firwin(N, cutoff_hz_nyq, window=(\"kaiser\", beta))\n    if sig is None:\n        sig = self.sig\n    fx = filtfilt(taps, [1.0], sig)\n    return fx\n</code></pre>"},{"location":"reference/#ephysiopy.common.rhythmicity.LFPOscillations.getFreqPhase","title":"<code>getFreqPhase(sig, band2filter, ford=3)</code>","text":"<p>Uses the Hilbert transform to calculate the instantaneous phase and amplitude of the time series in sig.</p> <p>Args:     sig (np.array): The signal to be analysed     ford (int): The order for the Butterworth filter     band2filter (list): The two frequencies to be filtered for</p> Source code in <code>ephysiopy/common/rhythmicity.py</code> <pre><code>def getFreqPhase(self, sig, band2filter: list, ford=3):\n    \"\"\"\n    Uses the Hilbert transform to calculate the instantaneous phase and\n    amplitude of the time series in sig.\n\n    Args:\n        sig (np.array): The signal to be analysed\n        ford (int): The order for the Butterworth filter\n        band2filter (list): The two frequencies to be filtered for\n    \"\"\"\n    if sig is None:\n        sig = self.sig\n    band2filter = np.array(band2filter, dtype=float)\n\n    b, a = signal.butter(ford, band2filter / (self.fs / 2), btype=\"bandpass\")\n\n    filt_sig = signal.filtfilt(b, a, sig, padtype=\"odd\")\n    hilbert_sig = signal.hilbert(filt_sig)\n    phase = np.angle(hilbert_sig)\n    amplitude = np.abs(hilbert_sig)\n    inst_freq = self.fs / (2 * np.pi) * np.diff(np.unwrap(phase))\n    inst_freq = np.insert(inst_freq, -1, inst_freq[-1])\n    amplitude_filtered = signal.filtfilt(b, a, amplitude, padtype=\"odd\")\n    return filt_sig, phase, amplitude, amplitude_filtered, inst_freq\n</code></pre>"},{"location":"reference/#ephysiopy.common.rhythmicity.LFPOscillations.get_theta_phase","title":"<code>get_theta_phase(cluster_times, **kwargs)</code>","text":"<p>Calculates the phase of theta at which a cluster emitted spikes and returns a fit to a vonmises distribution</p> <p>Parameters:</p> Name Type Description Default <code>cluster_times</code> <code>ndarray</code> <pre><code>                     seconds\n</code></pre> required Notes <p>kwargs can include:     low_theta (int) - low end for bandpass filter     high_theta (int) - high end for bandpass filter</p> Source code in <code>ephysiopy/common/rhythmicity.py</code> <pre><code>def get_theta_phase(self, cluster_times: np.ndarray, **kwargs):\n    \"\"\"\n    Calculates the phase of theta at which a cluster emitted spikes\n    and returns a fit to a vonmises distribution\n\n    Parameters\n    ----------\n    cluster_times (np.ndarray) - the times the cluster emitted spikes in\n                                 seconds\n\n    Notes\n    -----\n    kwargs can include:\n        low_theta (int) - low end for bandpass filter\n        high_theta (int) - high end for bandpass filter\n\n    \"\"\"\n    low_theta = kwargs.pop(\"low_theta\", 6)\n    high_theta = kwargs.pop(\"high_theta\", 12)\n    _, phase, _, _, _ = self.getFreqPhase(self.sig, [low_theta, high_theta])\n    # get indices into the phase vector\n    phase_idx = np.array(cluster_times * self.fs, dtype=int)\n    # It's possible that there are indices higher than the length of\n    # the phase vector so lets set them to the last index\n    bad_idx = np.nonzero(phase_idx &gt; len(phase))[0]\n    phase_idx[bad_idx] = len(phase) - 1\n    # get some stats for fitting to a vonmises\n    kappa, loc, _ = stats.vonmises.fit(phase[phase_idx])\n    x = np.linspace(-np.pi, np.pi, num=501)\n    y = np.exp(kappa * np.cos(x - loc)) / (2 * np.pi * i0(kappa))\n    return phase[phase_idx], x, y\n</code></pre>"},{"location":"reference/#ephysiopy.common.rhythmicity.LFPOscillations.modulationindex","title":"<code>modulationindex(sig=None, nbins=20, forder=2, thetaband=[4, 8], gammaband=[30, 80], plot=True)</code>","text":"<p>Calculates the modulation index of theta and gamma oscillations. Specifically this is the circular correlation between the phase of theta and the power of theta.</p> <p>Args:     sig (np.array): The LFP signal     nbins (int): The number of bins in the circular range 0 to 2*pi     forder (int): The order of the butterworth filter     thetaband (list): The lower/upper bands of the theta freq range     gammaband (list): The lower/upper bands of the gamma freq range     plot (bool): Show some pics or not</p> Source code in <code>ephysiopy/common/rhythmicity.py</code> <pre><code>def modulationindex(\n    self,\n    sig=None,\n    nbins=20,\n    forder=2,\n    thetaband=[4, 8],\n    gammaband=[30, 80],\n    plot=True,\n):\n    \"\"\"\n    Calculates the modulation index of theta and gamma oscillations.\n    Specifically this is the circular correlation between the phase of\n    theta and the power of theta.\n\n    Args:\n        sig (np.array): The LFP signal\n        nbins (int): The number of bins in the circular range 0 to 2*pi\n        forder (int): The order of the butterworth filter\n        thetaband (list): The lower/upper bands of the theta freq range\n        gammaband (list): The lower/upper bands of the gamma freq range\n        plot (bool): Show some pics or not\n    \"\"\"\n    if sig is None:\n        sig = self.sig\n    sig = sig - np.ma.mean(sig)\n    if np.ma.is_masked(sig):\n        sig = np.ma.compressed(sig)\n    _, lowphase, _, _, _ = self.getFreqPhase(sig, thetaband, forder)\n    _, _, highamp, _, _ = self.getFreqPhase(sig, gammaband, forder)\n    inc = 2 * np.pi / nbins\n    a = np.arange(-np.pi + inc / 2, np.pi, inc)\n    dt = np.array([-inc / 2, inc / 2])\n    pbins = a[:, np.newaxis] + dt[np.newaxis, :]\n    amp = np.zeros((nbins))\n    phaselen = np.arange(len(lowphase))\n    for i in range(nbins):\n        pts = np.nonzero(\n            (lowphase &gt;= pbins[i, 0]) * (lowphase &lt; pbins[i, 1]) * phaselen\n        )\n        amp[i] = np.mean(highamp[pts])\n    amp = amp / np.sum(amp)\n    from ephysiopy.common.statscalcs import circ_r\n\n    mi = circ_r(pbins[:, 1], amp)\n    if plot:\n        fig = plt.figure()\n        ax = fig.add_subplot(111, polar=True)\n        w = np.pi / (nbins / 2)\n        ax.bar(pbins[:, 1], amp, width=w)\n        ax.set_title(\"Modulation index={0:.5f}\".format(mi))\n    return mi\n</code></pre>"},{"location":"reference/#ephysiopy.common.rhythmicity.LFPOscillations.plv","title":"<code>plv(sig=None, forder=2, thetaband=[4, 8], gammaband=[30, 80], plot=True, **kwargs)</code>","text":"<p>Computes the phase-amplitude coupling (PAC) of nested oscillations. More specifically this is the phase-locking value (PLV) between two nested oscillations in EEG data, in this case theta (default 4-8Hz) and gamma (defaults to 30-80Hz). A PLV of unity indicates perfect phase locking (here PAC) and a value of zero indicates no locking (no PAC)</p> <p>Args:     eeg (numpy array): The eeg data itself. This is a 1-d array which     can be masked or not     forder (int): The order of the filter(s) applied to the eeg data     thetaband, gammaband (list/array): The range of values to bandpass     filter for for the theta and gamma ranges     plot (bool, optional): Whether to plot the resulting binned up     polar plot which shows the amplitude of the gamma oscillation     found at different phases of the theta oscillation.     Default is True.</p> <p>Returns:     plv (float): The value of the phase-amplitude coupling</p> Source code in <code>ephysiopy/common/rhythmicity.py</code> <pre><code>def plv(\n    self,\n    sig=None,\n    forder=2,\n    thetaband=[4, 8],\n    gammaband=[30, 80],\n    plot=True,\n    **kwargs,\n):\n    \"\"\"\n    Computes the phase-amplitude coupling (PAC) of nested oscillations.\n    More specifically this is the phase-locking value (PLV) between two\n    nested oscillations in EEG data, in this case theta (default 4-8Hz)\n    and gamma (defaults to 30-80Hz). A PLV of unity indicates perfect phase\n    locking (here PAC) and a value of zero indicates no locking (no PAC)\n\n    Args:\n        eeg (numpy array): The eeg data itself. This is a 1-d array which\n        can be masked or not\n        forder (int): The order of the filter(s) applied to the eeg data\n        thetaband, gammaband (list/array): The range of values to bandpass\n        filter for for the theta and gamma ranges\n        plot (bool, optional): Whether to plot the resulting binned up\n        polar plot which shows the amplitude of the gamma oscillation\n        found at different phases of the theta oscillation.\n        Default is True.\n\n    Returns:\n        plv (float): The value of the phase-amplitude coupling\n    \"\"\"\n\n    if sig is None:\n        sig = self.sig\n    sig = sig - np.ma.mean(sig)\n    if np.ma.is_masked(sig):\n        sig = np.ma.compressed(sig)\n\n    _, lowphase, _, _, _ = self.getFreqPhase(sig, thetaband, forder)\n    _, _, _, highamp_f, _ = self.getFreqPhase(sig, gammaband, forder)\n\n    highampphase = np.angle(signal.hilbert(highamp_f))\n    phasedf = highampphase - lowphase\n    phasedf = np.exp(1j * phasedf)\n    phasedf = np.angle(phasedf)\n    from ephysiopy.common.statscalcs import circ_r\n\n    plv = circ_r(phasedf)\n    th = np.linspace(0.0, 2 * np.pi, 20, endpoint=False)\n    h, _ = np.histogram(phasedf, bins=20)\n    h = h / float(len(phasedf))\n\n    if plot:\n        fig = plt.figure()\n        ax = fig.add_subplot(111, polar=True)\n        w = np.pi / 10\n        ax.bar(th, h, width=w, bottom=0.0)\n    return plv, th, h\n</code></pre>"},{"location":"reference/#ephysiopy.common.rhythmicity.LFPOscillations.spike_xy_phase_plot","title":"<code>spike_xy_phase_plot(cluster, pos_data, phy_data, lfp_data)</code>","text":"<p>Produces a plot of the phase of theta at which each spike was emitted. Each spike is plotted according to the x-y location the animal was in when it was fired and the colour of the marker corresponds to the phase of theta at which it fired.</p> Source code in <code>ephysiopy/common/rhythmicity.py</code> <pre><code>def spike_xy_phase_plot(\n    self,\n    cluster: int,\n    pos_data: PosCalcsGeneric,\n    phy_data: TemplateModel,\n    lfp_data: EEGCalcsGeneric,\n) -&gt; plt.Axes:\n    \"\"\"\n    Produces a plot of the phase of theta at which each spike was\n    emitted. Each spike is plotted according to the x-y location the\n    animal was in when it was fired and the colour of the marker\n    corresponds to the phase of theta at which it fired.\n    \"\"\"\n    _, phase, _, _, _ = self.getFreqPhase(lfp_data.sig, [6, 12])\n    cluster_times = phy_data.spike_times[\n        phy_data.spike_clusters == cluster\n    ]  # in seconds\n    # get indices into the phase vector\n    phase_idx = np.array(cluster_times * self.fs, dtype=int)\n    # It's possible that there are indices higher than the length of\n    # the phase vector so lets set them to the last index\n    bad_idx = np.nonzero(phase_idx &gt; len(phase))[0]\n    phase_idx[bad_idx] = len(phase) - 1\n    # get indices into the position data\n    pos_idx = np.array(cluster_times * pos_data.sample_rate, dtype=int)\n    bad_idx = np.nonzero(pos_idx &gt;= len(pos_data.xyTS))[0]\n    pos_idx[bad_idx] = len(pos_data.xyTS) - 1\n    # add PI to phases to remove negativity\n    # cluster_phases = phase[phase_idx]\n    # TODO: create the colour map for phase and plot\n    spike_xy = pos_data.xy[:, pos_idx]\n    spike_phase = phase[phase_idx]\n    cmap = matplotlib.colormaps[\"hsv\"]\n    fig, ax = plt.subplots()\n    ax.plot(pos_data.xy[0], pos_data.xy[1], color=\"lightgrey\", zorder=0)\n    ax.scatter(spike_xy[0], spike_xy[1], c=spike_phase, cmap=cmap, zorder=1)\n    return ax\n</code></pre>"},{"location":"reference/#ephysiopy.common.rhythmicity.LFPOscillations.theta_running","title":"<code>theta_running(pos_data, lfp_data, **kwargs)</code>","text":"<p>Returns metrics to do with the theta frequency/ power and running speed/ acceleration</p> Source code in <code>ephysiopy/common/rhythmicity.py</code> <pre><code>def theta_running(\n    self, pos_data: PosCalcsGeneric, lfp_data: EEGCalcsGeneric, **kwargs\n) -&gt; tuple[np.ma.MaskedArray, ...]:\n    \"\"\"\n    Returns metrics to do with the theta frequency/ power and running speed/ acceleration\n\n\n    \"\"\"\n    low_theta = kwargs.pop(\"low_theta\", 6)\n    high_theta = kwargs.pop(\"high_theta\", 12)\n    low_speed = kwargs.pop(\"low_speed\", 2)\n    high_speed = kwargs.pop(\"high_speed\", 35)\n    nbins = kwargs.pop(\"nbins\", 13)\n    _, _, _, _, inst_freq = self.getFreqPhase(\n        lfp_data.sig, band2filter=[low_theta, high_theta]\n    )\n    # interpolate speed to match the frequency of the LFP data\n    eeg_time = np.linspace(\n        0, lfp_data.sig.shape[0] / lfp_data.fs, len(lfp_data.sig)\n    )\n    pos_time = np.linspace(0, pos_data.duration, pos_data.npos)\n    interpolated_speed = np.interp(eeg_time, pos_time, pos_data.speed)\n    h, e = np.histogramdd(\n        [inst_freq, interpolated_speed],\n        bins=(\n            np.linspace(low_theta, high_theta, nbins),\n            np.linspace(low_speed, high_speed, nbins),\n        ),\n    )\n    # overlay the mean points for each speed bin\n    spd_bins = np.linspace(low_speed, high_speed, nbins)\n\n    def __freq_calc__(fn: Callable) -&gt; list:\n        return [\n            fn(\n                inst_freq[\n                    np.logical_and(interpolated_speed &gt; s1, interpolated_speed &lt; s2)\n                ]\n            )\n            for s1, s2 in zip(spd_bins[:-1], spd_bins[1:])\n        ]\n\n    mean_freqs = __freq_calc__(np.mean)\n    counts = [\n        np.count_nonzero(np.logical_and(pos_data.speed &gt;= s1, pos_data.speed &lt; s2))\n        for s1, s2 in zip(spd_bins[:-1], spd_bins[1:])\n    ]\n    std_freqs = __freq_calc__(np.std) / np.sqrt(counts)\n\n    plt.pcolormesh(\n        e[1],\n        e[0],\n        h,\n        cmap=matplotlib.colormaps[\"bone_r\"],\n        norm=matplotlib.colors.LogNorm(),\n    )\n    plt.colorbar()\n    plt.errorbar(x=spd_bins[1:] - 2, y=mean_freqs, yerr=std_freqs, fmt=\"r.\")\n    ax = plt.gca()\n    ax.set_ylim((low_theta, high_theta))\n    ax.set_ylabel(\"Frequency (Hz)\")\n    ax.set_xlabel(\"Running speed (cm/s)\")\n    # mask the speed and lfp vectors so we can return these based\n    # on the low/high bounds of speed &amp; theta for doing correlations/\n    # stats later\n    speed_masked = np.ma.masked_outside(interpolated_speed, low_speed, high_speed)\n    theta_masked = np.ma.masked_outside(inst_freq, low_theta, high_theta)\n    # extract both masks, combine and re-apply\n    mask = np.logical_or(speed_masked.mask, theta_masked.mask)\n    speed_masked.mask = mask\n    theta_masked.mask = mask\n    # do the linear regression to add to the plot\n    # alternative argument here says we expect the correlation to be positive\n    res = linregress(\n        speed_masked.compressed(), theta_masked.compressed(), alternative=\"greater\"\n    )\n    ax.plot(spd_bins[1:] - 2, res.intercept + res.slope * (spd_bins[1:] - 2), \"r--\")\n    ax.set_title(\n        f\"r = {res.rvalue:.2f}, p = {res.pvalue:.2f}, intercept = {res.intercept:.2f}\"\n    )\n    return speed_masked, theta_masked\n</code></pre>"},{"location":"reference/#ephysiopy.common.rhythmicity.Rippler","title":"<code>Rippler</code>","text":"<p>               Bases: <code>object</code></p> <p>Does some spectrographic analysis and plots of LFP data looking specifically at the ripple band</p> <p>Until I modified the Ripple Detector plugin the duration of the TTL pulses was variable with a more or less bimodal distribution which is why there is a separate treatment of short and long duration TTL pulses below</p> Source code in <code>ephysiopy/common/rhythmicity.py</code> <pre><code>class Rippler(object):\n    \"\"\"\n    Does some spectrographic analysis and plots of LFP data\n    looking specifically at the ripple band\n\n    Until I modified the Ripple Detector plugin the duration of the TTL\n    pulses was variable with a more or less bimodal distribution which\n    is why there is a separate treatment of short and long duration TTL pulses below\n\n    \"\"\"\n\n    n_channels = 64\n    # time units are seconds, frequencies and sample rates in Hz\n    pre_ttl = 0.05\n    post_ttl = 0.2\n    min_ttl_duration = 0.01\n    # Not all TTL \"events\" in OE parlance result in a laser pulse as I modified\n    # the plugin so that only x percent are sent to the ttl \"out\" line that goes\n    # to the laser. All TTL events *are* recorded however on a separate TTL line\n    # that here I am calling ttl_all_line as opposed to ttl_out_line which is the\n    # line that goes to the laser - these values are overwritten when the Ripple\n    # Detector plugin settings are loaded in __init__\n    ttl_all_line = 4\n    ttl_out_line = 1\n    ttl_percent = (\n        100  # percentage of the ripple detections that get propagated to laser\n    )\n    ttl_duration = 0.05  # minimum duration of TTL pulse in seconds\n    low_band = 120  # Hz\n    high_band = 250  # Hz\n    bit_volts = 0.1949999928474426  # available in the structure.oebin file\n    # some parameters for the FFT stuff\n    gaussian_window = 12  # in samples\n    gaussian_std = 5\n    lfp_plotting_scale = (\n        500  # this is the scale/range I was looking at the ripple filtered lfp signal\n    )\n    ripple_std_dev = 2\n    ripple_min_duration_ms = 20\n\n    def __init__(self, trial_root: Path, signal: np.ndarray, fs: int):\n        \"\"\"\n        trial_root (Path) - location of the root recording directory, used to load ttls etc\n        signal (np.ndarray) - the LFP signal (usually downsampled to about 500-1000Hz)\n        fs (int) - the sampling rate of the signal\n        \"\"\"\n\n        self.pname_for_trial = trial_root\n        self.orig_sig = signal\n        self.fs = fs\n        self.settings = Settings(str(trial_root))\n        LFP = EEGCalcsGeneric(signal, fs)\n        self.LFP = LFP\n        detector_settings = self.settings.get_processor(\"Ripple\")\n        self.ttl_duration = (\n            float(detector_settings.ttl_duration) / 1000\n        )  # in seconds now\n\n        pname_for_ttl_data = self._find_path_to_ripple_ttl(self.pname_for_trial)\n        sync_file = pname_for_ttl_data.parents[2] / Path(\"sync_messages.txt\")\n        recording_start_time = self._load_start_time(sync_file)\n        ttl_ts = np.load(pname_for_ttl_data / \"timestamps.npy\") - recording_start_time\n        ttl_states = np.load(pname_for_ttl_data / \"states.npy\")\n        all_ons = ttl_ts[ttl_states == int(detector_settings.Ripple_save)]\n        laser_ons = ttl_ts[ttl_states == int(detector_settings.Ripple_Out)]\n        laser_offs = ttl_ts[ttl_states == -int(detector_settings.Ripple_Out)]\n        no_laser_ons = np.lib.setdiff1d(all_ons, laser_ons)\n\n        self.all_on_ts = all_ons\n        self.ttl_states = ttl_states\n        self.all_ts = ttl_ts\n        self.laser_on_ts = laser_ons\n        self.laser_off_ts = laser_offs\n        self.no_laser_on_ts = no_laser_ons\n\n        filtered_eeg = LFP.butterFilter(self.low_band, self.high_band)\n        filtered_eeg *= self.bit_volts\n        self.filtered_eeg = filtered_eeg\n        self.eeg_time = np.linspace(\n            0,\n            LFP.sig.shape[0] / self.fs,\n            LFP.sig.shape[0],\n        )  # in seconds\n\n    def update_bandpass(self, low=None, high=None):\n        if low is None:\n            low = self.low_band\n        self.low_band = low\n        if high is None:\n            high = self.high_band\n        self.high_band = high\n        filtered_eeg = self.LFP.butterFilter(low, high)\n        filtered_eeg *= self.bit_volts\n        self.filtered_eeg = filtered_eeg\n\n    def _load_start_time(self, path_to_sync_message_file: Path):\n        \"\"\"\n        Returns the start time contained in a sync file from OE\n        \"\"\"\n        recording_start_time = 0\n        with open(path_to_sync_message_file, \"r\") as f:\n            sync_strs = f.read()\n            sync_lines = sync_strs.split(\"\\n\")\n            for line in sync_lines:\n                if \"Start Time\" in line:\n                    tokens = line.split(\":\")\n                    start_time = int(tokens[-1])\n                    sample_rate = int(tokens[0].split(\"@\")[-1].strip().split()[0])\n                    recording_start_time = start_time / float(sample_rate)\n        return recording_start_time\n\n    def _find_path_to_continuous(self, trial_root: Path, **kwargs) -&gt; Path:\n        \"\"\"\n        Iterates through a directory tree and finds the path to the\n        Ripple Detector plugin data and returns its location\n        \"\"\"\n        exp_name = kwargs.pop(\"experiment\", \"experiment1\")\n        rec_name = kwargs.pop(\"recording\", \"recording1\")\n        folder_match = (\n            trial_root\n            / Path(\"Record Node [0-9][0-9][0-9]\")\n            / Path(exp_name)\n            / Path(rec_name)\n            / Path(\"events\")\n            / Path(\"Acquisition_Board-[0-9][0-9][0-9].*\")\n        )\n        for d, c, f in os.walk(trial_root):\n            for ff in f:\n                if \".\" not in c:  # ignore hidden directories\n                    if \"continuous.dat\" in ff:\n                        if PurePath(d).match(str(folder_match)):\n                            return Path(d)\n        return Path()\n\n    def _find_path_to_ripple_ttl(self, trial_root: Path, **kwargs) -&gt; Path:\n        \"\"\"\n        Iterates through a directory tree and finds the path to the\n        Ripple Detector plugin data and returns its location\n        \"\"\"\n        exp_name = kwargs.pop(\"experiment\", \"experiment1\")\n        rec_name = kwargs.pop(\"recording\", \"recording1\")\n        ripple_match = (\n            trial_root\n            / Path(\"Record Node [0-9][0-9][0-9]\")\n            / Path(exp_name)\n            / Path(rec_name)\n            / Path(\"events\")\n            / Path(\"Ripple_Detector-[0-9][0-9][0-9].*\")\n            / Path(\"TTL\")\n        )\n        for d, c, f in os.walk(trial_root):\n            for ff in f:\n                if \".\" not in c:  # ignore hidden directories\n                    if \"timestamps.npy\" in ff:\n                        if PurePath(d).match(str(ripple_match)):\n                            return Path(d)\n        return Path()\n\n    @savePlot\n    def plot_filtered_lfp_chunk(\n        self, start_time: float, end_time: float, **kwargs\n    ) -&gt; plt.Axes:\n        idx = np.logical_and(\n            self.eeg_time &gt; start_time - self.pre_ttl,\n            self.eeg_time &lt; end_time + self.post_ttl,\n        )\n\n        eeg_chunk = self.filtered_eeg[idx]\n\n        normed_time = np.linspace(\n            -int(self.pre_ttl * 1000), int(self.post_ttl * 1000), len(eeg_chunk)\n        )  # in ms\n        _, ax1 = plt.subplots(figsize=(6.0, 4.0))  # enlarge plot a bit\n        ax1.plot(normed_time, eeg_chunk)\n\n        trans = transforms.blended_transform_factory(ax1.transData, ax1.transAxes)\n        ax1.vlines(\n            [0, int(self.post_ttl * 1000)],\n            ymin=0,\n            ymax=1,\n            colors=\"r\",\n            linestyles=\"--\",\n            transform=trans,\n        )\n        ax1.set_xlabel(\"Time to TTL(ms)\")\n        return ax1\n\n    def plot_rasters(self, laser_on: bool):\n        F = FigureMaker()\n        self.path2APdata = self._find_path_to_continuous(self.pname_for_trial)\n        K = KiloSortSession(self.path2APdata)\n        F.ttl_data = {}\n        if laser_on:\n            F.ttl_data[\"ttl_timestamps\"] = self.laser_on_ts\n            ttls = np.array([self.laser_on_ts, self.laser_off_ts]).T\n            F.ttl_data[\"stim_duration\"] = (\n                np.max(np.diff(ttls)) * 1000\n            )  # needs to be in ms\n        else:\n            F.ttl_data[\"ttl_timestamps\"] = self.no_laser_on_ts\n            F.ttl_data[\"stim_duration\"] = self.ttl_duration\n        K.load()\n        K.removeNoiseClusters()\n        K.removeKSNoiseClusters()\n        for c in K.good_clusters:\n            ts = K.get_cluster_spike_times(c) / 3e4\n            F._getRasterPlot(spk_times=ts, cluster=c)\n            plt.show()\n\n    @savePlot\n    def plot_and_save_ripple_band_lfp_with_ttl(self):\n        for i_time in self.laser_on_ts:\n            eeg_chunk = self.filtered_eeg[\n                np.logical_and(\n                    self.eeg_time &gt; i_time - self.pre_ttl,\n                    self.eeg_time &lt; i_time + self.post_ttl,\n                )\n            ]\n            eeg_chunk_time = self.eeg_time[\n                np.logical_and(\n                    self.eeg_time &gt; i_time - self.pre_ttl,\n                    self.eeg_time &lt; i_time + self.post_ttl,\n                )\n            ]\n            fig = plt.figure()\n            ax = fig.add_subplot(111)\n            axTrans = transforms.blended_transform_factory(ax.transData, ax.transData)\n            ax.plot(eeg_chunk_time, eeg_chunk)\n            ax.add_patch(\n                Rectangle(\n                    (i_time, -self.lfp_plotting_scale),\n                    width=0.1,\n                    height=1000,\n                    transform=axTrans,\n                    color=[0, 0, 1],\n                    alpha=0.3,\n                )\n            )\n\n            ax.set_ylim(-self.lfp_plotting_scale, self.lfp_plotting_scale)\n            plt.show()\n            return fig\n\n    @savePlot\n    def plot_mean_spectrograms(self, **kwargs) -&gt; plt.Figure:\n        fig = plt.figure(figsize=(12.0, 4.0))\n        ax, ax1 = fig.subplots(1, 2)\n        fig, im, spec = self.plot_mean_spectrogram(laser_on=False, ax=ax, **kwargs)\n        fig, im1, spec1 = self.plot_mean_spectrogram(laser_on=True, ax=ax1, **kwargs)\n        self.laser_off_spectrogram = spec\n        self.laser_on_spectrogram = spec1\n        spec = np.mean(spec, 0)\n        spec1 = np.mean(spec1, 0)\n        min_im = np.min([np.min(spec), np.min(spec1)])\n        max_im = np.max([np.max(spec), np.max(spec1)])\n        im.set_clim((min_im, max_im))\n        im1.set_clim((min_im, max_im))\n        ax1.set_ylabel(\"\")\n        n_no_laser_ttls = len(self.no_laser_on_ts)\n        n_laser_ttls = len(self.laser_on_ts)\n        ax.set_title(f\"Laser off ({n_no_laser_ttls} events)\")\n        ax1.set_title(f\"Laser on ({n_laser_ttls} events)\")\n        cb_ax = fig.add_axes([0.91, 0.124, 0.01, 0.754])\n        fig.colorbar(\n            im1,\n            label=\"Power Spectral Density \" + r\"$20\\,\\log_{10}|S_x(t, f)|$ in dB\",\n            cax=cb_ax,\n        )\n\n        plt.show()\n        return fig\n\n    def plot_mean_spectrogram(self, laser_on: bool = False, ax=None, **kwargs):\n        \"\"\"\n        Plots the mean spectrogram for either 'long' or 'short' ttl events\n        \"\"\"\n        norm = kwargs.pop(\"norm\", None)\n        ttls = np.array([self.laser_on_ts, self.laser_off_ts]).T\n        # max_duration used in plotting output below\n        ttl_duration = np.mean(np.diff(ttls))\n\n        if not laser_on:\n            ttls = np.array(\n                [self.no_laser_on_ts, self.no_laser_on_ts + (self.ttl_duration)]\n            ).T\n        # breakpoint()\n        spectrograms = []\n        rows = []\n        cols = []\n        for ttl in ttls:\n            (\n                SFT,\n                N,\n                spec,\n            ) = self.get_spectrogram(ttl[0], ttl[1])\n            r, c = np.shape(spec)\n            rows.append(r)\n            cols.append(c)\n            spectrograms.append(spec)\n\n        # some spectrograms might be slightly different shapes so\n        # truncate to the shortest length in each dimension\n        min_rows = np.min(rows)\n        min_cols = np.min(cols)\n        spec_array = np.empty(shape=[len(ttls), min_rows, min_cols])\n        for i, s in enumerate(spectrograms):\n            spec_array[i, :, :] = s[0:min_rows, 0:min_cols]\n\n        if ax is None:\n            fig1, ax1 = plt.subplots(figsize=(6.0, 4.0))  # enlarge plot a bit\n        else:\n            ax1 = ax\n            fig1 = plt.gcf()\n        t_lo, t_hi = SFT.extent(N)[:2]  # time range of plot\n        breakpoint()\n        ax1.set(\n            xlabel=f\"Time $t$ in seconds ({SFT.p_num(N)} slices, \"\n            + rf\"$\\Delta t = {SFT.delta_t:g}\\,$s)\",\n            ylabel=f\"Freq. $f$ in Hz ({SFT.f_pts} bins, \"\n            + rf\"$\\Delta f = {SFT.delta_f:g}\\,$Hz)\",\n            xlim=(t_lo, t_hi),\n        )\n        trans = transforms.blended_transform_factory(ax1.transData, ax1.transAxes)\n        ax1.vlines(\n            [\n                0,\n                self.pre_ttl + ttl_duration,\n            ],\n            ymin=0,\n            ymax=1,\n            colors=\"r\",\n            linestyles=\"--\",\n            transform=trans,\n        )\n        # add an annotation for the ttl duration next in between\n        # the vertical red dashed lines\n        ttl_duration_ms = ttl_duration  # * 1000\n        ax1.annotate(\n            f\"{ttl_duration_ms:.2f}\\n ms\",\n            xy=(self.pre_ttl + ttl_duration / 2, 0.8),\n            xytext=(self.pre_ttl + ttl_duration / 2, 0.8),\n            xycoords=trans,\n            textcoords=trans,\n            ha=\"center\",\n            va=\"bottom\",\n            color=\"r\",\n            fontsize=\"small\",\n        )\n        # imshow not respecting the image extents so use pcolormesh\n        # mean_spec_array = np.mean(spec_array, 0)\n        # X, Y = np.meshgrid(\n        #     np.linspace(SFT.extent(N)[0], SFT.extent(N)[1], mean_spec_array.shape[1]),\n        #     np.linspace(SFT.extent(N)[2], SFT.extent(N)[3], mean_spec_array.shape[0]),\n        # )\n        # breakpoint()\n        # im1 = ax1.pcolormesh(\n        #     X, Y, np.mean(spec_array, 0), cmap=\"magma\", norm=norm, edgecolors=\"face\"\n        # )\n        im1 = ax1.imshow(\n            np.mean(spec_array, 0),\n            origin=\"lower\",\n            aspect=\"auto\",\n            extent=SFT.extent(N),\n            cmap=\"magma\",\n            norm=norm,\n        )\n        return fig1, im1, spec_array\n\n    def get_spectrogram(self, start_time: float, end_time: float, plot=False):\n        eeg_chunk = self.filtered_eeg[\n            np.logical_and(\n                self.eeg_time &gt; start_time - self.pre_ttl,\n                self.eeg_time &lt; start_time + self.post_ttl,\n            )\n        ]\n        # breakpoint()\n\n        win = gaussian(self.gaussian_window, std=self.gaussian_std, sym=True)\n        SFT = ShortTimeFFT(win, hop=1, fs=self.fs, mfft=256, scale_to=\"psd\")\n        Sx2 = SFT.spectrogram(eeg_chunk)\n        N = len(eeg_chunk)\n\n        if plot:\n            fig1, ax1 = plt.subplots(figsize=(6.0, 4.0))  # enlarge plot a bit\n            t_lo, t_hi = SFT.extent(N)[:2]  # time range of plot\n            ax1.set_title(\n                rf\"Spectrogram ({SFT.m_num*SFT.T:g}$\\,s$ Gaussian \"\n                + rf\"window, $\\sigma_t={self.gaussian_std*SFT.T:g}\\,$s)\"\n            )\n            ax1.set(\n                xlabel=f\"Time $t$ in seconds ({SFT.p_num(N)} slices, \"\n                + rf\"$\\Delta t = {SFT.delta_t:g}\\,$s)\",\n                ylabel=f\"Freq. $f$ in Hz ({SFT.f_pts} bins, \"\n                + rf\"$\\Delta f = {SFT.delta_f:g}\\,$Hz)\",\n                xlim=(t_lo, t_hi),\n            )\n            trans = transforms.blended_transform_factory(ax1.transData, ax1.transAxes)\n            ax1.vlines(\n                [self.pre_ttl, self.pre_ttl + (start_time)],\n                ymin=0,\n                ymax=1,\n                colors=\"r\",\n                linestyles=\"--\",\n                transform=trans,\n            )\n\n            im1 = ax1.imshow(\n                np.abs(Sx2),\n                origin=\"lower\",\n                aspect=\"auto\",\n                extent=SFT.extent(N),\n                cmap=\"magma\",\n            )\n            fig1.colorbar(\n                im1,\n                label=\"Power Spectral Density \" + r\"$20\\,\\log_{10}|S_x(t, f)|$ in dB\",\n            )\n            plt.show()\n        return SFT, N, np.abs(Sx2)\n\n    @savePlot\n    def plot_mean_rippleband_power(self, **kwargs) -&gt; plt.Axes:\n        \"\"\"\n        Plots the mean power in the ripple band for the laser on and no laser\n        conditions\n        \"\"\"\n        if np.any(self.laser_on_spectrogram) and np.any(self.laser_off_spectrogram):\n            ax = kwargs.pop(\"ax\", None)\n            freqs = np.linspace(\n                0, int(self.fs / 2), int(self.laser_off_spectrogram.shape[1])\n            )\n            idx = np.logical_and(freqs &gt;= self.low_band, freqs &lt;= self.high_band)\n            mean_power_on = np.mean(self.laser_on_spectrogram[:, idx, :], axis=(0, 1))\n            mean_power_no = np.mean(self.laser_off_spectrogram[:, idx, :], axis=(0, 1))\n            mean_power_on_time = np.linspace(\n                0 - self.pre_ttl, self.post_ttl, len(mean_power_on)\n            )\n            mean_power_off_time = np.linspace(\n                0 - self.pre_ttl, self.post_ttl, len(mean_power_no)\n            )\n            if ax is None:\n                fig = plt.figure()\n                ax = fig.add_subplot(111)\n\n            plt.plot(\n                mean_power_on_time,\n                mean_power_on,\n                \"blue\",\n                label=\"on\",\n            )\n            plt.plot(\n                mean_power_off_time,\n                mean_power_no,\n                \"k\",\n                label=\"off\",\n            )\n            ax = plt.gca()\n            ax.set_xlabel(\"Time(s)\")\n            ax.set_ylabel(\"Power\")\n            ax.set_title(f\"Mean power between {self.low_band} - {self.high_band}Hz\")\n            axTrans = transforms.blended_transform_factory(ax.transData, ax.transAxes)\n            ax.add_patch(\n                Rectangle(\n                    (0, 0),\n                    width=0.1,\n                    height=1,\n                    transform=axTrans,\n                    color=[0, 0, 1],\n                    alpha=0.3,\n                    label=\"Laser on\",\n                )\n            )\n            plt.legend()\n            plt.show()\n            return ax\n\n    def _find_high_power_periods(self, n: int = 3, t: int = 10) -&gt; np.ndarray:\n        \"\"\"\n        Find periods where the power in the ripple band is above n standard deviations\n        for t samples. Meant to recapitulate the algorithm from the Ripple Detector\n        plugin\n\n        \"\"\"\n        # get some detection parameters from the Ripple Detector plugin\n        settings = Settings(self.pname_for_trial)\n        proc = settings.get_processor(\"Ripple\")\n        rms_window = float(getattr(proc, \"rms_samples\"))\n        ripple_detect_channel = int(getattr(proc, \"Ripple_Input\"))\n        ripple_std = int(getattr(proc, \"ripple_std\"))\n        time_thresh = int(getattr(proc, \"time_thresh\"))\n        rms_sig = window_rms(self.filtered_eeg, rms_window)\n\n    def filter_timestamps_for_real_ripples(self):\n        \"\"\"\n        Filter out low power and short duration events from the list of timestamps\n        \"\"\"\n        laser_on_keep_indices, laser_on_run_lens = (\n            self._calc_ripple_chunks_duration_power(\"laser\")\n        )\n        no_laser_keep_indices, no_laser_run_lens = (\n            self._calc_ripple_chunks_duration_power(\"no_laser\")\n        )\n        self.laser_on_run_lens = laser_on_run_lens\n        self.no_laser_run_lens = no_laser_run_lens\n        self.laser_on_ts = self.laser_on_ts[laser_on_keep_indices]\n        self.laser_off_ts = self.laser_off_ts[laser_on_keep_indices]\n        self.no_laser_on_ts = self.no_laser_on_ts[no_laser_keep_indices]\n\n    def _calc_ripple_chunks_duration_power(self, ttl_type=\"no_laser\") -&gt; tuple:\n        \"\"\"\n        Find the indices and durations of the events that have sufficient\n        duration and power to be considered ripples.\n\n        Parameters\n        ----------\n        ttl_type (str) - which bit of the trial to do the calculation for\n                         Either 'no_laser' or 'laser'\n\n        Returns\n        -------\n        tuple: the run indices to keep and the run durations in ms\n\n        \"\"\"\n        n_samples = int((self.post_ttl + self.pre_ttl) * 1000)\n        times = [0]\n        if ttl_type == \"no_laser\":\n            times = self.no_laser_on_ts\n        elif ttl_type == \"laser\":\n            times = self.laser_on_ts\n        else:\n            warnings.warn(\n                \"ttl_type not recognised. Must be one of 'laser' or 'no_laser'\"\n            )\n            return ([],)\n        eeg_chunks = np.zeros(shape=[len(times), n_samples])\n        rms_signal = window_rms(self.filtered_eeg, 12)\n\n        # Get segments of the root mean squared and smoothed LFP signal\n        for i, t in enumerate(times):\n            idx = np.logical_and(\n                self.eeg_time &gt; t - self.pre_ttl, self.eeg_time &lt; t + self.post_ttl\n            )\n            eeg_chunks[i, :] = rms_signal[idx]\n\n        # Square the whole filtered LFP signal and calculate the mean power\n        mean_power = np.mean(rms_signal)\n        std_dev_power = np.std(rms_signal)\n\n        # Find ripples that are ripple_std_dev standard deviations over the\n        # mean power to demarcate the start and end of the ripples and longer\n        # than ripple_min_duration_ms\n        indices_to_keep = []\n        run_lens = []\n        for idx, chunk in enumerate(eeg_chunks):\n            high_power = chunk &gt; mean_power + std_dev_power * self.ripple_std_dev\n            run_vals, _, run_lengths = find_runs(high_power)\n            if len(run_vals &gt; 1):\n                try:\n                    if run_vals[0] is True:\n                        run_length = run_lengths[0]\n                    else:  # second run_val must be True\n                        run_length = run_lengths[1]\n                    if run_length &gt; self.ripple_min_duration_ms:\n                        indices_to_keep.append(idx)\n                        run_lens.append(run_length)\n                except IndexError:\n                    pass\n        return indices_to_keep, run_lens\n</code></pre>"},{"location":"reference/#ephysiopy.common.rhythmicity.Rippler.__init__","title":"<code>__init__(trial_root, signal, fs)</code>","text":"<p>trial_root (Path) - location of the root recording directory, used to load ttls etc signal (np.ndarray) - the LFP signal (usually downsampled to about 500-1000Hz) fs (int) - the sampling rate of the signal</p> Source code in <code>ephysiopy/common/rhythmicity.py</code> <pre><code>def __init__(self, trial_root: Path, signal: np.ndarray, fs: int):\n    \"\"\"\n    trial_root (Path) - location of the root recording directory, used to load ttls etc\n    signal (np.ndarray) - the LFP signal (usually downsampled to about 500-1000Hz)\n    fs (int) - the sampling rate of the signal\n    \"\"\"\n\n    self.pname_for_trial = trial_root\n    self.orig_sig = signal\n    self.fs = fs\n    self.settings = Settings(str(trial_root))\n    LFP = EEGCalcsGeneric(signal, fs)\n    self.LFP = LFP\n    detector_settings = self.settings.get_processor(\"Ripple\")\n    self.ttl_duration = (\n        float(detector_settings.ttl_duration) / 1000\n    )  # in seconds now\n\n    pname_for_ttl_data = self._find_path_to_ripple_ttl(self.pname_for_trial)\n    sync_file = pname_for_ttl_data.parents[2] / Path(\"sync_messages.txt\")\n    recording_start_time = self._load_start_time(sync_file)\n    ttl_ts = np.load(pname_for_ttl_data / \"timestamps.npy\") - recording_start_time\n    ttl_states = np.load(pname_for_ttl_data / \"states.npy\")\n    all_ons = ttl_ts[ttl_states == int(detector_settings.Ripple_save)]\n    laser_ons = ttl_ts[ttl_states == int(detector_settings.Ripple_Out)]\n    laser_offs = ttl_ts[ttl_states == -int(detector_settings.Ripple_Out)]\n    no_laser_ons = np.lib.setdiff1d(all_ons, laser_ons)\n\n    self.all_on_ts = all_ons\n    self.ttl_states = ttl_states\n    self.all_ts = ttl_ts\n    self.laser_on_ts = laser_ons\n    self.laser_off_ts = laser_offs\n    self.no_laser_on_ts = no_laser_ons\n\n    filtered_eeg = LFP.butterFilter(self.low_band, self.high_band)\n    filtered_eeg *= self.bit_volts\n    self.filtered_eeg = filtered_eeg\n    self.eeg_time = np.linspace(\n        0,\n        LFP.sig.shape[0] / self.fs,\n        LFP.sig.shape[0],\n    )  # in seconds\n</code></pre>"},{"location":"reference/#ephysiopy.common.rhythmicity.Rippler._calc_ripple_chunks_duration_power","title":"<code>_calc_ripple_chunks_duration_power(ttl_type='no_laser')</code>","text":"<p>Find the indices and durations of the events that have sufficient duration and power to be considered ripples.</p> <p>Parameters:</p> Name Type Description Default <code>ttl_type</code> <pre><code>         Either 'no_laser' or 'laser'\n</code></pre> <code>'no_laser'</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>the run indices to keep and the run durations in ms</code> Source code in <code>ephysiopy/common/rhythmicity.py</code> <pre><code>def _calc_ripple_chunks_duration_power(self, ttl_type=\"no_laser\") -&gt; tuple:\n    \"\"\"\n    Find the indices and durations of the events that have sufficient\n    duration and power to be considered ripples.\n\n    Parameters\n    ----------\n    ttl_type (str) - which bit of the trial to do the calculation for\n                     Either 'no_laser' or 'laser'\n\n    Returns\n    -------\n    tuple: the run indices to keep and the run durations in ms\n\n    \"\"\"\n    n_samples = int((self.post_ttl + self.pre_ttl) * 1000)\n    times = [0]\n    if ttl_type == \"no_laser\":\n        times = self.no_laser_on_ts\n    elif ttl_type == \"laser\":\n        times = self.laser_on_ts\n    else:\n        warnings.warn(\n            \"ttl_type not recognised. Must be one of 'laser' or 'no_laser'\"\n        )\n        return ([],)\n    eeg_chunks = np.zeros(shape=[len(times), n_samples])\n    rms_signal = window_rms(self.filtered_eeg, 12)\n\n    # Get segments of the root mean squared and smoothed LFP signal\n    for i, t in enumerate(times):\n        idx = np.logical_and(\n            self.eeg_time &gt; t - self.pre_ttl, self.eeg_time &lt; t + self.post_ttl\n        )\n        eeg_chunks[i, :] = rms_signal[idx]\n\n    # Square the whole filtered LFP signal and calculate the mean power\n    mean_power = np.mean(rms_signal)\n    std_dev_power = np.std(rms_signal)\n\n    # Find ripples that are ripple_std_dev standard deviations over the\n    # mean power to demarcate the start and end of the ripples and longer\n    # than ripple_min_duration_ms\n    indices_to_keep = []\n    run_lens = []\n    for idx, chunk in enumerate(eeg_chunks):\n        high_power = chunk &gt; mean_power + std_dev_power * self.ripple_std_dev\n        run_vals, _, run_lengths = find_runs(high_power)\n        if len(run_vals &gt; 1):\n            try:\n                if run_vals[0] is True:\n                    run_length = run_lengths[0]\n                else:  # second run_val must be True\n                    run_length = run_lengths[1]\n                if run_length &gt; self.ripple_min_duration_ms:\n                    indices_to_keep.append(idx)\n                    run_lens.append(run_length)\n            except IndexError:\n                pass\n    return indices_to_keep, run_lens\n</code></pre>"},{"location":"reference/#ephysiopy.common.rhythmicity.Rippler._find_high_power_periods","title":"<code>_find_high_power_periods(n=3, t=10)</code>","text":"<p>Find periods where the power in the ripple band is above n standard deviations for t samples. Meant to recapitulate the algorithm from the Ripple Detector plugin</p> Source code in <code>ephysiopy/common/rhythmicity.py</code> <pre><code>def _find_high_power_periods(self, n: int = 3, t: int = 10) -&gt; np.ndarray:\n    \"\"\"\n    Find periods where the power in the ripple band is above n standard deviations\n    for t samples. Meant to recapitulate the algorithm from the Ripple Detector\n    plugin\n\n    \"\"\"\n    # get some detection parameters from the Ripple Detector plugin\n    settings = Settings(self.pname_for_trial)\n    proc = settings.get_processor(\"Ripple\")\n    rms_window = float(getattr(proc, \"rms_samples\"))\n    ripple_detect_channel = int(getattr(proc, \"Ripple_Input\"))\n    ripple_std = int(getattr(proc, \"ripple_std\"))\n    time_thresh = int(getattr(proc, \"time_thresh\"))\n    rms_sig = window_rms(self.filtered_eeg, rms_window)\n</code></pre>"},{"location":"reference/#ephysiopy.common.rhythmicity.Rippler._find_path_to_continuous","title":"<code>_find_path_to_continuous(trial_root, **kwargs)</code>","text":"<p>Iterates through a directory tree and finds the path to the Ripple Detector plugin data and returns its location</p> Source code in <code>ephysiopy/common/rhythmicity.py</code> <pre><code>def _find_path_to_continuous(self, trial_root: Path, **kwargs) -&gt; Path:\n    \"\"\"\n    Iterates through a directory tree and finds the path to the\n    Ripple Detector plugin data and returns its location\n    \"\"\"\n    exp_name = kwargs.pop(\"experiment\", \"experiment1\")\n    rec_name = kwargs.pop(\"recording\", \"recording1\")\n    folder_match = (\n        trial_root\n        / Path(\"Record Node [0-9][0-9][0-9]\")\n        / Path(exp_name)\n        / Path(rec_name)\n        / Path(\"events\")\n        / Path(\"Acquisition_Board-[0-9][0-9][0-9].*\")\n    )\n    for d, c, f in os.walk(trial_root):\n        for ff in f:\n            if \".\" not in c:  # ignore hidden directories\n                if \"continuous.dat\" in ff:\n                    if PurePath(d).match(str(folder_match)):\n                        return Path(d)\n    return Path()\n</code></pre>"},{"location":"reference/#ephysiopy.common.rhythmicity.Rippler._find_path_to_ripple_ttl","title":"<code>_find_path_to_ripple_ttl(trial_root, **kwargs)</code>","text":"<p>Iterates through a directory tree and finds the path to the Ripple Detector plugin data and returns its location</p> Source code in <code>ephysiopy/common/rhythmicity.py</code> <pre><code>def _find_path_to_ripple_ttl(self, trial_root: Path, **kwargs) -&gt; Path:\n    \"\"\"\n    Iterates through a directory tree and finds the path to the\n    Ripple Detector plugin data and returns its location\n    \"\"\"\n    exp_name = kwargs.pop(\"experiment\", \"experiment1\")\n    rec_name = kwargs.pop(\"recording\", \"recording1\")\n    ripple_match = (\n        trial_root\n        / Path(\"Record Node [0-9][0-9][0-9]\")\n        / Path(exp_name)\n        / Path(rec_name)\n        / Path(\"events\")\n        / Path(\"Ripple_Detector-[0-9][0-9][0-9].*\")\n        / Path(\"TTL\")\n    )\n    for d, c, f in os.walk(trial_root):\n        for ff in f:\n            if \".\" not in c:  # ignore hidden directories\n                if \"timestamps.npy\" in ff:\n                    if PurePath(d).match(str(ripple_match)):\n                        return Path(d)\n    return Path()\n</code></pre>"},{"location":"reference/#ephysiopy.common.rhythmicity.Rippler._load_start_time","title":"<code>_load_start_time(path_to_sync_message_file)</code>","text":"<p>Returns the start time contained in a sync file from OE</p> Source code in <code>ephysiopy/common/rhythmicity.py</code> <pre><code>def _load_start_time(self, path_to_sync_message_file: Path):\n    \"\"\"\n    Returns the start time contained in a sync file from OE\n    \"\"\"\n    recording_start_time = 0\n    with open(path_to_sync_message_file, \"r\") as f:\n        sync_strs = f.read()\n        sync_lines = sync_strs.split(\"\\n\")\n        for line in sync_lines:\n            if \"Start Time\" in line:\n                tokens = line.split(\":\")\n                start_time = int(tokens[-1])\n                sample_rate = int(tokens[0].split(\"@\")[-1].strip().split()[0])\n                recording_start_time = start_time / float(sample_rate)\n    return recording_start_time\n</code></pre>"},{"location":"reference/#ephysiopy.common.rhythmicity.Rippler.filter_timestamps_for_real_ripples","title":"<code>filter_timestamps_for_real_ripples()</code>","text":"<p>Filter out low power and short duration events from the list of timestamps</p> Source code in <code>ephysiopy/common/rhythmicity.py</code> <pre><code>def filter_timestamps_for_real_ripples(self):\n    \"\"\"\n    Filter out low power and short duration events from the list of timestamps\n    \"\"\"\n    laser_on_keep_indices, laser_on_run_lens = (\n        self._calc_ripple_chunks_duration_power(\"laser\")\n    )\n    no_laser_keep_indices, no_laser_run_lens = (\n        self._calc_ripple_chunks_duration_power(\"no_laser\")\n    )\n    self.laser_on_run_lens = laser_on_run_lens\n    self.no_laser_run_lens = no_laser_run_lens\n    self.laser_on_ts = self.laser_on_ts[laser_on_keep_indices]\n    self.laser_off_ts = self.laser_off_ts[laser_on_keep_indices]\n    self.no_laser_on_ts = self.no_laser_on_ts[no_laser_keep_indices]\n</code></pre>"},{"location":"reference/#ephysiopy.common.rhythmicity.Rippler.plot_mean_rippleband_power","title":"<code>plot_mean_rippleband_power(**kwargs)</code>","text":"<p>Plots the mean power in the ripple band for the laser on and no laser conditions</p> Source code in <code>ephysiopy/common/rhythmicity.py</code> <pre><code>@savePlot\ndef plot_mean_rippleband_power(self, **kwargs) -&gt; plt.Axes:\n    \"\"\"\n    Plots the mean power in the ripple band for the laser on and no laser\n    conditions\n    \"\"\"\n    if np.any(self.laser_on_spectrogram) and np.any(self.laser_off_spectrogram):\n        ax = kwargs.pop(\"ax\", None)\n        freqs = np.linspace(\n            0, int(self.fs / 2), int(self.laser_off_spectrogram.shape[1])\n        )\n        idx = np.logical_and(freqs &gt;= self.low_band, freqs &lt;= self.high_band)\n        mean_power_on = np.mean(self.laser_on_spectrogram[:, idx, :], axis=(0, 1))\n        mean_power_no = np.mean(self.laser_off_spectrogram[:, idx, :], axis=(0, 1))\n        mean_power_on_time = np.linspace(\n            0 - self.pre_ttl, self.post_ttl, len(mean_power_on)\n        )\n        mean_power_off_time = np.linspace(\n            0 - self.pre_ttl, self.post_ttl, len(mean_power_no)\n        )\n        if ax is None:\n            fig = plt.figure()\n            ax = fig.add_subplot(111)\n\n        plt.plot(\n            mean_power_on_time,\n            mean_power_on,\n            \"blue\",\n            label=\"on\",\n        )\n        plt.plot(\n            mean_power_off_time,\n            mean_power_no,\n            \"k\",\n            label=\"off\",\n        )\n        ax = plt.gca()\n        ax.set_xlabel(\"Time(s)\")\n        ax.set_ylabel(\"Power\")\n        ax.set_title(f\"Mean power between {self.low_band} - {self.high_band}Hz\")\n        axTrans = transforms.blended_transform_factory(ax.transData, ax.transAxes)\n        ax.add_patch(\n            Rectangle(\n                (0, 0),\n                width=0.1,\n                height=1,\n                transform=axTrans,\n                color=[0, 0, 1],\n                alpha=0.3,\n                label=\"Laser on\",\n            )\n        )\n        plt.legend()\n        plt.show()\n        return ax\n</code></pre>"},{"location":"reference/#ephysiopy.common.rhythmicity.Rippler.plot_mean_spectrogram","title":"<code>plot_mean_spectrogram(laser_on=False, ax=None, **kwargs)</code>","text":"<p>Plots the mean spectrogram for either 'long' or 'short' ttl events</p> Source code in <code>ephysiopy/common/rhythmicity.py</code> <pre><code>def plot_mean_spectrogram(self, laser_on: bool = False, ax=None, **kwargs):\n    \"\"\"\n    Plots the mean spectrogram for either 'long' or 'short' ttl events\n    \"\"\"\n    norm = kwargs.pop(\"norm\", None)\n    ttls = np.array([self.laser_on_ts, self.laser_off_ts]).T\n    # max_duration used in plotting output below\n    ttl_duration = np.mean(np.diff(ttls))\n\n    if not laser_on:\n        ttls = np.array(\n            [self.no_laser_on_ts, self.no_laser_on_ts + (self.ttl_duration)]\n        ).T\n    # breakpoint()\n    spectrograms = []\n    rows = []\n    cols = []\n    for ttl in ttls:\n        (\n            SFT,\n            N,\n            spec,\n        ) = self.get_spectrogram(ttl[0], ttl[1])\n        r, c = np.shape(spec)\n        rows.append(r)\n        cols.append(c)\n        spectrograms.append(spec)\n\n    # some spectrograms might be slightly different shapes so\n    # truncate to the shortest length in each dimension\n    min_rows = np.min(rows)\n    min_cols = np.min(cols)\n    spec_array = np.empty(shape=[len(ttls), min_rows, min_cols])\n    for i, s in enumerate(spectrograms):\n        spec_array[i, :, :] = s[0:min_rows, 0:min_cols]\n\n    if ax is None:\n        fig1, ax1 = plt.subplots(figsize=(6.0, 4.0))  # enlarge plot a bit\n    else:\n        ax1 = ax\n        fig1 = plt.gcf()\n    t_lo, t_hi = SFT.extent(N)[:2]  # time range of plot\n    breakpoint()\n    ax1.set(\n        xlabel=f\"Time $t$ in seconds ({SFT.p_num(N)} slices, \"\n        + rf\"$\\Delta t = {SFT.delta_t:g}\\,$s)\",\n        ylabel=f\"Freq. $f$ in Hz ({SFT.f_pts} bins, \"\n        + rf\"$\\Delta f = {SFT.delta_f:g}\\,$Hz)\",\n        xlim=(t_lo, t_hi),\n    )\n    trans = transforms.blended_transform_factory(ax1.transData, ax1.transAxes)\n    ax1.vlines(\n        [\n            0,\n            self.pre_ttl + ttl_duration,\n        ],\n        ymin=0,\n        ymax=1,\n        colors=\"r\",\n        linestyles=\"--\",\n        transform=trans,\n    )\n    # add an annotation for the ttl duration next in between\n    # the vertical red dashed lines\n    ttl_duration_ms = ttl_duration  # * 1000\n    ax1.annotate(\n        f\"{ttl_duration_ms:.2f}\\n ms\",\n        xy=(self.pre_ttl + ttl_duration / 2, 0.8),\n        xytext=(self.pre_ttl + ttl_duration / 2, 0.8),\n        xycoords=trans,\n        textcoords=trans,\n        ha=\"center\",\n        va=\"bottom\",\n        color=\"r\",\n        fontsize=\"small\",\n    )\n    # imshow not respecting the image extents so use pcolormesh\n    # mean_spec_array = np.mean(spec_array, 0)\n    # X, Y = np.meshgrid(\n    #     np.linspace(SFT.extent(N)[0], SFT.extent(N)[1], mean_spec_array.shape[1]),\n    #     np.linspace(SFT.extent(N)[2], SFT.extent(N)[3], mean_spec_array.shape[0]),\n    # )\n    # breakpoint()\n    # im1 = ax1.pcolormesh(\n    #     X, Y, np.mean(spec_array, 0), cmap=\"magma\", norm=norm, edgecolors=\"face\"\n    # )\n    im1 = ax1.imshow(\n        np.mean(spec_array, 0),\n        origin=\"lower\",\n        aspect=\"auto\",\n        extent=SFT.extent(N),\n        cmap=\"magma\",\n        norm=norm,\n    )\n    return fig1, im1, spec_array\n</code></pre>"},{"location":"reference/#spike-calculations","title":"Spike calculations","text":""},{"location":"reference/#ephysiopy.common.spikecalcs.SpikeCalcsAxona","title":"<code>SpikeCalcsAxona</code>","text":"<p>               Bases: <code>SpikeCalcsGeneric</code></p> <p>Replaces SpikeCalcs from ephysiopy.axona.spikecalcs</p> Source code in <code>ephysiopy/common/spikecalcs.py</code> <pre><code>class SpikeCalcsAxona(SpikeCalcsGeneric):\n    \"\"\"\n    Replaces SpikeCalcs from ephysiopy.axona.spikecalcs\n    \"\"\"\n\n    def half_amp_dur(self, waveforms):\n        \"\"\"\n        Calculates the half amplitude duration of a spike.\n\n        Args:\n            A (ndarray): An nSpikes x nElectrodes x nSamples array.\n\n        Returns:\n            had (float): The half-amplitude duration for the channel\n                (electrode) that has the strongest (highest amplitude)\n                signal. Units are ms.\n        \"\"\"\n        from scipy import optimize\n\n        best_chan = np.argmax(np.max(np.mean(waveforms, 0), 1))\n        mn_wvs = np.mean(waveforms, 0)\n        wvs = mn_wvs[best_chan, :]\n        half_amp = np.max(wvs) / 2\n        half_amp = np.zeros_like(wvs) + half_amp\n        t = np.linspace(0, 1 / 1000.0, 50)\n        # create functions from the data using PiecewisePolynomial\n        from scipy.interpolate import BPoly\n\n        p1 = BPoly.from_derivatives(t, wvs[:, np.newaxis])\n        p2 = BPoly.from_derivatives(t, half_amp[:, np.newaxis])\n        xs = np.r_[t, t]\n        xs.sort()\n        x_min = xs.min()\n        x_max = xs.max()\n        x_mid = xs[:-1] + np.diff(xs) / 2\n        roots = set()\n        for val in x_mid:\n            root, infodict, ier, mesg = optimize.fsolve(\n                lambda x: p1(x) - p2(x), val, full_output=True\n            )\n            if ier == 1 and x_min &lt; root &lt; x_max:\n                roots.add(root[0])\n        roots = list(roots)\n        if len(roots) &gt; 1:\n            r = np.abs(np.diff(roots[0:2]))[0]\n        else:\n            r = np.nan\n        return r\n\n    def p2t_time(self, waveforms):\n        \"\"\"\n        The peak to trough time of a spike in ms\n\n        Args:\n            cluster (int): The cluster whose waveforms are to be analysed\n\n        Returns:\n            p2t (float): The mean peak-to-trough time for the channel\n                (electrode) that has the strongest (highest amplitude) signal.\n                Units are ms.\n        \"\"\"\n        best_chan = np.argmax(np.max(np.mean(waveforms, 0), 1))\n        tP = get_param(waveforms, param=\"tP\")\n        tT = get_param(waveforms, param=\"tT\")\n        mn_tP = np.mean(tP, 0)\n        mn_tT = np.mean(tT, 0)\n        p2t = np.abs(mn_tP[best_chan] - mn_tT[best_chan])\n        return p2t * 1000\n\n    def plotClusterSpace(\n        self,\n        waveforms,\n        param=\"Amp\",\n        clusts: None | int | list = None,\n        cluster_vec: None | np.ndarray | list = None,\n        **kwargs,\n    ):\n        \"\"\"\n        Assumes the waveform data is signed 8-bit ints\n\n        NB THe above assumption is mostly broken as waveforms by default are now\n        in volts so you need to construct the trial object (AxonaTrial, OpenEphysBase\n        etc) with volts=False (works for Axona, less sure about OE)\n        TODO: aspect of plot boxes in ImageGrid not right as scaled by range of\n        values now\n\n        Parameters\n        ----------\n        waveforms (np.ndarray) - the array of waveform data. For Axona recordings this\n                                 is nSpikes x nChannels x nSamplesPerWaveform\n        param (str) - the parameter to plot. See get_param at the top of this file\n                      for valid args\n        clusts (optional - int or list) - which clusters to colour in\n        cluster_vec (optional - np.ndarray or list) - the cluster identity of each spike in waveforms\n                                           Must be nSpikes long\n        \"\"\"\n        if cluster_vec is not None:\n            assert np.shape(waveforms)[0] == len(cluster_vec)\n\n        from itertools import combinations\n\n        from mpl_toolkits.axes_grid1 import ImageGrid\n        from matplotlib.collections import RegularPolyCollection\n        from ephysiopy.axona.tintcolours import colours as tcols\n        from numpy.version import version as np_vers\n\n        try:\n            from numpy.lib.arraysetops import isin\n        except:\n            from numpy import isin as isin\n\n        c_vec = np.zeros(shape=(np.shape(waveforms)[0]))\n        if clusts is not None:\n            idx = isin(cluster_vec, clusts)\n            c_vec[idx] = cluster_vec[idx]\n        c_vec = [[np.floor(t * 255) for t in tcols[i]] for i in c_vec.astype(int)]\n\n        self.scaling = np.full(4, 15)\n\n        amps = get_param(waveforms, param=param)\n        cmb = combinations(range(4), 2)\n        if \"fig\" in kwargs:\n            fig = kwargs[\"fig\"]\n        else:\n            fig = plt.figure(figsize=(8, 6))\n        grid = ImageGrid(fig, 111, nrows_ncols=(2, 3), axes_pad=0.1, aspect=False)\n        for i, c in enumerate(cmb):\n            if np.sum(amps[:, c[0]]) &gt; 0 and np.sum(amps[:, c[1]]) &gt; 0:\n                xy = np.array([amps[:, c[0]], amps[:, c[1]]]).T\n                rects = RegularPolyCollection(\n                    numsides=4,\n                    rotation=0,\n                    facecolors=c_vec,\n                    edgecolors=c_vec,\n                    offsets=xy,\n                    offset_transform=grid[i].transData,\n                )\n                grid[i].add_collection(rects)\n            s = str(c[0] + 1) + \" v \" + str(c[1] + 1)\n            grid[i].text(\n                0.05,\n                0.95,\n                s,\n                va=\"top\",\n                ha=\"left\",\n                size=\"small\",\n                color=\"k\",\n                transform=grid[i].transAxes,\n            )\n            grid[i].set_xlim(0, 256)\n            grid[i].set_ylim(0, 256)\n        plt.setp([a.get_xticklabels() for a in grid], visible=False)\n        plt.setp([a.get_yticklabels() for a in grid], visible=False)\n        return fig\n</code></pre>"},{"location":"reference/#ephysiopy.common.spikecalcs.SpikeCalcsAxona.half_amp_dur","title":"<code>half_amp_dur(waveforms)</code>","text":"<p>Calculates the half amplitude duration of a spike.</p> <p>Args:     A (ndarray): An nSpikes x nElectrodes x nSamples array.</p> <p>Returns:     had (float): The half-amplitude duration for the channel         (electrode) that has the strongest (highest amplitude)         signal. Units are ms.</p> Source code in <code>ephysiopy/common/spikecalcs.py</code> <pre><code>def half_amp_dur(self, waveforms):\n    \"\"\"\n    Calculates the half amplitude duration of a spike.\n\n    Args:\n        A (ndarray): An nSpikes x nElectrodes x nSamples array.\n\n    Returns:\n        had (float): The half-amplitude duration for the channel\n            (electrode) that has the strongest (highest amplitude)\n            signal. Units are ms.\n    \"\"\"\n    from scipy import optimize\n\n    best_chan = np.argmax(np.max(np.mean(waveforms, 0), 1))\n    mn_wvs = np.mean(waveforms, 0)\n    wvs = mn_wvs[best_chan, :]\n    half_amp = np.max(wvs) / 2\n    half_amp = np.zeros_like(wvs) + half_amp\n    t = np.linspace(0, 1 / 1000.0, 50)\n    # create functions from the data using PiecewisePolynomial\n    from scipy.interpolate import BPoly\n\n    p1 = BPoly.from_derivatives(t, wvs[:, np.newaxis])\n    p2 = BPoly.from_derivatives(t, half_amp[:, np.newaxis])\n    xs = np.r_[t, t]\n    xs.sort()\n    x_min = xs.min()\n    x_max = xs.max()\n    x_mid = xs[:-1] + np.diff(xs) / 2\n    roots = set()\n    for val in x_mid:\n        root, infodict, ier, mesg = optimize.fsolve(\n            lambda x: p1(x) - p2(x), val, full_output=True\n        )\n        if ier == 1 and x_min &lt; root &lt; x_max:\n            roots.add(root[0])\n    roots = list(roots)\n    if len(roots) &gt; 1:\n        r = np.abs(np.diff(roots[0:2]))[0]\n    else:\n        r = np.nan\n    return r\n</code></pre>"},{"location":"reference/#ephysiopy.common.spikecalcs.SpikeCalcsAxona.p2t_time","title":"<code>p2t_time(waveforms)</code>","text":"<p>The peak to trough time of a spike in ms</p> <p>Args:     cluster (int): The cluster whose waveforms are to be analysed</p> <p>Returns:     p2t (float): The mean peak-to-trough time for the channel         (electrode) that has the strongest (highest amplitude) signal.         Units are ms.</p> Source code in <code>ephysiopy/common/spikecalcs.py</code> <pre><code>def p2t_time(self, waveforms):\n    \"\"\"\n    The peak to trough time of a spike in ms\n\n    Args:\n        cluster (int): The cluster whose waveforms are to be analysed\n\n    Returns:\n        p2t (float): The mean peak-to-trough time for the channel\n            (electrode) that has the strongest (highest amplitude) signal.\n            Units are ms.\n    \"\"\"\n    best_chan = np.argmax(np.max(np.mean(waveforms, 0), 1))\n    tP = get_param(waveforms, param=\"tP\")\n    tT = get_param(waveforms, param=\"tT\")\n    mn_tP = np.mean(tP, 0)\n    mn_tT = np.mean(tT, 0)\n    p2t = np.abs(mn_tP[best_chan] - mn_tT[best_chan])\n    return p2t * 1000\n</code></pre>"},{"location":"reference/#ephysiopy.common.spikecalcs.SpikeCalcsAxona.plotClusterSpace","title":"<code>plotClusterSpace(waveforms, param='Amp', clusts=None, cluster_vec=None, **kwargs)</code>","text":"<p>Assumes the waveform data is signed 8-bit ints</p> <p>NB THe above assumption is mostly broken as waveforms by default are now in volts so you need to construct the trial object (AxonaTrial, OpenEphysBase etc) with volts=False (works for Axona, less sure about OE) TODO: aspect of plot boxes in ImageGrid not right as scaled by range of values now</p> <p>Parameters:</p> Name Type Description Default <code>waveforms</code> <pre><code>                 is nSpikes x nChannels x nSamplesPerWaveform\n</code></pre> required <code>param</code> <pre><code>      for valid args\n</code></pre> <code>'Amp'</code> <code>clusts</code> <code>None | int | list</code> <code>None</code> <code>cluster_vec</code> <code>None | ndarray | list</code> <pre><code>                           Must be nSpikes long\n</code></pre> <code>None</code> Source code in <code>ephysiopy/common/spikecalcs.py</code> <pre><code>def plotClusterSpace(\n    self,\n    waveforms,\n    param=\"Amp\",\n    clusts: None | int | list = None,\n    cluster_vec: None | np.ndarray | list = None,\n    **kwargs,\n):\n    \"\"\"\n    Assumes the waveform data is signed 8-bit ints\n\n    NB THe above assumption is mostly broken as waveforms by default are now\n    in volts so you need to construct the trial object (AxonaTrial, OpenEphysBase\n    etc) with volts=False (works for Axona, less sure about OE)\n    TODO: aspect of plot boxes in ImageGrid not right as scaled by range of\n    values now\n\n    Parameters\n    ----------\n    waveforms (np.ndarray) - the array of waveform data. For Axona recordings this\n                             is nSpikes x nChannels x nSamplesPerWaveform\n    param (str) - the parameter to plot. See get_param at the top of this file\n                  for valid args\n    clusts (optional - int or list) - which clusters to colour in\n    cluster_vec (optional - np.ndarray or list) - the cluster identity of each spike in waveforms\n                                       Must be nSpikes long\n    \"\"\"\n    if cluster_vec is not None:\n        assert np.shape(waveforms)[0] == len(cluster_vec)\n\n    from itertools import combinations\n\n    from mpl_toolkits.axes_grid1 import ImageGrid\n    from matplotlib.collections import RegularPolyCollection\n    from ephysiopy.axona.tintcolours import colours as tcols\n    from numpy.version import version as np_vers\n\n    try:\n        from numpy.lib.arraysetops import isin\n    except:\n        from numpy import isin as isin\n\n    c_vec = np.zeros(shape=(np.shape(waveforms)[0]))\n    if clusts is not None:\n        idx = isin(cluster_vec, clusts)\n        c_vec[idx] = cluster_vec[idx]\n    c_vec = [[np.floor(t * 255) for t in tcols[i]] for i in c_vec.astype(int)]\n\n    self.scaling = np.full(4, 15)\n\n    amps = get_param(waveforms, param=param)\n    cmb = combinations(range(4), 2)\n    if \"fig\" in kwargs:\n        fig = kwargs[\"fig\"]\n    else:\n        fig = plt.figure(figsize=(8, 6))\n    grid = ImageGrid(fig, 111, nrows_ncols=(2, 3), axes_pad=0.1, aspect=False)\n    for i, c in enumerate(cmb):\n        if np.sum(amps[:, c[0]]) &gt; 0 and np.sum(amps[:, c[1]]) &gt; 0:\n            xy = np.array([amps[:, c[0]], amps[:, c[1]]]).T\n            rects = RegularPolyCollection(\n                numsides=4,\n                rotation=0,\n                facecolors=c_vec,\n                edgecolors=c_vec,\n                offsets=xy,\n                offset_transform=grid[i].transData,\n            )\n            grid[i].add_collection(rects)\n        s = str(c[0] + 1) + \" v \" + str(c[1] + 1)\n        grid[i].text(\n            0.05,\n            0.95,\n            s,\n            va=\"top\",\n            ha=\"left\",\n            size=\"small\",\n            color=\"k\",\n            transform=grid[i].transAxes,\n        )\n        grid[i].set_xlim(0, 256)\n        grid[i].set_ylim(0, 256)\n    plt.setp([a.get_xticklabels() for a in grid], visible=False)\n    plt.setp([a.get_yticklabels() for a in grid], visible=False)\n    return fig\n</code></pre>"},{"location":"reference/#ephysiopy.common.spikecalcs.SpikeCalcsGeneric","title":"<code>SpikeCalcsGeneric</code>","text":"<p>               Bases: <code>object</code></p> <p>Deals with the processing and analysis of spike data. There should be one instance of this class per cluster in the recording session. NB this differs from previous versions of this class where there was one instance per recording session and clusters were selected by passing in the cluster id to the methods.</p> <p>Args:     spike_times (array_like): The times of spikes in the trial in seconds     waveforms (np.array, optional): An nSpikes x nChannels x nSamples array</p> Source code in <code>ephysiopy/common/spikecalcs.py</code> <pre><code>class SpikeCalcsGeneric(object):\n    \"\"\"\n    Deals with the processing and analysis of spike data.\n    There should be one instance of this class per cluster in the\n    recording session. NB this differs from previous versions of this\n    class where there was one instance per recording session and clusters\n    were selected by passing in the cluster id to the methods.\n\n    Args:\n        spike_times (array_like): The times of spikes in the trial in seconds\n        waveforms (np.array, optional): An nSpikes x nChannels x nSamples array\n\n    \"\"\"\n\n    def __init__(\n        self,\n        spike_times: np.ndarray,\n        cluster: int,\n        waveforms: np.ndarray = None,\n        **kwargs,\n    ):\n        self.spike_times = np.ma.MaskedArray(spike_times)  # IN SECONDS\n        if waveforms is not None:\n            self._waves = np.ma.MaskedArray(waveforms)\n        else:\n            self._waves = None\n        self.cluster = cluster\n        self._event_ts = None  # the times that events occured IN SECONDS\n        # window, in seconds, either side of the stimulus, to examine\n        self._event_window = np.array((-0.050, 0.100))\n        self._stim_width = None  # the width, in ms, of the stimulus\n        # used to increase / decrease size of bins in psth\n        self._secs_per_bin = 0.001\n        self._sample_rate = 30000\n        self._pos_sample_rate = 50\n        self._duration = None\n        # these values should be specific to OE data\n        self._pre_spike_samples = 16\n        self._post_spike_samples = 34\n        # values from running KS\n        self._ksmeta = KSMetaTuple(None, None, None, None)\n        # update the __dict__ attribute with the kwargs\n        self.__dict__.update(kwargs)\n\n    @property\n    def sample_rate(self) -&gt; int | float:\n        return self._sample_rate\n\n    @sample_rate.setter\n    def sample_rate(self, value: int | float) -&gt; None:\n        self._sample_rate = value\n\n    @property\n    def pos_sample_rate(self) -&gt; int | float:\n        return self._pos_sample_rate\n\n    @pos_sample_rate.setter\n    def pos_sample_rate(self, value: int | float) -&gt; None:\n        self._pos_sample_rate = value\n\n    @property\n    def pre_spike_samples(self) -&gt; int:\n        return self._pre_spike_samples\n\n    @pre_spike_samples.setter\n    def pre_spike_samples(self, value: int) -&gt; None:\n        self._pre_spike_samples = int(value)\n\n    @property\n    def post_spike_samples(self) -&gt; int:\n        return self._post_spike_samples\n\n    @post_spike_samples.setter\n    def post_spike_samples(self, value: int) -&gt; None:\n        self._post_spike_samples = int(self._post_spike_samples)\n\n    def waveforms(self, channel_id: Sequence = None):\n        if self._waves is not None:\n            if channel_id is None:\n                return self._waves[:, :, :]\n            else:\n                if isinstance(channel_id, int):\n                    channel_id = [channel_id]\n                return self._waves[:, channel_id, :]\n        else:\n            return None\n\n    @property\n    def n_spikes(self):\n        \"\"\"\n        Returns the number of spikes in the cluster\n\n        Returns:\n            int: The number of spikes in the cluster\n        \"\"\"\n        return np.ma.count(self.spike_times)\n\n    @property\n    def event_ts(self) -&gt; np.ndarray:\n        return self._event_ts\n\n    @event_ts.setter\n    def event_ts(self, value: np.ndarray) -&gt; None:\n        self._event_ts = value\n\n    @property\n    def duration(self) -&gt; float | int | None:\n        return self._duration\n\n    @duration.setter\n    def duration(self, value: float | int | None):\n        self._duration = value\n\n    @property\n    def KSMeta(self) -&gt; KSMetaTuple:\n        return self._ksmeta\n\n    def update_KSMeta(self, value: dict) -&gt; None:\n        \"\"\"\n        Takes in a TemplateModel instance from a phy session and\n        parses out the relevant metrics for the cluster and places\n        into the namedtuple KSMeta\n        \"\"\"\n        metavals = []\n        for f in KSMetaTuple._fields:\n            if f in value.keys():\n                if self.cluster in value[f].keys():\n                    metavals.append(value[f][self.cluster])\n                else:\n                    metavals.append(None)\n            else:\n                metavals.append(None)\n        self._ksmeta = KSMetaTuple(*metavals)\n\n    @property\n    def event_window(self) -&gt; np.ndarray:\n        return self._event_window\n\n    @event_window.setter\n    def event_window(self, value: np.ndarray):\n        self._event_window = value\n\n    @property\n    def stim_width(self) -&gt; int | float | None:\n        return self._stim_width\n\n    @stim_width.setter\n    def stim_width(self, value: int | float | None):\n        self._stim_width = value\n\n    @property\n    def secs_per_bin(self) -&gt; float | int:\n        return self._secs_per_bin\n\n    @secs_per_bin.setter\n    def secs_per_bin(self, value: float | int):\n        self._secs_per_bin = value\n\n    def apply_filter(self, *trial_filter: TrialFilter) -&gt; None:\n        \"\"\"\n        Applies a mask to the spike times\n\n        Args\n            mask (list or tuple): The mask to apply to the spike times\n        \"\"\"\n        if trial_filter:\n            for i_filter in trial_filter:\n                assert isinstance(i_filter, TrialFilter), \"Filter must be a TrialFilter\"\n                assert i_filter.name == \"time\", \"Only time filters are supported\"\n        self.spike_times.mask = False\n        if self._waves and self._waves is not None:\n            self._waves.mask = False\n        if not trial_filter or len(trial_filter) == 0:\n            if self._waves and self._waves is not None:\n                self._waves.mask = False\n            self.spike_times.mask = False\n        else:\n            mask = np.zeros_like(self.spike_times, dtype=bool)\n            for i_filter in trial_filter:\n                i_mask = np.logical_and(\n                    self.spike_times &gt; i_filter.start, self.spike_times &lt; i_filter.end\n                )\n                mask = np.logical_or(mask, i_mask)\n            self.spike_times.mask = mask\n            if self._waves is not None:\n                self._waves.mask = mask\n\n    def acorr(self, Trange: np.ndarray = np.array([-0.5, 0.5]), **kwargs) -&gt; BinnedData:\n        \"\"\"\n        Calculates the autocorrelogram of a spike train\n\n        Args:\n            ts (np.ndarray): The spike times\n            Trange (np.ndarray): The range of times to calculate the\n                autocorrelogram over\n\n        Returns:\n        result: (BinnedData): Container for the binned data\n        \"\"\"\n        return xcorr(self.spike_times, Trange=Trange, **kwargs)\n\n    def trial_mean_fr(self) -&gt; float:\n        # Returns the trial mean firing rate for the cluster\n        if self.duration is None:\n            raise IndexError(\"No duration provided, give me one!\")\n        return self.n_spikes / self.duration\n\n    def mean_isi_range(self, isi_range: int) -&gt; float:\n        \"\"\"\n        Calculates the mean of the autocorrelation from 0 to n milliseconds\n        Used to help classify a neurons type (principal, interneuron etc)\n\n        Args:\n            isi_range (int): The range in ms to calculate the mean over\n\n        Returns:\n            float: The mean of the autocorrelogram between 0 and n milliseconds\n        \"\"\"\n        bins = 201\n        trange = np.array((-500, 500))\n        ac = self.acorr(Trange=trange)\n        bins = ac.bin_edges[0]\n        counts = ac.binned_data[0]\n        mask = np.logical_and(bins &gt; 0, bins &lt; isi_range)\n        return np.mean(counts[mask[1:]])\n\n    def mean_waveform(self, channel_id: Sequence = None):\n        \"\"\"\n        Returns the mean waveform and sem for a given spike train on a\n        particular channel\n\n        Args:\n            cluster_id (int): The cluster to get the mean waveform for\n\n        Returns:\n            mn_wvs (ndarray): The mean waveforms, usually 4x50 for tetrode\n                                recordings\n            std_wvs (ndarray): The standard deviations of the waveforms,\n                                usually 4x50 for tetrode recordings\n        \"\"\"\n        x = self.waveforms(channel_id)\n        if x is not None:\n            return np.mean(x, axis=0), np.std(x, axis=0)\n        else:\n            return None\n\n    def psth(self) -&gt; tuple[list, ...]:\n        \"\"\"\n        Calculate the PSTH of event_ts against the spiking of a cell\n\n        Args:\n            cluster_id (int): The cluster for which to calculate the psth\n\n        Returns:\n            x, y (list): The list of time differences between the spikes of\n                            the cluster and the events (x) and the trials (y)\n        \"\"\"\n        if self._event_ts is None:\n            raise Exception(\"Need some event timestamps! Aborting\")\n        event_ts = self.event_ts\n        event_ts.sort()\n        if isinstance(event_ts, list):\n            event_ts = np.array(event_ts)\n\n        irange = event_ts[:, np.newaxis] + self.event_window[np.newaxis, :]\n        dts = np.searchsorted(self.spike_times, irange)\n        x = []\n        y = []\n        for i, t in enumerate(dts):\n            tmp = self.spike_times[t[0] : t[1]] - event_ts[i]\n            x.extend(tmp)\n            y.extend(np.repeat(i, len(tmp)))\n        return x, y\n\n    def psch(self, bin_width_secs: float) -&gt; np.ndarray:\n        \"\"\"\n        Calculate the peri-stimulus *count* histogram of a cell's spiking\n        against event times.\n\n        Args:\n            bin_width_secs (float): The width of each bin in seconds.\n\n        Returns:\n            result (np.ndarray): Rows are counts of spikes per bin_width_secs.\n            Size of columns ranges from self.event_window[0] to\n            self.event_window[1] with bin_width_secs steps;\n            so x is count, y is \"event\".\n        \"\"\"\n        if self._event_ts is None:\n            raise Exception(\"Need some event timestamps! Aborting\")\n        event_ts = self.event_ts\n        event_ts.sort()\n        if isinstance(event_ts, list):\n            event_ts = np.array(event_ts)\n\n        irange = event_ts[:, np.newaxis] + self.event_window[np.newaxis, :]\n        dts = np.searchsorted(self.spike_times, irange)\n        bins = np.arange(self.event_window[0], self.event_window[1], bin_width_secs)\n        result = np.empty(shape=(len(bins) - 1, len(event_ts)), dtype=np.int64)\n        for i, t in enumerate(dts):\n            tmp = self.spike_times[t[0] : t[1]] - event_ts[i]\n            indices = np.digitize(tmp, bins=bins)\n            counts = np.bincount(indices, minlength=len(bins))\n            result[:, i] = counts[1:]\n        return result\n\n    def get_shuffled_ifr_sp_corr(\n        self, ts: np.array, speed: np.array, nShuffles: int = 100, **kwargs\n    ):\n        # shift spikes by at least 30 seconds after trial start and\n        # 30 seconds before trial end\n        nSamples = len(speed)\n        random_seed = kwargs.get(\"random_seed\", None)\n        r = np.random.default_rng(random_seed)\n        timeSteps = r.integers(low=30, high=ts[-1] - 30, size=nShuffles)\n        shuffled_ifr_sp_corrs = []\n        for t in timeSteps:\n            shift_ts = shift_vector(ts, t, maxlen=nSamples)\n            res = self.ifr_sp_corr(shift_ts, speed, **kwargs)\n            shuffled_ifr_sp_corrs.append(res.statistic)\n        return np.array(shuffled_ifr_sp_corrs)\n\n    def ifr_sp_corr(\n        self,\n        ts,\n        speed,\n        minSpeed=2.0,\n        maxSpeed=40.0,\n        sigma=3,\n        nShuffles=100,\n        plot=False,\n        **kwargs,\n    ):\n        \"\"\"\n        Calculates the correlation between the instantaneous firing rate and\n        speed.\n\n        Args:\n            ts (np.array): The times in seconds at which the cluster fired.\n            speed (np.array): Instantaneous speed (1 x nSamples).\n            minSpeed (float, optional): Speeds below this value are ignored.\n                Defaults to 2.0 cm/s as with Kropff et al., 2015.\n            maxSpeed (float, optional): Speeds above this value are ignored.\n                Defaults to 40.0 cm/s.\n            sigma (int, optional): The standard deviation of the gaussian used\n                to smooth the spike train. Defaults to 3.\n            nShuffles (int, optional): The number of resamples to feed into\n                the permutation test. Defaults to 9999.\n                See scipy.stats.PermutationMethod.\n            plot (bool, optional): Whether to plot the result.\n                Defaults to False.\n        kwargs:\n            method: how the significance of the speed vs firing rate correlation\n                    is calculated - see the documentation for scipy.stats.PermutationMethod\n\n                    An example of how I was calculating this is:\n\n                    &gt;&gt; rng = np.random.default_rng()\n                    &gt;&gt; method = stats.PermutationMethod(n_resamples=nShuffles, random_state=rng)\n        \"\"\"\n        speed = speed.ravel()\n        orig_speed_mask = speed.mask\n        posSampRate = self.pos_sample_rate\n        nSamples = len(speed)\n        x1 = np.floor(ts * posSampRate).astype(int)\n        # crop the end of the timestamps if longer than the pos data\n        x1 = np.delete(x1, np.nonzero(x1 &gt;= nSamples))\n        spk_hist = np.bincount(x1, minlength=nSamples)\n        # smooth the spk_hist (which is a temporal histogram) with a 250ms\n        # gaussian as with Kropff et al., 2015\n        h = signal.windows.gaussian(13, sigma)\n        h = h / float(np.sum(h))\n        # filter for low and high speeds\n        speed_mask = np.logical_or(speed &lt; minSpeed, speed &gt; maxSpeed)\n        # make sure the original mask is preserved\n        speed_mask = np.logical_or(speed_mask, orig_speed_mask)\n        speed_filt = np.ma.MaskedArray(speed, speed_mask)\n        ## speed might contain nans so mask these too\n        speed_filt = np.ma.fix_invalid(speed_filt)\n        speed_mask = speed_filt.mask\n        spk_hist_filt = np.ma.MaskedArray(spk_hist, speed_mask)\n        spk_sm = signal.filtfilt(h.ravel(), 1, spk_hist_filt)\n        sm_spk_rate = np.ma.MaskedArray(spk_sm * posSampRate, speed_mask)\n        # the permutation test for significance, only perform\n        # on the non-masked data\n        rng = np.random.default_rng()\n        method = stats.PermutationMethod(n_resamples=nShuffles, random_state=rng)\n        method = kwargs.get(\"method\", method)\n        res = stats.pearsonr(\n            sm_spk_rate.compressed(), speed_filt.compressed(), method=method\n        )\n        return res\n\n    def get_ifr(self, spike_times: np.array, n_samples: int, **kwargs):\n        \"\"\"\n        Returns the instantaneous firing rate of the cluster\n\n        Args:\n            ts (np.array): The times in seconds at which the cluster fired.\n            n_samples (int): The number of samples to use in the calculation.\n                             Practically this should be the number of position\n                             samples in the recording.\n\n        Returns:\n            ifr (np.array): The instantaneous firing rate of the cluster\n        \"\"\"\n        posSampRate = self.pos_sample_rate\n        x1 = np.floor(spike_times * posSampRate).astype(int)\n        spk_hist = np.bincount(x1, minlength=n_samples)\n        sigma = kwargs.get(\"sigma\", 3)\n        h = signal.windows.gaussian(13, sigma)\n        h = h / float(np.sum(h))\n        spk_sm = signal.filtfilt(h.ravel(), 1, spk_hist)\n        ifr = spk_sm * posSampRate\n        return ifr\n\n    def responds_to_stimulus(\n        self,\n        threshold: float,\n        min_contiguous: int,\n        return_activity: bool = False,\n        return_magnitude: bool = False,\n        **kwargs,\n    ) -&gt; tuple:\n        \"\"\"\n        Checks whether a cluster responds to a laser stimulus.\n\n        Args:\n            cluster (int): The cluster to check.\n            threshold (float): The amount of activity the cluster needs to go\n                beyond to be classified as a responder (1.5 = 50% more or less\n                than the baseline activity).\n            min_contiguous (int): The number of contiguous samples in the\n                post-stimulus period for which the cluster needs to be active\n                beyond the threshold value to be classed as a responder.\n            return_activity (bool): Whether to return the mean reponse curve.\n            return_magnitude (int): Whether to return the magnitude of the\n                response. NB this is either +1 for excited or -1 for inhibited.\n\n        Returns:\n            responds (bool): Whether the cell responds or not.\n            OR\n            tuple: responds (bool), normed_response_curve (np.ndarray).\n            OR\n            tuple: responds (bool), normed_response_curve (np.ndarray),\n                response_magnitude (np.ndarray).\n        \"\"\"\n        spk_count_by_trial = self.psch(self._secs_per_bin)\n        firing_rate_by_trial = spk_count_by_trial / self.secs_per_bin\n        mean_firing_rate = np.mean(firing_rate_by_trial, 1)\n        # smooth with a moving average\n        # check nothing in kwargs first\n        if \"window_len\" in kwargs.keys():\n            window_len = kwargs[\"window_len\"]\n        else:\n            window_len = 5\n        if \"window\" in kwargs.keys():\n            window = kwargs[\"window\"]\n        else:\n            window = \"flat\"\n        if \"flat\" in window:\n            kernel = Box1DKernel(window_len)\n        if \"gauss\" in window:\n            kernel = Gaussian1DKernel(1, window_len)\n        if \"do_smooth\" in kwargs.keys():\n            do_smooth = kwargs.get(\"do_smooth\")\n        else:\n            do_smooth = True\n\n        if do_smooth:\n            smoothed_binned_spikes = convolve(mean_firing_rate, kernel, boundary=\"wrap\")\n        else:\n            smoothed_binned_spikes = mean_firing_rate\n        nbins = np.floor(np.sum(np.abs(self.event_window)) / self.secs_per_bin)\n        bins = np.linspace(self.event_window[0], self.event_window[1], int(nbins))\n        # normalize all activity by activity in the time before\n        # the laser onset\n        idx = bins &lt; 0\n        normd = min_max_norm(\n            smoothed_binned_spikes,\n            np.min(smoothed_binned_spikes[idx]),\n            np.max(smoothed_binned_spikes[idx]),\n        )\n        # mask the array outside of a threshold value so that\n        # only True values in the masked array are those that\n        # exceed the threshold (positively or negatively)\n        # the threshold provided to this function is expressed\n        # as a % above / below unit normality so adjust that now\n        # so it is expressed as a pre-stimulus firing rate mean\n        # pre_stim_mean = np.mean(smoothed_binned_spikes[idx])\n        # pre_stim_max = pre_stim_mean * threshold\n        # pre_stim_min = pre_stim_mean * (threshold-1.0)\n        # updated so threshold is double (+ or -) the pre-stim\n        # norm (lies between )\n        normd_masked = np.ma.masked_inside(normd, -threshold, 1 + threshold)\n        # find the contiguous runs in the masked array\n        # that are at least as long as the min_contiguous value\n        # and classify this as a True response\n        slices = np.ma.notmasked_contiguous(normd_masked)\n        if slices and np.any(np.isfinite(normd)):\n            # make sure that slices are within the first 25ms post-stim\n            if ~np.any([s.start &gt; 50 and s.start &lt; 75 for s in slices]):\n                if not return_activity:\n                    return False\n                else:\n                    if return_magnitude:\n                        return False, normd, 0\n                    return False, normd\n            max_runlength = max([len(normd_masked[s]) for s in slices])\n            if max_runlength &gt;= min_contiguous:\n                if not return_activity:\n                    return True\n                else:\n                    if return_magnitude:\n                        sl = [\n                            slc\n                            for slc in slices\n                            if (slc.stop - slc.start) == max_runlength\n                        ]\n                        mag = [-1 if np.mean(normd[sl[0]]) &lt; 0 else 1][0]\n                        return True, normd, mag\n                    else:\n                        return True, normd\n        if not return_activity:\n            return False\n        else:\n            if return_magnitude:\n                return False, normd, 0\n            return False, normd\n\n    def theta_mod_idx(self, **kwargs) -&gt; float:\n        \"\"\"\n        Calculates a theta modulation index of a spike train based on the cells\n        autocorrelogram.\n\n        The difference of the mean power in the theta frequency band (6-11 Hz) and\n        the mean power in the 1-50 Hz frequency band is divided by their sum to give\n        a metric that lives between 0 and 1\n\n        Args:\n            x1 (np.array): The spike time-series.\n\n        Returns:\n            thetaMod (float): The difference of the values at the first peak\n            and trough of the autocorrelogram.\n\n        NB This is a fairly skewed metric with a distribution strongly biased\n        to -1 (although more evenly distributed than theta_mod_idxV2 below)\n        \"\"\"\n        ac = self.acorr(**kwargs)\n        # Take the fft of the spike train autocorr (from -500 to +500ms)\n        from scipy.signal import periodogram\n\n        fs = 1.0 / kwargs.get(\"binsize\", 0.0001)\n        freqs, power = periodogram(ac.binned_data[0], fs=fs, return_onesided=True)\n        # Smooth the power over +/- 1Hz when fs=200\n        b = signal.windows.boxcar(3)  # another filter type - blackman?\n        h = signal.filtfilt(b, 3, power)\n\n        # Square the amplitude first to get power\n        sqd_amp = h**2\n        # Get the mean theta band power - mtbp\n        mtbp = np.mean(sqd_amp[np.logical_and(freqs &gt;= 6, freqs &lt;= 11)])\n        # Find the mean amplitude in the 1-50Hz range\n        # Get the mean in the other band - mobp\n        mobp = np.mean(sqd_amp[np.logical_and(freqs &gt; 2, freqs &lt; 50)])\n        # Find the ratio of these two - this is the theta modulation index\n        return float((mtbp - mobp) / (mtbp + mobp))\n\n    def theta_mod_idxV2(self) -&gt; float:\n        \"\"\"\n        This is a simpler alternative to the theta_mod_idx method in that it\n        calculates the difference between the normalized temporal\n        autocorrelogram at the trough between 50-70ms and the\n        peak between 100-140ms over their sum (data is binned into 5ms bins)\n\n        Measure used in Cacucci et al., 2004 and Kropff et al 2015\n        \"\"\"\n        ac = self.acorr()\n        bins = ac.bin_edges[0]\n        corr = ac.binned_data[0]\n        # 'close' the right-hand bin\n        bins = bins[0:-1]\n        # normalise corr so max is 1.0\n        corr = corr / float(np.max(corr))\n        thetaAntiPhase = np.min(\n            corr[np.logical_and(bins &gt; 50 / 1000.0, bins &lt; 70 / 1000.0)]\n        )\n        thetaPhase = np.max(\n            corr[np.logical_and(bins &gt; 100 / 1000.0, bins &lt; 140 / 1000.0)]\n        )\n        return float((thetaPhase - thetaAntiPhase) / (thetaPhase + thetaAntiPhase))\n\n    def theta_mod_idxV3(self, **kwargs) -&gt; float:\n        \"\"\"\n        Another theta modulation index score this time based on the method used\n        by Kornienko et al., (2024) (Kevin Allens lab)\n        see https://doi.org/10.7554/eLife.35949.001\n\n        Basically uses the binned spike train instead of the autocorrelogram as\n        the input to the periodogram function (they use pwelch in R; periodogram is a\n        simplified call to welch in scipy.signal)\n\n        The resulting metric is similar to the one in theta_mod_idx above except\n        that the frequency bands compared to the theta band are narrower and\n        exclusive of the theta band\n\n        Produces a fairly normally distributed looking score with a mean and median\n        pretty close to 0\n        \"\"\"\n        freqs, power = self.get_ifr_power_spectrum(**kwargs)\n        # smooth with a boxcar filter with a 0.5Hz window\n        win_len = np.count_nonzero(np.logical_and(freqs &gt;= 0, freqs &lt;= 0.5))\n        w = signal.windows.boxcar(win_len)\n        b = signal.filtfilt(w, 1, power)\n        sqd_amp = b**2\n        mtbp = np.mean(sqd_amp[np.logical_and(freqs &gt;= 6, freqs &lt;= 10)])\n        mobp = np.mean(\n            sqd_amp[\n                np.logical_or(\n                    np.logical_and(freqs &gt; 3, freqs &lt; 5),\n                    np.logical_and(freqs &gt; 11, freqs &lt; 13),\n                )\n            ]\n        )\n        return float((mtbp - mobp) / (mtbp + mobp))\n\n    def get_ifr_power_spectrum(self, **kwargs) -&gt; tuple[np.ndarray, ...]:\n        \"\"\"\n        Returns the power spectrum of the instantaneous firing rate of a cell\n\n        This is what is used to calculate the theta_mod_idxV3 score above\n        \"\"\"\n        binned_spikes = np.bincount(\n            np.array(self.spike_times * self.pos_sample_rate, dtype=int).ravel(),\n            minlength=int(self.pos_sample_rate * self.duration),\n        )\n        # possibly smooth the spike train...\n        freqs, power = signal.periodogram(binned_spikes, fs=self.pos_sample_rate)\n        freqs = freqs.ravel()\n        power = power.ravel()\n        return freqs, power\n\n    def theta_band_max_freq(self):\n        \"\"\"\n        Calculates the frequency with the maximum power in the theta band (6-12Hz)\n        of a spike train's autocorrelogram.\n\n        This function is used to look for differences in theta frequency in\n        different running directions as per Blair.\n        See Welday paper - https://doi.org/10.1523/jneurosci.0712-11.2011\n\n        Args:\n            x1 (np.ndarray): The spike train for which the autocorrelogram will be\n                calculated.\n\n        Returns:\n            float: The frequency with the maximum power in the theta band.\n\n        Raises:\n            ValueError: If the input spike train is not valid.\n        \"\"\"\n        ac = self.acorr()\n        # Take the fft of the spike train autocorr (from -500 to +500ms)\n        from scipy.signal import periodogram\n\n        freqs, power = periodogram(ac.binned_data[0], fs=200, return_onesided=True)\n        power_masked = np.ma.MaskedArray(power, np.logical_or(freqs &lt; 6, freqs &gt; 12))\n        return freqs[np.argmax(power_masked)]\n\n    def smooth_spike_train(self, npos, sigma=3.0, shuffle=None):\n        \"\"\"\n        Returns a spike train the same length as num pos samples that has been\n        smoothed in time with a gaussian kernel M in width and standard\n        deviation equal to sigma.\n\n        Args:\n            x1 (np.array): The pos indices the spikes occurred at.\n            npos (int): The number of position samples captured.\n            sigma (float): The standard deviation of the gaussian used to\n                smooth the spike train.\n            shuffle (int, optional): The number of seconds to shift the spike\n                train by. Default is None.\n\n        Returns:\n            smoothed_spikes (np.array): The smoothed spike train.\n        \"\"\"\n        spk_hist = np.bincount(self.spike_times, minlength=npos)\n        if shuffle is not None:\n            spk_hist = np.roll(spk_hist, int(shuffle * 50))\n        # smooth the spk_hist (which is a temporal histogram) with a 250ms\n        # gaussian as with Kropff et al., 2015\n        h = signal.windows.gaussian(13, sigma)\n        h = h / float(np.sum(h))\n        return signal.filtfilt(h.ravel(), 1, spk_hist)\n\n    def contamination_percent(self, **kwargs) -&gt; tuple:\n\n        _, Qi, Q00, Q01, Ri = contamination_percent(self.spike_times, **kwargs)\n        Q = min(Qi / (max(Q00, Q01)))  # this is a measure of refractoriness\n        # this is a second measure of refractoriness (kicks in for very low\n        # firing rates)\n        R = min(Ri)\n        return Q, R\n</code></pre>"},{"location":"reference/#ephysiopy.common.spikecalcs.SpikeCalcsGeneric.n_spikes","title":"<code>n_spikes</code>  <code>property</code>","text":"<p>Returns the number of spikes in the cluster</p> <p>Returns:     int: The number of spikes in the cluster</p>"},{"location":"reference/#ephysiopy.common.spikecalcs.SpikeCalcsGeneric.acorr","title":"<code>acorr(Trange=np.array([-0.5, 0.5]), **kwargs)</code>","text":"<p>Calculates the autocorrelogram of a spike train</p> <p>Args:     ts (np.ndarray): The spike times     Trange (np.ndarray): The range of times to calculate the         autocorrelogram over</p> <p>Returns: result: (BinnedData): Container for the binned data</p> Source code in <code>ephysiopy/common/spikecalcs.py</code> <pre><code>def acorr(self, Trange: np.ndarray = np.array([-0.5, 0.5]), **kwargs) -&gt; BinnedData:\n    \"\"\"\n    Calculates the autocorrelogram of a spike train\n\n    Args:\n        ts (np.ndarray): The spike times\n        Trange (np.ndarray): The range of times to calculate the\n            autocorrelogram over\n\n    Returns:\n    result: (BinnedData): Container for the binned data\n    \"\"\"\n    return xcorr(self.spike_times, Trange=Trange, **kwargs)\n</code></pre>"},{"location":"reference/#ephysiopy.common.spikecalcs.SpikeCalcsGeneric.apply_filter","title":"<code>apply_filter(*trial_filter)</code>","text":"<p>Applies a mask to the spike times</p> <p>Args     mask (list or tuple): The mask to apply to the spike times</p> Source code in <code>ephysiopy/common/spikecalcs.py</code> <pre><code>def apply_filter(self, *trial_filter: TrialFilter) -&gt; None:\n    \"\"\"\n    Applies a mask to the spike times\n\n    Args\n        mask (list or tuple): The mask to apply to the spike times\n    \"\"\"\n    if trial_filter:\n        for i_filter in trial_filter:\n            assert isinstance(i_filter, TrialFilter), \"Filter must be a TrialFilter\"\n            assert i_filter.name == \"time\", \"Only time filters are supported\"\n    self.spike_times.mask = False\n    if self._waves and self._waves is not None:\n        self._waves.mask = False\n    if not trial_filter or len(trial_filter) == 0:\n        if self._waves and self._waves is not None:\n            self._waves.mask = False\n        self.spike_times.mask = False\n    else:\n        mask = np.zeros_like(self.spike_times, dtype=bool)\n        for i_filter in trial_filter:\n            i_mask = np.logical_and(\n                self.spike_times &gt; i_filter.start, self.spike_times &lt; i_filter.end\n            )\n            mask = np.logical_or(mask, i_mask)\n        self.spike_times.mask = mask\n        if self._waves is not None:\n            self._waves.mask = mask\n</code></pre>"},{"location":"reference/#ephysiopy.common.spikecalcs.SpikeCalcsGeneric.get_ifr","title":"<code>get_ifr(spike_times, n_samples, **kwargs)</code>","text":"<p>Returns the instantaneous firing rate of the cluster</p> <p>Args:     ts (np.array): The times in seconds at which the cluster fired.     n_samples (int): The number of samples to use in the calculation.                      Practically this should be the number of position                      samples in the recording.</p> <p>Returns:     ifr (np.array): The instantaneous firing rate of the cluster</p> Source code in <code>ephysiopy/common/spikecalcs.py</code> <pre><code>def get_ifr(self, spike_times: np.array, n_samples: int, **kwargs):\n    \"\"\"\n    Returns the instantaneous firing rate of the cluster\n\n    Args:\n        ts (np.array): The times in seconds at which the cluster fired.\n        n_samples (int): The number of samples to use in the calculation.\n                         Practically this should be the number of position\n                         samples in the recording.\n\n    Returns:\n        ifr (np.array): The instantaneous firing rate of the cluster\n    \"\"\"\n    posSampRate = self.pos_sample_rate\n    x1 = np.floor(spike_times * posSampRate).astype(int)\n    spk_hist = np.bincount(x1, minlength=n_samples)\n    sigma = kwargs.get(\"sigma\", 3)\n    h = signal.windows.gaussian(13, sigma)\n    h = h / float(np.sum(h))\n    spk_sm = signal.filtfilt(h.ravel(), 1, spk_hist)\n    ifr = spk_sm * posSampRate\n    return ifr\n</code></pre>"},{"location":"reference/#ephysiopy.common.spikecalcs.SpikeCalcsGeneric.get_ifr_power_spectrum","title":"<code>get_ifr_power_spectrum(**kwargs)</code>","text":"<p>Returns the power spectrum of the instantaneous firing rate of a cell</p> <p>This is what is used to calculate the theta_mod_idxV3 score above</p> Source code in <code>ephysiopy/common/spikecalcs.py</code> <pre><code>def get_ifr_power_spectrum(self, **kwargs) -&gt; tuple[np.ndarray, ...]:\n    \"\"\"\n    Returns the power spectrum of the instantaneous firing rate of a cell\n\n    This is what is used to calculate the theta_mod_idxV3 score above\n    \"\"\"\n    binned_spikes = np.bincount(\n        np.array(self.spike_times * self.pos_sample_rate, dtype=int).ravel(),\n        minlength=int(self.pos_sample_rate * self.duration),\n    )\n    # possibly smooth the spike train...\n    freqs, power = signal.periodogram(binned_spikes, fs=self.pos_sample_rate)\n    freqs = freqs.ravel()\n    power = power.ravel()\n    return freqs, power\n</code></pre>"},{"location":"reference/#ephysiopy.common.spikecalcs.SpikeCalcsGeneric.ifr_sp_corr","title":"<code>ifr_sp_corr(ts, speed, minSpeed=2.0, maxSpeed=40.0, sigma=3, nShuffles=100, plot=False, **kwargs)</code>","text":"<p>Calculates the correlation between the instantaneous firing rate and speed.</p> <p>Args:     ts (np.array): The times in seconds at which the cluster fired.     speed (np.array): Instantaneous speed (1 x nSamples).     minSpeed (float, optional): Speeds below this value are ignored.         Defaults to 2.0 cm/s as with Kropff et al., 2015.     maxSpeed (float, optional): Speeds above this value are ignored.         Defaults to 40.0 cm/s.     sigma (int, optional): The standard deviation of the gaussian used         to smooth the spike train. Defaults to 3.     nShuffles (int, optional): The number of resamples to feed into         the permutation test. Defaults to 9999.         See scipy.stats.PermutationMethod.     plot (bool, optional): Whether to plot the result.         Defaults to False. kwargs:     method: how the significance of the speed vs firing rate correlation             is calculated - see the documentation for scipy.stats.PermutationMethod</p> <pre><code>        An example of how I was calculating this is:\n\n        &gt;&gt; rng = np.random.default_rng()\n        &gt;&gt; method = stats.PermutationMethod(n_resamples=nShuffles, random_state=rng)\n</code></pre> Source code in <code>ephysiopy/common/spikecalcs.py</code> <pre><code>def ifr_sp_corr(\n    self,\n    ts,\n    speed,\n    minSpeed=2.0,\n    maxSpeed=40.0,\n    sigma=3,\n    nShuffles=100,\n    plot=False,\n    **kwargs,\n):\n    \"\"\"\n    Calculates the correlation between the instantaneous firing rate and\n    speed.\n\n    Args:\n        ts (np.array): The times in seconds at which the cluster fired.\n        speed (np.array): Instantaneous speed (1 x nSamples).\n        minSpeed (float, optional): Speeds below this value are ignored.\n            Defaults to 2.0 cm/s as with Kropff et al., 2015.\n        maxSpeed (float, optional): Speeds above this value are ignored.\n            Defaults to 40.0 cm/s.\n        sigma (int, optional): The standard deviation of the gaussian used\n            to smooth the spike train. Defaults to 3.\n        nShuffles (int, optional): The number of resamples to feed into\n            the permutation test. Defaults to 9999.\n            See scipy.stats.PermutationMethod.\n        plot (bool, optional): Whether to plot the result.\n            Defaults to False.\n    kwargs:\n        method: how the significance of the speed vs firing rate correlation\n                is calculated - see the documentation for scipy.stats.PermutationMethod\n\n                An example of how I was calculating this is:\n\n                &gt;&gt; rng = np.random.default_rng()\n                &gt;&gt; method = stats.PermutationMethod(n_resamples=nShuffles, random_state=rng)\n    \"\"\"\n    speed = speed.ravel()\n    orig_speed_mask = speed.mask\n    posSampRate = self.pos_sample_rate\n    nSamples = len(speed)\n    x1 = np.floor(ts * posSampRate).astype(int)\n    # crop the end of the timestamps if longer than the pos data\n    x1 = np.delete(x1, np.nonzero(x1 &gt;= nSamples))\n    spk_hist = np.bincount(x1, minlength=nSamples)\n    # smooth the spk_hist (which is a temporal histogram) with a 250ms\n    # gaussian as with Kropff et al., 2015\n    h = signal.windows.gaussian(13, sigma)\n    h = h / float(np.sum(h))\n    # filter for low and high speeds\n    speed_mask = np.logical_or(speed &lt; minSpeed, speed &gt; maxSpeed)\n    # make sure the original mask is preserved\n    speed_mask = np.logical_or(speed_mask, orig_speed_mask)\n    speed_filt = np.ma.MaskedArray(speed, speed_mask)\n    ## speed might contain nans so mask these too\n    speed_filt = np.ma.fix_invalid(speed_filt)\n    speed_mask = speed_filt.mask\n    spk_hist_filt = np.ma.MaskedArray(spk_hist, speed_mask)\n    spk_sm = signal.filtfilt(h.ravel(), 1, spk_hist_filt)\n    sm_spk_rate = np.ma.MaskedArray(spk_sm * posSampRate, speed_mask)\n    # the permutation test for significance, only perform\n    # on the non-masked data\n    rng = np.random.default_rng()\n    method = stats.PermutationMethod(n_resamples=nShuffles, random_state=rng)\n    method = kwargs.get(\"method\", method)\n    res = stats.pearsonr(\n        sm_spk_rate.compressed(), speed_filt.compressed(), method=method\n    )\n    return res\n</code></pre>"},{"location":"reference/#ephysiopy.common.spikecalcs.SpikeCalcsGeneric.mean_isi_range","title":"<code>mean_isi_range(isi_range)</code>","text":"<p>Calculates the mean of the autocorrelation from 0 to n milliseconds Used to help classify a neurons type (principal, interneuron etc)</p> <p>Args:     isi_range (int): The range in ms to calculate the mean over</p> <p>Returns:     float: The mean of the autocorrelogram between 0 and n milliseconds</p> Source code in <code>ephysiopy/common/spikecalcs.py</code> <pre><code>def mean_isi_range(self, isi_range: int) -&gt; float:\n    \"\"\"\n    Calculates the mean of the autocorrelation from 0 to n milliseconds\n    Used to help classify a neurons type (principal, interneuron etc)\n\n    Args:\n        isi_range (int): The range in ms to calculate the mean over\n\n    Returns:\n        float: The mean of the autocorrelogram between 0 and n milliseconds\n    \"\"\"\n    bins = 201\n    trange = np.array((-500, 500))\n    ac = self.acorr(Trange=trange)\n    bins = ac.bin_edges[0]\n    counts = ac.binned_data[0]\n    mask = np.logical_and(bins &gt; 0, bins &lt; isi_range)\n    return np.mean(counts[mask[1:]])\n</code></pre>"},{"location":"reference/#ephysiopy.common.spikecalcs.SpikeCalcsGeneric.mean_waveform","title":"<code>mean_waveform(channel_id=None)</code>","text":"<p>Returns the mean waveform and sem for a given spike train on a particular channel</p> <p>Args:     cluster_id (int): The cluster to get the mean waveform for</p> <p>Returns:     mn_wvs (ndarray): The mean waveforms, usually 4x50 for tetrode                         recordings     std_wvs (ndarray): The standard deviations of the waveforms,                         usually 4x50 for tetrode recordings</p> Source code in <code>ephysiopy/common/spikecalcs.py</code> <pre><code>def mean_waveform(self, channel_id: Sequence = None):\n    \"\"\"\n    Returns the mean waveform and sem for a given spike train on a\n    particular channel\n\n    Args:\n        cluster_id (int): The cluster to get the mean waveform for\n\n    Returns:\n        mn_wvs (ndarray): The mean waveforms, usually 4x50 for tetrode\n                            recordings\n        std_wvs (ndarray): The standard deviations of the waveforms,\n                            usually 4x50 for tetrode recordings\n    \"\"\"\n    x = self.waveforms(channel_id)\n    if x is not None:\n        return np.mean(x, axis=0), np.std(x, axis=0)\n    else:\n        return None\n</code></pre>"},{"location":"reference/#ephysiopy.common.spikecalcs.SpikeCalcsGeneric.psch","title":"<code>psch(bin_width_secs)</code>","text":"<p>Calculate the peri-stimulus count histogram of a cell's spiking against event times.</p> <p>Args:     bin_width_secs (float): The width of each bin in seconds.</p> <p>Returns:     result (np.ndarray): Rows are counts of spikes per bin_width_secs.     Size of columns ranges from self.event_window[0] to     self.event_window[1] with bin_width_secs steps;     so x is count, y is \"event\".</p> Source code in <code>ephysiopy/common/spikecalcs.py</code> <pre><code>def psch(self, bin_width_secs: float) -&gt; np.ndarray:\n    \"\"\"\n    Calculate the peri-stimulus *count* histogram of a cell's spiking\n    against event times.\n\n    Args:\n        bin_width_secs (float): The width of each bin in seconds.\n\n    Returns:\n        result (np.ndarray): Rows are counts of spikes per bin_width_secs.\n        Size of columns ranges from self.event_window[0] to\n        self.event_window[1] with bin_width_secs steps;\n        so x is count, y is \"event\".\n    \"\"\"\n    if self._event_ts is None:\n        raise Exception(\"Need some event timestamps! Aborting\")\n    event_ts = self.event_ts\n    event_ts.sort()\n    if isinstance(event_ts, list):\n        event_ts = np.array(event_ts)\n\n    irange = event_ts[:, np.newaxis] + self.event_window[np.newaxis, :]\n    dts = np.searchsorted(self.spike_times, irange)\n    bins = np.arange(self.event_window[0], self.event_window[1], bin_width_secs)\n    result = np.empty(shape=(len(bins) - 1, len(event_ts)), dtype=np.int64)\n    for i, t in enumerate(dts):\n        tmp = self.spike_times[t[0] : t[1]] - event_ts[i]\n        indices = np.digitize(tmp, bins=bins)\n        counts = np.bincount(indices, minlength=len(bins))\n        result[:, i] = counts[1:]\n    return result\n</code></pre>"},{"location":"reference/#ephysiopy.common.spikecalcs.SpikeCalcsGeneric.psth","title":"<code>psth()</code>","text":"<p>Calculate the PSTH of event_ts against the spiking of a cell</p> <p>Args:     cluster_id (int): The cluster for which to calculate the psth</p> <p>Returns:     x, y (list): The list of time differences between the spikes of                     the cluster and the events (x) and the trials (y)</p> Source code in <code>ephysiopy/common/spikecalcs.py</code> <pre><code>def psth(self) -&gt; tuple[list, ...]:\n    \"\"\"\n    Calculate the PSTH of event_ts against the spiking of a cell\n\n    Args:\n        cluster_id (int): The cluster for which to calculate the psth\n\n    Returns:\n        x, y (list): The list of time differences between the spikes of\n                        the cluster and the events (x) and the trials (y)\n    \"\"\"\n    if self._event_ts is None:\n        raise Exception(\"Need some event timestamps! Aborting\")\n    event_ts = self.event_ts\n    event_ts.sort()\n    if isinstance(event_ts, list):\n        event_ts = np.array(event_ts)\n\n    irange = event_ts[:, np.newaxis] + self.event_window[np.newaxis, :]\n    dts = np.searchsorted(self.spike_times, irange)\n    x = []\n    y = []\n    for i, t in enumerate(dts):\n        tmp = self.spike_times[t[0] : t[1]] - event_ts[i]\n        x.extend(tmp)\n        y.extend(np.repeat(i, len(tmp)))\n    return x, y\n</code></pre>"},{"location":"reference/#ephysiopy.common.spikecalcs.SpikeCalcsGeneric.responds_to_stimulus","title":"<code>responds_to_stimulus(threshold, min_contiguous, return_activity=False, return_magnitude=False, **kwargs)</code>","text":"<p>Checks whether a cluster responds to a laser stimulus.</p> <p>Args:     cluster (int): The cluster to check.     threshold (float): The amount of activity the cluster needs to go         beyond to be classified as a responder (1.5 = 50% more or less         than the baseline activity).     min_contiguous (int): The number of contiguous samples in the         post-stimulus period for which the cluster needs to be active         beyond the threshold value to be classed as a responder.     return_activity (bool): Whether to return the mean reponse curve.     return_magnitude (int): Whether to return the magnitude of the         response. NB this is either +1 for excited or -1 for inhibited.</p> <p>Returns:     responds (bool): Whether the cell responds or not.     OR     tuple: responds (bool), normed_response_curve (np.ndarray).     OR     tuple: responds (bool), normed_response_curve (np.ndarray),         response_magnitude (np.ndarray).</p> Source code in <code>ephysiopy/common/spikecalcs.py</code> <pre><code>def responds_to_stimulus(\n    self,\n    threshold: float,\n    min_contiguous: int,\n    return_activity: bool = False,\n    return_magnitude: bool = False,\n    **kwargs,\n) -&gt; tuple:\n    \"\"\"\n    Checks whether a cluster responds to a laser stimulus.\n\n    Args:\n        cluster (int): The cluster to check.\n        threshold (float): The amount of activity the cluster needs to go\n            beyond to be classified as a responder (1.5 = 50% more or less\n            than the baseline activity).\n        min_contiguous (int): The number of contiguous samples in the\n            post-stimulus period for which the cluster needs to be active\n            beyond the threshold value to be classed as a responder.\n        return_activity (bool): Whether to return the mean reponse curve.\n        return_magnitude (int): Whether to return the magnitude of the\n            response. NB this is either +1 for excited or -1 for inhibited.\n\n    Returns:\n        responds (bool): Whether the cell responds or not.\n        OR\n        tuple: responds (bool), normed_response_curve (np.ndarray).\n        OR\n        tuple: responds (bool), normed_response_curve (np.ndarray),\n            response_magnitude (np.ndarray).\n    \"\"\"\n    spk_count_by_trial = self.psch(self._secs_per_bin)\n    firing_rate_by_trial = spk_count_by_trial / self.secs_per_bin\n    mean_firing_rate = np.mean(firing_rate_by_trial, 1)\n    # smooth with a moving average\n    # check nothing in kwargs first\n    if \"window_len\" in kwargs.keys():\n        window_len = kwargs[\"window_len\"]\n    else:\n        window_len = 5\n    if \"window\" in kwargs.keys():\n        window = kwargs[\"window\"]\n    else:\n        window = \"flat\"\n    if \"flat\" in window:\n        kernel = Box1DKernel(window_len)\n    if \"gauss\" in window:\n        kernel = Gaussian1DKernel(1, window_len)\n    if \"do_smooth\" in kwargs.keys():\n        do_smooth = kwargs.get(\"do_smooth\")\n    else:\n        do_smooth = True\n\n    if do_smooth:\n        smoothed_binned_spikes = convolve(mean_firing_rate, kernel, boundary=\"wrap\")\n    else:\n        smoothed_binned_spikes = mean_firing_rate\n    nbins = np.floor(np.sum(np.abs(self.event_window)) / self.secs_per_bin)\n    bins = np.linspace(self.event_window[0], self.event_window[1], int(nbins))\n    # normalize all activity by activity in the time before\n    # the laser onset\n    idx = bins &lt; 0\n    normd = min_max_norm(\n        smoothed_binned_spikes,\n        np.min(smoothed_binned_spikes[idx]),\n        np.max(smoothed_binned_spikes[idx]),\n    )\n    # mask the array outside of a threshold value so that\n    # only True values in the masked array are those that\n    # exceed the threshold (positively or negatively)\n    # the threshold provided to this function is expressed\n    # as a % above / below unit normality so adjust that now\n    # so it is expressed as a pre-stimulus firing rate mean\n    # pre_stim_mean = np.mean(smoothed_binned_spikes[idx])\n    # pre_stim_max = pre_stim_mean * threshold\n    # pre_stim_min = pre_stim_mean * (threshold-1.0)\n    # updated so threshold is double (+ or -) the pre-stim\n    # norm (lies between )\n    normd_masked = np.ma.masked_inside(normd, -threshold, 1 + threshold)\n    # find the contiguous runs in the masked array\n    # that are at least as long as the min_contiguous value\n    # and classify this as a True response\n    slices = np.ma.notmasked_contiguous(normd_masked)\n    if slices and np.any(np.isfinite(normd)):\n        # make sure that slices are within the first 25ms post-stim\n        if ~np.any([s.start &gt; 50 and s.start &lt; 75 for s in slices]):\n            if not return_activity:\n                return False\n            else:\n                if return_magnitude:\n                    return False, normd, 0\n                return False, normd\n        max_runlength = max([len(normd_masked[s]) for s in slices])\n        if max_runlength &gt;= min_contiguous:\n            if not return_activity:\n                return True\n            else:\n                if return_magnitude:\n                    sl = [\n                        slc\n                        for slc in slices\n                        if (slc.stop - slc.start) == max_runlength\n                    ]\n                    mag = [-1 if np.mean(normd[sl[0]]) &lt; 0 else 1][0]\n                    return True, normd, mag\n                else:\n                    return True, normd\n    if not return_activity:\n        return False\n    else:\n        if return_magnitude:\n            return False, normd, 0\n        return False, normd\n</code></pre>"},{"location":"reference/#ephysiopy.common.spikecalcs.SpikeCalcsGeneric.smooth_spike_train","title":"<code>smooth_spike_train(npos, sigma=3.0, shuffle=None)</code>","text":"<p>Returns a spike train the same length as num pos samples that has been smoothed in time with a gaussian kernel M in width and standard deviation equal to sigma.</p> <p>Args:     x1 (np.array): The pos indices the spikes occurred at.     npos (int): The number of position samples captured.     sigma (float): The standard deviation of the gaussian used to         smooth the spike train.     shuffle (int, optional): The number of seconds to shift the spike         train by. Default is None.</p> <p>Returns:     smoothed_spikes (np.array): The smoothed spike train.</p> Source code in <code>ephysiopy/common/spikecalcs.py</code> <pre><code>def smooth_spike_train(self, npos, sigma=3.0, shuffle=None):\n    \"\"\"\n    Returns a spike train the same length as num pos samples that has been\n    smoothed in time with a gaussian kernel M in width and standard\n    deviation equal to sigma.\n\n    Args:\n        x1 (np.array): The pos indices the spikes occurred at.\n        npos (int): The number of position samples captured.\n        sigma (float): The standard deviation of the gaussian used to\n            smooth the spike train.\n        shuffle (int, optional): The number of seconds to shift the spike\n            train by. Default is None.\n\n    Returns:\n        smoothed_spikes (np.array): The smoothed spike train.\n    \"\"\"\n    spk_hist = np.bincount(self.spike_times, minlength=npos)\n    if shuffle is not None:\n        spk_hist = np.roll(spk_hist, int(shuffle * 50))\n    # smooth the spk_hist (which is a temporal histogram) with a 250ms\n    # gaussian as with Kropff et al., 2015\n    h = signal.windows.gaussian(13, sigma)\n    h = h / float(np.sum(h))\n    return signal.filtfilt(h.ravel(), 1, spk_hist)\n</code></pre>"},{"location":"reference/#ephysiopy.common.spikecalcs.SpikeCalcsGeneric.theta_band_max_freq","title":"<code>theta_band_max_freq()</code>","text":"<p>Calculates the frequency with the maximum power in the theta band (6-12Hz) of a spike train's autocorrelogram.</p> <p>This function is used to look for differences in theta frequency in different running directions as per Blair. See Welday paper - https://doi.org/10.1523/jneurosci.0712-11.2011</p> <p>Args:     x1 (np.ndarray): The spike train for which the autocorrelogram will be         calculated.</p> <p>Returns:     float: The frequency with the maximum power in the theta band.</p> <p>Raises:     ValueError: If the input spike train is not valid.</p> Source code in <code>ephysiopy/common/spikecalcs.py</code> <pre><code>def theta_band_max_freq(self):\n    \"\"\"\n    Calculates the frequency with the maximum power in the theta band (6-12Hz)\n    of a spike train's autocorrelogram.\n\n    This function is used to look for differences in theta frequency in\n    different running directions as per Blair.\n    See Welday paper - https://doi.org/10.1523/jneurosci.0712-11.2011\n\n    Args:\n        x1 (np.ndarray): The spike train for which the autocorrelogram will be\n            calculated.\n\n    Returns:\n        float: The frequency with the maximum power in the theta band.\n\n    Raises:\n        ValueError: If the input spike train is not valid.\n    \"\"\"\n    ac = self.acorr()\n    # Take the fft of the spike train autocorr (from -500 to +500ms)\n    from scipy.signal import periodogram\n\n    freqs, power = periodogram(ac.binned_data[0], fs=200, return_onesided=True)\n    power_masked = np.ma.MaskedArray(power, np.logical_or(freqs &lt; 6, freqs &gt; 12))\n    return freqs[np.argmax(power_masked)]\n</code></pre>"},{"location":"reference/#ephysiopy.common.spikecalcs.SpikeCalcsGeneric.theta_mod_idx","title":"<code>theta_mod_idx(**kwargs)</code>","text":"<p>Calculates a theta modulation index of a spike train based on the cells autocorrelogram.</p> <p>The difference of the mean power in the theta frequency band (6-11 Hz) and the mean power in the 1-50 Hz frequency band is divided by their sum to give a metric that lives between 0 and 1</p> <p>Args:     x1 (np.array): The spike time-series.</p> <p>Returns:     thetaMod (float): The difference of the values at the first peak     and trough of the autocorrelogram.</p> <p>NB This is a fairly skewed metric with a distribution strongly biased to -1 (although more evenly distributed than theta_mod_idxV2 below)</p> Source code in <code>ephysiopy/common/spikecalcs.py</code> <pre><code>def theta_mod_idx(self, **kwargs) -&gt; float:\n    \"\"\"\n    Calculates a theta modulation index of a spike train based on the cells\n    autocorrelogram.\n\n    The difference of the mean power in the theta frequency band (6-11 Hz) and\n    the mean power in the 1-50 Hz frequency band is divided by their sum to give\n    a metric that lives between 0 and 1\n\n    Args:\n        x1 (np.array): The spike time-series.\n\n    Returns:\n        thetaMod (float): The difference of the values at the first peak\n        and trough of the autocorrelogram.\n\n    NB This is a fairly skewed metric with a distribution strongly biased\n    to -1 (although more evenly distributed than theta_mod_idxV2 below)\n    \"\"\"\n    ac = self.acorr(**kwargs)\n    # Take the fft of the spike train autocorr (from -500 to +500ms)\n    from scipy.signal import periodogram\n\n    fs = 1.0 / kwargs.get(\"binsize\", 0.0001)\n    freqs, power = periodogram(ac.binned_data[0], fs=fs, return_onesided=True)\n    # Smooth the power over +/- 1Hz when fs=200\n    b = signal.windows.boxcar(3)  # another filter type - blackman?\n    h = signal.filtfilt(b, 3, power)\n\n    # Square the amplitude first to get power\n    sqd_amp = h**2\n    # Get the mean theta band power - mtbp\n    mtbp = np.mean(sqd_amp[np.logical_and(freqs &gt;= 6, freqs &lt;= 11)])\n    # Find the mean amplitude in the 1-50Hz range\n    # Get the mean in the other band - mobp\n    mobp = np.mean(sqd_amp[np.logical_and(freqs &gt; 2, freqs &lt; 50)])\n    # Find the ratio of these two - this is the theta modulation index\n    return float((mtbp - mobp) / (mtbp + mobp))\n</code></pre>"},{"location":"reference/#ephysiopy.common.spikecalcs.SpikeCalcsGeneric.theta_mod_idxV2","title":"<code>theta_mod_idxV2()</code>","text":"<p>This is a simpler alternative to the theta_mod_idx method in that it calculates the difference between the normalized temporal autocorrelogram at the trough between 50-70ms and the peak between 100-140ms over their sum (data is binned into 5ms bins)</p> <p>Measure used in Cacucci et al., 2004 and Kropff et al 2015</p> Source code in <code>ephysiopy/common/spikecalcs.py</code> <pre><code>def theta_mod_idxV2(self) -&gt; float:\n    \"\"\"\n    This is a simpler alternative to the theta_mod_idx method in that it\n    calculates the difference between the normalized temporal\n    autocorrelogram at the trough between 50-70ms and the\n    peak between 100-140ms over their sum (data is binned into 5ms bins)\n\n    Measure used in Cacucci et al., 2004 and Kropff et al 2015\n    \"\"\"\n    ac = self.acorr()\n    bins = ac.bin_edges[0]\n    corr = ac.binned_data[0]\n    # 'close' the right-hand bin\n    bins = bins[0:-1]\n    # normalise corr so max is 1.0\n    corr = corr / float(np.max(corr))\n    thetaAntiPhase = np.min(\n        corr[np.logical_and(bins &gt; 50 / 1000.0, bins &lt; 70 / 1000.0)]\n    )\n    thetaPhase = np.max(\n        corr[np.logical_and(bins &gt; 100 / 1000.0, bins &lt; 140 / 1000.0)]\n    )\n    return float((thetaPhase - thetaAntiPhase) / (thetaPhase + thetaAntiPhase))\n</code></pre>"},{"location":"reference/#ephysiopy.common.spikecalcs.SpikeCalcsGeneric.theta_mod_idxV3","title":"<code>theta_mod_idxV3(**kwargs)</code>","text":"<p>Another theta modulation index score this time based on the method used by Kornienko et al., (2024) (Kevin Allens lab) see https://doi.org/10.7554/eLife.35949.001</p> <p>Basically uses the binned spike train instead of the autocorrelogram as the input to the periodogram function (they use pwelch in R; periodogram is a simplified call to welch in scipy.signal)</p> <p>The resulting metric is similar to the one in theta_mod_idx above except that the frequency bands compared to the theta band are narrower and exclusive of the theta band</p> <p>Produces a fairly normally distributed looking score with a mean and median pretty close to 0</p> Source code in <code>ephysiopy/common/spikecalcs.py</code> <pre><code>def theta_mod_idxV3(self, **kwargs) -&gt; float:\n    \"\"\"\n    Another theta modulation index score this time based on the method used\n    by Kornienko et al., (2024) (Kevin Allens lab)\n    see https://doi.org/10.7554/eLife.35949.001\n\n    Basically uses the binned spike train instead of the autocorrelogram as\n    the input to the periodogram function (they use pwelch in R; periodogram is a\n    simplified call to welch in scipy.signal)\n\n    The resulting metric is similar to the one in theta_mod_idx above except\n    that the frequency bands compared to the theta band are narrower and\n    exclusive of the theta band\n\n    Produces a fairly normally distributed looking score with a mean and median\n    pretty close to 0\n    \"\"\"\n    freqs, power = self.get_ifr_power_spectrum(**kwargs)\n    # smooth with a boxcar filter with a 0.5Hz window\n    win_len = np.count_nonzero(np.logical_and(freqs &gt;= 0, freqs &lt;= 0.5))\n    w = signal.windows.boxcar(win_len)\n    b = signal.filtfilt(w, 1, power)\n    sqd_amp = b**2\n    mtbp = np.mean(sqd_amp[np.logical_and(freqs &gt;= 6, freqs &lt;= 10)])\n    mobp = np.mean(\n        sqd_amp[\n            np.logical_or(\n                np.logical_and(freqs &gt; 3, freqs &lt; 5),\n                np.logical_and(freqs &gt; 11, freqs &lt; 13),\n            )\n        ]\n    )\n    return float((mtbp - mobp) / (mtbp + mobp))\n</code></pre>"},{"location":"reference/#ephysiopy.common.spikecalcs.SpikeCalcsGeneric.update_KSMeta","title":"<code>update_KSMeta(value)</code>","text":"<p>Takes in a TemplateModel instance from a phy session and parses out the relevant metrics for the cluster and places into the namedtuple KSMeta</p> Source code in <code>ephysiopy/common/spikecalcs.py</code> <pre><code>def update_KSMeta(self, value: dict) -&gt; None:\n    \"\"\"\n    Takes in a TemplateModel instance from a phy session and\n    parses out the relevant metrics for the cluster and places\n    into the namedtuple KSMeta\n    \"\"\"\n    metavals = []\n    for f in KSMetaTuple._fields:\n        if f in value.keys():\n            if self.cluster in value[f].keys():\n                metavals.append(value[f][self.cluster])\n            else:\n                metavals.append(None)\n        else:\n            metavals.append(None)\n    self._ksmeta = KSMetaTuple(*metavals)\n</code></pre>"},{"location":"reference/#ephysiopy.common.spikecalcs.SpikeCalcsOpenEphys","title":"<code>SpikeCalcsOpenEphys</code>","text":"<p>               Bases: <code>SpikeCalcsGeneric</code></p> Source code in <code>ephysiopy/common/spikecalcs.py</code> <pre><code>class SpikeCalcsOpenEphys(SpikeCalcsGeneric):\n    def __init__(self, spike_times, cluster, waveforms=None, **kwargs):\n        super().__init__(spike_times, cluster, waveforms, **kwargs)\n        self.n_samples = [-40, 41]\n        self.TemplateModel = None\n\n    def get_waveforms(\n        self,\n        cluster: int,\n        cluster_data: KiloSortSession,\n        n_waveforms: int = 2000,\n        n_channels: int = 64,\n        channel_range=None,\n        **kwargs,\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Returns waveforms for a cluster.\n\n        Args:\n            cluster (int): The cluster to return the waveforms for.\n            cluster_data (KiloSortSession): The KiloSortSession object for the\n                session that contains the cluster.\n            n_waveforms (int, optional): The number of waveforms to return.\n                Defaults to 2000.\n            n_channels (int, optional): The number of channels in the\n                recording. Defaults to 64.\n        \"\"\"\n        # instantiate the TemplateModel - this is used to get the waveforms\n        # for the cluster. TemplateModel encapsulates the results of KiloSort\n        if self.TemplateModel is None:\n            self.TemplateModel = TemplateModel(\n                dir_path=os.path.join(cluster_data.fname_root),\n                sample_rate=3e4,\n                dat_path=os.path.join(cluster_data.fname_root, \"continuous.dat\"),\n                n_channels_dat=n_channels,\n            )\n        # get the waveforms for the given cluster on the best channel only\n        waveforms = self.TemplateModel.get_cluster_spike_waveforms(cluster)\n        # get a random subset of the waveforms\n        rng = np.random.default_rng()\n        total_waves = waveforms.shape[0]\n        n_waveforms = n_waveforms if n_waveforms &lt; total_waves else total_waves\n        waveforms_subset = rng.choice(waveforms, n_waveforms)\n        # return the waveforms\n        if channel_range is None:\n            return np.squeeze(waveforms_subset[:, :, 0])\n        else:\n            if isinstance(channel_range, Sequence):\n                return np.squeeze(waveforms_subset[:, :, channel_range])\n            else:\n                warnings.warn(\"Invalid channel_range sequence\")\n\n    def get_channel_depth_from_templates(self, pname: Path):\n        \"\"\"\n        Determine depth of template as well as closest channel. Adopted from\n        'templatePositionsAmplitudes' by N. Steinmetz\n        (https://github.com/cortex-lab/spikes)\n        \"\"\"\n        # Load inverse whitening matrix\n        Winv = np.load(os.path.join(pname, \"whitening_mat_inv.npy\"))\n        # Load templates\n        templates = np.load(os.path.join(pname, \"templates.npy\"))\n        # Load channel_map and positions\n        channel_map = np.load(os.path.join(pname, \"channel_map.npy\"))\n        channel_positions = np.load(os.path.join(pname, \"channel_positions.npy\"))\n        map_and_pos = np.array([np.squeeze(channel_map), channel_positions[:, 1]])\n        # unwhiten all the templates\n        tempsUnW = np.zeros(np.shape(templates))\n        for i in np.shape(templates)[0]:\n            tempsUnW[i, :, :] = np.squeeze(templates[i, :, :]) @ Winv\n\n        tempAmp = np.squeeze(np.max(tempsUnW, 1)) - np.squeeze(np.min(tempsUnW, 1))\n        tempAmpsUnscaled = np.max(tempAmp, 1)\n        # need to zero-out the potentially-many low values on distant channels\n        threshVals = tempAmpsUnscaled * 0.3\n        tempAmp[tempAmp &lt; threshVals[:, None]] = 0\n        # Compute the depth as a centre of mass\n        templateDepths = np.sum(tempAmp * map_and_pos[1, :], -1) / np.sum(tempAmp, 1)\n        maxChanIdx = np.argmin(\n            np.abs((templateDepths[:, None] - map_and_pos[1, :].T)), 1\n        )\n        return templateDepths, maxChanIdx\n\n    def get_template_id_for_cluster(self, pname: Path, cluster: int):\n        \"\"\"\n        Determine the best channel (one with highest amplitude spikes)\n        for a given cluster.\n        \"\"\"\n        spike_templates = np.load(os.path.join(pname, \"spike_templates.npy\"))\n        spike_times = np.load(os.path.join(pname, \"spike_times.npy\"))\n        spike_clusters = np.load(os.path.join(pname, \"spike_clusters.npy\"))\n        cluster_times = spike_times[spike_clusters == cluster]\n        rez_mat = h5py.File(os.path.join(pname, \"rez.mat\"), \"r\")\n        st3 = rez_mat[\"rez\"][\"st3\"]\n        st_spike_times = st3[0, :]\n        idx = np.searchsorted(st_spike_times, cluster_times)\n        template_idx, counts = np.unique(spike_templates[idx], return_counts=True)\n        ind = np.argmax(counts)\n        return template_idx[ind]\n</code></pre>"},{"location":"reference/#ephysiopy.common.spikecalcs.SpikeCalcsOpenEphys.get_channel_depth_from_templates","title":"<code>get_channel_depth_from_templates(pname)</code>","text":"<p>Determine depth of template as well as closest channel. Adopted from 'templatePositionsAmplitudes' by N. Steinmetz (https://github.com/cortex-lab/spikes)</p> Source code in <code>ephysiopy/common/spikecalcs.py</code> <pre><code>def get_channel_depth_from_templates(self, pname: Path):\n    \"\"\"\n    Determine depth of template as well as closest channel. Adopted from\n    'templatePositionsAmplitudes' by N. Steinmetz\n    (https://github.com/cortex-lab/spikes)\n    \"\"\"\n    # Load inverse whitening matrix\n    Winv = np.load(os.path.join(pname, \"whitening_mat_inv.npy\"))\n    # Load templates\n    templates = np.load(os.path.join(pname, \"templates.npy\"))\n    # Load channel_map and positions\n    channel_map = np.load(os.path.join(pname, \"channel_map.npy\"))\n    channel_positions = np.load(os.path.join(pname, \"channel_positions.npy\"))\n    map_and_pos = np.array([np.squeeze(channel_map), channel_positions[:, 1]])\n    # unwhiten all the templates\n    tempsUnW = np.zeros(np.shape(templates))\n    for i in np.shape(templates)[0]:\n        tempsUnW[i, :, :] = np.squeeze(templates[i, :, :]) @ Winv\n\n    tempAmp = np.squeeze(np.max(tempsUnW, 1)) - np.squeeze(np.min(tempsUnW, 1))\n    tempAmpsUnscaled = np.max(tempAmp, 1)\n    # need to zero-out the potentially-many low values on distant channels\n    threshVals = tempAmpsUnscaled * 0.3\n    tempAmp[tempAmp &lt; threshVals[:, None]] = 0\n    # Compute the depth as a centre of mass\n    templateDepths = np.sum(tempAmp * map_and_pos[1, :], -1) / np.sum(tempAmp, 1)\n    maxChanIdx = np.argmin(\n        np.abs((templateDepths[:, None] - map_and_pos[1, :].T)), 1\n    )\n    return templateDepths, maxChanIdx\n</code></pre>"},{"location":"reference/#ephysiopy.common.spikecalcs.SpikeCalcsOpenEphys.get_template_id_for_cluster","title":"<code>get_template_id_for_cluster(pname, cluster)</code>","text":"<p>Determine the best channel (one with highest amplitude spikes) for a given cluster.</p> Source code in <code>ephysiopy/common/spikecalcs.py</code> <pre><code>def get_template_id_for_cluster(self, pname: Path, cluster: int):\n    \"\"\"\n    Determine the best channel (one with highest amplitude spikes)\n    for a given cluster.\n    \"\"\"\n    spike_templates = np.load(os.path.join(pname, \"spike_templates.npy\"))\n    spike_times = np.load(os.path.join(pname, \"spike_times.npy\"))\n    spike_clusters = np.load(os.path.join(pname, \"spike_clusters.npy\"))\n    cluster_times = spike_times[spike_clusters == cluster]\n    rez_mat = h5py.File(os.path.join(pname, \"rez.mat\"), \"r\")\n    st3 = rez_mat[\"rez\"][\"st3\"]\n    st_spike_times = st3[0, :]\n    idx = np.searchsorted(st_spike_times, cluster_times)\n    template_idx, counts = np.unique(spike_templates[idx], return_counts=True)\n    ind = np.argmax(counts)\n    return template_idx[ind]\n</code></pre>"},{"location":"reference/#ephysiopy.common.spikecalcs.SpikeCalcsOpenEphys.get_waveforms","title":"<code>get_waveforms(cluster, cluster_data, n_waveforms=2000, n_channels=64, channel_range=None, **kwargs)</code>","text":"<p>Returns waveforms for a cluster.</p> <p>Args:     cluster (int): The cluster to return the waveforms for.     cluster_data (KiloSortSession): The KiloSortSession object for the         session that contains the cluster.     n_waveforms (int, optional): The number of waveforms to return.         Defaults to 2000.     n_channels (int, optional): The number of channels in the         recording. Defaults to 64.</p> Source code in <code>ephysiopy/common/spikecalcs.py</code> <pre><code>def get_waveforms(\n    self,\n    cluster: int,\n    cluster_data: KiloSortSession,\n    n_waveforms: int = 2000,\n    n_channels: int = 64,\n    channel_range=None,\n    **kwargs,\n) -&gt; np.ndarray:\n    \"\"\"\n    Returns waveforms for a cluster.\n\n    Args:\n        cluster (int): The cluster to return the waveforms for.\n        cluster_data (KiloSortSession): The KiloSortSession object for the\n            session that contains the cluster.\n        n_waveforms (int, optional): The number of waveforms to return.\n            Defaults to 2000.\n        n_channels (int, optional): The number of channels in the\n            recording. Defaults to 64.\n    \"\"\"\n    # instantiate the TemplateModel - this is used to get the waveforms\n    # for the cluster. TemplateModel encapsulates the results of KiloSort\n    if self.TemplateModel is None:\n        self.TemplateModel = TemplateModel(\n            dir_path=os.path.join(cluster_data.fname_root),\n            sample_rate=3e4,\n            dat_path=os.path.join(cluster_data.fname_root, \"continuous.dat\"),\n            n_channels_dat=n_channels,\n        )\n    # get the waveforms for the given cluster on the best channel only\n    waveforms = self.TemplateModel.get_cluster_spike_waveforms(cluster)\n    # get a random subset of the waveforms\n    rng = np.random.default_rng()\n    total_waves = waveforms.shape[0]\n    n_waveforms = n_waveforms if n_waveforms &lt; total_waves else total_waves\n    waveforms_subset = rng.choice(waveforms, n_waveforms)\n    # return the waveforms\n    if channel_range is None:\n        return np.squeeze(waveforms_subset[:, :, 0])\n    else:\n        if isinstance(channel_range, Sequence):\n            return np.squeeze(waveforms_subset[:, :, channel_range])\n        else:\n            warnings.warn(\"Invalid channel_range sequence\")\n</code></pre>"},{"location":"reference/#ephysiopy.common.spikecalcs.SpikeCalcsProbe","title":"<code>SpikeCalcsProbe</code>","text":"<p>               Bases: <code>SpikeCalcsGeneric</code></p> <p>Encapsulates methods specific to probe-based recordings</p> Source code in <code>ephysiopy/common/spikecalcs.py</code> <pre><code>class SpikeCalcsProbe(SpikeCalcsGeneric):\n    \"\"\"\n    Encapsulates methods specific to probe-based recordings\n    \"\"\"\n\n    def __init__(self):\n        pass\n</code></pre>"},{"location":"reference/#ephysiopy.common.spikecalcs.cluster_quality","title":"<code>cluster_quality(waveforms=None, spike_clusters=None, cluster_id=None, fet=1)</code>","text":"<p>Returns the L-ratio and Isolation Distance measures calculated on the principal components of the energy in a spike matrix.</p> <p>Args:     waveforms (np.ndarray, optional): The waveforms to be processed.         If None, the function will return None.     spike_clusters (np.ndarray, optional): The spike clusters to be         processed.     cluster_id (int, optional): The ID of the cluster to be processed.     fet (int, default=1): The feature to be used in the PCA calculation.</p> <p>Returns:     tuple: A tuple containing the L-ratio and Isolation Distance of the         cluster.</p> <p>Raises:     Exception: If an error occurs during the calculation of the L-ratio or         Isolation Distance.</p> Source code in <code>ephysiopy/common/spikecalcs.py</code> <pre><code>def cluster_quality(\n    waveforms: np.ndarray = None,\n    spike_clusters: np.ndarray = None,\n    cluster_id: int = None,\n    fet: int = 1,\n):\n    \"\"\"\n    Returns the L-ratio and Isolation Distance measures calculated\n    on the principal components of the energy in a spike matrix.\n\n    Args:\n        waveforms (np.ndarray, optional): The waveforms to be processed.\n            If None, the function will return None.\n        spike_clusters (np.ndarray, optional): The spike clusters to be\n            processed.\n        cluster_id (int, optional): The ID of the cluster to be processed.\n        fet (int, default=1): The feature to be used in the PCA calculation.\n\n    Returns:\n        tuple: A tuple containing the L-ratio and Isolation Distance of the\n            cluster.\n\n    Raises:\n        Exception: If an error occurs during the calculation of the L-ratio or\n            Isolation Distance.\n    \"\"\"\n    if waveforms is None:\n        return None\n    nSpikes, nElectrodes, _ = waveforms.shape\n    wvs = waveforms.copy()\n    E = np.sqrt(np.nansum(waveforms**2, axis=2))\n    zeroIdx = np.sum(E, 0) == [0, 0, 0, 0]\n    E = E[:, ~zeroIdx]\n    wvs = wvs[:, ~zeroIdx, :]\n    normdWaves = (wvs.T / E.T).T\n    PCA_m = get_param(normdWaves, \"PCA\", fet=fet)\n    badIdx = np.sum(PCA_m, axis=0) == 0\n    PCA_m = PCA_m[:, ~badIdx]\n    # get mahalanobis distance\n    idx = spike_clusters == cluster_id\n    nClustSpikes = np.count_nonzero(idx)\n    try:\n        d = mahal(PCA_m, PCA_m[idx, :])\n        # get the indices of the spikes not in the cluster\n        M_noise = d[~idx]\n        df = np.prod((fet, nElectrodes))\n        from scipy import stats\n\n        L = np.sum(1 - stats.chi2.cdf(M_noise, df))\n        L_ratio = L / nClustSpikes\n        # calculate isolation distance\n        if nClustSpikes &lt; nSpikes / 2:\n            M_noise.sort()\n            isolation_dist = M_noise[nClustSpikes]\n        else:\n            isolation_dist = np.nan\n    except Exception:\n        isolation_dist = L_ratio = np.nan\n    return L_ratio, isolation_dist\n</code></pre>"},{"location":"reference/#ephysiopy.common.spikecalcs.contamination_percent","title":"<code>contamination_percent(x1, x2=None, **kwargs)</code>","text":"<p>Computes the cross-correlogram between two sets of spikes and estimates how refractory the cross-correlogram is.</p> <p>Args:     st1 (np.array): The first set of spikes.     st2 (np.array): The second set of spikes.</p> <p>kwargs:     Anything that can be fed into xcorr above</p> <p>Returns:     Q (float): a measure of refractoriness     R (float): a second measure of refractoriness             (kicks in for very low firing rates)</p> <p>Notes:     Taken from KiloSorts ccg.m</p> <pre><code>The contamination metrics are calculated based on\nan analysis of the 'shoulders' of the cross-correlogram.\nSpecifically, the spike counts in the ranges +/-5-25ms and\n+/-250-500ms are compared for refractoriness\n</code></pre> Source code in <code>ephysiopy/common/spikecalcs.py</code> <pre><code>def contamination_percent(\n    x1: np.ndarray, x2: np.ndarray | None = None, **kwargs\n) -&gt; tuple:\n    \"\"\"\n    Computes the cross-correlogram between two sets of spikes and\n    estimates how refractory the cross-correlogram is.\n\n    Args:\n        st1 (np.array): The first set of spikes.\n        st2 (np.array): The second set of spikes.\n\n    kwargs:\n        Anything that can be fed into xcorr above\n\n    Returns:\n        Q (float): a measure of refractoriness\n        R (float): a second measure of refractoriness\n                (kicks in for very low firing rates)\n\n    Notes:\n        Taken from KiloSorts ccg.m\n\n        The contamination metrics are calculated based on\n        an analysis of the 'shoulders' of the cross-correlogram.\n        Specifically, the spike counts in the ranges +/-5-25ms and\n        +/-250-500ms are compared for refractoriness\n    \"\"\"\n    if x2 is None:\n        x2 = x1.copy()\n    xc = xcorr(x1, x2, **kwargs)\n    b = xc.bin_edges[0]\n    c = xc.binned_data[0]\n    left = [[-0.05, -0.01]]\n    right = [[0.01, 0.051]]\n    far = [[-0.5, -0.249], [0.25, 0.501]]\n\n    def get_shoulder(bins, vals):\n        all = np.array([np.logical_and(bins &gt;= i[0], bins &lt; i[1]) for i in vals])\n        return np.any(all, 0)\n\n    inner_left = get_shoulder(b, left)\n    inner_right = get_shoulder(b, right)\n    outer = get_shoulder(b, far)\n\n    tbin = 1000\n    Tr = max(np.concatenate([x1, x2])) - min(np.concatenate([x1, x2]))\n\n    def get_normd_shoulder(idx):\n        return np.sum(c[idx[:-1]]) / (\n            len(np.nonzero(idx)[0]) * tbin * len(x1) * len(x2) / Tr\n        )\n\n    Q00 = get_normd_shoulder(outer)\n    Q01 = max(get_normd_shoulder(inner_left), get_normd_shoulder(inner_right))\n\n    R00 = max(\n        np.mean(c[outer[:-1]]), np.mean(c[inner_left[:-1]]), np.mean(c[inner_right[1:]])\n    )\n\n    middle_idx = np.nonzero(b == 0)[0]\n    a = c[middle_idx]\n    c[middle_idx] = 0\n    Qi = np.zeros(10)\n    Ri = np.zeros(10)\n    # enumerate through the central range of the xcorr\n    # saving the same calculation as done above\n    for i, t in enumerate(np.linspace(0.001, 0.01, 10)):\n        irange = [[-t, t]]\n        chunk = get_shoulder(b, irange)\n        # compute the same normalized ratio as above;\n        # this should be 1 if there is no refractoriness\n        Qi[i] = get_normd_shoulder(chunk)  # save the normd prob\n        n = np.sum(c[chunk[:-1]]) / 2\n        lam = R00 * i\n        # this is tricky: we approximate the Poisson likelihood with a\n        # gaussian of equal mean and variance\n        # that allows us to integrate the probability that we would see &lt;N\n        # spikes in the center of the\n        # cross-correlogram from a distribution with mean R00*i spikes\n        p = 1 / 2 * (1 + erf((n - lam) / np.sqrt(2 * lam)))\n\n        Ri[i] = p  # keep track of p for each bin size i\n\n    c[middle_idx] = a  # restore the center value of the cross-correlogram\n    return c, Qi, Q00, Q01, Ri\n</code></pre>"},{"location":"reference/#ephysiopy.common.spikecalcs.fit_smoothed_curve_to_xcorr","title":"<code>fit_smoothed_curve_to_xcorr(xc, **kwargs)</code>","text":"<p>Idea is to smooth out the result of an auto- or cross-correlogram with a view to correlating the result with another auto- or cross-correlogram to see how similar two of these things are.</p> Source code in <code>ephysiopy/common/spikecalcs.py</code> <pre><code>def fit_smoothed_curve_to_xcorr(xc: BinnedData, **kwargs) -&gt; BinnedData:\n    \"\"\"\n    Idea is to smooth out the result of an auto- or cross-correlogram with\n    a view to correlating the result with another auto- or cross-correlogram\n    to see how similar two of these things are.\n\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/#ephysiopy.common.spikecalcs.get_param","title":"<code>get_param(waveforms, param='Amp', t=200, fet=1)</code>","text":"<p>Returns the requested parameter from a spike train as a numpy array</p> <p>Args:     waveforms (numpy array): Shape of array can be nSpikes x nSamples         OR         a nSpikes x nElectrodes x nSamples     param (str): Valid values are:         'Amp' - peak-to-trough amplitude (default)         'P' - height of peak         'T' - depth of trough         'Vt' height at time t         'tP' - time of peak (in seconds)         'tT' - time of trough (in seconds)         'PCA' - first n fet principal components (defaults to 1)     t (int): The time used for Vt     fet (int): The number of principal components         (use with param 'PCA')</p> Source code in <code>ephysiopy/common/spikecalcs.py</code> <pre><code>def get_param(waveforms, param=\"Amp\", t=200, fet=1) -&gt; np.ndarray:\n    \"\"\"\n    Returns the requested parameter from a spike train as a numpy array\n\n    Args:\n        waveforms (numpy array): Shape of array can be nSpikes x nSamples\n            OR\n            a nSpikes x nElectrodes x nSamples\n        param (str): Valid values are:\n            'Amp' - peak-to-trough amplitude (default)\n            'P' - height of peak\n            'T' - depth of trough\n            'Vt' height at time t\n            'tP' - time of peak (in seconds)\n            'tT' - time of trough (in seconds)\n            'PCA' - first n fet principal components (defaults to 1)\n        t (int): The time used for Vt\n        fet (int): The number of principal components\n            (use with param 'PCA')\n    \"\"\"\n    from scipy import interpolate\n    from sklearn.decomposition import PCA\n\n    if param == \"Amp\":\n        return np.ptp(waveforms + 128, axis=-1)\n    elif param == \"P\":\n        return np.max(waveforms, axis=-1)\n    elif param == \"T\":\n        return np.min(waveforms, axis=-1)\n    elif param == \"Vt\":\n        times = np.arange(0, 1000, 20)\n        f = interpolate.interp1d(times, range(50), \"nearest\")\n        if waveforms.ndim == 2:\n            return waveforms[:, int(f(t))]\n        elif waveforms.ndim == 3:\n            return waveforms[:, :, int(f(t))]\n    elif param == \"tP\":\n        idx = np.argmax(waveforms, axis=-1)\n        m = interpolate.interp1d([0, waveforms.shape[-1] - 1], [0, 1 / 1000.0])\n        return m(idx)\n    elif param == \"tT\":\n        idx = np.argmin(waveforms, axis=-1)\n        m = interpolate.interp1d([0, waveforms.shape[-1] - 1], [0, 1 / 1000.0])\n        return m(idx)\n    elif param == \"PCA\":\n        pca = PCA(n_components=fet)\n        if waveforms.ndim == 2:\n            return pca.fit(waveforms).transform(waveforms).squeeze()\n        elif waveforms.ndim == 3:\n            out = np.zeros((waveforms.shape[0], waveforms.shape[1] * fet))\n            st = np.arange(0, waveforms.shape[1] * fet, fet)\n            en = np.arange(fet, fet + (waveforms.shape[1] * fet), fet)\n            rng = np.vstack((st, en))\n            for i in range(waveforms.shape[1]):\n                if ~np.any(np.isnan(waveforms[:, i, :])):\n                    A = np.squeeze(\n                        pca.fit(waveforms[:, i, :].squeeze()).transform(\n                            waveforms[:, i, :].squeeze()\n                        )\n                    )\n                    if A.ndim &lt; 2:\n                        out[:, rng[0, i] : rng[1, i]] = np.atleast_2d(A).T\n                    else:\n                        out[:, rng[0, i] : rng[1, i]] = A\n            return out\n</code></pre>"},{"location":"reference/#ephysiopy.common.spikecalcs.mahal","title":"<code>mahal(u, v)</code>","text":"<p>Returns the L-ratio and Isolation Distance measures calculated on the principal components of the energy in a spike matrix.</p> <p>Args:     waveforms (np.ndarray, optional): The waveforms to be processed. If         None, the function will return None.     spike_clusters (np.ndarray, optional): The spike clusters to be         processed.     cluster_id (int, optional): The ID of the cluster to be processed.     fet (int, default=1): The feature to be used in the PCA calculation.</p> <p>Returns:     tuple: A tuple containing the L-ratio and Isolation Distance of the         cluster.</p> <p>Raises:     Exception: If an error occurs during the calculation of the L-ratio or         Isolation Distance.</p> Source code in <code>ephysiopy/common/spikecalcs.py</code> <pre><code>def mahal(u, v):\n    \"\"\"\n    Returns the L-ratio and Isolation Distance measures calculated on the\n    principal components of the energy in a spike matrix.\n\n    Args:\n        waveforms (np.ndarray, optional): The waveforms to be processed. If\n            None, the function will return None.\n        spike_clusters (np.ndarray, optional): The spike clusters to be\n            processed.\n        cluster_id (int, optional): The ID of the cluster to be processed.\n        fet (int, default=1): The feature to be used in the PCA calculation.\n\n    Returns:\n        tuple: A tuple containing the L-ratio and Isolation Distance of the\n            cluster.\n\n    Raises:\n        Exception: If an error occurs during the calculation of the L-ratio or\n            Isolation Distance.\n    \"\"\"\n    u_sz = u.shape\n    v_sz = v.shape\n    if u_sz[1] != v_sz[1]:\n        warnings.warn(\n            \"Input size mismatch: \\\n                        matrices must have same num of columns\"\n        )\n    if v_sz[0] &lt; v_sz[1]:\n        warnings.warn(\"Too few rows: v must have more rows than columns\")\n    if np.any(np.imag(u)) or np.any(np.imag(v)):\n        warnings.warn(\"No complex inputs are allowed\")\n    m = np.nanmean(v, axis=0)\n    M = np.tile(m, reps=(u_sz[0], 1))\n    C = v - np.tile(m, reps=(v_sz[0], 1))\n    _, R = np.linalg.qr(C)\n    ri = np.linalg.solve(R.T, (u - M).T)\n    d = np.sum(ri * ri, 0).T * (v_sz[0] - 1)\n    return d\n</code></pre>"},{"location":"reference/#ephysiopy.common.spikecalcs.xcorr","title":"<code>xcorr(x1, x2=None, Trange=np.array([-0.5, 0.5]), binsize=0.001, normed=False, **kwargs)</code>","text":"<p>Calculates the ISIs in x1 or x1 vs x2 within a given range</p> <p>Args:     x1, x2 (array_like): The times of the spikes emitted by the                         cluster(s) in seconds     Trange (array_like): Range of times to bin up in seconds                             Defaults to [-0.5, +0.5]     binsize (float): The size of the bins in seconds     normed (bool): Whether to divide the counts by the total                     number of spikes to give a probabilty     **kwargs - just there to suck up spare parameters</p> <p>Returns:     BinnedData: A BinnedData object containing the binned data and the                 bin edges</p> Source code in <code>ephysiopy/common/spikecalcs.py</code> <pre><code>def xcorr(\n    x1: np.ndarray,\n    x2: np.ndarray | None = None,\n    Trange: np.ndarray | list = np.array([-0.5, 0.5]),\n    binsize: float = 0.001,\n    normed=False,\n    **kwargs,\n) -&gt; BinnedData:\n    \"\"\"\n    Calculates the ISIs in x1 or x1 vs x2 within a given range\n\n    Args:\n        x1, x2 (array_like): The times of the spikes emitted by the\n                            cluster(s) in seconds\n        Trange (array_like): Range of times to bin up in seconds\n                                Defaults to [-0.5, +0.5]\n        binsize (float): The size of the bins in seconds\n        normed (bool): Whether to divide the counts by the total\n                        number of spikes to give a probabilty\n        **kwargs - just there to suck up spare parameters\n\n    Returns:\n        BinnedData: A BinnedData object containing the binned data and the\n                    bin edges\n    \"\"\"\n    if x2 is None:\n        x2 = x1.copy()\n    if isinstance(Trange, list):\n        Trange = np.array(Trange)\n    y = []\n    irange = x2[:, np.newaxis] + Trange[np.newaxis, :]\n    dts = np.searchsorted(x1, irange)\n    for i, t in enumerate(dts):\n        y.extend((x1[t[0] : t[1]] - x2[i]))\n    y = np.array(y, dtype=float)\n    counts, bins = np.histogram(\n        y[y != 0], bins=int(np.ptp(Trange) / binsize) + 1, range=(Trange[0], Trange[1])\n    )\n    if normed:\n        counts = counts / len(x1)\n    return BinnedData(\n        variable=VariableToBin.TIME,\n        map_type=MapType.SPK,\n        binned_data=[counts],\n        bin_edges=[bins],\n    )\n</code></pre>"},{"location":"reference/#statistics","title":"Statistics","text":""},{"location":"reference/#ephysiopy.common.statscalcs.V_test","title":"<code>V_test(angles, test_direction)</code>","text":"<p>The Watson U2 tests whether the observed angles have a tendency to cluster around a given angle indicating a lack of randomness in the distribution. Also known as the modified Rayleigh test.</p> <p>Args:     angles (array_like): Vector of angular values in degrees.     test_direction (int): A single angular value in degrees.</p> <p>Notes:     For grouped data the length of the mean vector must be adjusted,     and for axial data all angles must be doubled.</p> Source code in <code>ephysiopy/common/statscalcs.py</code> <pre><code>def V_test(angles, test_direction):\n    \"\"\"\n    The Watson U2 tests whether the observed angles have a tendency to\n    cluster around a given angle indicating a lack of randomness in the\n    distribution. Also known as the modified Rayleigh test.\n\n    Args:\n        angles (array_like): Vector of angular values in degrees.\n        test_direction (int): A single angular value in degrees.\n\n    Notes:\n        For grouped data the length of the mean vector must be adjusted,\n        and for axial data all angles must be doubled.\n    \"\"\"\n    n = len(angles)\n    x_hat = np.sum(np.cos(np.radians(angles))) / float(n)\n    y_hat = np.sum(np.sin(np.radians(angles))) / float(n)\n    r = np.sqrt(x_hat**2 + y_hat**2)\n    theta_hat = np.degrees(np.arctan(y_hat / x_hat))\n    v_squiggle = r * np.cos(np.radians(theta_hat) - np.radians(test_direction))\n    V = np.sqrt(2 * n) * v_squiggle\n    return V\n</code></pre>"},{"location":"reference/#ephysiopy.common.statscalcs.circ_r","title":"<code>circ_r(alpha, w=None, d=0, axis=0)</code>","text":"<p>Computes the mean resultant vector length for circular data.</p> <p>Args:     alpha (array or list): Sample of angles in radians.     w (array or list): Counts in the case of binned data.         Must be same length as alpha.     d (array or list, optional): Spacing of bin centres for binned data; if         supplied, correction factor is used to correct for bias in         estimation of r, in radians.     axis (int, optional): The dimension along which to compute.         Default is 0.</p> <p>Returns:     r (float): The mean resultant vector length.</p> Source code in <code>ephysiopy/common/statscalcs.py</code> <pre><code>def circ_r(alpha, w=None, d=0, axis=0):\n    \"\"\"\n    Computes the mean resultant vector length for circular data.\n\n    Args:\n        alpha (array or list): Sample of angles in radians.\n        w (array or list): Counts in the case of binned data.\n            Must be same length as alpha.\n        d (array or list, optional): Spacing of bin centres for binned data; if\n            supplied, correction factor is used to correct for bias in\n            estimation of r, in radians.\n        axis (int, optional): The dimension along which to compute.\n            Default is 0.\n\n    Returns:\n        r (float): The mean resultant vector length.\n    \"\"\"\n\n    if w is None:\n        w = np.ones_like(alpha, dtype=float)\n    # TODO: error check for size constancy\n    r = np.sum(w * np.exp(1j * alpha))\n    r = np.abs(r) / np.sum(w)\n    if d != 0:\n        c = d / 2.0 / np.sin(d / 2.0)\n        r = c * r\n    return r\n</code></pre>"},{"location":"reference/#ephysiopy.common.statscalcs.duplicates_as_complex","title":"<code>duplicates_as_complex(x, already_sorted=False)</code>","text":"<p>Finds duplicates in x</p> <p>Args:     x (array_like): The list to find duplicates in.     already_sorted (bool, optional): Whether x is already sorted.         Default False.</p> <p>Returns:     x (array_like): A complex array where the complex part is the count of         the number of duplicates of the real value.</p> <p>Examples:     &gt;&gt;&gt;     x = [9.9, 9.9, 12.3, 15.2, 15.2, 15.2]     &gt;&gt;&gt; ret = duplicates_as_complex(x)     &gt;&gt;&gt;     print(ret)     [9.9+0j, 9.9+1j,  12.3+0j, 15.2+0j, 15.2+1j, 15.2+2j]</p> Source code in <code>ephysiopy/common/statscalcs.py</code> <pre><code>def duplicates_as_complex(x, already_sorted=False):\n    \"\"\"\n    Finds duplicates in x\n\n    Args:\n        x (array_like): The list to find duplicates in.\n        already_sorted (bool, optional): Whether x is already sorted.\n            Default False.\n\n    Returns:\n        x (array_like): A complex array where the complex part is the count of\n            the number of duplicates of the real value.\n\n    Examples:\n        &gt;&gt;&gt;\tx = [9.9, 9.9, 12.3, 15.2, 15.2, 15.2]\n        &gt;&gt;&gt; ret = duplicates_as_complex(x)\n        &gt;&gt;&gt;\tprint(ret)\n        [9.9+0j, 9.9+1j,  12.3+0j, 15.2+0j, 15.2+1j, 15.2+2j]\n    \"\"\"\n\n    if not already_sorted:\n        x = np.sort(x)\n    is_start = np.empty(len(x), dtype=bool)\n    is_start[0], is_start[1:] = True, x[:-1] != x[1:]\n    labels = np.cumsum(is_start) - 1\n    sub_idx = np.arange(len(x)) - np.nonzero(is_start)[0][labels]\n    return x + 1j * sub_idx\n</code></pre>"},{"location":"reference/#ephysiopy.common.statscalcs.mean_resultant_vector","title":"<code>mean_resultant_vector(angles)</code>","text":"<p>Calculate the mean resultant length and direction for angles.</p> <p>Args:     angles (np.array): Sample of angles in radians.</p> <p>Returns:     r (float): The mean resultant vector length.     th (float): The mean resultant vector direction.</p> <p>Notes: Taken from Directional Statistics by Mardia &amp; Jupp, 2000</p> Source code in <code>ephysiopy/common/statscalcs.py</code> <pre><code>def mean_resultant_vector(angles):\n    \"\"\"\n    Calculate the mean resultant length and direction for angles.\n\n    Args:\n        angles (np.array): Sample of angles in radians.\n\n    Returns:\n        r (float): The mean resultant vector length.\n        th (float): The mean resultant vector direction.\n\n    Notes:\n    Taken from Directional Statistics by Mardia &amp; Jupp, 2000\n    \"\"\"\n    if len(angles) == 0:\n        return 0, 0\n    S = np.sum(np.sin(angles)) * (1 / float(len(angles)))\n    C = np.sum(np.cos(angles)) * (1 / float(len(angles)))\n    r = np.hypot(S, C)\n    th = np.arctan2(S, C)\n    if C &lt; 0:\n        th = np.pi + th\n    return r, th\n</code></pre>"},{"location":"reference/#ephysiopy.common.statscalcs.watsonWilliams","title":"<code>watsonWilliams(a, b)</code>","text":"<p>The Watson-Williams F test tests whether a set of mean directions are equal given that the concentrations are unknown, but equal, given that the groups each follow a von Mises distribution.</p> <p>Args:     a, b (array_like): The directional samples</p> <p>Returns:     F_stat (float): The F-statistic</p> Source code in <code>ephysiopy/common/statscalcs.py</code> <pre><code>def watsonWilliams(a, b):\n    \"\"\"\n    The Watson-Williams F test tests whether a set of mean directions are\n    equal given that the concentrations are unknown, but equal, given that\n    the groups each follow a von Mises distribution.\n\n    Args:\n        a, b (array_like): The directional samples\n\n    Returns:\n        F_stat (float): The F-statistic\n    \"\"\"\n\n    n = len(a)\n    m = len(b)\n    N = n + m\n    # v_1 = 1 # needed to do p-value lookup in table of critical values\n    #  of F distribution\n    # v_2 = N - 2 # needed to do p-value lookup in table of critical values\n    # of F distribution\n    C_1 = np.sum(np.cos(np.radians(a)))\n    S_1 = np.sum(np.sin(np.radians(a)))\n    C_2 = np.sum(np.cos(np.radians(b)))\n    S_2 = np.sum(np.sin(np.radians(b)))\n    C = C_1 + C_2\n    S = S_1 + S_2\n    R_1 = np.hypot(C_1, S_1)\n    R_2 = np.hypot(C_2, S_2)\n    R = np.hypot(C, S)\n    R_hat = (R_1 + R_2) / float(N)\n    from ephysiopy.common.mle_von_mises_vals import vals\n\n    mle_von_mises = np.array(vals)\n    mle_von_mises = np.sort(mle_von_mises, 0)\n    k_hat = mle_von_mises[(np.abs(mle_von_mises[:, 0] - R_hat)).argmin(), 1]\n    g = 1 - (3 / 8 * k_hat)\n    F = g * (N - 2) * ((R_1 + R_2 - R) / (N - (R_1 + R_2)))\n    return F\n</code></pre>"},{"location":"reference/#ephysiopy.common.statscalcs.watsonsU2","title":"<code>watsonsU2(a, b)</code>","text":"<p>Tests whether two samples from circular observations differ significantly from each other with regard to mean direction or angular variance.</p> <p>Args:     a, b (array_like): The two samples to be tested</p> <p>Returns:     U2 (float): The test statistic</p> <p>Notes:     Both samples must come from a continuous distribution. In the case of     grouping the class interval should not exceed 5.     Taken from '100 Statistical Tests' G.J.Kanji, 2006 Sage Publications</p> Source code in <code>ephysiopy/common/statscalcs.py</code> <pre><code>def watsonsU2(a, b):\n    \"\"\"\n    Tests whether two samples from circular observations differ significantly\n    from each other with regard to mean direction or angular variance.\n\n    Args:\n        a, b (array_like): The two samples to be tested\n\n    Returns:\n        U2 (float): The test statistic\n\n    Notes:\n        Both samples must come from a continuous distribution. In the case of\n        grouping the class interval should not exceed 5.\n        Taken from '100 Statistical Tests' G.J.Kanji, 2006 Sage Publications\n    \"\"\"\n\n    a = np.sort(np.ravel(a))\n    b = np.sort(np.ravel(b))\n    n_a = len(a)\n    n_b = len(b)\n    N = float(n_a + n_b)\n    a_complex = duplicates_as_complex(a, True)\n    b_complex = duplicates_as_complex(b, True)\n    a_and_b = np.union1d(a_complex, b_complex)\n\n    # get index for a\n    a_ind = np.zeros(len(a_and_b), dtype=int)\n    a_ind[np.searchsorted(a_and_b, a_complex)] = 1\n    a_ind = np.cumsum(a_ind)\n\n    # same for b\n    b_ind = np.zeros(len(a_and_b), dtype=int)\n    b_ind[np.searchsorted(a_and_b, b_complex)] = 1\n    b_ind = np.cumsum(b_ind)\n\n    d_k = (a_ind / float(n_a)) - (b_ind / float(n_b))\n\n    d_k_sq = d_k**2\n\n    U2 = ((n_a * n_b) / N**2) * (np.sum(d_k_sq) - ((np.sum(d_k) ** 2) / N))\n    return U2\n</code></pre>"},{"location":"reference/#ephysiopy.common.statscalcs.watsonsU2n","title":"<code>watsonsU2n(angles)</code>","text":"<p>Tests whether the given distribution fits a random sample of angular values.</p> <p>Args:     angles (array_like): The angular samples.</p> <p>Returns:     U2n (float): The test statistic.</p> <p>Notes:     This test is suitable for both unimodal and the multimodal cases.     It can be used as a test for randomness.     Taken from '100 Statistical Tests' G.J.Kanji, 2006 Sage Publications.</p> Source code in <code>ephysiopy/common/statscalcs.py</code> <pre><code>def watsonsU2n(angles):\n    \"\"\"\n    Tests whether the given distribution fits a random sample of angular\n    values.\n\n    Args:\n        angles (array_like): The angular samples.\n\n    Returns:\n        U2n (float): The test statistic.\n\n    Notes:\n        This test is suitable for both unimodal and the multimodal cases.\n        It can be used as a test for randomness.\n        Taken from '100 Statistical Tests' G.J.Kanji, 2006 Sage Publications.\n    \"\"\"\n\n    angles = np.sort(angles)\n    n = len(angles)\n    Vi = angles / float(360)\n    sum_Vi = np.sum(Vi)\n    sum_sq_Vi = np.sum(Vi**2)\n    Ci = (2 * np.arange(1, n + 1)) - 1\n    sum_Ci_Vi_ov_n = np.sum(Ci * Vi / n)\n    V_bar = (1 / float(n)) * sum_Vi\n    U2n = sum_sq_Vi - sum_Ci_Vi_ov_n + (n * (1 / float(3) - (V_bar - 0.5) ** 2))\n    test_vals = {\n        \"0.1\": 0.152,\n        \"0.05\": 0.187,\n        \"0.025\": 0.221,\n        \"0.01\": 0.267,\n        \"0.005\": 0.302,\n    }\n    for key, val in test_vals.items():\n        if U2n &gt; val:\n            print(\n                \"The Watsons U2 statistic is {0} which is \\\n                greater than\\n the critical value of {1} at p={2}\".format(\n                    U2n, val, key\n                )\n            )\n        else:\n            print(\n                \"The Watsons U2 statistic is not \\\n                significant at p={0}\".format(\n                    key\n                )\n            )\n    return U2n\n</code></pre>"},{"location":"reference/#utility-functions","title":"Utility functions","text":""},{"location":"reference/#ephysiopy.common.utils.BinnedData","title":"<code>BinnedData</code>  <code>dataclass</code>","text":"Source code in <code>ephysiopy/common/utils.py</code> <pre><code>@dataclass\nclass BinnedData:\n    variable: VariableToBin = VariableToBin.XY\n    map_type: MapType = MapType.RATE\n    binned_data: list[np.ndarray] = field(default_factory=list)\n    bin_edges: list[np.ndarray] = field(default_factory=list)\n    # bin_units: int = 1\n\n    def __iter__(self):\n        yield from self.binned_data\n\n    def __assert_equal_bin_edges__(self, other):\n        assert np.all(\n            [np.all(sbe == obe) for sbe, obe in zip(self.bin_edges, other.bin_edges)]\n        ), \"Bin edges do not match\"\n\n    def __len__(self):\n        return len(self.binned_data)\n\n    def __getitem__(self, i):\n        return BinnedData(\n            variable=self.variable,\n            map_type=self.map_type,\n            binned_data=copy.deepcopy(self.binned_data[i]),\n            bin_edges=self.bin_edges,\n        )\n\n    def __truediv__(self, other):\n        if isinstance(other, BinnedData):\n            self.__assert_equal_bin_edges__(other)\n            if len(self.binned_data) &gt; len(other.binned_data):\n                if (other.map_type.value == MapType.POS.value) and (\n                    self.map_type.value == MapType.SPK.value\n                ):\n                    if len(other.binned_data) == 1:\n                        return BinnedData(\n                            variable=self.variable,\n                            map_type=MapType.RATE,\n                            binned_data=[\n                                a / b\n                                for a in self.binned_data\n                                for b in other.binned_data\n                            ],\n                            bin_edges=self.bin_edges,\n                        )\n\n            return BinnedData(\n                variable=self.variable,\n                map_type=MapType.RATE,\n                binned_data=[\n                    a / b for a, b in zip(self.binned_data, other.binned_data)\n                ],\n                bin_edges=self.bin_edges,\n            )\n\n    def __add__(self, other):\n        if isinstance(other, BinnedData):\n            self.__assert_equal_bin_edges__(other)\n            return BinnedData(\n                variable=self.variable,\n                map_type=self.map_type,\n                binned_data=self.binned_data + other.binned_data,\n                bin_edges=self.bin_edges,\n            )\n\n    def __eq__(self, other) -&gt; bool:\n        assert isinstance(other, BinnedData)\n        self.__assert_equal_bin_edges__(other)\n        if np.all(\n            [\n                np.all(np.isfinite(sbd) == np.isfinite(obd))\n                for sbd, obd in zip(self.binned_data, other.binned_data)\n            ]\n        ):\n            if np.all(\n                [\n                    np.all(sbd[np.isfinite(sbd)] == obd[np.isfinite(obd)])\n                    for sbd, obd in zip(self.binned_data, other.binned_data)\n                ]\n            ):\n                return True\n            else:\n                return False\n        else:\n            return False\n\n    def set_nan_indices(self, indices):\n        for i in range(len(self.binned_data)):\n            self.binned_data[i][indices] = np.nan\n\n    def T(self):\n        return BinnedData(\n            variable=self.variable,\n            map_type=self.map_type,\n            binned_data=[a.T for a in self.binned_data],\n            bin_edges=self.bin_edges[::-1],\n        )\n\n    def correlate(self, other=None, as_matrix=False) -&gt; list[float] | np.ndarray:\n        \"\"\"\n        This method is used to correlate the binned data of this BinnedData\n        instance with the binned data of another BinnedData instance.\n\n        Args:\n            other (BinnedData): The other BinnedData instance to correlate with.\n                If None, then correlations are performed between all the data held\n                in the list self.binned_data\n            as_matrix (bool): If True will return the full correlation matrix for\n                all of the correlations in the list of data in self.binned_data. If\n                False, a list of the unique correlations for the comparisons in\n                self.binned_data are returned.\n\n        Returns:\n            BinnedData: A new BinnedData instance with the correlation of the\n                binned data of this instance and the other instance.\n        \"\"\"\n        if other is not None:\n            assert isinstance(other, BinnedData)\n            self.__assert_equal_bin_edges__(other)\n        if other is not None:\n            result = np.reshape(\n                [corr_maps(a, b) for a in self.binned_data for b in other.binned_data],\n                newshape=(len(self.binned_data), len(other.binned_data)),\n            )\n        else:\n            result = np.reshape(\n                [corr_maps(a, b) for a in self.binned_data for b in self.binned_data],\n                newshape=(len(self.binned_data), len(self.binned_data)),\n            )\n        if as_matrix:\n            return result\n        else:\n            # pick out the relevant diagonal\n            k = -1\n            if len(self.binned_data) == 1:\n                k = 0\n            if other is not None:\n                if len(other.binned_data) == 1:\n                    k = 0\n                idx = np.tril_indices(\n                    n=len(self.binned_data), m=len(other.binned_data), k=k\n                )\n            else:\n                idx = np.tril_indices(n=len(self.binned_data), k=k)\n            return result[idx]\n</code></pre>"},{"location":"reference/#ephysiopy.common.utils.BinnedData.correlate","title":"<code>correlate(other=None, as_matrix=False)</code>","text":"<p>This method is used to correlate the binned data of this BinnedData instance with the binned data of another BinnedData instance.</p> <p>Args:     other (BinnedData): The other BinnedData instance to correlate with.         If None, then correlations are performed between all the data held         in the list self.binned_data     as_matrix (bool): If True will return the full correlation matrix for         all of the correlations in the list of data in self.binned_data. If         False, a list of the unique correlations for the comparisons in         self.binned_data are returned.</p> <p>Returns:     BinnedData: A new BinnedData instance with the correlation of the         binned data of this instance and the other instance.</p> Source code in <code>ephysiopy/common/utils.py</code> <pre><code>def correlate(self, other=None, as_matrix=False) -&gt; list[float] | np.ndarray:\n    \"\"\"\n    This method is used to correlate the binned data of this BinnedData\n    instance with the binned data of another BinnedData instance.\n\n    Args:\n        other (BinnedData): The other BinnedData instance to correlate with.\n            If None, then correlations are performed between all the data held\n            in the list self.binned_data\n        as_matrix (bool): If True will return the full correlation matrix for\n            all of the correlations in the list of data in self.binned_data. If\n            False, a list of the unique correlations for the comparisons in\n            self.binned_data are returned.\n\n    Returns:\n        BinnedData: A new BinnedData instance with the correlation of the\n            binned data of this instance and the other instance.\n    \"\"\"\n    if other is not None:\n        assert isinstance(other, BinnedData)\n        self.__assert_equal_bin_edges__(other)\n    if other is not None:\n        result = np.reshape(\n            [corr_maps(a, b) for a in self.binned_data for b in other.binned_data],\n            newshape=(len(self.binned_data), len(other.binned_data)),\n        )\n    else:\n        result = np.reshape(\n            [corr_maps(a, b) for a in self.binned_data for b in self.binned_data],\n            newshape=(len(self.binned_data), len(self.binned_data)),\n        )\n    if as_matrix:\n        return result\n    else:\n        # pick out the relevant diagonal\n        k = -1\n        if len(self.binned_data) == 1:\n            k = 0\n        if other is not None:\n            if len(other.binned_data) == 1:\n                k = 0\n            idx = np.tril_indices(\n                n=len(self.binned_data), m=len(other.binned_data), k=k\n            )\n        else:\n            idx = np.tril_indices(n=len(self.binned_data), k=k)\n        return result[idx]\n</code></pre>"},{"location":"reference/#ephysiopy.common.utils.applyFilter2Labels","title":"<code>applyFilter2Labels(M, x)</code>","text":"<p>M is a logical mask specifying which label numbers to keep x is an array of positive integer labels</p> <p>This method sets the undesired labels to 0 and renumbers the remaining labels 1 to n when n is the number of trues in M</p> Source code in <code>ephysiopy/common/utils.py</code> <pre><code>def applyFilter2Labels(M, x):\n    \"\"\"\n    M is a logical mask specifying which label numbers to keep\n    x is an array of positive integer labels\n\n    This method sets the undesired labels to 0 and renumbers the remaining\n    labels 1 to n when n is the number of trues in M\n    \"\"\"\n    newVals = M * np.cumsum(M)\n    x[x &gt; 0] = newVals[x[x &gt; 0] - 1]\n    return x\n</code></pre>"},{"location":"reference/#ephysiopy.common.utils.blur_image","title":"<code>blur_image(im, n, ny=0, ftype='boxcar', **kwargs)</code>","text":"<p>Smooths a 2D image by convolving with a filter.</p> <p>Args:     im (BinnedData): Contains the array to smooth.     n, ny (int): The size of the smoothing kernel.     ftype (str): The type of smoothing kernel.         Either 'boxcar' or 'gaussian'.</p> <p>Returns:     res (BinnedData): BinnedData instance with the smoothed data.</p> <p>Notes:     This essentially does the smoothing in-place</p> Source code in <code>ephysiopy/common/utils.py</code> <pre><code>def blur_image(\n    im: BinnedData, n: int, ny: int = 0, ftype: str = \"boxcar\", **kwargs\n) -&gt; BinnedData:\n    \"\"\"\n    Smooths a 2D image by convolving with a filter.\n\n    Args:\n        im (BinnedData): Contains the array to smooth.\n        n, ny (int): The size of the smoothing kernel.\n        ftype (str): The type of smoothing kernel.\n            Either 'boxcar' or 'gaussian'.\n\n    Returns:\n        res (BinnedData): BinnedData instance with the smoothed data.\n\n    Notes:\n        This essentially does the smoothing in-place\n    \"\"\"\n    stddev = kwargs.pop(\"stddev\", 5)\n    boundary = kwargs.pop(\"boundary\", \"extend\")\n    n = int(n)\n    if n % 2 == 0:\n        n += 1\n    if ny == 0:\n        ny = n\n    else:\n        ny = int(ny)\n        if ny % 2 == 0:\n            ny += 1\n    ndims = len(im.bin_edges)\n    g = cnv.Box2DKernel(n)\n    if \"box\" in ftype:\n        if ndims == 1:\n            g = cnv.Box1DKernel(n)\n        if ndims == 2:\n            g = np.atleast_2d(g)\n    elif \"gaussian\" in ftype:\n        if ndims == 1:\n            g = cnv.Gaussian1DKernel(stddev, x_size=n)\n        if ndims == 2:\n            g = cnv.Gaussian2DKernel(stddev, x_size=n, y_size=ny)\n    g = np.array(g)\n    for i, m in enumerate(im.binned_data):\n        im.binned_data[i] = cnv.convolve(m, g, boundary=boundary)\n    return im\n</code></pre>"},{"location":"reference/#ephysiopy.common.utils.bwperim","title":"<code>bwperim(bw, n=4)</code>","text":"<p>Finds the perimeter of objects in binary images.</p> <p>A pixel is part of an object perimeter if its value is one and there is at least one zero-valued pixel in its neighborhood.</p> <p>By default the neighborhood of a pixel is 4 nearest pixels, but if <code>n</code> is set to 8 the 8 nearest pixels will be considered.</p> <p>Args:     bw (array_like): A black-and-white image.     n (int, optional): Connectivity. Must be 4 or 8. Default is 8.</p> <p>Returns:     perim (array_like): A boolean image.</p> Source code in <code>ephysiopy/common/utils.py</code> <pre><code>def bwperim(bw, n=4):\n    \"\"\"\n    Finds the perimeter of objects in binary images.\n\n    A pixel is part of an object perimeter if its value is one and there\n    is at least one zero-valued pixel in its neighborhood.\n\n    By default the neighborhood of a pixel is 4 nearest pixels, but\n    if `n` is set to 8 the 8 nearest pixels will be considered.\n\n    Args:\n        bw (array_like): A black-and-white image.\n        n (int, optional): Connectivity. Must be 4 or 8. Default is 8.\n\n    Returns:\n        perim (array_like): A boolean image.\n    \"\"\"\n\n    if n not in (4, 8):\n        raise ValueError(\"mahotas.bwperim: n must be 4 or 8\")\n    rows, cols = bw.shape\n\n    # Translate image by one pixel in all directions\n    north = np.zeros((rows, cols))\n    south = np.zeros((rows, cols))\n    west = np.zeros((rows, cols))\n    east = np.zeros((rows, cols))\n\n    north[:-1, :] = bw[1:, :]\n    south[1:, :] = bw[:-1, :]\n    west[:, :-1] = bw[:, 1:]\n    east[:, 1:] = bw[:, :-1]\n    idx = (north == bw) &amp; (south == bw) &amp; (west == bw) &amp; (east == bw)\n    if n == 8:\n        north_east = np.zeros((rows, cols))\n        north_west = np.zeros((rows, cols))\n        south_east = np.zeros((rows, cols))\n        south_west = np.zeros((rows, cols))\n        north_east[:-1, 1:] = bw[1:, :-1]\n        north_west[:-1, :-1] = bw[1:, 1:]\n        south_east[1:, 1:] = bw[:-1, :-1]\n        south_west[1:, :-1] = bw[:-1, 1:]\n        idx &amp;= (\n            (north_east == bw)\n            &amp; (south_east == bw)\n            &amp; (south_west == bw)\n            &amp; (north_west == bw)\n        )\n    return ~idx * bw\n</code></pre>"},{"location":"reference/#ephysiopy.common.utils.clean_kwargs","title":"<code>clean_kwargs(func, kwargs)</code>","text":"<p>This function is used to remove any keyword arguments that are not accepted by the function. It is useful for passing keyword arguments to other functions without having to worry about whether they are accepted by the function or not.</p> <p>Args:     func (function): The function to check for keyword arguments.     kwargs (dict): The keyword arguments to check.</p> <p>Returns:     dict: A dictionary containing only the keyword arguments that are     accepted by the function.</p> Source code in <code>ephysiopy/common/utils.py</code> <pre><code>def clean_kwargs(func, kwargs):\n    \"\"\"\n    This function is used to remove any keyword arguments that are not\n    accepted by the function. It is useful for passing keyword arguments\n    to other functions without having to worry about whether they are\n    accepted by the function or not.\n\n    Args:\n        func (function): The function to check for keyword arguments.\n        kwargs (dict): The keyword arguments to check.\n\n    Returns:\n        dict: A dictionary containing only the keyword arguments that are\n        accepted by the function.\n    \"\"\"\n    valid_kwargs = inspect.getfullargspec(func).kwonlyargs\n    return {k: v for k, v in kwargs.items() if k in valid_kwargs}\n</code></pre>"},{"location":"reference/#ephysiopy.common.utils.corr_maps","title":"<code>corr_maps(map1, map2, maptype='normal')</code>","text":"<p>correlates two ratemaps together ignoring areas that have zero sampling</p> Source code in <code>ephysiopy/common/utils.py</code> <pre><code>def corr_maps(map1, map2, maptype=\"normal\") -&gt; float:\n    \"\"\"\n    correlates two ratemaps together ignoring areas that have zero sampling\n    \"\"\"\n    if map1.shape &gt; map2.shape:\n        map2 = skimage.transform.resize(map2, map1.shape, mode=\"reflect\")\n    elif map1.shape &lt; map2.shape:\n        map1 = skimage.transform.resize(map1, map2.shape, mode=\"reflect\")\n    map1 = map1.flatten()\n    map2 = map2.flatten()\n    valid_map1 = np.zeros_like(map1)\n    valid_map2 = np.zeros_like(map2)\n    if \"normal\" in maptype:\n        np.logical_or((map1 &gt; 0), ~np.isnan(map1), out=valid_map1)\n        np.logical_or((map2 &gt; 0), ~np.isnan(map2), out=valid_map2)\n    elif \"grid\" in maptype:\n        np.isfinite(map1, out=valid_map1)\n        np.isfinite(map2, out=valid_map2)\n    valid = np.logical_and(valid_map1, valid_map2)\n    r = np.corrcoef(map1[valid], map2[valid])\n    return r[1][0]\n</code></pre>"},{"location":"reference/#ephysiopy.common.utils.count_runs_and_unique_numbers","title":"<code>count_runs_and_unique_numbers(arr)</code>","text":"<p>Counts the number of continuous runs of numbers in a 1D numpy array and returns the count of runs for each unique number and the unique numbers.</p> <p>Args:     arr (np.ndarray): The input 1D numpy array of numbers.</p> <p>Returns:     tuple: A tuple containing a dictionary with the count of runs for     each unique number and the set of unique numbers in the array.</p> Source code in <code>ephysiopy/common/utils.py</code> <pre><code>def count_runs_and_unique_numbers(arr: np.ndarray) -&gt; tuple:\n    \"\"\"\n    Counts the number of continuous runs of numbers in a 1D numpy array\n    and returns the count of runs for each unique number and the unique\n    numbers.\n\n    Args:\n        arr (np.ndarray): The input 1D numpy array of numbers.\n\n    Returns:\n        tuple: A tuple containing a dictionary with the count of runs for\n        each unique number and the set of unique numbers in the array.\n    \"\"\"\n    if arr.size == 0:\n        return {}, set()\n\n    unique_numbers = set(arr)\n    runs_count = defaultdict(int)\n    for num in unique_numbers:\n        runs = np.diff(np.where(arr == num)) != 1\n        # Add 1 because diff reduces the size by 1\n        runs_count[num] = np.count_nonzero(runs) + 1\n\n    return runs_count, unique_numbers\n</code></pre>"},{"location":"reference/#ephysiopy.common.utils.count_to","title":"<code>count_to(n)</code>","text":"<p>This function is equivalent to hstack((arange(n_i) for n_i in n)). It seems to be faster for some possible inputs and encapsulates a task in a function.</p> <p>Example:     Given n = [0, 0, 3, 0, 0, 2, 0, 2, 1],     the result would be [0, 1, 2, 0, 1, 0, 1, 0].</p> Source code in <code>ephysiopy/common/utils.py</code> <pre><code>def count_to(n):\n    \"\"\"\n    This function is equivalent to hstack((arange(n_i) for n_i in n)).\n    It seems to be faster for some possible inputs and encapsulates\n    a task in a function.\n\n    Example:\n        Given n = [0, 0, 3, 0, 0, 2, 0, 2, 1],\n        the result would be [0, 1, 2, 0, 1, 0, 1, 0].\n    \"\"\"\n    if n.ndim != 1:\n        raise Exception(\"n is supposed to be 1d array.\")\n\n    n_mask = n.astype(bool)\n    n_cumsum = np.cumsum(n)\n    ret = np.ones(n_cumsum[-1] + 1, dtype=int)\n    ret[n_cumsum[n_mask]] -= n[n_mask]\n    ret[0] -= 1\n    return np.cumsum(ret)[:-1]\n</code></pre>"},{"location":"reference/#ephysiopy.common.utils.find_runs","title":"<code>find_runs(x)</code>","text":"<p>Find runs of consecutive items in an array.</p> <p>Taken from: https://gist.github.com/alimanfoo/c5977e87111abe8127453b21204c1065</p> Source code in <code>ephysiopy/common/utils.py</code> <pre><code>def find_runs(x):\n    \"\"\"\n    Find runs of consecutive items in an array.\n\n    Taken from:\n    https://gist.github.com/alimanfoo/c5977e87111abe8127453b21204c1065\n    \"\"\"\n\n    # ensure array\n    x = np.asanyarray(x)\n    if x.ndim != 1:\n        raise ValueError(\"only 1D array supported\")\n    n = x.shape[0]\n\n    # handle empty array\n    if n == 0:\n        return np.array([]), np.array([]), np.array([])\n\n    else:\n        # find run starts\n        loc_run_start = np.empty(n, dtype=bool)\n        loc_run_start[0] = True\n        np.not_equal(x[:-1], x[1:], out=loc_run_start[1:])\n        run_starts = np.nonzero(loc_run_start)[0]\n\n        # find run values\n        run_values = x[loc_run_start]\n\n        # find run lengths\n        run_lengths = np.diff(np.append(run_starts, n))\n\n        return run_values, run_starts, run_lengths\n</code></pre>"},{"location":"reference/#ephysiopy.common.utils.fixAngle","title":"<code>fixAngle(a)</code>","text":"<p>Ensure angles lie between -pi and pi a must be in radians</p> Source code in <code>ephysiopy/common/utils.py</code> <pre><code>def fixAngle(a):\n    \"\"\"\n    Ensure angles lie between -pi and pi\n    a must be in radians\n    \"\"\"\n    b = np.mod(a + np.pi, 2 * np.pi) - np.pi\n    return b\n</code></pre>"},{"location":"reference/#ephysiopy.common.utils.get_z_score","title":"<code>get_z_score(x, mean=None, sd=None, axis=0)</code>","text":"<p>Calculate the z-scores for array x based on the mean and standard deviation in that sample, unless stated</p> Source code in <code>ephysiopy/common/utils.py</code> <pre><code>def get_z_score(x: np.ndarray, mean=None, sd=None, axis=0) -&gt; np.ndarray:\n    \"\"\"\n    Calculate the z-scores for array x based on the mean\n    and standard deviation in that sample, unless stated\n    \"\"\"\n    if mean is None:\n        mean = np.nanmean(x, axis=axis)\n    if sd is None:\n        sd = np.nanstd(x, axis=axis)\n    if axis == -1:\n        return (x - mean[..., None]) / sd[..., None]\n    return (x - mean) / sd\n</code></pre>"},{"location":"reference/#ephysiopy.common.utils.min_max_norm","title":"<code>min_max_norm(x, min=None, max=None, axis=0)</code>","text":"<p>Normalise the input array x to lie between min and max</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> required <code>min</code> <code>None</code> <code>max</code> <code>None</code> <code>axis</code> <code>0</code> <p>Returns:</p> Type Description <code>out (np.ndarray) - the normalised array</code> Source code in <code>ephysiopy/common/utils.py</code> <pre><code>def min_max_norm(x: np.ndarray, min=None, max=None, axis=0) -&gt; np.ndarray:\n    \"\"\"\n    Normalise the input array x to lie between min and max\n\n    Parameters\n    ----------\n    x (np.ndarray) - the array to normalise\n    min (float) - the minimun value in the returned array\n    max (float) - the maximum value in the returned array\n    axis - the axis along which to operate. Default 0\n\n    Returns\n    -------\n    out (np.ndarray) - the normalised array\n    \"\"\"\n    if min is None:\n        min = np.nanmin(x, axis)\n    if max is None:\n        max = np.nanmax(x, axis)\n    if axis == -1:\n        return (x - np.array(min)[..., None]) / np.array(max - min)[..., None]\n    return (x - min) / (max - min)\n</code></pre>"},{"location":"reference/#ephysiopy.common.utils.polar","title":"<code>polar(x, y, deg=False)</code>","text":"<p>Converts from rectangular coordinates to polar ones.</p> <p>Args:     x, y (array_like, list_like): The x and y coordinates.     deg (int): Radian if deg=0; degree if deg=1.</p> <p>Returns:     p (array_like): The polar version of x and y.</p> Source code in <code>ephysiopy/common/utils.py</code> <pre><code>def polar(x, y, deg=False):\n    \"\"\"\n    Converts from rectangular coordinates to polar ones.\n\n    Args:\n        x, y (array_like, list_like): The x and y coordinates.\n        deg (int): Radian if deg=0; degree if deg=1.\n\n    Returns:\n        p (array_like): The polar version of x and y.\n    \"\"\"\n    if deg:\n        return np.hypot(x, y), 180.0 * np.arctan2(y, x) / np.pi\n    else:\n        return np.hypot(x, y), np.arctan2(y, x)\n</code></pre>"},{"location":"reference/#ephysiopy.common.utils.rect","title":"<code>rect(r, w, deg=False)</code>","text":"<p>Convert from polar (r,w) to rectangular (x,y) x = r cos(w) y = r sin(w)</p> Source code in <code>ephysiopy/common/utils.py</code> <pre><code>def rect(r, w, deg=False):\n    \"\"\"\n    Convert from polar (r,w) to rectangular (x,y)\n    x = r cos(w)\n    y = r sin(w)\n    \"\"\"\n    # radian if deg=0; degree if deg=1\n    if deg:\n        w = np.pi * w / 180.0\n    return r * np.cos(w), r * np.sin(w)\n</code></pre>"},{"location":"reference/#ephysiopy.common.utils.remap_to_range","title":"<code>remap_to_range(x, new_min=0, new_max=1, axis=0)</code>","text":"<p>Remap the values of x to the range [new_min, new_max].</p> Source code in <code>ephysiopy/common/utils.py</code> <pre><code>def remap_to_range(x: np.ndarray, new_min=0, new_max=1, axis=0) -&gt; np.ndarray:\n    \"\"\"\n    Remap the values of x to the range [new_min, new_max].\n    \"\"\"\n    min = np.nanmin(x, axis)\n    max = np.nanmax(x, axis)\n    if axis == -1:\n        return np.array((x.T - min) / (max - min) * (new_max - new_min) + new_min).T\n    return (x - min) / (max - min) * (new_max - new_min) + new_min\n</code></pre>"},{"location":"reference/#ephysiopy.common.utils.repeat_ind","title":"<code>repeat_ind(n)</code>","text":"<p>Examples:     &gt;&gt;&gt; n = [0, 0, 3, 0, 0, 2, 0, 2, 1]     &gt;&gt;&gt; res = repeat_ind(n)     &gt;&gt;&gt; res = [2, 2, 2, 5, 5, 7, 7, 8]</p> <p>The input specifies how many times to repeat the given index. It is equivalent to something like this:</p> <pre><code>hstack((zeros(n_i,dtype=int)+i for i, n_i in enumerate(n)))\n</code></pre> <p>But this version seems to be faster, and probably scales better. At any rate, it encapsulates a task in a function.</p> Source code in <code>ephysiopy/common/utils.py</code> <pre><code>def repeat_ind(n: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Examples:\n        &gt;&gt;&gt; n = [0, 0, 3, 0, 0, 2, 0, 2, 1]\n        &gt;&gt;&gt; res = repeat_ind(n)\n        &gt;&gt;&gt; res = [2, 2, 2, 5, 5, 7, 7, 8]\n\n    The input specifies how many times to repeat the given index.\n    It is equivalent to something like this:\n\n        hstack((zeros(n_i,dtype=int)+i for i, n_i in enumerate(n)))\n\n    But this version seems to be faster, and probably scales better.\n    At any rate, it encapsulates a task in a function.\n    \"\"\"\n    if n.ndim != 1:\n        raise Exception(\"n is supposed to be 1d array.\")\n\n    res = [[idx] * a for idx, a in enumerate(n) if a != 0]\n    return np.concatenate(res)\n</code></pre>"},{"location":"reference/#ephysiopy.common.utils.shift_vector","title":"<code>shift_vector(v, shift, maxlen=None)</code>","text":"<p>Shifts the elements of a vector by a given amount. A bit like numpys roll function but when the shift goes beyond some limit that limit is subtracted from the shift. The result is then sorted and returned.</p> <p>Args:     v (array_like): The input vector.     shift (int): The amount to shift the elements.     fill_value (int): The value to fill the empty spaces.</p> <p>Returns:     array_like: The shifted vector.</p> Source code in <code>ephysiopy/common/utils.py</code> <pre><code>def shift_vector(v, shift, maxlen=None):\n    \"\"\"\n    Shifts the elements of a vector by a given amount.\n    A bit like numpys roll function but when the shift goes\n    beyond some limit that limit is subtracted from the shift.\n    The result is then sorted and returned.\n\n    Args:\n        v (array_like): The input vector.\n        shift (int): The amount to shift the elements.\n        fill_value (int): The value to fill the empty spaces.\n\n    Returns:\n        array_like: The shifted vector.\n    \"\"\"\n    if shift == 0:\n        return v\n    if maxlen is None:\n        return v\n    if shift &gt; 0:\n        shifted = v + shift\n        shifted[shifted &gt;= maxlen] -= maxlen\n        return np.sort(shifted)\n</code></pre>"},{"location":"reference/#ephysiopy.common.utils.smooth","title":"<code>smooth(x, window_len=9, window='hanning')</code>","text":"<p>Smooth the data using a window with requested size.</p> <p>This method is based on the convolution of a scaled window with the signal. The signal is prepared by introducing reflected copies of the signal (with the window size) in both ends so that transient parts are minimized in the beginning and end part of the output signal.</p> <p>Args:     x (array_like): The input signal.     window_len (int): The length of the smoothing window.     window (str): The type of window from 'flat', 'hanning', 'hamming',         'bartlett', 'blackman'. 'flat' window will produce a moving average         smoothing.</p> <p>Returns:     out (array_like): The smoothed signal.</p> <p>Example:     &gt;&gt;&gt; t=linspace(-2,2,0.1)     &gt;&gt;&gt; x=sin(t)+randn(len(t))*0.1     &gt;&gt;&gt; y=smooth(x)</p> <p>See Also:     numpy.hanning, numpy.hamming, numpy.bartlett, numpy.blackman,     numpy.convolve, scipy.signal.lfilter</p> <p>Notes:     The window parameter could be the window itself if an array instead of     a string.</p> Source code in <code>ephysiopy/common/utils.py</code> <pre><code>def smooth(x, window_len=9, window=\"hanning\"):\n    \"\"\"\n    Smooth the data using a window with requested size.\n\n    This method is based on the convolution of a scaled window with the signal.\n    The signal is prepared by introducing reflected copies of the signal\n    (with the window size) in both ends so that transient parts are minimized\n    in the beginning and end part of the output signal.\n\n    Args:\n        x (array_like): The input signal.\n        window_len (int): The length of the smoothing window.\n        window (str): The type of window from 'flat', 'hanning', 'hamming',\n            'bartlett', 'blackman'. 'flat' window will produce a moving average\n            smoothing.\n\n    Returns:\n        out (array_like): The smoothed signal.\n\n    Example:\n        &gt;&gt;&gt; t=linspace(-2,2,0.1)\n        &gt;&gt;&gt; x=sin(t)+randn(len(t))*0.1\n        &gt;&gt;&gt; y=smooth(x)\n\n    See Also:\n        numpy.hanning, numpy.hamming, numpy.bartlett, numpy.blackman,\n        numpy.convolve, scipy.signal.lfilter\n\n    Notes:\n        The window parameter could be the window itself if an array instead of\n        a string.\n    \"\"\"\n\n    if isinstance(x, list):\n        x = np.array(x)\n\n    if x.ndim != 1:\n        raise ValueError(\"smooth only accepts 1 dimension arrays.\")\n\n    if len(x) &lt; window_len:\n        print(\"length of x: \", len(x))\n        print(\"window_len: \", window_len)\n        raise ValueError(\"Input vector needs to be bigger than window size.\")\n    if window_len &lt; 3:\n        return x\n\n    if (window_len % 2) == 0:\n        window_len = window_len + 1\n\n    if window not in [\"flat\", \"hanning\", \"hamming\", \"bartlett\", \"blackman\"]:\n        raise ValueError(\n            \"Window is on of 'flat', 'hanning', \\\n                'hamming', 'bartlett', 'blackman'\"\n        )\n\n    if window == \"flat\":  # moving average\n        w = np.ones(window_len, \"d\")\n    else:\n        w = eval(\"np.\" + window + \"(window_len)\")\n    y = cnv.convolve(x, w / w.sum(), normalize_kernel=False, boundary=\"extend\")\n    # return the smoothed signal\n    return y\n</code></pre>"},{"location":"reference/#ephysiopy.common.utils.window_rms","title":"<code>window_rms(a, window_size)</code>","text":"<p>Returns the root mean square of the input a over a window of size window_size</p> Source code in <code>ephysiopy/common/utils.py</code> <pre><code>def window_rms(a: np.ndarray, window_size: int | float) -&gt; np.ndarray:\n    \"\"\"\n    Returns the root mean square of the input a over a window of\n    size window_size\n    \"\"\"\n    window_size = int(window_size)\n    a2 = np.power(a, 2)\n    window = np.ones(window_size) / float(window_size)\n    return np.sqrt(np.convolve(a2, window, \"same\"))\n</code></pre>"},{"location":"reference/#axona-input-output","title":"Axona input/ output","text":""},{"location":"reference/#ephysiopy.axona.axonaIO.ClusterSession","title":"<code>ClusterSession</code>","text":"<p>               Bases: <code>object</code></p> <p>Loads all the cut file data and timestamps from the data associated with the *.set filename given to init</p> <p>Meant to be a method-replica of the KiloSortSession class but really both should inherit from the same meta-class</p> Source code in <code>ephysiopy/axona/axonaIO.py</code> <pre><code>class ClusterSession(object):\n    \"\"\"\n    Loads all the cut file data and timestamps from the data\n    associated with the *.set filename given to __init__\n\n    Meant to be a method-replica of the KiloSortSession class\n    but really both should inherit from the same meta-class\n    \"\"\"\n\n    def __init__(self, fname_root) -&gt; None:\n        fname_root = Path(fname_root)\n        assert fname_root.suffix == \".set\"\n        assert fname_root.exists()\n        self.fname_root = fname_root\n\n        self.cluster_id = None\n        self.spk_clusters = None\n        self.spike_times = None\n        self.good_clusters = {}\n\n    def load(self):\n        pname = self.fname_root.parent\n        pattern = re.compile(\n            r\"^\" + str(self.fname_root.with_suffix(\"\")) + r\"_[1-9][0-9].cut\"\n        )\n        pattern1 = re.compile(\n            r\"^\" + str(self.fname_root.with_suffix(\"\")) + r\"_[1-9]$.cut\"\n        )\n        cut_files = sorted(\n            list(\n                f\n                for f in pname.iterdir()\n                if pattern.search(str(f)) or pattern1.search(str(f))\n            )\n        )\n        # extract the clusters from each cut file\n        # get the corresponding tetrode files\n        tet_files = [str(c.with_suffix(\"\")) for c in cut_files]\n        tet_files = [t[::-1].replace(\"_\", \".\", 1)[::-1] for t in tet_files]\n        tetrode_clusters = {}\n        for fname in tet_files:\n            T = IO(fname)\n            idx = fname.rfind(\".\")\n            tetnum = int(fname[idx + 1 :])\n            cut = T.getCut(tetnum)\n            tetrode_clusters[tetnum] = cut\n\n        self.good_clusters = tetrode_clusters\n</code></pre>"},{"location":"reference/#ephysiopy.axona.axonaIO.EEG","title":"<code>EEG</code>","text":"<p>               Bases: <code>IO</code></p> <p>Processes eeg data collected with the Axona recording system</p> <p>Parameters:</p> Name Type Description Default <code>filename_root</code> <code>str</code> <p>The fully qualified filename without the suffix</p> required <code>egf</code> <p>Whether to read the 'eeg' file or the 'egf' file. 0 is False, 1 is True</p> <code>0</code> <code>eeg_file</code> <p>If more than one eeg channel was recorded from then they are numbered from 1 onwards i.e. trial.eeg, trial.eeg1, trial.eeg2 etc This number specifies that</p> <code>1</code> Source code in <code>ephysiopy/axona/axonaIO.py</code> <pre><code>class EEG(IO):\n    \"\"\"\n    Processes eeg data collected with the Axona recording system\n\n    Parameters\n    ---------\n    filename_root : str\n        The fully qualified filename without the suffix\n    egf: int\n        Whether to read the 'eeg' file or the 'egf' file. 0 is False, 1 is True\n    eeg_file: int\n        If more than one eeg channel was recorded from then they are numbered\n        from 1 onwards i.e. trial.eeg, trial.eeg1, trial.eeg2 etc\n        This number specifies that\n\n    \"\"\"\n\n    def __init__(self, filename_root: Path, eeg_file=1, egf=0):\n        self.showfigs = 0\n        filename_root = Path(os.path.splitext(filename_root)[0])\n        self.filename_root = filename_root\n        if egf == 0:\n            if eeg_file == 1:\n                eeg_suffix = \".eeg\"\n            else:\n                eeg_suffix = \".eeg\" + str(eeg_file)\n        elif egf == 1:\n            if eeg_file == 1:\n                eeg_suffix = \".egf\"\n            else:\n                eeg_suffix = \".egf\" + str(eeg_file)\n        self.header = self.getHeader(self.filename_root.with_suffix(eeg_suffix))\n        self.eeg = self.getData(filename_root.with_suffix(eeg_suffix))[\"eeg\"]\n        # sometimes the eeg record is longer than reported in\n        # the 'num_EEG_samples'\n        # value of the header so eeg record should be truncated\n        # to match 'num_EEG_samples'\n        # TODO: this could be taken care of in the IO base class\n        if egf:\n            self.eeg = self.eeg[0 : int(self.header[\"num_EGF_samples\"])]\n        else:\n            self.eeg = self.eeg[0 : int(self.header[\"num_EEG_samples\"])]\n        self.sample_rate = int(self.getHeaderVal(self.header, \"sample_rate\"))\n        set_header = self.getHeader(self.filename_root.with_suffix(\".set\"))\n        eeg_ch = int(set_header[\"EEG_ch_1\"]) - 1\n        eeg_gain = int(set_header[\"gain_ch_\" + str(eeg_ch)])\n        # EEG polarity is determined by the \"mode_ch_n\" key in the setfile\n        # where n is the channel # for the eeg. The possibles values to these\n        # keys are as follows:\n        # 0 = Signal\n        # 1 = Ref\n        # 2 = -Signal\n        # 3 = -Ref\n        # 4 = Sig-Ref\n        # 5 = Ref-Sig\n        # 6 = grounded\n        # So if the EEG has been recorded with -Signal (2) then the recorded\n        # polarity is inverted with respect to that in the brain\n        eeg_mode = int(set_header[\"mode_ch_\" + set_header[\"EEG_ch_1\"]])\n        polarity = 1  # ensure it always has a value\n        if eeg_mode == 2:\n            polarity = -1\n        ADC_mv = float(set_header[\"ADC_fullscale_mv\"])\n        scaling = (ADC_mv / 1000.0) * eeg_gain\n        self.scaling = scaling\n        self.gain = eeg_gain\n        self.polarity = polarity\n        denom = 128.0\n        self.sig = (self.eeg / denom) * scaling * polarity  # eeg in microvolts\n        self.EEGphase = None\n        # x1 / x2 are the lower and upper limits of the eeg filter\n        self.x1 = 6\n        self.x2 = 12\n</code></pre>"},{"location":"reference/#ephysiopy.axona.axonaIO.IO","title":"<code>IO</code>","text":"<p>               Bases: <code>object</code></p> <p>Axona data I/O. Also reads .clu files generated from KlustaKwik</p> <p>Parameters:</p> Name Type Description Default <code>filename_root</code> <code>str</code> <p>The fully-qualified filename</p> <code>''</code> Source code in <code>ephysiopy/axona/axonaIO.py</code> <pre><code>class IO(object):\n    \"\"\"\n    Axona data I/O. Also reads .clu files generated from KlustaKwik\n\n    Parameters\n    ----------\n    filename_root : str\n        The fully-qualified filename\n    \"\"\"\n\n    tetrode_files = dict.fromkeys(\n        [\".\" + str(i) for i in range(1, 17)],\n        # ts is a big-endian 32-bit integer\n        # waveform is 50 signed 8-bit ints (a signed byte)\n        [(\"ts\", \"&gt;i\"), (\"waveform\", \"50b\")],\n    )\n    other_files = {\n        \".pos\": [(\"ts\", \"&gt;i\"), (\"pos\", \"&gt;8h\")],\n        \".eeg\": [(\"eeg\", \"=b\")],\n        \".eeg2\": [(\"eeg\", \"=b\")],\n        \".egf\": [(\"eeg\", \"int16\")],\n        \".egf2\": [(\"eeg\", \"int16\")],\n        \".inp\": [(\"ts\", \"&gt;i4\"), (\"type\", \"&gt;b\"), (\"value\", \"&gt;2b\")],\n        \".log\": [(\"state\", \"S3\"), (\"ts\", \"&gt;i\")],\n        \".stm\": [(\"ts\", \"&gt;i\")],\n    }\n\n    # this only works in &gt;= Python3.5\n    axona_files = {**other_files, **tetrode_files}\n\n    def __init__(self, filename_root: Path = \"\"):\n        self.filename_root = filename_root\n\n    def getData(self, filename_root: str) -&gt; np.ndarray:\n        \"\"\"\n        Returns the data part of an Axona data file i.e. from \"data_start\" to\n        \"data_end\"\n\n        Parameters\n        ----------\n        filename_root:  str\n            Fully qualified path name to the data file\n\n        Returns\n        -------\n        output : ndarray\n            The data part of whatever file was fed in\n        \"\"\"\n        n_samps = -1\n        fType = os.path.splitext(filename_root)[1]\n        if fType in self.axona_files:\n            header = self.getHeader(filename_root)\n            for key in header.keys():\n                if len(fType) &gt; 2:\n                    if fnmatch.fnmatch(key, \"num_*_samples\"):\n                        n_samps = int(header[key])\n                else:\n                    if key.startswith(\"num_spikes\"):\n                        n_samps = int(header[key]) * 4\n            f = open(filename_root, \"rb\")\n            data = f.read()\n            st = data.find(b\"data_start\") + len(\"data_start\")\n            f.seek(st)\n            dt = np.dtype(self.axona_files[fType])\n            a = np.fromfile(f, dtype=dt, count=n_samps)\n            f.close()\n        else:\n            raise IOError(\"File not in list of recognised Axona files\")\n        return a\n\n    def getCluCut(self, tet: int) -&gt; np.ndarray:\n        \"\"\"\n        Load a clu file and return as an array of integers\n\n        Parameters\n        ----------\n        tet : int\n            The tetrode the clu file relates to\n\n        Returns\n        -------\n        out : ndarray\n            Data read from the clu file\n        \"\"\"\n        filename_root = self.filename_root.with_suffix(\".\" + \"clu.\" + str(tet))\n        if os.path.exists(filename_root):\n            dt = np.dtype([(\"data\", \"&lt;i\")])\n            clu_data = np.loadtxt(filename_root, dtype=dt)\n            return clu_data[\"data\"][1::]  # first entry is num of clusters\n        else:\n            return None\n\n    def getCut(self, tet: int) -&gt; list:\n        \"\"\"\n        Returns the cut file as a list of integers\n\n        Parameters\n        ----------\n        tet : int\n            The tetrode the cut file relates to\n\n        Returns\n        -------\n        out : ndarray\n            The data read from the cut file\n        \"\"\"\n        a = []\n        filename_root = Path(\n            os.path.splitext(self.filename_root)[0] + \"_\" + str(tet) + \".cut\"\n        )\n\n        if not os.path.exists(filename_root):\n            cut = self.getCluCut(tet)\n            if cut is not None:\n                return cut - 1  # clusters 1 indexed in clu\n            return cut\n        with open(filename_root, \"r\") as f:\n            cut_data = f.read()\n            f.close()\n        tmp = cut_data.split(\"spikes: \")\n        tmp1 = tmp[1].split(\"\\n\")\n        cut = tmp1[1:]\n        for line in cut:\n            m = line.split()\n            for i in m:\n                a.append(int(i))\n        return a\n\n    def setHeader(self, filename_root: str, header: dataclass):\n        \"\"\"\n        Writes out the header to the specified file\n\n        Parameters\n        ------------\n        filename_root : str\n            A fully qualified path to a file with the relevant suffix at\n            the end (e.g. \".set\", \".pos\" or whatever)\n\n        header : dataclass\n            See ephysiopy.axona.file_headers\n        \"\"\"\n        with open(filename_root, \"w\") as f:\n            with redirect_stdout(f):\n                header.print()\n            f.write(\"data_start\")\n            f.write(\"\\r\\n\")\n            f.write(\"data_end\")\n            f.write(\"\\r\\n\")\n\n    def setCut(self, filename_root: str, cut_header: dataclass, cut_data: np.array):\n        fpath = Path(filename_root)\n        n_clusters = len(np.unique(cut_data))\n        cluster_entries = make_cluster_cut_entries(n_clusters)\n        with open(filename_root, \"w\") as f:\n            with redirect_stdout(f):\n                cut_header.print()\n            print(cluster_entries, file=f)\n            print(f\"Exact_cut_for: {fpath.stem}    spikes: {len(cut_data)}\", file=f)\n            for num in cut_data:\n                f.write(str(num))\n                f.write(\" \")\n\n    def setData(self, filename_root: str, data: np.array):\n        \"\"\"\n        Writes Axona format data to the given filename\n\n        Parameters\n        ----------\n        filename_root : str\n            The fully qualified filename including the suffix\n\n        data : ndarray\n            The data that will be saved\n        \"\"\"\n        fType = os.path.splitext(filename_root)[1]\n        if fType in self.axona_files:\n            f = open(filename_root, \"rb+\")\n            d = f.read()\n            st = d.find(b\"data_start\") + len(\"data_start\")\n            f.seek(st)\n            data.tofile(f)\n            f.close()\n            f = open(filename_root, \"a\")\n            f.write(\"\\r\\n\")\n            f.write(\"data_end\")\n            f.write(\"\\r\\n\")\n            f.close()\n\n    def getHeader(self, filename_root: str) -&gt; dict:\n        \"\"\"\n        Reads and returns the header of a specified data file as a dictionary\n\n        Parameters\n        ----------\n        filename_root : str\n            Fully qualified filename of Axona type\n\n        Returns\n        -------\n        headerDict : dict\n            key - value pairs of the header part of an Axona type file\n        \"\"\"\n        with open(filename_root, \"rb\") as f:\n            data = f.read()\n            f.close()\n        if os.path.splitext(filename_root)[1] != \".set\":\n            st = data.find(b\"data_start\") + len(\"data_start\")\n            header = data[0 : st - len(\"data_start\") - 2]\n        else:\n            header = data\n        headerDict = {}\n        lines = header.splitlines()\n        for line in lines:\n            line = str(line.decode(\"ISO-8859-1\")).rstrip()\n            line = line.split(\" \", 1)\n            try:\n                headerDict[line[0]] = line[1]\n            except IndexError:\n                headerDict[line[0]] = \"\"\n        return headerDict\n\n    def getHeaderVal(self, header: dict, key: str) -&gt; int:\n        \"\"\"\n        Get a value from the header as an int\n\n        Parameters\n        ----------\n        header : dict\n            The header dictionary to read\n        key : str\n            The key to look up\n\n        Returns\n        -------\n        value : int\n            The value of `key` as an int\n        \"\"\"\n        tmp = header[key]\n        val = tmp.split(\" \")\n        val = val[0].split(\".\")\n        val = int(val[0])\n        return val\n</code></pre>"},{"location":"reference/#ephysiopy.axona.axonaIO.IO.getCluCut","title":"<code>getCluCut(tet)</code>","text":"<p>Load a clu file and return as an array of integers</p> <p>Parameters:</p> Name Type Description Default <code>tet</code> <code>int</code> <p>The tetrode the clu file relates to</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>ndarray</code> <p>Data read from the clu file</p> Source code in <code>ephysiopy/axona/axonaIO.py</code> <pre><code>def getCluCut(self, tet: int) -&gt; np.ndarray:\n    \"\"\"\n    Load a clu file and return as an array of integers\n\n    Parameters\n    ----------\n    tet : int\n        The tetrode the clu file relates to\n\n    Returns\n    -------\n    out : ndarray\n        Data read from the clu file\n    \"\"\"\n    filename_root = self.filename_root.with_suffix(\".\" + \"clu.\" + str(tet))\n    if os.path.exists(filename_root):\n        dt = np.dtype([(\"data\", \"&lt;i\")])\n        clu_data = np.loadtxt(filename_root, dtype=dt)\n        return clu_data[\"data\"][1::]  # first entry is num of clusters\n    else:\n        return None\n</code></pre>"},{"location":"reference/#ephysiopy.axona.axonaIO.IO.getCut","title":"<code>getCut(tet)</code>","text":"<p>Returns the cut file as a list of integers</p> <p>Parameters:</p> Name Type Description Default <code>tet</code> <code>int</code> <p>The tetrode the cut file relates to</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>ndarray</code> <p>The data read from the cut file</p> Source code in <code>ephysiopy/axona/axonaIO.py</code> <pre><code>def getCut(self, tet: int) -&gt; list:\n    \"\"\"\n    Returns the cut file as a list of integers\n\n    Parameters\n    ----------\n    tet : int\n        The tetrode the cut file relates to\n\n    Returns\n    -------\n    out : ndarray\n        The data read from the cut file\n    \"\"\"\n    a = []\n    filename_root = Path(\n        os.path.splitext(self.filename_root)[0] + \"_\" + str(tet) + \".cut\"\n    )\n\n    if not os.path.exists(filename_root):\n        cut = self.getCluCut(tet)\n        if cut is not None:\n            return cut - 1  # clusters 1 indexed in clu\n        return cut\n    with open(filename_root, \"r\") as f:\n        cut_data = f.read()\n        f.close()\n    tmp = cut_data.split(\"spikes: \")\n    tmp1 = tmp[1].split(\"\\n\")\n    cut = tmp1[1:]\n    for line in cut:\n        m = line.split()\n        for i in m:\n            a.append(int(i))\n    return a\n</code></pre>"},{"location":"reference/#ephysiopy.axona.axonaIO.IO.getData","title":"<code>getData(filename_root)</code>","text":"<p>Returns the data part of an Axona data file i.e. from \"data_start\" to \"data_end\"</p> <p>Parameters:</p> Name Type Description Default <code>filename_root</code> <code>str</code> <p>Fully qualified path name to the data file</p> required <p>Returns:</p> Name Type Description <code>output</code> <code>ndarray</code> <p>The data part of whatever file was fed in</p> Source code in <code>ephysiopy/axona/axonaIO.py</code> <pre><code>def getData(self, filename_root: str) -&gt; np.ndarray:\n    \"\"\"\n    Returns the data part of an Axona data file i.e. from \"data_start\" to\n    \"data_end\"\n\n    Parameters\n    ----------\n    filename_root:  str\n        Fully qualified path name to the data file\n\n    Returns\n    -------\n    output : ndarray\n        The data part of whatever file was fed in\n    \"\"\"\n    n_samps = -1\n    fType = os.path.splitext(filename_root)[1]\n    if fType in self.axona_files:\n        header = self.getHeader(filename_root)\n        for key in header.keys():\n            if len(fType) &gt; 2:\n                if fnmatch.fnmatch(key, \"num_*_samples\"):\n                    n_samps = int(header[key])\n            else:\n                if key.startswith(\"num_spikes\"):\n                    n_samps = int(header[key]) * 4\n        f = open(filename_root, \"rb\")\n        data = f.read()\n        st = data.find(b\"data_start\") + len(\"data_start\")\n        f.seek(st)\n        dt = np.dtype(self.axona_files[fType])\n        a = np.fromfile(f, dtype=dt, count=n_samps)\n        f.close()\n    else:\n        raise IOError(\"File not in list of recognised Axona files\")\n    return a\n</code></pre>"},{"location":"reference/#ephysiopy.axona.axonaIO.IO.getHeader","title":"<code>getHeader(filename_root)</code>","text":"<p>Reads and returns the header of a specified data file as a dictionary</p> <p>Parameters:</p> Name Type Description Default <code>filename_root</code> <code>str</code> <p>Fully qualified filename of Axona type</p> required <p>Returns:</p> Name Type Description <code>headerDict</code> <code>dict</code> <p>key - value pairs of the header part of an Axona type file</p> Source code in <code>ephysiopy/axona/axonaIO.py</code> <pre><code>def getHeader(self, filename_root: str) -&gt; dict:\n    \"\"\"\n    Reads and returns the header of a specified data file as a dictionary\n\n    Parameters\n    ----------\n    filename_root : str\n        Fully qualified filename of Axona type\n\n    Returns\n    -------\n    headerDict : dict\n        key - value pairs of the header part of an Axona type file\n    \"\"\"\n    with open(filename_root, \"rb\") as f:\n        data = f.read()\n        f.close()\n    if os.path.splitext(filename_root)[1] != \".set\":\n        st = data.find(b\"data_start\") + len(\"data_start\")\n        header = data[0 : st - len(\"data_start\") - 2]\n    else:\n        header = data\n    headerDict = {}\n    lines = header.splitlines()\n    for line in lines:\n        line = str(line.decode(\"ISO-8859-1\")).rstrip()\n        line = line.split(\" \", 1)\n        try:\n            headerDict[line[0]] = line[1]\n        except IndexError:\n            headerDict[line[0]] = \"\"\n    return headerDict\n</code></pre>"},{"location":"reference/#ephysiopy.axona.axonaIO.IO.getHeaderVal","title":"<code>getHeaderVal(header, key)</code>","text":"<p>Get a value from the header as an int</p> <p>Parameters:</p> Name Type Description Default <code>header</code> <code>dict</code> <p>The header dictionary to read</p> required <code>key</code> <code>str</code> <p>The key to look up</p> required <p>Returns:</p> Name Type Description <code>value</code> <code>int</code> <p>The value of <code>key</code> as an int</p> Source code in <code>ephysiopy/axona/axonaIO.py</code> <pre><code>def getHeaderVal(self, header: dict, key: str) -&gt; int:\n    \"\"\"\n    Get a value from the header as an int\n\n    Parameters\n    ----------\n    header : dict\n        The header dictionary to read\n    key : str\n        The key to look up\n\n    Returns\n    -------\n    value : int\n        The value of `key` as an int\n    \"\"\"\n    tmp = header[key]\n    val = tmp.split(\" \")\n    val = val[0].split(\".\")\n    val = int(val[0])\n    return val\n</code></pre>"},{"location":"reference/#ephysiopy.axona.axonaIO.IO.setData","title":"<code>setData(filename_root, data)</code>","text":"<p>Writes Axona format data to the given filename</p> <p>Parameters:</p> Name Type Description Default <code>filename_root</code> <code>str</code> <p>The fully qualified filename including the suffix</p> required <code>data</code> <code>ndarray</code> <p>The data that will be saved</p> required Source code in <code>ephysiopy/axona/axonaIO.py</code> <pre><code>def setData(self, filename_root: str, data: np.array):\n    \"\"\"\n    Writes Axona format data to the given filename\n\n    Parameters\n    ----------\n    filename_root : str\n        The fully qualified filename including the suffix\n\n    data : ndarray\n        The data that will be saved\n    \"\"\"\n    fType = os.path.splitext(filename_root)[1]\n    if fType in self.axona_files:\n        f = open(filename_root, \"rb+\")\n        d = f.read()\n        st = d.find(b\"data_start\") + len(\"data_start\")\n        f.seek(st)\n        data.tofile(f)\n        f.close()\n        f = open(filename_root, \"a\")\n        f.write(\"\\r\\n\")\n        f.write(\"data_end\")\n        f.write(\"\\r\\n\")\n        f.close()\n</code></pre>"},{"location":"reference/#ephysiopy.axona.axonaIO.IO.setHeader","title":"<code>setHeader(filename_root, header)</code>","text":"<p>Writes out the header to the specified file</p> <p>Parameters:</p> Name Type Description Default <code>filename_root</code> <code>str</code> <p>A fully qualified path to a file with the relevant suffix at the end (e.g. \".set\", \".pos\" or whatever)</p> required <code>header</code> <code>dataclass</code> <p>See ephysiopy.axona.file_headers</p> required Source code in <code>ephysiopy/axona/axonaIO.py</code> <pre><code>def setHeader(self, filename_root: str, header: dataclass):\n    \"\"\"\n    Writes out the header to the specified file\n\n    Parameters\n    ------------\n    filename_root : str\n        A fully qualified path to a file with the relevant suffix at\n        the end (e.g. \".set\", \".pos\" or whatever)\n\n    header : dataclass\n        See ephysiopy.axona.file_headers\n    \"\"\"\n    with open(filename_root, \"w\") as f:\n        with redirect_stdout(f):\n            header.print()\n        f.write(\"data_start\")\n        f.write(\"\\r\\n\")\n        f.write(\"data_end\")\n        f.write(\"\\r\\n\")\n</code></pre>"},{"location":"reference/#ephysiopy.axona.axonaIO.Pos","title":"<code>Pos</code>","text":"<p>               Bases: <code>IO</code></p> <p>Processs position data recorded with the Axona recording system</p> <p>Parameters:</p> Name Type Description Default <code>filename_root</code> <code>str</code> <p>The basename of the file i.e mytrial as opposed to mytrial.pos</p> required Notes <p>Currently the only arg that does anything is 'cm' which will convert the xy data to cm, assuming that the pixels per metre value has been set correctly</p> Source code in <code>ephysiopy/axona/axonaIO.py</code> <pre><code>class Pos(IO):\n    \"\"\"\n    Processs position data recorded with the Axona recording system\n\n    Parameters\n    ----------\n    filename_root : str\n        The basename of the file i.e mytrial as opposed to mytrial.pos\n\n    Notes\n    -----\n    Currently the only arg that does anything is 'cm' which will convert\n    the xy data to cm, assuming that the pixels per metre value has been\n    set correctly\n    \"\"\"\n\n    def __init__(self, filename_root: Path, *args, **kwargs):\n        filename_root = Path(filename_root)\n        if filename_root.suffix == \".set\":\n            filename_root = Path(os.path.splitext(filename_root)[0])\n        self.filename_root = filename_root\n        self.header = self.getHeader(filename_root.with_suffix(\".pos\"))\n        self.setheader = None\n        self.setheader = self.getHeader(filename_root.with_suffix(\".set\"))\n        self.posProcessed = False\n        posData = self.getData(filename_root.with_suffix(\".pos\"))\n        self.nLEDs = 1\n        if self.setheader is not None:\n            self.nLEDs = sum(\n                [\n                    self.getHeaderVal(self.setheader, \"colactive_1\"),\n                    self.getHeaderVal(self.setheader, \"colactive_2\"),\n                ]\n            )\n        if self.nLEDs == 1:\n            self.led_pos = np.ma.MaskedArray(posData[\"pos\"][:, 0:2])\n            self.led_pix = np.ma.MaskedArray([posData[\"pos\"][:, 4]])\n        if self.nLEDs == 2:\n            self.led_pos = np.ma.MaskedArray(posData[\"pos\"][:, 0:4])\n            self.led_pix = np.ma.MaskedArray(posData[\"pos\"][:, 4:6])\n        self.led_pos = np.ma.masked_equal(self.led_pos, 1023)\n        self.led_pix = np.ma.masked_equal(self.led_pix, 1023)\n        self.ts = np.array(posData[\"ts\"])\n        self.npos = len(self.led_pos[0])\n        self.xy = np.ones([2, self.npos]) * np.nan\n        self.dir = np.ones([self.npos]) * np.nan\n        self.dir_disp = np.ones([self.npos]) * np.nan\n        self.speed = np.ones([self.npos]) * np.nan\n        self.pos_sample_rate = self.getHeaderVal(self.header, \"sample_rate\")\n        self._ppm = None\n        if \"cm\" in kwargs:\n            self.cm = kwargs[\"cm\"]\n        else:\n            self.cm = False\n\n    @property\n    def ppm(self):\n        if self._ppm is None:\n            self._ppm = self.getHeaderVal(self.header, \"pixels_per_metre\")\n        return self._ppm\n\n    @ppm.setter\n    def ppm(self, value):\n        self._ppm = value\n</code></pre>"},{"location":"reference/#ephysiopy.axona.axonaIO.Stim","title":"<code>Stim</code>","text":"<p>               Bases: <code>dict</code>, <code>IO</code></p> <p>Processes the stimulation data recorded using Axona</p> <p>Parameters:</p> Name Type Description Default <code>filename_root</code> <code>str</code> <p>The fully qualified filename without the suffix</p> required Source code in <code>ephysiopy/axona/axonaIO.py</code> <pre><code>class Stim(dict, IO):\n    \"\"\"\n    Processes the stimulation data recorded using Axona\n\n    Parameters\n    ----------\n    filename_root : str\n        The fully qualified filename without the suffix\n    \"\"\"\n\n    def __init__(self, filename_root: Path, *args, **kwargs):\n        self.update(*args, **kwargs)\n        filename_root = Path(os.path.splitext(filename_root)[0])\n        self.filename_root = filename_root\n        stmData = self.getData(filename_root.with_suffix(\".stm\"))\n        times = stmData[\"ts\"]\n        stmHdr = self.getHeader(filename_root.with_suffix(\".stm\"))\n        for k, v in stmHdr.items():\n            self.__setitem__(k, v)\n        tb = int(self[\"timebase\"].split(\" \")[0])\n        self.timebase = tb\n        times = times / tb\n        self.__setitem__(\"ttl_timestamps\", times)  # in SECONDS\n        # the 'duration' value in the header of the .stm file\n        # is not correct so we need to read this from the .set\n        # file and update\n        setHdr = self.getHeader(filename_root.with_suffix(\".set\"))\n        stim_duration = [setHdr[k] for k in setHdr.keys() if \"stim_pwidth\" in k][0]\n        stim_duration = int(stim_duration)\n        stim_duration = stim_duration  # in seconds\n        self.__setitem__(\"stim_duration\", stim_duration)\n\n    def update(self, *args, **kwargs):\n        for k, v in dict(*args, **kwargs).items():\n            self[k] = v\n\n    def __getitem__(self, key):\n        val = dict.__getitem__(self, key)\n        return val\n\n    def __setitem__(self, key, val):\n        dict.__setitem__(self, key, val)\n</code></pre>"},{"location":"reference/#ephysiopy.axona.axonaIO.Tetrode","title":"<code>Tetrode</code>","text":"<p>               Bases: <code>IO</code></p> <p>Processes tetrode files recorded with the Axona recording system</p> <p>Mostly this class deals with interpolating tetrode and position timestamps and getting indices for particular clusters.</p> <p>Parameters:</p> Name Type Description Default <code>filename_root</code> <code>str</code> <p>The fully qualified name of the file without it's suffix</p> required <code>tetrode</code> <code>int</code> <p>The number of the tetrode</p> required <code>volts</code> <code>bool</code> <p>Whether to convert the data values volts. Default True</p> <code>True</code> Source code in <code>ephysiopy/axona/axonaIO.py</code> <pre><code>class Tetrode(IO):\n    \"\"\"\n    Processes tetrode files recorded with the Axona recording system\n\n    Mostly this class deals with interpolating tetrode and position timestamps\n    and getting indices for particular clusters.\n\n    Parameters\n    ---------\n    filename_root : str\n        The fully qualified name of the file without it's suffix\n    tetrode : int\n        The number of the tetrode\n    volts : bool, optional\n        Whether to convert the data values volts. Default True\n    \"\"\"\n\n    def __init__(self, filename_root: Path, tetrode, volts=True):\n        filename_root = Path(filename_root)\n        if filename_root.suffix == \".set\":\n            filename_root = Path(os.path.splitext(filename_root)[0])\n        self.filename_root = filename_root\n        self.tetrode = tetrode\n        self.volts = volts\n        self.header = self.getHeader(self.filename_root.with_suffix(\".\" + str(tetrode)))\n        data = self.getData(filename_root.with_suffix(\".\" + str(tetrode)))\n        self.spike_times = np.ma.MaskedArray(data[\"ts\"][::4])\n        self.nChans = self.getHeaderVal(self.header, \"num_chans\")\n        self.samples = self.getHeaderVal(self.header, \"samples_per_spike\")\n        self.nSpikes = self.getHeaderVal(self.header, \"num_spikes\")\n        self.duration = self.getHeaderVal(self.header, \"duration\")\n        self.posSampleRate = self.getHeaderVal(\n            self.getHeader(self.filename_root.with_suffix(\".pos\")), \"sample_rate\"\n        )\n        self.waveforms = np.ma.MaskedArray(\n            data[\"waveform\"].reshape(self.nSpikes, self.nChans, self.samples)\n        )\n        del data\n        if volts:\n            set_header = self.getHeader(self.filename_root.with_suffix(\".set\"))\n            gains = np.zeros(4)\n            st = (tetrode - 1) * 4\n            for i, g in enumerate(np.arange(st, st + 4)):\n                gains[i] = int(set_header[\"gain_ch_\" + str(g)])\n            ADC_mv = int(set_header[\"ADC_fullscale_mv\"])\n            scaling = (ADC_mv / 1000.0) / gains\n            self.scaling = scaling\n            self.gains = gains\n            self.waveforms = (self.waveforms / 128.0) * scaling[:, np.newaxis]\n        self.timebase = self.getHeaderVal(self.header, \"timebase\")\n        cut = np.array(self.getCut(self.tetrode), dtype=int)\n        self.cut = np.ma.MaskedArray(cut)\n        self.clusters = np.unique(self.cut)\n        self.pos_samples = None\n\n    def getSpkTS(self):\n        \"\"\"\n        Return all the timestamps for all the spikes on the tetrode\n        \"\"\"\n        return np.ma.compressed(self.spike_times)\n\n    def getClustTS(self, cluster: int = None):\n        \"\"\"\n        Returns the timestamps for a cluster on the tetrode\n\n        Parameters\n        ----------\n        cluster : int\n            The cluster whose timestamps we want\n\n        Returns\n        -------\n        clustTS : ndarray\n            The timestamps\n\n        Notes\n        -----\n        If None is supplied as input then all timestamps for all clusters\n        is returned i.e. getSpkTS() is called\n        \"\"\"\n        clustTS = None\n        if cluster is None:\n            clustTS = self.getSpkTS()\n        else:\n            if self.cut is None:\n                cut = np.array(self.getCut(self.tetrode), dtype=int)\n                self.cut = cut\n            if self.cut is not None:\n                clustTS = self.spike_times[self.cut == cluster]\n                # clustTS = np.ma.compressed(self.spike_times[self.cut == cluster])\n        return clustTS\n\n    def getPosSamples(self):\n        \"\"\"\n        Returns the pos samples at which the spikes were captured\n        \"\"\"\n        self.pos_samples = np.floor(\n            self.getSpkTS() / float(self.timebase) * self.posSampleRate\n        ).astype(int)\n        return np.ma.compressed(self.pos_samples)\n\n    def getClustSpks(self, cluster: int):\n        \"\"\"\n        Returns the waveforms of `cluster`\n\n        Parameters\n        ----------\n        cluster : int\n            The cluster whose waveforms we want\n\n        Returns\n        -------\n        waveforms : ndarray\n            The waveforms on all 4 electrodes of the tgtrode so the shape of\n            the returned array is [nClusterSpikes, 4, 50]\n        \"\"\"\n        if self.cut is None:\n            self.getClustTS(cluster)\n        return self.waveforms[self.cut == cluster, :, :]\n\n    def getClustIdx(self, cluster: int):\n        \"\"\"\n        Get the indices of the position samples corresponding to the cluster\n\n        Parameters\n        ----------\n        cluster : int\n            The cluster whose position indices we want\n\n        Returns\n        -------\n        pos_samples : ndarray\n            The indices of the position samples, dtype is int\n        \"\"\"\n        if self.cut is None:\n            cut = np.array(self.getCut(self.tetrode), dtype=int)\n            self.cut = cut\n            if self.cut is None:\n                return None\n        if self.pos_samples is None:\n            self.getPosSamples()  # sets self.pos_samples\n        return self.pos_samples[self.cut == cluster].astype(int)\n\n    def getUniqueClusters(self):\n        \"\"\"\n        Returns the unique clusters\n        \"\"\"\n        if self.cut is None:\n            cut = np.array(self.getCut(self.tetrode), dtype=int)\n            self.cut = cut\n        return np.unique(self.cut)\n\n    def apply_mask(self, mask, **kwargs) -&gt; None:\n        \"\"\"Apply a mask to the data\n\n        Args:\n            mask (np.ndarray): The mask to be applied. For use with\n                               np.ma.MaskedArray's mask attribute\n\n        Returns:\n            None\n\n        Note:\n        The times inside the bounds are masked ie the mask is set to True\n        The mask can be a list of tuples, in which case the mask is applied\n        for each tuple in the list.\n        mask can be an empty tuple, in which case the mask is removed\n\n        \"\"\"\n        if np.any(mask):\n            sample_rate = kwargs.get(\"sample_rate\", 50)\n            timebase = self.getHeaderVal(self.header, \"timebase\")\n            spike_pos_samples = np.ma.MaskedArray(\n                self.spike_times.data / timebase * sample_rate, dtype=int\n            )\n            pos_times_in_samples = np.nonzero(mask)[1]\n            mask = np.ma.isin(spike_pos_samples, pos_times_in_samples)\n        if mask is not None:\n            self.spike_times.mask = mask.data\n        if mask is not None:\n            self.waveforms.mask = mask.data\n        if mask is not None:\n            self.cut.mask = mask.data\n</code></pre>"},{"location":"reference/#ephysiopy.axona.axonaIO.Tetrode.apply_mask","title":"<code>apply_mask(mask, **kwargs)</code>","text":"<p>Apply a mask to the data</p> <p>Args:     mask (np.ndarray): The mask to be applied. For use with                        np.ma.MaskedArray's mask attribute</p> <p>Returns:     None</p> <p>Note: The times inside the bounds are masked ie the mask is set to True The mask can be a list of tuples, in which case the mask is applied for each tuple in the list. mask can be an empty tuple, in which case the mask is removed</p> Source code in <code>ephysiopy/axona/axonaIO.py</code> <pre><code>def apply_mask(self, mask, **kwargs) -&gt; None:\n    \"\"\"Apply a mask to the data\n\n    Args:\n        mask (np.ndarray): The mask to be applied. For use with\n                           np.ma.MaskedArray's mask attribute\n\n    Returns:\n        None\n\n    Note:\n    The times inside the bounds are masked ie the mask is set to True\n    The mask can be a list of tuples, in which case the mask is applied\n    for each tuple in the list.\n    mask can be an empty tuple, in which case the mask is removed\n\n    \"\"\"\n    if np.any(mask):\n        sample_rate = kwargs.get(\"sample_rate\", 50)\n        timebase = self.getHeaderVal(self.header, \"timebase\")\n        spike_pos_samples = np.ma.MaskedArray(\n            self.spike_times.data / timebase * sample_rate, dtype=int\n        )\n        pos_times_in_samples = np.nonzero(mask)[1]\n        mask = np.ma.isin(spike_pos_samples, pos_times_in_samples)\n    if mask is not None:\n        self.spike_times.mask = mask.data\n    if mask is not None:\n        self.waveforms.mask = mask.data\n    if mask is not None:\n        self.cut.mask = mask.data\n</code></pre>"},{"location":"reference/#ephysiopy.axona.axonaIO.Tetrode.getClustIdx","title":"<code>getClustIdx(cluster)</code>","text":"<p>Get the indices of the position samples corresponding to the cluster</p> <p>Parameters:</p> Name Type Description Default <code>cluster</code> <code>int</code> <p>The cluster whose position indices we want</p> required <p>Returns:</p> Name Type Description <code>pos_samples</code> <code>ndarray</code> <p>The indices of the position samples, dtype is int</p> Source code in <code>ephysiopy/axona/axonaIO.py</code> <pre><code>def getClustIdx(self, cluster: int):\n    \"\"\"\n    Get the indices of the position samples corresponding to the cluster\n\n    Parameters\n    ----------\n    cluster : int\n        The cluster whose position indices we want\n\n    Returns\n    -------\n    pos_samples : ndarray\n        The indices of the position samples, dtype is int\n    \"\"\"\n    if self.cut is None:\n        cut = np.array(self.getCut(self.tetrode), dtype=int)\n        self.cut = cut\n        if self.cut is None:\n            return None\n    if self.pos_samples is None:\n        self.getPosSamples()  # sets self.pos_samples\n    return self.pos_samples[self.cut == cluster].astype(int)\n</code></pre>"},{"location":"reference/#ephysiopy.axona.axonaIO.Tetrode.getClustSpks","title":"<code>getClustSpks(cluster)</code>","text":"<p>Returns the waveforms of <code>cluster</code></p> <p>Parameters:</p> Name Type Description Default <code>cluster</code> <code>int</code> <p>The cluster whose waveforms we want</p> required <p>Returns:</p> Name Type Description <code>waveforms</code> <code>ndarray</code> <p>The waveforms on all 4 electrodes of the tgtrode so the shape of the returned array is [nClusterSpikes, 4, 50]</p> Source code in <code>ephysiopy/axona/axonaIO.py</code> <pre><code>def getClustSpks(self, cluster: int):\n    \"\"\"\n    Returns the waveforms of `cluster`\n\n    Parameters\n    ----------\n    cluster : int\n        The cluster whose waveforms we want\n\n    Returns\n    -------\n    waveforms : ndarray\n        The waveforms on all 4 electrodes of the tgtrode so the shape of\n        the returned array is [nClusterSpikes, 4, 50]\n    \"\"\"\n    if self.cut is None:\n        self.getClustTS(cluster)\n    return self.waveforms[self.cut == cluster, :, :]\n</code></pre>"},{"location":"reference/#ephysiopy.axona.axonaIO.Tetrode.getClustTS","title":"<code>getClustTS(cluster=None)</code>","text":"<p>Returns the timestamps for a cluster on the tetrode</p> <p>Parameters:</p> Name Type Description Default <code>cluster</code> <code>int</code> <p>The cluster whose timestamps we want</p> <code>None</code> <p>Returns:</p> Name Type Description <code>clustTS</code> <code>ndarray</code> <p>The timestamps</p> Notes <p>If None is supplied as input then all timestamps for all clusters is returned i.e. getSpkTS() is called</p> Source code in <code>ephysiopy/axona/axonaIO.py</code> <pre><code>def getClustTS(self, cluster: int = None):\n    \"\"\"\n    Returns the timestamps for a cluster on the tetrode\n\n    Parameters\n    ----------\n    cluster : int\n        The cluster whose timestamps we want\n\n    Returns\n    -------\n    clustTS : ndarray\n        The timestamps\n\n    Notes\n    -----\n    If None is supplied as input then all timestamps for all clusters\n    is returned i.e. getSpkTS() is called\n    \"\"\"\n    clustTS = None\n    if cluster is None:\n        clustTS = self.getSpkTS()\n    else:\n        if self.cut is None:\n            cut = np.array(self.getCut(self.tetrode), dtype=int)\n            self.cut = cut\n        if self.cut is not None:\n            clustTS = self.spike_times[self.cut == cluster]\n            # clustTS = np.ma.compressed(self.spike_times[self.cut == cluster])\n    return clustTS\n</code></pre>"},{"location":"reference/#ephysiopy.axona.axonaIO.Tetrode.getPosSamples","title":"<code>getPosSamples()</code>","text":"<p>Returns the pos samples at which the spikes were captured</p> Source code in <code>ephysiopy/axona/axonaIO.py</code> <pre><code>def getPosSamples(self):\n    \"\"\"\n    Returns the pos samples at which the spikes were captured\n    \"\"\"\n    self.pos_samples = np.floor(\n        self.getSpkTS() / float(self.timebase) * self.posSampleRate\n    ).astype(int)\n    return np.ma.compressed(self.pos_samples)\n</code></pre>"},{"location":"reference/#ephysiopy.axona.axonaIO.Tetrode.getSpkTS","title":"<code>getSpkTS()</code>","text":"<p>Return all the timestamps for all the spikes on the tetrode</p> Source code in <code>ephysiopy/axona/axonaIO.py</code> <pre><code>def getSpkTS(self):\n    \"\"\"\n    Return all the timestamps for all the spikes on the tetrode\n    \"\"\"\n    return np.ma.compressed(self.spike_times)\n</code></pre>"},{"location":"reference/#ephysiopy.axona.axonaIO.Tetrode.getUniqueClusters","title":"<code>getUniqueClusters()</code>","text":"<p>Returns the unique clusters</p> Source code in <code>ephysiopy/axona/axonaIO.py</code> <pre><code>def getUniqueClusters(self):\n    \"\"\"\n    Returns the unique clusters\n    \"\"\"\n    if self.cut is None:\n        cut = np.array(self.getCut(self.tetrode), dtype=int)\n        self.cut = cut\n    return np.unique(self.cut)\n</code></pre>"},{"location":"reference/#conversion-code","title":"Conversion code","text":""},{"location":"reference/#openephys-to-axona","title":"OpenEphys to Axona","text":""},{"location":"reference/#ephysiopy.format_converters.OE_Axona.OE2Axona","title":"<code>OE2Axona</code>","text":"<p>               Bases: <code>object</code></p> <p>Converts openephys data into Axona files</p> <p>Example workflow:</p> <p>You have recorded some openephys data using the binary format leading to a directory structure something like this:</p> <p>M4643_2023-07-21_11-52-02 \u251c\u2500\u2500 Record Node 101 \u2502 \u251c\u2500\u2500 experiment1 \u2502 \u2502 \u2514\u2500\u2500 recording1 \u2502 \u2502     \u251c\u2500\u2500 continuous \u2502 \u2502     \u2502 \u2514\u2500\u2500 Acquisition_Board-100.Rhythm Data \u2502 \u2502     \u2502     \u251c\u2500\u2500 amplitudes.npy \u2502 \u2502     \u2502     \u251c\u2500\u2500 channel_map.npy \u2502 \u2502     \u2502     \u251c\u2500\u2500 channel_positions.npy \u2502 \u2502     \u2502     \u251c\u2500\u2500 cluster_Amplitude.tsv \u2502 \u2502     \u2502     \u251c\u2500\u2500 cluster_ContamPct.tsv \u2502 \u2502     \u2502     \u251c\u2500\u2500 cluster_KSLabel.tsv \u2502 \u2502     \u2502     \u251c\u2500\u2500 continuous.dat \u2502 \u2502     \u2502     \u251c\u2500\u2500 params.py \u2502 \u2502     \u2502     \u251c\u2500\u2500 pc_feature_ind.npy \u2502 \u2502     \u2502     \u251c\u2500\u2500 pc_features.npy \u2502 \u2502     \u2502     \u251c\u2500\u2500 phy.log \u2502 \u2502     \u2502     \u251c\u2500\u2500 rez.mat \u2502 \u2502     \u2502     \u251c\u2500\u2500 similar_templates.npy \u2502 \u2502     \u2502     \u251c\u2500\u2500 spike_clusters.npy \u2502 \u2502     \u2502     \u251c\u2500\u2500 spike_templates.npy \u2502 \u2502     \u2502     \u251c\u2500\u2500 spike_times.npy \u2502 \u2502     \u2502     \u251c\u2500\u2500 template_feature_ind.npy \u2502 \u2502     \u2502     \u251c\u2500\u2500 template_features.npy \u2502 \u2502     \u2502     \u251c\u2500\u2500 templates_ind.npy \u2502 \u2502     \u2502     \u251c\u2500\u2500 templates.npy \u2502 \u2502     \u2502     \u251c\u2500\u2500 whitening_mat_inv.npy \u2502 \u2502     \u2502     \u2514\u2500\u2500 whitening_mat.npy \u2502 \u2502     \u251c\u2500\u2500 events \u2502 \u2502     \u2502 \u251c\u2500\u2500 Acquisition_Board-100.Rhythm Data \u2502 \u2502     \u2502 \u2502 \u2514\u2500\u2500 TTL \u2502 \u2502     \u2502 \u2502     \u251c\u2500\u2500 full_words.npy \u2502 \u2502     \u2502 \u2502     \u251c\u2500\u2500 sample_numbers.npy \u2502 \u2502     \u2502 \u2502     \u251c\u2500\u2500 states.npy \u2502 \u2502     \u2502 \u2502     \u2514\u2500\u2500 timestamps.npy \u2502 \u2502     \u2502 \u2514\u2500\u2500 MessageCenter \u2502 \u2502     \u2502     \u251c\u2500\u2500 sample_numbers.npy \u2502 \u2502     \u2502     \u251c\u2500\u2500 text.npy \u2502 \u2502     \u2502     \u2514\u2500\u2500 timestamps.npy \u2502 \u2502     \u251c\u2500\u2500 structure.oebin \u2502 \u2502     \u2514\u2500\u2500 sync_messages.txt \u2502 \u2514\u2500\u2500 settings.xml \u2514\u2500\u2500 Record Node 104     \u251c\u2500\u2500 experiment1     \u2502 \u2514\u2500\u2500 recording1     \u2502     \u251c\u2500\u2500 continuous     \u2502     \u2502 \u2514\u2500\u2500 TrackMe-103.TrackingNode     \u2502     \u2502     \u251c\u2500\u2500 continuous.dat     \u2502     \u2502     \u251c\u2500\u2500 sample_numbers.npy     \u2502     \u2502     \u2514\u2500\u2500 timestamps.npy     \u2502     \u251c\u2500\u2500 events     \u2502     \u2502 \u251c\u2500\u2500 MessageCenter     \u2502     \u2502 \u2502 \u251c\u2500\u2500 sample_numbers.npy     \u2502     \u2502 \u2502 \u251c\u2500\u2500 text.npy     \u2502     \u2502 \u2502 \u2514\u2500\u2500 timestamps.npy     \u2502     \u2502 \u2514\u2500\u2500 TrackMe-103.TrackingNode     \u2502     \u2502     \u2514\u2500\u2500 TTL     \u2502     \u2502         \u251c\u2500\u2500 full_words.npy     \u2502     \u2502         \u251c\u2500\u2500 sample_numbers.npy     \u2502     \u2502         \u251c\u2500\u2500 states.npy     \u2502     \u2502         \u2514\u2500\u2500 timestamps.npy     \u2502     \u251c\u2500\u2500 structure.oebin     \u2502     \u2514\u2500\u2500 sync_messages.txt     \u2514\u2500\u2500 settings.xml</p> <p>The binary data file is called \"continuous.dat\" in the continuous/Acquisition_Board-100.Rhythm Data folder. There is also a collection of files resulting from a KiloSort session in that directory.</p> <p>Run the conversion code like so:</p> <p>from ephysiopy.format_converters.OE_Axona import OE2Axona from pathlib import Path nChannels = 64 apData = Path(\"M4643_2023-07-21_11-52-02/Record Node 101/experiment1/recording1/continuous/Acquisition_Board-100.Rhythm Data\") OE = OE2Axona(Path(\"M4643_2023-07-21_11-52-02\"), path2APData=apData, channels=nChannels) OE.getOEData()</p> <p>The last command will attempt to load position data and also load up something called a TemplateModel (from the package phylib) which should grab a handle to the neural data. If that doesn't throw out errors then try:</p> <p>OE.exportPos()</p> <p>There are a few arguments you can provide the exportPos() function - see the docstring for it below. Basically, it calls a function called convertPosData(xy, xyts) where xy is the xy data with shape nsamples x 2 and xyts is a vector of timestamps. So if the call to exportPos() fails, you could try calling convertPosData() directly which returns axona formatted position data. If the variable returned from convertPosData() is called axona_pos_data then you can call the function:</p> <p>writePos2AxonaFormat(pos_header, axona_pos_data)</p> <p>Providing the pos_header to it - see the last half of the exportPos function for how to create and modify the pos_header as that will need to have user-specific information added to it.</p> <p>OE.convertTemplateDataToAxonaTetrode()</p> <p>This is the main function for creating the tetrode files. It has an optional argument called max_n_waves which is used to limit the maximum number of spikes that make up a cluster. This defaults to 2000 which means that if a cluster has 12000 spikes, it will have 2000 spikes randomly drawn from those 12000 (without replacement), that will then be saved to a tetrode file. This is mostly a time-saving device as if you have 250 clusters and many consist of 10,000's of spikes, processing that data will take a long time.</p> <p>OE.exportLFP()</p> <p>This will save either a .eeg or .egf file depending on the arguments. Check the docstring for how to change what channel is chosen for the LFP etc.</p> <p>OE.exportSetFile()</p> <p>This should save the .set file with all the metadata for the trial.</p> Source code in <code>ephysiopy/format_converters/OE_Axona.py</code> <pre><code>class OE2Axona(object):\n    \"\"\"\n    Converts openephys data into Axona files\n\n    Example workflow:\n\n    You have recorded some openephys data using the binary\n    format leading to a directory structure something like this:\n\n    M4643_2023-07-21_11-52-02\n    \u251c\u2500\u2500 Record Node 101\n    \u2502 \u251c\u2500\u2500 experiment1\n    \u2502 \u2502 \u2514\u2500\u2500 recording1\n    \u2502 \u2502     \u251c\u2500\u2500 continuous\n    \u2502 \u2502     \u2502 \u2514\u2500\u2500 Acquisition_Board-100.Rhythm Data\n    \u2502 \u2502     \u2502     \u251c\u2500\u2500 amplitudes.npy\n    \u2502 \u2502     \u2502     \u251c\u2500\u2500 channel_map.npy\n    \u2502 \u2502     \u2502     \u251c\u2500\u2500 channel_positions.npy\n    \u2502 \u2502     \u2502     \u251c\u2500\u2500 cluster_Amplitude.tsv\n    \u2502 \u2502     \u2502     \u251c\u2500\u2500 cluster_ContamPct.tsv\n    \u2502 \u2502     \u2502     \u251c\u2500\u2500 cluster_KSLabel.tsv\n    \u2502 \u2502     \u2502     \u251c\u2500\u2500 continuous.dat\n    \u2502 \u2502     \u2502     \u251c\u2500\u2500 params.py\n    \u2502 \u2502     \u2502     \u251c\u2500\u2500 pc_feature_ind.npy\n    \u2502 \u2502     \u2502     \u251c\u2500\u2500 pc_features.npy\n    \u2502 \u2502     \u2502     \u251c\u2500\u2500 phy.log\n    \u2502 \u2502     \u2502     \u251c\u2500\u2500 rez.mat\n    \u2502 \u2502     \u2502     \u251c\u2500\u2500 similar_templates.npy\n    \u2502 \u2502     \u2502     \u251c\u2500\u2500 spike_clusters.npy\n    \u2502 \u2502     \u2502     \u251c\u2500\u2500 spike_templates.npy\n    \u2502 \u2502     \u2502     \u251c\u2500\u2500 spike_times.npy\n    \u2502 \u2502     \u2502     \u251c\u2500\u2500 template_feature_ind.npy\n    \u2502 \u2502     \u2502     \u251c\u2500\u2500 template_features.npy\n    \u2502 \u2502     \u2502     \u251c\u2500\u2500 templates_ind.npy\n    \u2502 \u2502     \u2502     \u251c\u2500\u2500 templates.npy\n    \u2502 \u2502     \u2502     \u251c\u2500\u2500 whitening_mat_inv.npy\n    \u2502 \u2502     \u2502     \u2514\u2500\u2500 whitening_mat.npy\n    \u2502 \u2502     \u251c\u2500\u2500 events\n    \u2502 \u2502     \u2502 \u251c\u2500\u2500 Acquisition_Board-100.Rhythm Data\n    \u2502 \u2502     \u2502 \u2502 \u2514\u2500\u2500 TTL\n    \u2502 \u2502     \u2502 \u2502     \u251c\u2500\u2500 full_words.npy\n    \u2502 \u2502     \u2502 \u2502     \u251c\u2500\u2500 sample_numbers.npy\n    \u2502 \u2502     \u2502 \u2502     \u251c\u2500\u2500 states.npy\n    \u2502 \u2502     \u2502 \u2502     \u2514\u2500\u2500 timestamps.npy\n    \u2502 \u2502     \u2502 \u2514\u2500\u2500 MessageCenter\n    \u2502 \u2502     \u2502     \u251c\u2500\u2500 sample_numbers.npy\n    \u2502 \u2502     \u2502     \u251c\u2500\u2500 text.npy\n    \u2502 \u2502     \u2502     \u2514\u2500\u2500 timestamps.npy\n    \u2502 \u2502     \u251c\u2500\u2500 structure.oebin\n    \u2502 \u2502     \u2514\u2500\u2500 sync_messages.txt\n    \u2502 \u2514\u2500\u2500 settings.xml\n    \u2514\u2500\u2500 Record Node 104\n        \u251c\u2500\u2500 experiment1\n        \u2502 \u2514\u2500\u2500 recording1\n        \u2502     \u251c\u2500\u2500 continuous\n        \u2502     \u2502 \u2514\u2500\u2500 TrackMe-103.TrackingNode\n        \u2502     \u2502     \u251c\u2500\u2500 continuous.dat\n        \u2502     \u2502     \u251c\u2500\u2500 sample_numbers.npy\n        \u2502     \u2502     \u2514\u2500\u2500 timestamps.npy\n        \u2502     \u251c\u2500\u2500 events\n        \u2502     \u2502 \u251c\u2500\u2500 MessageCenter\n        \u2502     \u2502 \u2502 \u251c\u2500\u2500 sample_numbers.npy\n        \u2502     \u2502 \u2502 \u251c\u2500\u2500 text.npy\n        \u2502     \u2502 \u2502 \u2514\u2500\u2500 timestamps.npy\n        \u2502     \u2502 \u2514\u2500\u2500 TrackMe-103.TrackingNode\n        \u2502     \u2502     \u2514\u2500\u2500 TTL\n        \u2502     \u2502         \u251c\u2500\u2500 full_words.npy\n        \u2502     \u2502         \u251c\u2500\u2500 sample_numbers.npy\n        \u2502     \u2502         \u251c\u2500\u2500 states.npy\n        \u2502     \u2502         \u2514\u2500\u2500 timestamps.npy\n        \u2502     \u251c\u2500\u2500 structure.oebin\n        \u2502     \u2514\u2500\u2500 sync_messages.txt\n        \u2514\u2500\u2500 settings.xml\n\n    The binary data file is called \"continuous.dat\" in the\n    continuous/Acquisition_Board-100.Rhythm Data folder. There\n    is also a collection of files resulting from a KiloSort session\n    in that directory.\n\n    Run the conversion code like so:\n\n    &gt;&gt;&gt; from ephysiopy.format_converters.OE_Axona import OE2Axona\n    &gt;&gt;&gt; from pathlib import Path\n    &gt;&gt;&gt; nChannels = 64\n    &gt;&gt;&gt; apData = Path(\"M4643_2023-07-21_11-52-02/Record Node 101/experiment1/recording1/continuous/Acquisition_Board-100.Rhythm Data\")\n    &gt;&gt;&gt; OE = OE2Axona(Path(\"M4643_2023-07-21_11-52-02\"), path2APData=apData, channels=nChannels)\n    &gt;&gt;&gt; OE.getOEData()\n\n    The last command will attempt to load position data and also load up\n    something called a TemplateModel (from the package phylib) which\n    should grab a handle to the neural data. If that doesn't throw\n    out errors then try:\n\n    &gt;&gt;&gt; OE.exportPos()\n\n    There are a few arguments you can provide the exportPos() function - see\n    the docstring for it below. Basically, it calls a function called\n    convertPosData(xy, xyts) where xy is the xy data with shape nsamples x 2\n    and xyts is a vector of timestamps. So if the call to exportPos() fails, you\n    could try calling convertPosData() directly which returns axona formatted\n    position data. If the variable returned from convertPosData() is called axona_pos_data\n    then you can call the function:\n\n    writePos2AxonaFormat(pos_header, axona_pos_data)\n\n    Providing the pos_header to it - see the last half of the exportPos function\n    for how to create and modify the pos_header as that will need to have\n    user-specific information added to it.\n\n    &gt;&gt;&gt; OE.convertTemplateDataToAxonaTetrode()\n\n    This is the main function for creating the tetrode files. It has an optional\n    argument called max_n_waves which is used to limit the maximum number of spikes\n    that make up a cluster. This defaults to 2000 which means that if a cluster has\n    12000 spikes, it will have 2000 spikes randomly drawn from those 12000 (without\n    replacement), that will then be saved to a tetrode file. This is mostly a time-saving\n    device as if you have 250 clusters and many consist of 10,000's of spikes,\n    processing that data will take a long time.\n\n    &gt;&gt;&gt; OE.exportLFP()\n\n    This will save either a .eeg or .egf file depending on the arguments. Check the\n    docstring for how to change what channel is chosen for the LFP etc.\n\n    &gt;&gt;&gt; OE.exportSetFile()\n\n    This should save the .set file with all the metadata for the trial.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        pname: Path,\n        path2APData: Path = None,\n        pos_sample_rate: int = 50,\n        channels: int = 0,\n        **kwargs,\n    ):\n        \"\"\"\n        Args:\n            pname (Path): The base directory of the openephys recording.\n                e.g. '/home/robin/Data/M4643_2023-07-21_11-52-02'\n            path2APData (Path, optional): Path to AP data. Defaults to None.\n            pos_sample_rate (int, optional): Position sample rate. Defaults to 50.\n            channels (int, optional): Number of channels. Defaults to 0.\n            **kwargs: Variable length argument list.\n        \"\"\"\n        pname = Path(pname)\n        assert pname.exists()\n        self.pname: Path = pname\n        self.path2APdata: Path = path2APData\n        self.pos_sample_rate: int = pos_sample_rate\n        # 'experiment_1.nwb'\n        self.experiment_name: Path = self.pname or Path(kwargs[\"experiment_name\"])\n        self.recording_name = None  # will become 'recording1' etc\n        self.OE_data = None  # becomes instance of io.recording.OpenEphysBase\n        self._settings = None  # will become an instance of OESettings.Settings\n        # Create a basename for Axona file names\n        # e.g.'/home/robin/Data/experiment_1'\n        # that we can append '.pos' or '.eeg' or whatever onto\n        self.axona_root_name = self.experiment_name\n        # need to instantiated now for later\n        self.AxonaData = axonaIO.IO(self.axona_root_name.name + \".pos\")\n        # THIS IS TEMPORARY AND WILL BE MORE USER-SPECIFIABLE IN THE FUTURE\n        # it is used to scale the spikes\n        self.hp_gain = 500\n        self.lp_gain = 15000\n        self.bitvolts = 0.195\n        # if left as None some default values for the next 3 params are loaded\n        #  from top-level __init__.py\n        # these are only used in self.__filterLFP__\n        self.fs = None\n        # if lfp_channel is set to None then the .set file will reflect that\n        #  no EEG was recorded\n        # this should mean that you can load data into Tint without a .eeg file\n        self.lfp_channel = 1 or kwargs[\"lfp_channel\"]\n        self.lfp_lowcut = None\n        self.lfp_highcut = None\n        # set the tetrodes to record from\n        # defaults to 1 through 4 - see self.makeSetData below\n        self.tetrodes = [\"1\", \"2\", \"3\", \"4\"]\n        self.channel_count = channels\n\n    def resample(self, data, src_rate=30, dst_rate=50, axis=0):\n        \"\"\"\n        Resamples data using FFT\n        \"\"\"\n        denom = np.gcd(dst_rate, src_rate)\n        new_data = signal.resample_poly(data, dst_rate / denom, src_rate / denom, axis)\n        return new_data\n\n    @property\n    def settings(self):\n        \"\"\"\n        Loads the settings data from the settings.xml file\n        \"\"\"\n        if self._settings is None:\n            self._settings = OESettings.Settings(self.pname)\n        return self._settings\n\n    @settings.setter\n    def settings(self, value):\n        self._settings = value\n\n    def getOEData(self) -&gt; OpenEphysBase:\n        \"\"\"\n        Loads the nwb file names in filename_root and returns a dict\n        containing some of the nwb data relevant for converting to Axona file formats.\n\n        Args:\n            filename_root (str): Fully qualified name of the nwb file.\n            recording_name (str): The name of the recording in the nwb file. Note that\n                the default has changed in different versions of OE from 'recording0'\n                to 'recording1'.\n        \"\"\"\n        OE_data = OpenEphysBase(self.pname)\n        try:\n            OE_data.load_pos_data(sample_rate=self.pos_sample_rate)\n            # It's likely that spikes have been collected after the last\n            # position sample\n            # due to buffering issues I can't be bothered to resolve.\n            # Get the last pos\n            # timestamps here and check that spikes don't go beyond\n            #  this when writing data\n            # out later\n            # Also the pos and spike data timestamps almost never start at\n            #  0 as the user\n            # usually acquires data for a while before recording.\n            # Grab the first timestamp\n            # here with a view to subtracting this from everything (including\n            # the spike data)\n            # and figuring out what to keep later\n            first_pos_ts = OE_data.PosCalcs.xyTS[0]\n            last_pos_ts = OE_data.PosCalcs.xyTS[-1]\n            self.first_pos_ts = first_pos_ts\n            self.last_pos_ts = last_pos_ts\n        except Exception:\n            OE_data.load_neural_data()  # will create TemplateModel instance\n            self.first_pos_ts = 0\n            self.last_pos_ts = self.OE_data.template_model.duration\n        print(f\"First pos ts: {self.first_pos_ts}\")\n        print(f\"Last pos ts: {self.last_pos_ts}\")\n        self.OE_data = OE_data\n        if self.path2APdata is None:\n            self.path2APdata = self.OE_data.path2APdata\n        # extract number of channels from settings\n        for item in self.settings.record_nodes.items():\n            if \"Rhythm Data\" in item[1].name:\n                self.channel_count = int(item[1].channel_count)\n        return OE_data\n\n    def exportSetFile(self, **kwargs):\n        \"\"\"\n        Wrapper for makeSetData below\n        \"\"\"\n        print(\"Exporting set file data...\")\n        self.makeSetData(**kwargs)\n        print(\"Done exporting set file.\")\n\n    def exportPos(self, ppm=300, jumpmax=100, as_text=False, **kwargs):\n        #\n        # Step 1) Deal with the position data first:\n        #\n        # Grab the settings of the pos tracker and do some post-processing\n        # on the position\n        # data (discard jumpy data, do some smoothing etc)\n        self.settings.parse()\n        if not self.OE_data:\n            self.getOEData()\n        if not self.OE_data.PosCalcs:\n            self.OE_data.load_pos_data(sample_rate=self.pos_sample_rate)\n        print(\"Post-processing position data...\")\n        self.OE_data.PosCalcs.jumpmax = jumpmax\n        self.OE_data.PosCalcs.tracker_params[\"AxonaBadValue\"] = 1023\n        self.OE_data.PosCalcs.postprocesspos(self.OE_data.PosCalcs.tracker_params)\n        xy = self.OE_data.PosCalcs.xy.T\n        xyTS = self.OE_data.PosCalcs.xyTS  # in seconds\n        xyTS = xyTS * self.pos_sample_rate\n        # extract some values from PosCalcs or overrides given\n        # when calling this method\n        ppm = self.OE_data.PosCalcs.ppm or ppm\n        sample_rate = self.OE_data.PosCalcs.sample_rate or kwargs[\"sample_rate\"]\n        if as_text is True:\n            print(\"Beginning export of position data to text format...\")\n            pos_file_name = self.axona_root_name + \".txt\"\n            np.savetxt(pos_file_name, xy, fmt=\"%1.u\")\n            print(\"Completed export of position data\")\n            return\n        # Do the upsampling of both xy and the timestamps\n        print(\"Beginning export of position data to Axona format...\")\n        axona_pos_data = self.convertPosData(xy, xyTS)\n        # make sure pos data length is same as duration * num_samples\n        axona_pos_data = axona_pos_data[\n            0 : int(self.last_pos_ts - self.first_pos_ts) * self.pos_sample_rate\n        ]\n        # Create an empty header for the pos data\n        from ephysiopy.axona.file_headers import PosHeader\n\n        pos_header = PosHeader()\n        tracker_params = self.OE_data.PosCalcs.tracker_params\n        min_xy = np.floor(np.min(xy, 0)).astype(int).data\n        max_xy = np.ceil(np.max(xy, 0)).astype(int).data\n        pos_header.pos[\"min_x\"] = pos_header.pos[\"window_min_x\"] = (\n            str(tracker_params[\"LeftBorder\"])\n            if \"LeftBorder\" in tracker_params.keys()\n            else str(min_xy[0])\n        )\n        pos_header.pos[\"min_y\"] = pos_header.pos[\"window_min_y\"] = (\n            str(tracker_params[\"TopBorder\"])\n            if \"TopBorder\" in tracker_params.keys()\n            else str(min_xy[1])\n        )\n        pos_header.pos[\"max_x\"] = pos_header.pos[\"window_max_x\"] = (\n            str(tracker_params[\"RightBorder\"])\n            if \"RightBorder\" in tracker_params.keys()\n            else str(max_xy[0])\n        )\n        pos_header.pos[\"max_y\"] = pos_header.pos[\"window_max_y\"] = (\n            str(tracker_params[\"BottomBorder\"])\n            if \"BottomBorder\" in tracker_params.keys()\n            else str(max_xy[1])\n        )\n        pos_header.common[\"duration\"] = str(int(self.last_pos_ts - self.first_pos_ts))\n        pos_header.pos[\"pixels_per_metre\"] = str(ppm)\n        pos_header.pos[\"num_pos_samples\"] = str(len(axona_pos_data))\n        pos_header.pos[\"pixels_per_metre\"] = str(ppm)\n        pos_header.pos[\"sample_rate\"] = str(sample_rate)\n\n        self.writePos2AxonaFormat(pos_header, axona_pos_data)\n        print(\"Exported position data to Axona format\")\n\n    def exportSpikes(self):\n        print(\"Beginning conversion of spiking data...\")\n        self.convertSpikeData(\n            self.OE_data.nwbData[\"acquisition\"][\n                \"\\\n                timeseries\"\n            ][self.recording_name][\"spikes\"]\n        )\n        print(\"Completed exporting spiking data\")\n\n    def exportLFP(\n        self, channel: int = 0, lfp_type: str = \"eeg\", gain: int = 5000, **kwargs\n    ):\n        \"\"\"\n        Exports LFP data to file.\n\n        Args:\n            channel (int): The channel number.\n            lfp_type (str): The type of LFP data. Legal values are 'egf' or 'eeg'.\n            gain (int): Multiplier for the LFP data.\n        \"\"\"\n        print(\"Beginning conversion and exporting of LFP data...\")\n        if not self.settings.processors:\n            self.settings.parse()\n        from ephysiopy.io.recording import memmapBinaryFile\n\n        try:\n            data = memmapBinaryFile(\n                Path(self.path2APdata).joinpath(\"continuous.dat\"),\n                n_channels=self.channel_count,\n            )\n            self.makeLFPData(data[channel, :], eeg_type=lfp_type, gain=gain)\n            print(\"Completed exporting LFP data to \" + lfp_type + \" format\")\n        except Exception as e:\n            print(f\"Couldn't load raw data:\\n{e}\")\n\n    def convertPosData(self, xy: np.array, xy_ts: np.array) -&gt; np.array:\n        \"\"\"\n        Performs the conversion of the array parts of the data.\n\n        Note: As well as upsampling the data to the Axona pos sampling rate (50Hz),\n        we have to insert some columns into the pos array as Axona format\n        expects it like: pos_format: t,x1,y1,x2,y2,numpix1,numpix2\n        We can make up some of the info and ignore other bits.\n        \"\"\"\n        n_new_pts = int(\n            np.floor((self.last_pos_ts - self.first_pos_ts) * self.pos_sample_rate)\n        )\n        t = xy_ts - self.first_pos_ts\n        new_ts = np.linspace(t[0], t[-1], n_new_pts)\n        new_x = np.interp(new_ts, t, xy[:, 0])\n        new_y = np.interp(new_ts, t, xy[:, 1])\n        new_x[np.isnan(new_x)] = 1023\n        new_y[np.isnan(new_y)] = 1023\n        # Expand the pos bit of the data to make it look like Axona data\n        new_pos = np.vstack([new_x, new_y]).T\n        new_pos = np.c_[\n            new_pos,\n            np.ones_like(new_pos) * 1023,\n            np.zeros_like(new_pos),\n            np.zeros_like(new_pos),\n        ]\n        new_pos[:, 4] = 40  # just made this value up - it's numpix i think\n        new_pos[:, 6] = 40  # same\n        # Squeeze this data into Axona pos format array\n        dt = self.AxonaData.axona_files[\".pos\"]\n        new_data = np.zeros(n_new_pts, dtype=dt)\n        # Timestamps in Axona are pos_samples (monotonic, linear integer)\n        new_data[\"ts\"] = new_ts\n        new_data[\"pos\"] = new_pos\n        return new_data\n\n    def convertTemplateDataToAxonaTetrode(self, max_n_waves=2000, **kwargs):\n        \"\"\"\n        Converts the data held in a TemplateModel instance into tetrode\n        format Axona data files.\n\n        For each cluster, there'll be a channel that has a peak amplitude and this contains that peak channel.\n        While the other channels with a large signal in might be on the same tetrode, KiloSort (or whatever) might find\n        channels *not* within the same tetrode. For a given cluster, we can extract from the TemplateModel the 12 channels across\n        which the signal is strongest using Model.get_cluster_channels(). If a channel from a tetrode is missing from this list then the\n        spikes for that channel(s) will be zeroed when saved to Axona format.\n\n        Example:\n            If cluster 3 has a peak channel of 1 then get_cluster_channels() might look like:\n            [ 1,  2,  0,  6, 10, 11,  4,  12,  7,  5,  8,  9]\n            Here the cluster has the best signal on 1, then 2, 0 etc, but note that channel 3 isn't in the list.\n            In this case the data for channel 3 will be zeroed when saved to Axona format.\n\n        References:\n            1) https://phy.readthedocs.io/en/latest/api/#phyappstemplatetemplatemodel\n        \"\"\"\n        # First lets get the datatype for tetrode files as this will be the\n        # same for all tetrodes...\n        dt = self.AxonaData.axona_files[\".1\"]\n        # Load the TemplateModel\n        if \"path2APdata\" in kwargs.keys():\n            self.OE_data.load_neural_data(**kwargs)\n        else:\n            self.OE_data.load_neural_data()\n        model = self.OE_data.template_model\n        clusts = model.cluster_ids\n        # have to pre-process the channels / clusters to determine\n        # which tetrodes clusters belong to - this is based on\n        # the 'best' channel for a given cluster\n        clusters_channels = OrderedDict(dict.fromkeys(clusts, np.ndarray))\n        for c in clusts:\n            clusters_channels[c] = model.get_cluster_channels(c)\n        tetrodes_clusters = OrderedDict(\n            dict.fromkeys(range(0, int(self.channel_count / 4)), [])\n        )\n        for t in tetrodes_clusters.items():\n            this_tetrodes_clusters = []\n            for c in clusters_channels.items():\n                if int(c[1][0] / 4) == t[0]:\n                    this_tetrodes_clusters.append(c[0])\n            tetrodes_clusters[t[0]] = this_tetrodes_clusters\n        # limit the number of spikes to max_n_waves in the\n        # interests of speed. Randomly select spikes across\n        # the period they fire\n        rng = np.random.default_rng()\n\n        for i, i_tet_item in enumerate(tetrodes_clusters.items()):\n            this_tetrode = i_tet_item[0]\n            times_to_sort = []\n            new_clusters = []\n            new_waves = []\n            for clust in tqdm(i_tet_item[1], desc=\"Tetrode \" + str(i + 1)):\n                clust_chans = model.get_cluster_channels(clust)\n                idx = np.logical_and(\n                    clust_chans &gt;= this_tetrode, clust_chans &lt; this_tetrode + 4\n                )\n                # clust_chans is an ordered list of the channels\n                # the cluster was most active on. idx has True\n                # where there is overlap between that and the\n                # currently active tetrode channel numbers (0:4, 4:8\n                # or whatever)\n                spike_idx = model.get_cluster_spikes(clust)\n                # limit the number of spikes to max_n_waves in the\n                # interests of speed. Randomly select spikes across\n                # the period they fire\n                total_n_waves = len(spike_idx)\n                max_num_waves = (\n                    max_n_waves if max_n_waves &lt; total_n_waves else total_n_waves\n                )\n                # grab spike times (in seconds) so the random sampling of\n                # spikes matches their times\n                times = model.spike_times[model.spike_clusters == clust]\n                spike_idx_times_subset = rng.choice(\n                    (spike_idx, times), max_num_waves, axis=1, replace=False\n                )\n                # spike_idx_times_subset is unsorted as it's just been drawn\n                # from a random distribution, so sort it now\n                spike_idx_times_subset = np.sort(spike_idx_times_subset, 1)\n                # split out into spikes and times\n                spike_idx_subset = spike_idx_times_subset[0, :].astype(int)\n                times = spike_idx_times_subset[1, :]\n                waves = model.get_waveforms(spike_idx_subset, clust_chans[idx])\n                # Given a spike at time T, Axona takes T-200us and T+800us\n                # from the buffer to make up a waveform. From OE\n                # take 30 samples which corresponds to a 1ms sample\n                # if the data is sampled at 30kHz. Interpolate this so the\n                # length is 50 samples as with Axona\n                waves = waves[:, 30:60, :]\n                # waves go from int16 to float as a result of the resampling\n                waves = self.resample(waves.astype(float), axis=1)\n                # multiply by bitvolts to get microvolts\n                waves = waves * self.bitvolts\n                # scale appropriately for Axona and invert as\n                # OE seems to be inverted wrt Axona\n                waves = waves / (self.hp_gain / 4 / 128.0) * (-1)\n                # check the shape of waves to make sure it has 4\n                # channels, if not add some to make it so and make\n                # sure they are in the correct order for the tetrode\n                ordered_chans = np.argsort(clust_chans[idx])\n                if waves.shape[-1] != 4:\n                    z = np.zeros(shape=(waves.shape[0], waves.shape[1], 4))\n                    z[:, :, ordered_chans] = waves\n                    waves = z\n                else:\n                    waves = waves[:, :, ordered_chans]\n                # Axona format tetrode waveforms are nSpikes x 4 x 50\n                waves = np.transpose(waves, (0, 2, 1))\n                # Append clusters to a list to sort later for saving a\n                # cluster/ cut file\n                new_clusters.append(np.repeat(clust, len(times)))\n                # Axona times are sampled at 96KHz\n                times = times * 96000\n                # There is a time for each spike despite the repetition\n                # get the indices for sorting\n                times_to_sort.append(times)\n                # i_clust_data = np.zeros(len(new_times), dtype=dt)\n                new_waves.append(waves)\n            # Concatenate, order and reshape some of the lists/ arrays\n            if times_to_sort:  # apparently can be empty sometimes\n                _times = np.concatenate(times_to_sort)\n                _waves = np.concatenate(new_waves)\n                _clusters = np.concatenate(new_clusters)\n                indices = np.argsort(_times)\n                sorted_times = _times[indices]\n                sorted_waves = _waves[indices]\n                sorted_clusts = _clusters[indices]\n                output_times = np.repeat(sorted_times, 4)\n                output_waves = np.reshape(\n                    sorted_waves,\n                    [\n                        sorted_waves.shape[0] * sorted_waves.shape[1],\n                        sorted_waves.shape[2],\n                    ],\n                )\n                new_tetrode_data = np.zeros(len(output_times), dtype=dt)\n                new_tetrode_data[\"ts\"] = output_times\n                new_tetrode_data[\"waveform\"] = output_waves\n                header = TetrodeHeader()\n                header.common[\"duration\"] = str(int(model.duration))\n                header.tetrode_entries[\"num_spikes\"] = str(len(_clusters))\n                self.writeTetrodeData(str(i + 1), header, new_tetrode_data)\n                cut_header = CutHeader()\n                self.writeCutData(str(i + 1), cut_header, sorted_clusts)\n\n    def convertSpikeData(self, hdf5_tetrode_data: h5py._hl.group.Group):\n        \"\"\"\n        Does the spike conversion from OE Spike Sorter format to Axona format tetrode files.\n\n        Args:\n            hdf5_tetrode_data (h5py._hl.group.Group): This kind of looks like a dictionary and can,\n                it seems, be treated as one more or less. See http://docs.h5py.org/en/stable/high/group.html\n        \"\"\"\n        # First lets get the datatype for tetrode files as this will be the\n        # same for all tetrodes...\n        dt = self.AxonaData.axona_files[\".1\"]\n        # ... and a basic header for the tetrode file that use for each\n        # tetrode file, changing only the num_spikes value\n        header = TetrodeHeader()\n        header.common[\"duration\"] = str(int(self.last_pos_ts - self.first_pos_ts))\n\n        for key in hdf5_tetrode_data.keys():\n            spiking_data = np.array(hdf5_tetrode_data[key].get(\"data\"))\n            timestamps = np.array(hdf5_tetrode_data[key].get(\"timestamps\"))\n            # check if any of the spiking data is captured before/ after the\n            #  first/ last bit of position data\n            # if there is then discard this as we potentially have no valid\n            # position to align the spike to :(\n            idx = np.logical_or(\n                timestamps &lt; self.first_pos_ts, timestamps &gt; self.last_pos_ts\n            )\n            spiking_data = spiking_data[~idx, :, :]\n            timestamps = timestamps[~idx]\n            # subtract the first pos timestamp from the spiking timestamps\n            timestamps = timestamps - self.first_pos_ts\n            # get the number of spikes here for use below in the header\n            num_spikes = len(timestamps)\n            # repeat the timestamps in tetrode multiples ready for Axona export\n            new_timestamps = np.repeat(timestamps, 4)\n            new_spiking_data = spiking_data.astype(np.float64)\n            # Convert to microvolts...\n            new_spiking_data = new_spiking_data * self.bitvolts\n            # And upsample the spikes...\n            new_spiking_data = self.resample(new_spiking_data, 4, 5, -1)\n            # ... and scale appropriately for Axona and invert as\n            # OE seems to be inverted wrt Axona\n            new_spiking_data = new_spiking_data / (self.hp_gain / 4 / 128.0) * (-1)\n            # ... scale them to the gains specified somewhere\n            #  (not sure where / how to do this yet)\n            shp = new_spiking_data.shape\n            # then reshape them as Axona wants them a bit differently\n            new_spiking_data = np.reshape(new_spiking_data, [shp[0] * shp[1], shp[2]])\n            # Cap any values outside the range of int8\n            new_spiking_data[new_spiking_data &lt; -128] = -128\n            new_spiking_data[new_spiking_data &gt; 127] = 127\n            # create the new array\n            new_tetrode_data = np.zeros(len(new_timestamps), dtype=dt)\n            new_tetrode_data[\"ts\"] = new_timestamps * 96000\n            new_tetrode_data[\"waveform\"] = new_spiking_data\n            # change the header num_spikes field\n            header.tetrode_entries[\"num_spikes\"] = str(num_spikes)\n            i_tetnum = key.split(\"electrode\")[1]\n            print(\"Exporting tetrode {}\".format(i_tetnum))\n            self.writeTetrodeData(i_tetnum, header, new_tetrode_data)\n\n    def makeLFPData(self, data: np.ndarray, eeg_type=\"eeg\", gain=5000):\n        \"\"\"\n        Downsamples the data in data and saves the result as either an egf or eeg file\n        depending on the choice of either eeg_type which can take a value of either 'egf' or 'eeg'.\n        Gain is the scaling factor.\n\n        Args:\n            data (np.array): The data to be downsampled. Must have dtype as np.int16.\n        \"\"\"\n        if eeg_type == \"eeg\":\n            from ephysiopy.axona.file_headers import EEGHeader\n\n            header = EEGHeader()\n            dst_rate = 250\n        elif eeg_type == \"egf\":\n            from ephysiopy.axona.file_headers import EGFHeader\n\n            header = EGFHeader()\n            dst_rate = 4800\n        header.common[\"duration\"] = str(int(self.last_pos_ts - self.first_pos_ts))\n        print(f\"header.common[duration] = {header.common['duration']}\")\n        _lfp_data = self.resample(data.astype(float), 30000, dst_rate, -1)\n        # make sure data is same length as sample_rate * duration\n        nsamples = int(dst_rate * int(header.common[\"duration\"]))\n        # lfp_data might be shorter than nsamples. If so, fill the\n        # remaining values with zeros\n        if len(_lfp_data) &lt; nsamples:\n            lfp_data = np.zeros(nsamples)\n            lfp_data[0 : len(_lfp_data)] = _lfp_data\n        else:\n            lfp_data = _lfp_data[0:nsamples]\n        lfp_data = self.__filterLFP__(lfp_data, dst_rate)\n        # convert the data format\n        # lfp_data = lfp_data * self.bitvolts # in microvolts\n\n        if eeg_type == \"eeg\":\n            # probably BROKEN\n            # lfp_data starts out as int16 (see Parameters above)\n            # but gets converted into float64 as part of the\n            # resampling/ filtering process\n            lfp_data = lfp_data / 32768.0\n            lfp_data = lfp_data * gain\n            # cap the values at either end...\n            lfp_data[lfp_data &lt; -128] = -128\n            lfp_data[lfp_data &gt; 127] = 127\n            # and convert to int8\n            lfp_data = lfp_data.astype(np.int8)\n\n        elif eeg_type == \"egf\":\n            # probably works\n            # lfp_data = lfp_data / 256.\n            lfp_data = lfp_data.astype(np.int16)\n\n        header.n_samples = str(len(lfp_data))\n        self.writeLFP2AxonaFormat(header, lfp_data, eeg_type)\n\n    def makeSetData(self, lfp_channel=4, **kwargs):\n        if self.OE_data is None:\n            # to get the timestamps for duration key\n            self.getOEData(self.filename_root)\n        from ephysiopy.axona.file_headers import SetHeader\n\n        header = SetHeader()\n        # set some reasonable default values\n        from ephysiopy.__about__ import __version__\n\n        header.meta_info[\"sw_version\"] = str(__version__)\n        # ADC fullscale mv is 1500 in Axona and 0.195 in OE\n        # there is a division by 1000 that happens when processing\n        # spike data in Axona that looks like that has already\n        # happened in OE. So here the OE 0.195 value is multiplied\n        # by 1000 as it will get divided by 1000 later on to get\n        # the correct scaling of waveforms/ gains -&gt; mv values\n        header.meta_info[\"ADC_fullscale_mv\"] = \"195\"\n        header.meta_info[\"tracker_version\"] = \"1.1.0\"\n\n        for k, v in header.set_entries.items():\n            if \"gain\" in k:\n                header.set_entries[k] = str(self.hp_gain)\n            if \"collectMask\" in k:\n                header.set_entries[k] = \"0\"\n            if \"EEG_ch_1\" in k:\n                if lfp_channel is not None:\n                    header.set_entries[k] = str(int(lfp_channel))\n            if \"mode_ch_\" in k:\n                header.set_entries[k] = \"0\"\n        # iterate again to make sure lfp gain set correctly\n        for k, v in header.set_entries.items():\n            if lfp_channel is not None:\n                if k == \"gain_ch_\" + str(lfp_channel):\n                    header.set_entries[k] = str(self.lp_gain)\n\n        # Based on the data in the electrodes dict of the OESettings\n        # instance (self.settings - see __init__)\n        # determine which tetrodes we can let Tint load\n        # make sure we've parsed the electrodes\n        tetrode_count = int(self.channel_count / 4)\n        for i in range(1, tetrode_count + 1):\n            header.set_entries[\"collectMask_\" + str(i)] = \"1\"\n        # if self.lfp_channel is not None:\n        #     for chan in self.tetrodes:\n        #         key = \"collectMask_\" + str(chan)\n        #         header.set_entries[key] = \"1\"\n        header.set_entries[\"colactive_1\"] = \"1\"\n        header.set_entries[\"colactive_2\"] = \"0\"\n        header.set_entries[\"colactive_3\"] = \"0\"\n        header.set_entries[\"colactive_4\"] = \"0\"\n        header.set_entries[\"colmap_algorithm\"] = \"1\"\n        header.set_entries[\"duration\"] = str(int(self.last_pos_ts - self.first_pos_ts))\n        self.writeSetData(header)\n\n    def __filterLFP__(self, data: np.array, sample_rate: int):\n        from scipy.signal import filtfilt, firwin\n\n        if self.fs is None:\n            from ephysiopy import fs\n\n            self.fs = fs\n        if self.lfp_lowcut is None:\n            from ephysiopy import lfp_lowcut\n\n            self.lfp_lowcut = lfp_lowcut\n        if self.lfp_highcut is None:\n            from ephysiopy import lfp_highcut\n\n            self.lfp_highcut = lfp_highcut\n        nyq = sample_rate / 2.0\n        lowcut = self.lfp_lowcut / nyq\n        highcut = self.lfp_highcut / nyq\n        if highcut &gt;= 1.0:\n            highcut = 1.0 - np.finfo(float).eps\n        if lowcut &lt;= 0.0:\n            lowcut = np.finfo(float).eps\n        b = firwin(sample_rate + 1, [lowcut, highcut], window=\"black\", pass_zero=False)\n        y = filtfilt(b, [1], data.ravel(), padtype=\"odd\")\n        return y\n\n    def writeLFP2AxonaFormat(self, header: dataclass, data: np.array, eeg_type=\"eeg\"):\n        self.AxonaData.setHeader(str(self.axona_root_name) + \".\" + eeg_type, header)\n        self.AxonaData.setData(str(self.axona_root_name) + \".\" + eeg_type, data)\n\n    def writePos2AxonaFormat(self, header: dataclass, data: np.array):\n        self.AxonaData.setHeader(str(self.axona_root_name) + \".pos\", header)\n        self.AxonaData.setData(str(self.axona_root_name) + \".pos\", data)\n\n    def writeTetrodeData(self, itet: str, header: dataclass, data: np.array):\n        self.AxonaData.setHeader(str(self.axona_root_name) + \".\" + itet, header)\n        self.AxonaData.setData(str(self.axona_root_name) + \".\" + itet, data)\n\n    def writeSetData(self, header: dataclass):\n        self.AxonaData.setHeader(str(self.axona_root_name) + \".set\", header)\n\n    def writeCutData(self, itet: str, header: dataclass, data: np.array):\n        self.AxonaData.setCut(\n            str(self.axona_root_name) + \"_\" + str(itet) + \".cut\", header, data\n        )\n</code></pre>"},{"location":"reference/#ephysiopy.format_converters.OE_Axona.OE2Axona.settings","title":"<code>settings</code>  <code>property</code> <code>writable</code>","text":"<p>Loads the settings data from the settings.xml file</p>"},{"location":"reference/#ephysiopy.format_converters.OE_Axona.OE2Axona.__init__","title":"<code>__init__(pname, path2APData=None, pos_sample_rate=50, channels=0, **kwargs)</code>","text":"<p>Args:     pname (Path): The base directory of the openephys recording.         e.g. '/home/robin/Data/M4643_2023-07-21_11-52-02'     path2APData (Path, optional): Path to AP data. Defaults to None.     pos_sample_rate (int, optional): Position sample rate. Defaults to 50.     channels (int, optional): Number of channels. Defaults to 0.     **kwargs: Variable length argument list.</p> Source code in <code>ephysiopy/format_converters/OE_Axona.py</code> <pre><code>def __init__(\n    self,\n    pname: Path,\n    path2APData: Path = None,\n    pos_sample_rate: int = 50,\n    channels: int = 0,\n    **kwargs,\n):\n    \"\"\"\n    Args:\n        pname (Path): The base directory of the openephys recording.\n            e.g. '/home/robin/Data/M4643_2023-07-21_11-52-02'\n        path2APData (Path, optional): Path to AP data. Defaults to None.\n        pos_sample_rate (int, optional): Position sample rate. Defaults to 50.\n        channels (int, optional): Number of channels. Defaults to 0.\n        **kwargs: Variable length argument list.\n    \"\"\"\n    pname = Path(pname)\n    assert pname.exists()\n    self.pname: Path = pname\n    self.path2APdata: Path = path2APData\n    self.pos_sample_rate: int = pos_sample_rate\n    # 'experiment_1.nwb'\n    self.experiment_name: Path = self.pname or Path(kwargs[\"experiment_name\"])\n    self.recording_name = None  # will become 'recording1' etc\n    self.OE_data = None  # becomes instance of io.recording.OpenEphysBase\n    self._settings = None  # will become an instance of OESettings.Settings\n    # Create a basename for Axona file names\n    # e.g.'/home/robin/Data/experiment_1'\n    # that we can append '.pos' or '.eeg' or whatever onto\n    self.axona_root_name = self.experiment_name\n    # need to instantiated now for later\n    self.AxonaData = axonaIO.IO(self.axona_root_name.name + \".pos\")\n    # THIS IS TEMPORARY AND WILL BE MORE USER-SPECIFIABLE IN THE FUTURE\n    # it is used to scale the spikes\n    self.hp_gain = 500\n    self.lp_gain = 15000\n    self.bitvolts = 0.195\n    # if left as None some default values for the next 3 params are loaded\n    #  from top-level __init__.py\n    # these are only used in self.__filterLFP__\n    self.fs = None\n    # if lfp_channel is set to None then the .set file will reflect that\n    #  no EEG was recorded\n    # this should mean that you can load data into Tint without a .eeg file\n    self.lfp_channel = 1 or kwargs[\"lfp_channel\"]\n    self.lfp_lowcut = None\n    self.lfp_highcut = None\n    # set the tetrodes to record from\n    # defaults to 1 through 4 - see self.makeSetData below\n    self.tetrodes = [\"1\", \"2\", \"3\", \"4\"]\n    self.channel_count = channels\n</code></pre>"},{"location":"reference/#ephysiopy.format_converters.OE_Axona.OE2Axona.convertPosData","title":"<code>convertPosData(xy, xy_ts)</code>","text":"<p>Performs the conversion of the array parts of the data.</p> <p>Note: As well as upsampling the data to the Axona pos sampling rate (50Hz), we have to insert some columns into the pos array as Axona format expects it like: pos_format: t,x1,y1,x2,y2,numpix1,numpix2 We can make up some of the info and ignore other bits.</p> Source code in <code>ephysiopy/format_converters/OE_Axona.py</code> <pre><code>def convertPosData(self, xy: np.array, xy_ts: np.array) -&gt; np.array:\n    \"\"\"\n    Performs the conversion of the array parts of the data.\n\n    Note: As well as upsampling the data to the Axona pos sampling rate (50Hz),\n    we have to insert some columns into the pos array as Axona format\n    expects it like: pos_format: t,x1,y1,x2,y2,numpix1,numpix2\n    We can make up some of the info and ignore other bits.\n    \"\"\"\n    n_new_pts = int(\n        np.floor((self.last_pos_ts - self.first_pos_ts) * self.pos_sample_rate)\n    )\n    t = xy_ts - self.first_pos_ts\n    new_ts = np.linspace(t[0], t[-1], n_new_pts)\n    new_x = np.interp(new_ts, t, xy[:, 0])\n    new_y = np.interp(new_ts, t, xy[:, 1])\n    new_x[np.isnan(new_x)] = 1023\n    new_y[np.isnan(new_y)] = 1023\n    # Expand the pos bit of the data to make it look like Axona data\n    new_pos = np.vstack([new_x, new_y]).T\n    new_pos = np.c_[\n        new_pos,\n        np.ones_like(new_pos) * 1023,\n        np.zeros_like(new_pos),\n        np.zeros_like(new_pos),\n    ]\n    new_pos[:, 4] = 40  # just made this value up - it's numpix i think\n    new_pos[:, 6] = 40  # same\n    # Squeeze this data into Axona pos format array\n    dt = self.AxonaData.axona_files[\".pos\"]\n    new_data = np.zeros(n_new_pts, dtype=dt)\n    # Timestamps in Axona are pos_samples (monotonic, linear integer)\n    new_data[\"ts\"] = new_ts\n    new_data[\"pos\"] = new_pos\n    return new_data\n</code></pre>"},{"location":"reference/#ephysiopy.format_converters.OE_Axona.OE2Axona.convertSpikeData","title":"<code>convertSpikeData(hdf5_tetrode_data)</code>","text":"<p>Does the spike conversion from OE Spike Sorter format to Axona format tetrode files.</p> <p>Args:     hdf5_tetrode_data (h5py._hl.group.Group): This kind of looks like a dictionary and can,         it seems, be treated as one more or less. See http://docs.h5py.org/en/stable/high/group.html</p> Source code in <code>ephysiopy/format_converters/OE_Axona.py</code> <pre><code>def convertSpikeData(self, hdf5_tetrode_data: h5py._hl.group.Group):\n    \"\"\"\n    Does the spike conversion from OE Spike Sorter format to Axona format tetrode files.\n\n    Args:\n        hdf5_tetrode_data (h5py._hl.group.Group): This kind of looks like a dictionary and can,\n            it seems, be treated as one more or less. See http://docs.h5py.org/en/stable/high/group.html\n    \"\"\"\n    # First lets get the datatype for tetrode files as this will be the\n    # same for all tetrodes...\n    dt = self.AxonaData.axona_files[\".1\"]\n    # ... and a basic header for the tetrode file that use for each\n    # tetrode file, changing only the num_spikes value\n    header = TetrodeHeader()\n    header.common[\"duration\"] = str(int(self.last_pos_ts - self.first_pos_ts))\n\n    for key in hdf5_tetrode_data.keys():\n        spiking_data = np.array(hdf5_tetrode_data[key].get(\"data\"))\n        timestamps = np.array(hdf5_tetrode_data[key].get(\"timestamps\"))\n        # check if any of the spiking data is captured before/ after the\n        #  first/ last bit of position data\n        # if there is then discard this as we potentially have no valid\n        # position to align the spike to :(\n        idx = np.logical_or(\n            timestamps &lt; self.first_pos_ts, timestamps &gt; self.last_pos_ts\n        )\n        spiking_data = spiking_data[~idx, :, :]\n        timestamps = timestamps[~idx]\n        # subtract the first pos timestamp from the spiking timestamps\n        timestamps = timestamps - self.first_pos_ts\n        # get the number of spikes here for use below in the header\n        num_spikes = len(timestamps)\n        # repeat the timestamps in tetrode multiples ready for Axona export\n        new_timestamps = np.repeat(timestamps, 4)\n        new_spiking_data = spiking_data.astype(np.float64)\n        # Convert to microvolts...\n        new_spiking_data = new_spiking_data * self.bitvolts\n        # And upsample the spikes...\n        new_spiking_data = self.resample(new_spiking_data, 4, 5, -1)\n        # ... and scale appropriately for Axona and invert as\n        # OE seems to be inverted wrt Axona\n        new_spiking_data = new_spiking_data / (self.hp_gain / 4 / 128.0) * (-1)\n        # ... scale them to the gains specified somewhere\n        #  (not sure where / how to do this yet)\n        shp = new_spiking_data.shape\n        # then reshape them as Axona wants them a bit differently\n        new_spiking_data = np.reshape(new_spiking_data, [shp[0] * shp[1], shp[2]])\n        # Cap any values outside the range of int8\n        new_spiking_data[new_spiking_data &lt; -128] = -128\n        new_spiking_data[new_spiking_data &gt; 127] = 127\n        # create the new array\n        new_tetrode_data = np.zeros(len(new_timestamps), dtype=dt)\n        new_tetrode_data[\"ts\"] = new_timestamps * 96000\n        new_tetrode_data[\"waveform\"] = new_spiking_data\n        # change the header num_spikes field\n        header.tetrode_entries[\"num_spikes\"] = str(num_spikes)\n        i_tetnum = key.split(\"electrode\")[1]\n        print(\"Exporting tetrode {}\".format(i_tetnum))\n        self.writeTetrodeData(i_tetnum, header, new_tetrode_data)\n</code></pre>"},{"location":"reference/#ephysiopy.format_converters.OE_Axona.OE2Axona.convertTemplateDataToAxonaTetrode","title":"<code>convertTemplateDataToAxonaTetrode(max_n_waves=2000, **kwargs)</code>","text":"<p>Converts the data held in a TemplateModel instance into tetrode format Axona data files.</p> <p>For each cluster, there'll be a channel that has a peak amplitude and this contains that peak channel. While the other channels with a large signal in might be on the same tetrode, KiloSort (or whatever) might find channels not within the same tetrode. For a given cluster, we can extract from the TemplateModel the 12 channels across which the signal is strongest using Model.get_cluster_channels(). If a channel from a tetrode is missing from this list then the spikes for that channel(s) will be zeroed when saved to Axona format.</p> <p>Example:     If cluster 3 has a peak channel of 1 then get_cluster_channels() might look like:     [ 1,  2,  0,  6, 10, 11,  4,  12,  7,  5,  8,  9]     Here the cluster has the best signal on 1, then 2, 0 etc, but note that channel 3 isn't in the list.     In this case the data for channel 3 will be zeroed when saved to Axona format.</p> <p>References:     1) https://phy.readthedocs.io/en/latest/api/#phyappstemplatetemplatemodel</p> Source code in <code>ephysiopy/format_converters/OE_Axona.py</code> <pre><code>def convertTemplateDataToAxonaTetrode(self, max_n_waves=2000, **kwargs):\n    \"\"\"\n    Converts the data held in a TemplateModel instance into tetrode\n    format Axona data files.\n\n    For each cluster, there'll be a channel that has a peak amplitude and this contains that peak channel.\n    While the other channels with a large signal in might be on the same tetrode, KiloSort (or whatever) might find\n    channels *not* within the same tetrode. For a given cluster, we can extract from the TemplateModel the 12 channels across\n    which the signal is strongest using Model.get_cluster_channels(). If a channel from a tetrode is missing from this list then the\n    spikes for that channel(s) will be zeroed when saved to Axona format.\n\n    Example:\n        If cluster 3 has a peak channel of 1 then get_cluster_channels() might look like:\n        [ 1,  2,  0,  6, 10, 11,  4,  12,  7,  5,  8,  9]\n        Here the cluster has the best signal on 1, then 2, 0 etc, but note that channel 3 isn't in the list.\n        In this case the data for channel 3 will be zeroed when saved to Axona format.\n\n    References:\n        1) https://phy.readthedocs.io/en/latest/api/#phyappstemplatetemplatemodel\n    \"\"\"\n    # First lets get the datatype for tetrode files as this will be the\n    # same for all tetrodes...\n    dt = self.AxonaData.axona_files[\".1\"]\n    # Load the TemplateModel\n    if \"path2APdata\" in kwargs.keys():\n        self.OE_data.load_neural_data(**kwargs)\n    else:\n        self.OE_data.load_neural_data()\n    model = self.OE_data.template_model\n    clusts = model.cluster_ids\n    # have to pre-process the channels / clusters to determine\n    # which tetrodes clusters belong to - this is based on\n    # the 'best' channel for a given cluster\n    clusters_channels = OrderedDict(dict.fromkeys(clusts, np.ndarray))\n    for c in clusts:\n        clusters_channels[c] = model.get_cluster_channels(c)\n    tetrodes_clusters = OrderedDict(\n        dict.fromkeys(range(0, int(self.channel_count / 4)), [])\n    )\n    for t in tetrodes_clusters.items():\n        this_tetrodes_clusters = []\n        for c in clusters_channels.items():\n            if int(c[1][0] / 4) == t[0]:\n                this_tetrodes_clusters.append(c[0])\n        tetrodes_clusters[t[0]] = this_tetrodes_clusters\n    # limit the number of spikes to max_n_waves in the\n    # interests of speed. Randomly select spikes across\n    # the period they fire\n    rng = np.random.default_rng()\n\n    for i, i_tet_item in enumerate(tetrodes_clusters.items()):\n        this_tetrode = i_tet_item[0]\n        times_to_sort = []\n        new_clusters = []\n        new_waves = []\n        for clust in tqdm(i_tet_item[1], desc=\"Tetrode \" + str(i + 1)):\n            clust_chans = model.get_cluster_channels(clust)\n            idx = np.logical_and(\n                clust_chans &gt;= this_tetrode, clust_chans &lt; this_tetrode + 4\n            )\n            # clust_chans is an ordered list of the channels\n            # the cluster was most active on. idx has True\n            # where there is overlap between that and the\n            # currently active tetrode channel numbers (0:4, 4:8\n            # or whatever)\n            spike_idx = model.get_cluster_spikes(clust)\n            # limit the number of spikes to max_n_waves in the\n            # interests of speed. Randomly select spikes across\n            # the period they fire\n            total_n_waves = len(spike_idx)\n            max_num_waves = (\n                max_n_waves if max_n_waves &lt; total_n_waves else total_n_waves\n            )\n            # grab spike times (in seconds) so the random sampling of\n            # spikes matches their times\n            times = model.spike_times[model.spike_clusters == clust]\n            spike_idx_times_subset = rng.choice(\n                (spike_idx, times), max_num_waves, axis=1, replace=False\n            )\n            # spike_idx_times_subset is unsorted as it's just been drawn\n            # from a random distribution, so sort it now\n            spike_idx_times_subset = np.sort(spike_idx_times_subset, 1)\n            # split out into spikes and times\n            spike_idx_subset = spike_idx_times_subset[0, :].astype(int)\n            times = spike_idx_times_subset[1, :]\n            waves = model.get_waveforms(spike_idx_subset, clust_chans[idx])\n            # Given a spike at time T, Axona takes T-200us and T+800us\n            # from the buffer to make up a waveform. From OE\n            # take 30 samples which corresponds to a 1ms sample\n            # if the data is sampled at 30kHz. Interpolate this so the\n            # length is 50 samples as with Axona\n            waves = waves[:, 30:60, :]\n            # waves go from int16 to float as a result of the resampling\n            waves = self.resample(waves.astype(float), axis=1)\n            # multiply by bitvolts to get microvolts\n            waves = waves * self.bitvolts\n            # scale appropriately for Axona and invert as\n            # OE seems to be inverted wrt Axona\n            waves = waves / (self.hp_gain / 4 / 128.0) * (-1)\n            # check the shape of waves to make sure it has 4\n            # channels, if not add some to make it so and make\n            # sure they are in the correct order for the tetrode\n            ordered_chans = np.argsort(clust_chans[idx])\n            if waves.shape[-1] != 4:\n                z = np.zeros(shape=(waves.shape[0], waves.shape[1], 4))\n                z[:, :, ordered_chans] = waves\n                waves = z\n            else:\n                waves = waves[:, :, ordered_chans]\n            # Axona format tetrode waveforms are nSpikes x 4 x 50\n            waves = np.transpose(waves, (0, 2, 1))\n            # Append clusters to a list to sort later for saving a\n            # cluster/ cut file\n            new_clusters.append(np.repeat(clust, len(times)))\n            # Axona times are sampled at 96KHz\n            times = times * 96000\n            # There is a time for each spike despite the repetition\n            # get the indices for sorting\n            times_to_sort.append(times)\n            # i_clust_data = np.zeros(len(new_times), dtype=dt)\n            new_waves.append(waves)\n        # Concatenate, order and reshape some of the lists/ arrays\n        if times_to_sort:  # apparently can be empty sometimes\n            _times = np.concatenate(times_to_sort)\n            _waves = np.concatenate(new_waves)\n            _clusters = np.concatenate(new_clusters)\n            indices = np.argsort(_times)\n            sorted_times = _times[indices]\n            sorted_waves = _waves[indices]\n            sorted_clusts = _clusters[indices]\n            output_times = np.repeat(sorted_times, 4)\n            output_waves = np.reshape(\n                sorted_waves,\n                [\n                    sorted_waves.shape[0] * sorted_waves.shape[1],\n                    sorted_waves.shape[2],\n                ],\n            )\n            new_tetrode_data = np.zeros(len(output_times), dtype=dt)\n            new_tetrode_data[\"ts\"] = output_times\n            new_tetrode_data[\"waveform\"] = output_waves\n            header = TetrodeHeader()\n            header.common[\"duration\"] = str(int(model.duration))\n            header.tetrode_entries[\"num_spikes\"] = str(len(_clusters))\n            self.writeTetrodeData(str(i + 1), header, new_tetrode_data)\n            cut_header = CutHeader()\n            self.writeCutData(str(i + 1), cut_header, sorted_clusts)\n</code></pre>"},{"location":"reference/#ephysiopy.format_converters.OE_Axona.OE2Axona.exportLFP","title":"<code>exportLFP(channel=0, lfp_type='eeg', gain=5000, **kwargs)</code>","text":"<p>Exports LFP data to file.</p> <p>Args:     channel (int): The channel number.     lfp_type (str): The type of LFP data. Legal values are 'egf' or 'eeg'.     gain (int): Multiplier for the LFP data.</p> Source code in <code>ephysiopy/format_converters/OE_Axona.py</code> <pre><code>def exportLFP(\n    self, channel: int = 0, lfp_type: str = \"eeg\", gain: int = 5000, **kwargs\n):\n    \"\"\"\n    Exports LFP data to file.\n\n    Args:\n        channel (int): The channel number.\n        lfp_type (str): The type of LFP data. Legal values are 'egf' or 'eeg'.\n        gain (int): Multiplier for the LFP data.\n    \"\"\"\n    print(\"Beginning conversion and exporting of LFP data...\")\n    if not self.settings.processors:\n        self.settings.parse()\n    from ephysiopy.io.recording import memmapBinaryFile\n\n    try:\n        data = memmapBinaryFile(\n            Path(self.path2APdata).joinpath(\"continuous.dat\"),\n            n_channels=self.channel_count,\n        )\n        self.makeLFPData(data[channel, :], eeg_type=lfp_type, gain=gain)\n        print(\"Completed exporting LFP data to \" + lfp_type + \" format\")\n    except Exception as e:\n        print(f\"Couldn't load raw data:\\n{e}\")\n</code></pre>"},{"location":"reference/#ephysiopy.format_converters.OE_Axona.OE2Axona.exportSetFile","title":"<code>exportSetFile(**kwargs)</code>","text":"<p>Wrapper for makeSetData below</p> Source code in <code>ephysiopy/format_converters/OE_Axona.py</code> <pre><code>def exportSetFile(self, **kwargs):\n    \"\"\"\n    Wrapper for makeSetData below\n    \"\"\"\n    print(\"Exporting set file data...\")\n    self.makeSetData(**kwargs)\n    print(\"Done exporting set file.\")\n</code></pre>"},{"location":"reference/#ephysiopy.format_converters.OE_Axona.OE2Axona.getOEData","title":"<code>getOEData()</code>","text":"<p>Loads the nwb file names in filename_root and returns a dict containing some of the nwb data relevant for converting to Axona file formats.</p> <p>Args:     filename_root (str): Fully qualified name of the nwb file.     recording_name (str): The name of the recording in the nwb file. Note that         the default has changed in different versions of OE from 'recording0'         to 'recording1'.</p> Source code in <code>ephysiopy/format_converters/OE_Axona.py</code> <pre><code>def getOEData(self) -&gt; OpenEphysBase:\n    \"\"\"\n    Loads the nwb file names in filename_root and returns a dict\n    containing some of the nwb data relevant for converting to Axona file formats.\n\n    Args:\n        filename_root (str): Fully qualified name of the nwb file.\n        recording_name (str): The name of the recording in the nwb file. Note that\n            the default has changed in different versions of OE from 'recording0'\n            to 'recording1'.\n    \"\"\"\n    OE_data = OpenEphysBase(self.pname)\n    try:\n        OE_data.load_pos_data(sample_rate=self.pos_sample_rate)\n        # It's likely that spikes have been collected after the last\n        # position sample\n        # due to buffering issues I can't be bothered to resolve.\n        # Get the last pos\n        # timestamps here and check that spikes don't go beyond\n        #  this when writing data\n        # out later\n        # Also the pos and spike data timestamps almost never start at\n        #  0 as the user\n        # usually acquires data for a while before recording.\n        # Grab the first timestamp\n        # here with a view to subtracting this from everything (including\n        # the spike data)\n        # and figuring out what to keep later\n        first_pos_ts = OE_data.PosCalcs.xyTS[0]\n        last_pos_ts = OE_data.PosCalcs.xyTS[-1]\n        self.first_pos_ts = first_pos_ts\n        self.last_pos_ts = last_pos_ts\n    except Exception:\n        OE_data.load_neural_data()  # will create TemplateModel instance\n        self.first_pos_ts = 0\n        self.last_pos_ts = self.OE_data.template_model.duration\n    print(f\"First pos ts: {self.first_pos_ts}\")\n    print(f\"Last pos ts: {self.last_pos_ts}\")\n    self.OE_data = OE_data\n    if self.path2APdata is None:\n        self.path2APdata = self.OE_data.path2APdata\n    # extract number of channels from settings\n    for item in self.settings.record_nodes.items():\n        if \"Rhythm Data\" in item[1].name:\n            self.channel_count = int(item[1].channel_count)\n    return OE_data\n</code></pre>"},{"location":"reference/#ephysiopy.format_converters.OE_Axona.OE2Axona.makeLFPData","title":"<code>makeLFPData(data, eeg_type='eeg', gain=5000)</code>","text":"<p>Downsamples the data in data and saves the result as either an egf or eeg file depending on the choice of either eeg_type which can take a value of either 'egf' or 'eeg'. Gain is the scaling factor.</p> <p>Args:     data (np.array): The data to be downsampled. Must have dtype as np.int16.</p> Source code in <code>ephysiopy/format_converters/OE_Axona.py</code> <pre><code>def makeLFPData(self, data: np.ndarray, eeg_type=\"eeg\", gain=5000):\n    \"\"\"\n    Downsamples the data in data and saves the result as either an egf or eeg file\n    depending on the choice of either eeg_type which can take a value of either 'egf' or 'eeg'.\n    Gain is the scaling factor.\n\n    Args:\n        data (np.array): The data to be downsampled. Must have dtype as np.int16.\n    \"\"\"\n    if eeg_type == \"eeg\":\n        from ephysiopy.axona.file_headers import EEGHeader\n\n        header = EEGHeader()\n        dst_rate = 250\n    elif eeg_type == \"egf\":\n        from ephysiopy.axona.file_headers import EGFHeader\n\n        header = EGFHeader()\n        dst_rate = 4800\n    header.common[\"duration\"] = str(int(self.last_pos_ts - self.first_pos_ts))\n    print(f\"header.common[duration] = {header.common['duration']}\")\n    _lfp_data = self.resample(data.astype(float), 30000, dst_rate, -1)\n    # make sure data is same length as sample_rate * duration\n    nsamples = int(dst_rate * int(header.common[\"duration\"]))\n    # lfp_data might be shorter than nsamples. If so, fill the\n    # remaining values with zeros\n    if len(_lfp_data) &lt; nsamples:\n        lfp_data = np.zeros(nsamples)\n        lfp_data[0 : len(_lfp_data)] = _lfp_data\n    else:\n        lfp_data = _lfp_data[0:nsamples]\n    lfp_data = self.__filterLFP__(lfp_data, dst_rate)\n    # convert the data format\n    # lfp_data = lfp_data * self.bitvolts # in microvolts\n\n    if eeg_type == \"eeg\":\n        # probably BROKEN\n        # lfp_data starts out as int16 (see Parameters above)\n        # but gets converted into float64 as part of the\n        # resampling/ filtering process\n        lfp_data = lfp_data / 32768.0\n        lfp_data = lfp_data * gain\n        # cap the values at either end...\n        lfp_data[lfp_data &lt; -128] = -128\n        lfp_data[lfp_data &gt; 127] = 127\n        # and convert to int8\n        lfp_data = lfp_data.astype(np.int8)\n\n    elif eeg_type == \"egf\":\n        # probably works\n        # lfp_data = lfp_data / 256.\n        lfp_data = lfp_data.astype(np.int16)\n\n    header.n_samples = str(len(lfp_data))\n    self.writeLFP2AxonaFormat(header, lfp_data, eeg_type)\n</code></pre>"},{"location":"reference/#ephysiopy.format_converters.OE_Axona.OE2Axona.resample","title":"<code>resample(data, src_rate=30, dst_rate=50, axis=0)</code>","text":"<p>Resamples data using FFT</p> Source code in <code>ephysiopy/format_converters/OE_Axona.py</code> <pre><code>def resample(self, data, src_rate=30, dst_rate=50, axis=0):\n    \"\"\"\n    Resamples data using FFT\n    \"\"\"\n    denom = np.gcd(dst_rate, src_rate)\n    new_data = signal.resample_poly(data, dst_rate / denom, src_rate / denom, axis)\n    return new_data\n</code></pre>"},{"location":"reference/#openephys-to-numpy","title":"OpenEphys to numpy","text":""},{"location":"reference/#ephysiopy.format_converters.OE_numpy.OE2Numpy","title":"<code>OE2Numpy</code>","text":"<p>               Bases: <code>object</code></p> <p>Converts openephys data recorded in the nwb format into numpy files</p> <p>NB Only exports the LFP and TTL files at the moment</p> Source code in <code>ephysiopy/format_converters/OE_numpy.py</code> <pre><code>class OE2Numpy(object):\n    \"\"\"\n    Converts openephys data recorded in the nwb format into numpy files\n\n    NB Only exports the LFP and TTL files at the moment\n    \"\"\"\n    def __init__(self, filename_root: str):\n        self.filename_root = filename_root  # /home/robin/Data/experiment_1.nwb\n        self.dirname = os.path.dirname(filename_root)  # '/home/robin/Data'\n        # 'experiment_1.nwb'\n        self.experiment_name = os.path.basename(self.filename_root)\n        self.recording_name = None  # will become 'recording1' etc\n        self.OE_data = None  # becomes OpenEphysBase instance\n        self._settings = None  # will become an instance of OESettings.Settings\n        self.fs = None\n        self.lfp_lowcut = None\n        self.lfp_highcut = None\n\n    def resample(self, data, src_rate=30, dst_rate=50, axis=0):\n        \"\"\"\n        Upsamples data using FFT\n        \"\"\"\n        denom = np.gcd(dst_rate, src_rate)\n        new_data = signal.resample_poly(\n            data, dst_rate/denom, src_rate/denom, axis)\n        return new_data\n\n    @property\n    def settings(self):\n        \"\"\"\n        Loads the settings data from the settings.xml file\n        \"\"\"\n        if self._settings is None:\n            self._settings = OESettings.Settings(self.dirname)\n        return self._settings\n\n    @settings.setter\n    def settings(self, value):\n        self._settings = value\n\n    def getOEData(\n            self, filename_root: str, recording_name='recording1') -&gt; dict:\n        \"\"\"\n        Loads the nwb file names in filename_root and returns a dict\n        containing some of the nwb data relevant for converting to Axona\n        file formats.\n\n        Args:\n            filename_root (str): Fully qualified name of the nwb file.\n            recording_name (str): The name of the recording in the nwb file.\n            Note that the default has changed in different versions of OE from\n            'recording0' to 'recording1'.\n        \"\"\"\n        if os.path.isfile(filename_root):\n            OE_data = OpenEphysNWB(self.dirname)\n            print(\"Loading nwb data...\")\n            OE_data.load(\n                self.dirname, session_name=self.experiment_name,\n                recording_name=recording_name, loadspikes=False, loadraw=True)\n            print(\"Loaded nwb data from: {}\".format(filename_root))\n            # It's likely that spikes have been collected after the last\n            # position sample\n            # due to buffering issues I can't be bothered to resolve.\n            # Get the last pos\n            # timestamps here and check that spikes don't go beyond this\n            # when writing data out later\n            # Also the pos and spike data timestamps almost never start at\n            #  0 as the user\n            # usually acquires data for a while before recording.\n            # Grab the first timestamp\n            # here with a view to subtracting this from everything\n            # (including the spike data)\n            # and figuring out what to keep later\n            try:  # pos might not be present\n                first_pos_ts = OE_data.xyTS[0]\n                last_pos_ts = OE_data.xyTS[-1]\n                self.first_pos_ts = first_pos_ts\n                self.last_pos_ts = last_pos_ts\n            except Exception:\n                print(\"No position data in nwb file\")\n            self.recording_name = recording_name\n            self.OE_data = OE_data\n            return OE_data\n\n    def exportLFP(self, channels: list, output_freq: int):\n        print(\"Beginning conversion and exporting of LFP data...\")\n        channels = [int(c) for c in channels]\n        if not self.settings.processors:\n            self.settings.parse()\n        if self.settings.fpga_sample_rate is None:\n            self.settings.parseProcessor()\n        output_name = os.path.join(self.dirname, \"lfp.npy\")\n        output_ts_name = os.path.join(self.dirname, \"lfp_timestamps.npy\")\n        if len(channels) == 1:\n            # resample data\n            print(\"Resampling data from {0} to {1} Hz\".format(\n                self.settings.fpga_sample_rate, output_freq))\n            new_data = self.resample(\n                self.OE_data.rawData[:, channels],\n                self.settings.fpga_sample_rate, output_freq)\n            np.save(output_name, new_data, allow_pickle=False)\n        if len(channels) &gt; 1:\n            print(\"Resampling data from {0} to {1} Hz\".format(\n                self.settings.fpga_sample_rate, output_freq))\n            new_data = self.resample(\n                self.OE_data.rawData[:, channels[0]:channels[-1]],\n                self.settings.fpga_sample_rate, output_freq)\n            np.save(output_name, new_data, allow_pickle=False)\n        nsamples = np.shape(new_data)[0]\n        new_ts = np.linspace(self.OE_data.ts[0], self.OE_data.ts[-1], nsamples)\n        np.save(output_ts_name, new_ts, allow_pickle=False)\n        print(\"Finished exporting LFP data\")\n\n    def exportTTL(self):\n        print(\"Exporting TTL data...\")\n        ttl_state = self.OE_data.ttl_data\n        ttl_ts = self.OE_data.ttl_timestamps\n        np.save(os.path.join(\n            self.dirname, \"ttl_state.npy\"), ttl_state, allow_pickle=False)\n        np.save(os.path.join(\n            self.dirname, \"ttl_timestamps.npy\"), ttl_ts, allow_pickle=False)\n        print(\"Finished exporting TTL data\")\n\n    def exportRaw2Binary(self, output_fname=None):\n        if self.OE_data.rawData is None:\n            print(\"Load the data first. See getOEData()\")\n            return\n        if output_fname is None:\n            output_fname = os.path.splitext(self.filename_root)[0] + '.bin'\n        print(f\"Exporting raw data to:\\n{output_fname}\")\n        with open(output_fname, 'wb') as f:\n            np.save(f, self.OE_data.rawData)\n        print(\"Finished exporting\")\n</code></pre>"},{"location":"reference/#ephysiopy.format_converters.OE_numpy.OE2Numpy.settings","title":"<code>settings</code>  <code>property</code> <code>writable</code>","text":"<p>Loads the settings data from the settings.xml file</p>"},{"location":"reference/#ephysiopy.format_converters.OE_numpy.OE2Numpy.getOEData","title":"<code>getOEData(filename_root, recording_name='recording1')</code>","text":"<p>Loads the nwb file names in filename_root and returns a dict containing some of the nwb data relevant for converting to Axona file formats.</p> <p>Args:     filename_root (str): Fully qualified name of the nwb file.     recording_name (str): The name of the recording in the nwb file.     Note that the default has changed in different versions of OE from     'recording0' to 'recording1'.</p> Source code in <code>ephysiopy/format_converters/OE_numpy.py</code> <pre><code>def getOEData(\n        self, filename_root: str, recording_name='recording1') -&gt; dict:\n    \"\"\"\n    Loads the nwb file names in filename_root and returns a dict\n    containing some of the nwb data relevant for converting to Axona\n    file formats.\n\n    Args:\n        filename_root (str): Fully qualified name of the nwb file.\n        recording_name (str): The name of the recording in the nwb file.\n        Note that the default has changed in different versions of OE from\n        'recording0' to 'recording1'.\n    \"\"\"\n    if os.path.isfile(filename_root):\n        OE_data = OpenEphysNWB(self.dirname)\n        print(\"Loading nwb data...\")\n        OE_data.load(\n            self.dirname, session_name=self.experiment_name,\n            recording_name=recording_name, loadspikes=False, loadraw=True)\n        print(\"Loaded nwb data from: {}\".format(filename_root))\n        # It's likely that spikes have been collected after the last\n        # position sample\n        # due to buffering issues I can't be bothered to resolve.\n        # Get the last pos\n        # timestamps here and check that spikes don't go beyond this\n        # when writing data out later\n        # Also the pos and spike data timestamps almost never start at\n        #  0 as the user\n        # usually acquires data for a while before recording.\n        # Grab the first timestamp\n        # here with a view to subtracting this from everything\n        # (including the spike data)\n        # and figuring out what to keep later\n        try:  # pos might not be present\n            first_pos_ts = OE_data.xyTS[0]\n            last_pos_ts = OE_data.xyTS[-1]\n            self.first_pos_ts = first_pos_ts\n            self.last_pos_ts = last_pos_ts\n        except Exception:\n            print(\"No position data in nwb file\")\n        self.recording_name = recording_name\n        self.OE_data = OE_data\n        return OE_data\n</code></pre>"},{"location":"reference/#ephysiopy.format_converters.OE_numpy.OE2Numpy.resample","title":"<code>resample(data, src_rate=30, dst_rate=50, axis=0)</code>","text":"<p>Upsamples data using FFT</p> Source code in <code>ephysiopy/format_converters/OE_numpy.py</code> <pre><code>def resample(self, data, src_rate=30, dst_rate=50, axis=0):\n    \"\"\"\n    Upsamples data using FFT\n    \"\"\"\n    denom = np.gcd(dst_rate, src_rate)\n    new_data = signal.resample_poly(\n        data, dst_rate/denom, src_rate/denom, axis)\n    return new_data\n</code></pre>"}]}