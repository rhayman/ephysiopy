{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to ephysiopy","text":""},{"location":"#installation","title":"Installation","text":"<pre><code>pip install ephysiopy\n</code></pre>"},{"location":"#basic-usage","title":"Basic usage","text":"<p>python<code>from ephysiopy.io.recording import OpenEphysBase from pathlib import Path trial = OpenEphysBase(Path(\"/path/to/top/folder\")) trial.load_pos_data() trial.load_neural_data() trial.rate_map(1, 1)</code></p> <p>This will load the data recorded with openephys contained in the folder \"/path/to/top/folder\", load the position data and the neural data (KiloSort output) and plot the cluster 1 on channel 1</p>"},{"location":"reference/","title":"Reference","text":""},{"location":"reference/#recording-data","title":"Recording data","text":""},{"location":"reference/#ephysiopy.io.recording.AxonaTrial","title":"<code>AxonaTrial</code>","text":"<p>               Bases: <code>TrialInterface</code></p> Source code in <code>ephysiopy/io/recording.py</code> <pre><code>class AxonaTrial(TrialInterface):\n    def __init__(self, pname: Path, **kwargs) -&gt; None:\n        pname = Path(pname)\n        super().__init__(pname, **kwargs)\n        self._settings = None\n        use_volts = kwargs.get(\"volts\", True)\n        self.TETRODE = TetrodeDict(\n            str(self.pname.with_suffix(\"\")), volts=use_volts)\n        self.load_settings()\n\n    def load_lfp(self, *args, **kwargs):\n        from ephysiopy.axona.axonaIO import EEG\n\n        if \"egf\" in args:\n            lfp = EEG(self.pname, egf=1)\n        else:\n            lfp = EEG(self.pname)\n        if lfp is not None:\n            self.EEGCalcs = EEGCalcsGeneric(lfp.sig, lfp.sample_rate)\n\n    def load_neural_data(self, *args, **kwargs):\n        if \"tetrode\" in kwargs.keys():\n            use_volts = kwargs.get(\"volts\", True)\n            self.TETRODE[kwargs[\"tetrode\"], use_volts]  # lazy load\n\n    def load_cluster_data(self, *args, **kwargs):\n        return False\n\n    def load_settings(self, *args, **kwargs):\n        if self._settings is None:\n            try:\n                settings_io = IO()\n                self.settings = settings_io.getHeader(str(self.pname))\n            except IOError:\n                print(\".set file not loaded\")\n                self.settings = None\n\n    def load_pos_data(\n        self, ppm: int = 300, jumpmax: int = 100, *args, **kwargs\n    ) -&gt; None:\n        try:\n            AxonaPos = Pos(Path(self.pname))\n            P = PosCalcsGeneric(\n                AxonaPos.led_pos[:, 0],\n                AxonaPos.led_pos[:, 1],\n                cm=True,\n                ppm=ppm,\n                jumpmax=jumpmax,\n            )\n            P.sample_rate = AxonaPos.getHeaderVal(\n                AxonaPos.header, \"sample_rate\")\n            P.xyTS = AxonaPos.ts / P.sample_rate  # in seconds now\n            P.postprocesspos(tracker_params={\"SampleRate\": P.sample_rate})\n            print(\"Loaded pos data\")\n            self.PosCalcs = P\n        except IOError:\n            print(\"Couldn't load the pos data\")\n\n    def load_ttl(self, *args, **kwargs) -&gt; bool:\n        from ephysiopy.axona.axonaIO import Stim\n        try:\n            self.ttl_data = Stim(self.pname)\n            # ttl times in Stim are in ms\n        except IOError:\n            return False\n        return True\n\n    def get_spike_times(self,\n                        tetrode=None,\n                        cluster: int = None,\n                        *args,\n                        **kwargs):\n        \"\"\"\n        Args:\n            tetrode (int): \n            cluster (int): \n\n        Returns:\n            spike_times (np.ndarray): \n        \"\"\"\n        if tetrode is not None:\n            return self.TETRODE.get_spike_samples(int(tetrode), int(cluster))\n</code></pre>"},{"location":"reference/#ephysiopy.io.recording.AxonaTrial.get_spike_times","title":"<code>get_spike_times(tetrode=None, cluster=None, *args, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>tetrode</code> <code>int</code> <code>None</code> <code>cluster</code> <code>int</code> <code>None</code> <p>Returns:</p> Name Type Description <code>spike_times</code> <code>ndarray</code> Source code in <code>ephysiopy/io/recording.py</code> <pre><code>def get_spike_times(self,\n                    tetrode=None,\n                    cluster: int = None,\n                    *args,\n                    **kwargs):\n    \"\"\"\n    Args:\n        tetrode (int): \n        cluster (int): \n\n    Returns:\n        spike_times (np.ndarray): \n    \"\"\"\n    if tetrode is not None:\n        return self.TETRODE.get_spike_samples(int(tetrode), int(cluster))\n</code></pre>"},{"location":"reference/#ephysiopy.io.recording.OpenEphysBase","title":"<code>OpenEphysBase</code>","text":"<p>               Bases: <code>TrialInterface</code></p> Source code in <code>ephysiopy/io/recording.py</code> <pre><code>class OpenEphysBase(TrialInterface):\n    def __init__(self, pname: Path, **kwargs) -&gt; None:\n        pname = Path(pname)\n        super().__init__(pname, **kwargs)\n        setattr(self, \"sync_message_file\", None)\n        self.load_settings()\n        # The numbers after the strings in this list are the node id's\n        # in openephys\n        record_methods = [\n            \"Acquisition Board [0-9][0-9][0-9]\",\n            \"Acquisition Board\",\n            \"Neuropix-PXI [0-9][0-9][0-9]\",\n            \"Neuropix-PXI\",\n            \"Sources/Neuropix-PXI [0-9][0-9][0-9]\",\n            \"Rhythm FPGA [0-9][0-9][0-9]\",\n            \"Rhythm\",\n            \"Sources/Rhythm FPGA [0-9][0-9][0-9]\",\n        ]\n        rec_method = [\n            re.search(m, k).string\n            for k in self.settings.processors.keys()\n            for m in record_methods\n            if re.search(m, k) is not None\n        ][0]\n        if \"Sources/\" in rec_method:\n            rec_method = rec_method.lstrip(\"Sources/\")\n\n        self.rec_kind = Xml2RecordingKind[rec_method.rpartition(\" \")[0]]\n\n        # Attempt to find the files contained in the parent directory\n        # related to the recording with the default experiment and\n        # recording name\n        self.find_files(pname)\n        self.sample_rate = None\n        self.sample_rate = self.settings.processors[rec_method].sample_rate\n        if self.sample_rate is None:\n            if self.rec_kind == RecordingKind.NEUROPIXELS:\n                self.sample_rate = 30000\n        else:  # rubbish fix - many strs need casting to int/float\n            self.sample_rate = float(self.sample_rate)\n        self.channel_count = self.settings.processors[rec_method].channel_count\n        if self.channel_count is None:\n            if self.rec_kind == RecordingKind.NEUROPIXELS:\n                self.channel_count = 384\n        self.kilodata = None\n        self.template_model = None\n\n    def _get_recording_start_time(self) -&gt; float:\n        \"\"\"\n        Get the recording start time from the sync_messages.txt file\n        \"\"\"\n        recording_start_time = 0.0\n        if self.sync_message_file is not None:\n            with open(self.sync_message_file, \"r\") as f:\n                sync_strs = f.read()\n            sync_lines = sync_strs.split(\"\\n\")\n            for line in sync_lines:\n                if \"Start Time\" in line:\n                    tokens = line.split(\":\")\n                    start_time = int(tokens[-1])\n                    sample_rate = int(tokens[0].split(\n                        \"@\")[-1].strip().split()[0])\n                    recording_start_time = start_time / float(sample_rate)\n        return recording_start_time\n\n    @cache\n    def get_spike_times(self,\n                        tetrode: int = None,\n                        cluster: int = None,\n                        *args, **kwargs):\n        \"\"\"\n        Args:\n            tetrode (int): \n            cluster (int): \n\n        Returns:\n            spike_times (np.ndarray): \n        \"\"\"\n        if not self.clusterData:\n            self.load_cluster_data()\n        ts = self.clusterData.spk_times\n        if cluster in self.clusterData.spk_clusters:\n            times = ts[self.clusterData.spk_clusters == cluster]\n            return times.astype(np.int64) / self.sample_rate\n        else:\n            warnings.warn(\"Cluster not present\")\n\n    @cache\n    def load_lfp(self, *args, **kwargs):\n        \"\"\"\n        Valid kwargs are:\n        'target_sample_rate' - int\n            the sample rate to downsample to from the original\n        \"\"\"\n        from scipy import signal\n\n        if self.path2LFPdata is not None:\n            lfp = memmapBinaryFile(\n                os.path.join(self.path2LFPdata, \"continuous.dat\"),\n                n_channels=self.channel_count,\n            )\n            channel = 0\n            if \"channel\" in kwargs.keys():\n                channel = kwargs[\"channel\"]\n            target_sample_rate = 500\n            if \"target_sample_rate\" in kwargs.keys():\n                target_sample_rate = kwargs[\"target_sample_rate\"]\n            n_samples = np.shape(lfp[channel, :])[0]\n            sig = signal.resample(\n                lfp[channel, :], int(\n                    n_samples / self.sample_rate) * target_sample_rate\n            )\n            self.EEGCalcs = EEGCalcsGeneric(sig, target_sample_rate)\n\n    @cache\n    def load_neural_data(self, *args, **kwargs) -&gt; None:\n        if \"path2APdata\" in kwargs.keys():\n            self.path2APdata: Path = Path(kwargs[\"path2APdata\"])\n        n_channels: int = self.channel_count or kwargs[\"nChannels\"]\n        try:\n            self.template_model = TemplateModel(\n                dir_path=self.path2APdata,\n                sample_rate=3e4,\n                dat_path=Path(self.path2APdata).joinpath(\"continuous.dat\"),\n                n_channels_dat=int(n_channels),\n            )\n            print(\"Loaded neural data\")\n        except Exception:\n            warnings.warn(\"Could not find raw data file\")\n\n    @cache\n    def load_settings(self, *args, **kwargs):\n        if self._settings is None:\n            # pname_root gets walked through and over-written with\n            # correct location of settings.xml\n            self.settings = Settings(self.pname)\n            print(\"Loaded settings data\")\n\n    @cache\n    def load_cluster_data(\n            self, removeNoiseClusters=True, *args, **kwargs) -&gt; bool:\n        if self.path2KiloSortData is not None:\n            clusterData = KiloSortSession(self.pname)\n        else:\n            return False\n        if clusterData is not None:\n            if clusterData.load():\n                print(\"Loaded KiloSort data\")\n                if removeNoiseClusters:\n                    try:\n                        clusterData.removeKSNoiseClusters()\n                        print(\"Removed noise clusters\")\n                    except Exception:\n                        pass\n        else:\n            return False\n        self.clusterData = clusterData\n        return True\n\n    @cache\n    def load_pos_data(\n        self, ppm: int = 300, jumpmax: int = 100, *args, **kwargs\n    ) -&gt; None:\n        # kwargs valid keys = \"loadTTLPos\" - if present loads the ttl\n        # timestamps not the ones in the plugin folder\n\n        # Only sub-class that doesn't use this is OpenEphysNWB\n        # which needs updating\n        # TODO: Update / overhaul OpenEphysNWB\n        # Load the start time from the sync_messages file\n        if \"cm\" in kwargs:\n            cm = kwargs[\"cm\"]\n        else:\n            cm = True\n\n        recording_start_time = self._get_recording_start_time()\n\n        if self.path2PosData is not None:\n            pos_method = [\n                \"Pos Tracker [0-9][0-9][0-9]\",\n                \"PosTracker [0-9][0-9][0-9]\",\n                \"TrackMe [0-9][0-9][0-9]\",\n                \"TrackingPlugin [0-9][0-9][0-9]\",\n                \"Tracking Port\"\n            ]\n            pos_plugin_name = [\n                re.search(m, k).string\n                for k in self.settings.processors.keys()\n                for m in pos_method\n                if re.search(m, k) is not None\n            ][0]\n            if \"Sources/\" in pos_plugin_name:\n                pos_plugin_name = pos_plugin_name.lstrip(\"Sources/\")\n\n            self.pos_plugin_name = pos_plugin_name\n\n            if \"Tracker\" in pos_plugin_name:\n                print(\"Loading Tracker data...\")\n                pos_data = np.load(os.path.join(\n                    self.path2PosData, \"data_array.npy\"))\n            if \"Tracking Port\" in pos_plugin_name:\n                print(\"Loading Tracking Port data...\")\n                pos_data = loadTrackingPluginData(\n                    os.path.join(self.path2PosData, \"data_array.npy\"))\n            if \"TrackingPlugin\" in pos_plugin_name:\n                print(\"Loading TrackingPlugin data...\")\n                pos_data = loadTrackingPluginData(\n                    os.path.join(self.path2PosData, \"data_array.npy\")\n                )\n\n            pos_ts = np.load(os.path.join(self.path2PosData, \"timestamps.npy\"))\n            # pos_ts in seconds\n            pos_ts = np.ravel(pos_ts)\n            if \"TrackMe\" in pos_plugin_name:\n                print(\"Loading TrackMe data...\")\n                n_pos_chans = int(\n                    self.settings.processors[pos_plugin_name].channel_count\n                )\n                pos_data = loadTrackMePluginData(\n                    Path(os.path.join(self.path2PosData, \"continuous.dat\")),\n                    n_channels=n_pos_chans,\n                )\n                if \"loadTTLPos\" in kwargs.keys():\n                    pos_ts = loadTrackMeTTLTimestamps(\n                        Path(self.path2EventsData))\n                else:\n                    pos_ts = loadTrackMeTimestamps(Path(self.path2PosData))\n                pos_ts = pos_ts[0:len(pos_data)]\n            sample_rate = self.settings.processors[pos_plugin_name].sample_rate\n            sample_rate = float(sample_rate) if sample_rate is not None else 50\n            # the timestamps for the Tracker Port plugin are fucked so\n            # we have to infer from the shape of the position data\n            if \"Tracking Port\" in pos_plugin_name:\n                sample_rate = kwargs[\"sample_rate\"] or 50\n                # pos_ts in seconds\n                pos_ts = np.arange(\n                    0, pos_data.shape[0]/sample_rate, 1.0/sample_rate)\n                print(f\"Tracker first and last ts: {pos_ts[0]} &amp; {pos_ts[-1]}\")\n            if pos_plugin_name != \"TrackMe\":\n                xyTS = pos_ts - recording_start_time\n            else:\n                xyTS = pos_ts\n            if self.sync_message_file is not None:\n                recording_start_time = xyTS[0]\n            print(\n                f\"First &amp; last ts before PosCalcs: {pos_ts[0]} &amp; {pos_ts[-1]}\")\n            P = PosCalcsGeneric(\n                pos_data[:, 0],\n                pos_data[:, 1],\n                cm=cm,\n                ppm=ppm,\n                jumpmax=jumpmax,\n            )\n            P.xyTS = xyTS\n            P.sample_rate = sample_rate\n            P.postprocesspos({\"SampleRate\": sample_rate})\n            print(\"Loaded pos data\")\n            self.PosCalcs = P\n        else:\n            warnings.warn(\n                \"Could not find the pos data. \\\n                Make sure there is a pos_data folder with data_array.npy \\\n                and timestamps.npy in\"\n            )\n        self.recording_start_time = recording_start_time\n\n    @cache\n    def load_ttl(self, *args, **kwargs) -&gt; bool:\n        \"\"\"\n        Args:\n            StimControl_id (str): This is the string \n                \"StimControl [0-9][0-9][0-9]\" where the numbers\n                are the node id in the openephys signal chain\n            TTL_channel_number (int): The integer value in the \"states.npy\"\n                file that corresponds to the\n                identity of the TTL input on the Digital I/O board on the\n                openephys recording system. i.e. if there is input to BNC\n                port 3 on the digital I/O board then values of 3 in the\n                states.npy file are high TTL values on this input and -3\n                are low TTL values (I think)\n\n        Returns:\n            Nothing but sets some keys/values in a dict on 'self'\n            called ttl_data, namely:\n\n            ttl_timestamps (list): the times of high ttl pulses in ms\n            stim_duration (int): the duration of the ttl pulse in ms\n        \"\"\"\n        if not Path(self.path2EventsData).exists:\n            return False\n        ttl_ts = np.load(os.path.join(self.path2EventsData, \"timestamps.npy\"))\n        states = np.load(os.path.join(self.path2EventsData, \"states.npy\"))\n        recording_start_time = self._get_recording_start_time()\n        self.ttl_data = {}\n        if \"StimControl_id\" in kwargs.keys():\n            stim_id = kwargs[\"StimControl_id\"]\n            if stim_id in self.settings.processors.keys():\n                duration = getattr(\n                    self.settings.processors[stim_id], \"Duration\")\n            else:\n                return False\n            self.ttl_data[\"stim_duration\"] = int(duration)\n        if \"TTL_channel_number\" in kwargs.keys():\n            chan = kwargs[\"TTL_channel_number\"]\n            high_ttl = ttl_ts[states == chan]\n            # get into ms\n            high_ttl = (high_ttl * 1000.0) - recording_start_time\n            self.ttl_data['ttl_timestamps'] = high_ttl\n        if not self.ttl_data:\n            return False\n        print(\"Loaded ttl data\")\n        return True\n\n    @cache\n    def find_files(\n        self,\n        pname_root: str,\n        experiment_name: str = \"experiment1\",\n        rec_name: str = \"recording1\",\n    ):\n        exp_name = Path(experiment_name)\n        PosTracker_match = (\n            exp_name / rec_name / \"events\" / \"*Pos_Tracker*/BINARY_group*\"\n        )\n        TrackingPlugin_match = (\n            exp_name / rec_name / \"events\" / \"*Tracking_Port*/BINARY_group*\"\n        )\n        TrackMe_match = (\n            exp_name / rec_name / \"continuous\" /\n            \"TrackMe-[0-9][0-9][0-9].TrackingNode\"\n        )\n        sync_file_match = exp_name / rec_name\n        acq_method = \"\"\n        if self.rec_kind == RecordingKind.NEUROPIXELS:\n            # the old OE NPX plugins saved two forms of the data,\n            # one for AP @30kHz and one for LFP @??Hz\n            # the newer plugin saves only the 30kHz data. Also, the\n            # 2.0 probes are saved with Probe[A-Z] appended to the end\n            # of the folder\n            # the older way:\n            acq_method = \"Neuropix-PXI-[0-9][0-9][0-9].\"\n            APdata_match = exp_name / rec_name / \\\n                \"continuous\" / (acq_method + \"0\")\n            LFPdata_match = exp_name / rec_name / \\\n                \"continuous\" / (acq_method + \"1\")\n            # the new way:\n            Rawdata_match = (\n                exp_name / rec_name / \"continuous\" /\n                (acq_method + \"Probe[A-Z]\")\n            )\n        elif self.rec_kind == RecordingKind.FPGA:\n            acq_method = \"Rhythm_FPGA-[0-9][0-9][0-9].\"\n            APdata_match = exp_name / rec_name / \\\n                \"continuous\" / (acq_method + \"0\")\n            LFPdata_match = exp_name / rec_name / \\\n                \"continuous\" / (acq_method + \"1\")\n            Rawdata_match = (\n                exp_name / rec_name / \"continuous\" /\n                (acq_method + \"Probe[A-Z]\")\n            )\n        else:\n            acq_method = \"Acquisition_Board-[0-9][0-9][0-9].*\"\n            APdata_match = exp_name / rec_name / \"continuous\" / acq_method\n            LFPdata_match = exp_name / rec_name / \"continuous\" / acq_method\n            Rawdata_match = (\n                exp_name / rec_name / \"continuous\" /\n                (acq_method + \"Probe[A-Z]\")\n            )\n        Events_match = (\n            # only dealing with a single TTL channel at the moment\n            exp_name\n            / rec_name\n            / \"events\"\n            / acq_method\n            / \"TTL\"\n        )\n\n        if pname_root is None:\n            pname_root = self.pname_root\n\n        for d, c, f in os.walk(pname_root):\n            for ff in f:\n                if \".\" not in c:  # ignore hidden directories\n                    if \"data_array.npy\" in ff:\n                        if PurePath(d).match(str(PosTracker_match)):\n                            if self.path2PosData is None:\n                                self.path2PosData = os.path.join(d)\n                                print(f\"Pos data at: {self.path2PosData}\")\n                            self.path2PosOEBin = Path(d).parents[1]\n                        if PurePath(d).match(\"*pos_data*\"):\n                            if self.path2PosData is None:\n                                self.path2PosData = os.path.join(d)\n                                print(f\"Pos data at: {self.path2PosData}\")\n                        if PurePath(d).match(str(TrackingPlugin_match)):\n                            if self.path2PosData is None:\n                                self.path2PosData = os.path.join(d)\n                                print(f\"Pos data at: {self.path2PosData}\")\n                    if \"continuous.dat\" in ff:\n                        if PurePath(d).match(str(APdata_match)):\n                            self.path2APdata = os.path.join(d)\n                            print(f\"Continuous AP data at: {self.path2APdata}\")\n                            self.path2APOEBin = Path(d).parents[1]\n                        if PurePath(d).match(str(LFPdata_match)):\n                            self.path2LFPdata = os.path.join(d)\n                            print(\n                                f\"Continuous LFP data at: {self.path2LFPdata}\")\n                        if PurePath(d).match(str(Rawdata_match)):\n                            self.path2APdata = os.path.join(d)\n                            self.path2LFPdata = os.path.join(d)\n                        if PurePath(d).match(str(TrackMe_match)):\n                            self.path2PosData = os.path.join(d)\n                            print(f\"TrackMe posdata at: {self.path2PosData}\")\n                    if \"sync_messages.txt\" in ff:\n                        if PurePath(d).match(str(sync_file_match)):\n                            sync_file = os.path.join(d, \"sync_messages.txt\")\n                            if fileContainsString(sync_file, \"Start Time\"):\n                                self.sync_message_file = sync_file\n                                print(f\"sync_messages file at: {sync_file}\")\n                    if \"full_words.npy\" in ff:\n                        if PurePath(d).match(str(Events_match)):\n                            self.path2EventsData = os.path.join(d)\n                            print(f\"Event data at: {self.path2EventsData}\")\n                    if \".nwb\" in ff:\n                        self.path2NWBData = os.path.join(d, ff)\n                        print(f\"nwb data at: {self.path2NWBData}\")\n                    if \"spike_templates.npy\" in ff:\n                        self.path2KiloSortData = os.path.join(d)\n                        print(\n                            f\"Found KiloSort data at {self.path2KiloSortData}\")\n</code></pre>"},{"location":"reference/#ephysiopy.io.recording.OpenEphysBase.get_spike_times","title":"<code>get_spike_times(tetrode=None, cluster=None, *args, **kwargs)</code>  <code>cached</code>","text":"<p>Parameters:</p> Name Type Description Default <code>tetrode</code> <code>int</code> <code>None</code> <code>cluster</code> <code>int</code> <code>None</code> <p>Returns:</p> Name Type Description <code>spike_times</code> <code>ndarray</code> Source code in <code>ephysiopy/io/recording.py</code> <pre><code>@cache\ndef get_spike_times(self,\n                    tetrode: int = None,\n                    cluster: int = None,\n                    *args, **kwargs):\n    \"\"\"\n    Args:\n        tetrode (int): \n        cluster (int): \n\n    Returns:\n        spike_times (np.ndarray): \n    \"\"\"\n    if not self.clusterData:\n        self.load_cluster_data()\n    ts = self.clusterData.spk_times\n    if cluster in self.clusterData.spk_clusters:\n        times = ts[self.clusterData.spk_clusters == cluster]\n        return times.astype(np.int64) / self.sample_rate\n    else:\n        warnings.warn(\"Cluster not present\")\n</code></pre>"},{"location":"reference/#ephysiopy.io.recording.OpenEphysBase.load_lfp","title":"<code>load_lfp(*args, **kwargs)</code>  <code>cached</code>","text":"<p>Valid kwargs are: 'target_sample_rate' - int     the sample rate to downsample to from the original</p> Source code in <code>ephysiopy/io/recording.py</code> <pre><code>@cache\ndef load_lfp(self, *args, **kwargs):\n    \"\"\"\n    Valid kwargs are:\n    'target_sample_rate' - int\n        the sample rate to downsample to from the original\n    \"\"\"\n    from scipy import signal\n\n    if self.path2LFPdata is not None:\n        lfp = memmapBinaryFile(\n            os.path.join(self.path2LFPdata, \"continuous.dat\"),\n            n_channels=self.channel_count,\n        )\n        channel = 0\n        if \"channel\" in kwargs.keys():\n            channel = kwargs[\"channel\"]\n        target_sample_rate = 500\n        if \"target_sample_rate\" in kwargs.keys():\n            target_sample_rate = kwargs[\"target_sample_rate\"]\n        n_samples = np.shape(lfp[channel, :])[0]\n        sig = signal.resample(\n            lfp[channel, :], int(\n                n_samples / self.sample_rate) * target_sample_rate\n        )\n        self.EEGCalcs = EEGCalcsGeneric(sig, target_sample_rate)\n</code></pre>"},{"location":"reference/#ephysiopy.io.recording.OpenEphysBase.load_ttl","title":"<code>load_ttl(*args, **kwargs)</code>  <code>cached</code>","text":"<p>Parameters:</p> Name Type Description Default <code>StimControl_id</code> <code>str</code> <p>This is the string  \"StimControl 0-9[0-9]\" where the numbers are the node id in the openephys signal chain</p> required <code>TTL_channel_number</code> <code>int</code> <p>The integer value in the \"states.npy\" file that corresponds to the identity of the TTL input on the Digital I/O board on the openephys recording system. i.e. if there is input to BNC port 3 on the digital I/O board then values of 3 in the states.npy file are high TTL values on this input and -3 are low TTL values (I think)</p> required <p>Returns:</p> Name Type Description <code>bool</code> <p>Nothing but sets some keys/values in a dict on 'self'</p> <code>bool</code> <p>called ttl_data, namely:</p> <code>ttl_timestamps</code> <code>list</code> <p>the times of high ttl pulses in ms</p> <code>stim_duration</code> <code>int</code> <p>the duration of the ttl pulse in ms</p> Source code in <code>ephysiopy/io/recording.py</code> <pre><code>@cache\ndef load_ttl(self, *args, **kwargs) -&gt; bool:\n    \"\"\"\n    Args:\n        StimControl_id (str): This is the string \n            \"StimControl [0-9][0-9][0-9]\" where the numbers\n            are the node id in the openephys signal chain\n        TTL_channel_number (int): The integer value in the \"states.npy\"\n            file that corresponds to the\n            identity of the TTL input on the Digital I/O board on the\n            openephys recording system. i.e. if there is input to BNC\n            port 3 on the digital I/O board then values of 3 in the\n            states.npy file are high TTL values on this input and -3\n            are low TTL values (I think)\n\n    Returns:\n        Nothing but sets some keys/values in a dict on 'self'\n        called ttl_data, namely:\n\n        ttl_timestamps (list): the times of high ttl pulses in ms\n        stim_duration (int): the duration of the ttl pulse in ms\n    \"\"\"\n    if not Path(self.path2EventsData).exists:\n        return False\n    ttl_ts = np.load(os.path.join(self.path2EventsData, \"timestamps.npy\"))\n    states = np.load(os.path.join(self.path2EventsData, \"states.npy\"))\n    recording_start_time = self._get_recording_start_time()\n    self.ttl_data = {}\n    if \"StimControl_id\" in kwargs.keys():\n        stim_id = kwargs[\"StimControl_id\"]\n        if stim_id in self.settings.processors.keys():\n            duration = getattr(\n                self.settings.processors[stim_id], \"Duration\")\n        else:\n            return False\n        self.ttl_data[\"stim_duration\"] = int(duration)\n    if \"TTL_channel_number\" in kwargs.keys():\n        chan = kwargs[\"TTL_channel_number\"]\n        high_ttl = ttl_ts[states == chan]\n        # get into ms\n        high_ttl = (high_ttl * 1000.0) - recording_start_time\n        self.ttl_data['ttl_timestamps'] = high_ttl\n    if not self.ttl_data:\n        return False\n    print(\"Loaded ttl data\")\n    return True\n</code></pre>"},{"location":"reference/#ephysiopy.io.recording.TrialInterface","title":"<code>TrialInterface</code>","text":"<p>               Bases: <code>FigureMaker</code></p> <p>Defines a minimal and required set of methods for loading electrophysiology data recorded using Axona or OpenEphys (OpenEphysNWB is there but not used)</p> Source code in <code>ephysiopy/io/recording.py</code> <pre><code>class TrialInterface(FigureMaker, metaclass=abc.ABCMeta):\n    \"\"\"\n    Defines a minimal and required set of methods for loading\n    electrophysiology data recorded using Axona or OpenEphys\n    (OpenEphysNWB is there but not used)\n    \"\"\"\n\n    def __init__(self, pname: Path, **kwargs) -&gt; None:\n        assert Path(pname).exists(), f\"Path provided doesnt exist: {pname}\"\n        self._pname = pname\n        self._settings = None\n        self._PosCalcs = None\n        self._RateMap = None\n        self._EEGCalcs = None\n        self._sync_message_file = None\n        self._clusterData = None  # Kilosort or .cut / .clu file\n        self._recording_start_time = None  # float\n        self._ttl_data = None  # dict\n        self._accelerometer_data = None\n        self._path2PosData = None  # Path or str\n\n    @classmethod\n    def __subclasshook__(cls, subclass):\n        return (\n            hasattr(subclass, \"load_neural_data\")\n            and callable(subclass.load_neural_data)\n            and hasattr(subclass, \"load_lfp\")\n            and callable(subclass.load_lfp)\n            and hasattr(subclass, \"load_pos\")\n            and callable(subclass.load_pos)\n            and hasattr(subclass, \"load_cluster_data\")\n            and callable(subclass.load_cluster_data)\n            and hasattr(subclass, \"load_settings\")\n            and callable(subclass.load_settings)\n            and hasattr(subclass, \"get_spike_times\")\n            and callable(subclass.get_spike_times)\n            and hasattr(subclass, \"load_ttl\")\n            and callable(subclass.load_ttl)\n            or NotImplemented\n        )\n\n    @property\n    def pname(self):\n        return self._pname\n\n    @pname.setter\n    def pname(self, val):\n        self._pname = val\n\n    @property\n    def settings(self):\n        return self._settings\n\n    @settings.setter\n    def settings(self, val):\n        self._settings = val\n\n    @property\n    def PosCalcs(self):\n        return self._PosCalcs\n\n    @PosCalcs.setter\n    def PosCalcs(self, val):\n        self._PosCalcs = val\n\n    @property\n    def RateMap(self):\n        return self._RateMap\n\n    @RateMap.setter\n    def RateMap(self, value):\n        self._RateMap = value\n\n    @property\n    def EEGCalcs(self):\n        return self._EEGCalcs\n\n    @EEGCalcs.setter\n    def EEGCalcs(self, val):\n        self._EEGCalcs = val\n\n    @property\n    def clusterData(self):\n        return self._clusterData\n\n    @clusterData.setter\n    def clusterData(self, val):\n        self._clusterData = val\n\n    @property\n    def recording_start_time(self):\n        return self._recording_start_time\n\n    @recording_start_time.setter\n    def recording_start_time(self, val):\n        self._recording_start_time = val\n\n    @property\n    def sync_message_file(self):\n        return self._sync_message_file\n\n    @sync_message_file.setter\n    def sync_message_file(self, val):\n        self._sync_message_file = val\n\n    @property\n    def ttl_data(self):\n        return self._ttl_data\n\n    @ttl_data.setter\n    def ttl_data(self, val):\n        self._ttl_data = val\n\n    @property\n    def accelerometer_data(self):\n        return self._accelerometer_data\n\n    @accelerometer_data.setter\n    def accelerometer_data(self, val):\n        self._accelerometer_data = val\n\n    @property\n    def path2PosData(self):\n        return self._path2PosData\n\n    @path2PosData.setter\n    def path2PosData(self, val):\n        self._path2PosData = val\n\n    @abc.abstractmethod\n    def load_lfp(self, *args, **kwargs) -&gt; NoReturn:\n        \"\"\"Load the LFP data\"\"\"\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def load_neural_data(self, *args, **kwargs) -&gt; NoReturn:\n        \"\"\"Load the neural data\"\"\"\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def load_pos_data(\n        self, ppm: int = 300, jumpmax: int = 100, *args, **kwargs\n    ) -&gt; NoReturn:\n        \"\"\"\n        Load the position data\n\n        Args:\n            pname (Path): Path to base directory containing pos data\n            ppm (int): pixels per metre\n            jumpmax (int): max jump in pixels between positions, more\n                than this and the position is interpolated over\n        \"\"\"\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def load_cluster_data(self, *args, **kwargs) -&gt; bool:\n        \"\"\"Load the cluster data (Kilosort/ Axona cut/ whatever else\"\"\"\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def load_settings(self, *args, **kwargs) -&gt; NoReturn:\n        \"\"\"Loads the format specific settings file\"\"\"\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def load_ttl(self, *args, **kwargs) -&gt; bool:\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def get_spike_times(self, cluster: int, tetrode: int, *args, **kwargs):\n        \"\"\"Returns the times of an individual cluster\"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"reference/#ephysiopy.io.recording.TrialInterface.get_spike_times","title":"<code>get_spike_times(cluster, tetrode, *args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Returns the times of an individual cluster</p> Source code in <code>ephysiopy/io/recording.py</code> <pre><code>@abc.abstractmethod\ndef get_spike_times(self, cluster: int, tetrode: int, *args, **kwargs):\n    \"\"\"Returns the times of an individual cluster\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/#ephysiopy.io.recording.TrialInterface.load_cluster_data","title":"<code>load_cluster_data(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Load the cluster data (Kilosort/ Axona cut/ whatever else</p> Source code in <code>ephysiopy/io/recording.py</code> <pre><code>@abc.abstractmethod\ndef load_cluster_data(self, *args, **kwargs) -&gt; bool:\n    \"\"\"Load the cluster data (Kilosort/ Axona cut/ whatever else\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/#ephysiopy.io.recording.TrialInterface.load_lfp","title":"<code>load_lfp(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Load the LFP data</p> Source code in <code>ephysiopy/io/recording.py</code> <pre><code>@abc.abstractmethod\ndef load_lfp(self, *args, **kwargs) -&gt; NoReturn:\n    \"\"\"Load the LFP data\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/#ephysiopy.io.recording.TrialInterface.load_neural_data","title":"<code>load_neural_data(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Load the neural data</p> Source code in <code>ephysiopy/io/recording.py</code> <pre><code>@abc.abstractmethod\ndef load_neural_data(self, *args, **kwargs) -&gt; NoReturn:\n    \"\"\"Load the neural data\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/#ephysiopy.io.recording.TrialInterface.load_pos_data","title":"<code>load_pos_data(ppm=300, jumpmax=100, *args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Load the position data</p> <p>Parameters:</p> Name Type Description Default <code>pname</code> <code>Path</code> <p>Path to base directory containing pos data</p> required <code>ppm</code> <code>int</code> <p>pixels per metre</p> <code>300</code> <code>jumpmax</code> <code>int</code> <p>max jump in pixels between positions, more than this and the position is interpolated over</p> <code>100</code> Source code in <code>ephysiopy/io/recording.py</code> <pre><code>@abc.abstractmethod\ndef load_pos_data(\n    self, ppm: int = 300, jumpmax: int = 100, *args, **kwargs\n) -&gt; NoReturn:\n    \"\"\"\n    Load the position data\n\n    Args:\n        pname (Path): Path to base directory containing pos data\n        ppm (int): pixels per metre\n        jumpmax (int): max jump in pixels between positions, more\n            than this and the position is interpolated over\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/#ephysiopy.io.recording.TrialInterface.load_settings","title":"<code>load_settings(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Loads the format specific settings file</p> Source code in <code>ephysiopy/io/recording.py</code> <pre><code>@abc.abstractmethod\ndef load_settings(self, *args, **kwargs) -&gt; NoReturn:\n    \"\"\"Loads the format specific settings file\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/#ephysiopy.io.recording.memmapBinaryFile","title":"<code>memmapBinaryFile(path2file, n_channels=384, **kwargs)</code>","text":"<p>Returns a numpy memmap of the int16 data in the file path2file, if present</p> Source code in <code>ephysiopy/io/recording.py</code> <pre><code>def memmapBinaryFile(path2file: str, n_channels=384, **kwargs) -&gt; np.ndarray:\n    \"\"\"\n    Returns a numpy memmap of the int16 data in the\n    file path2file, if present\n    \"\"\"\n    import os\n\n    if \"data_type\" in kwargs.keys():\n        data_type = kwargs[\"data_type\"]\n    else:\n        data_type = np.int16\n\n    if os.path.exists(path2file):\n        # make sure n_channels is int as could be str\n        n_channels = int(n_channels)\n        status = os.stat(path2file)\n        n_samples = int(status.st_size / (2.0 * n_channels))\n        mmap = np.memmap(\n            path2file, data_type, \"r\", 0, (n_channels, n_samples), order=\"F\"\n        )\n        return mmap\n    else:\n        return np.empty(0)\n</code></pre>"},{"location":"reference/#plotting-the-results","title":"Plotting the results","text":""},{"location":"reference/#ephysiopy.visualise.plotting.FigureMaker","title":"<code>FigureMaker</code>","text":"<p>               Bases: <code>object</code></p> <p>A mixin class for TrialInterface that deals solely with producing graphical output.</p> Source code in <code>ephysiopy/visualise/plotting.py</code> <pre><code>class FigureMaker(object):\n    \"\"\"\n    A mixin class for TrialInterface that deals solely with\n    producing graphical output.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initializes the FigureMaker object.\n        \"\"\"\n        self.PosCalcs = None\n\n    def initialise(self):\n        \"\"\"\n        Initializes the FigureMaker object with data from PosCalcs.\n        \"\"\"\n        self.RateMap = RateMap(self.PosCalcs.xy,\n                               self.PosCalcs.dir,\n                               self.PosCalcs.speed,)\n        self.npos = self.PosCalcs.xy.shape[1]\n\n    def _plot_multiple_clusters(self,\n                                func,\n                                clusters: list,\n                                channel: int,\n                                **kwargs):\n        \"\"\"\n        Plots multiple clusters.\n\n        Args:\n            func (function): The function to apply to each cluster.\n            clusters (list): The list of clusters to plot.\n            channel (int): The channel number.\n            **kwargs: Additional keyword arguments for the function.\n        \"\"\"\n        fig = plt.figure()\n        nrows = int(np.ceil(len(clusters) / 5))\n        if 'projection' in kwargs.keys():\n            proj = kwargs.pop('projection')\n        else:\n            proj = None\n        for i, c in enumerate(clusters):\n            ax = fig.add_subplot(nrows, 5, i+1, projection=proj)\n            ts = self.get_spike_times(channel, c)\n            func(ts, ax=ax, **kwargs)\n\n    def rate_map(self, cluster: int | list, channel: int, **kwargs):\n        \"\"\"\n        Gets the rate map for the specified cluster(s) and channel.\n\n        Args:\n            cluster (int | list): The cluster(s) to get the rate map for.\n            channel (int): The channel number.\n            **kwargs: Additional keyword arguments for the function.\n        \"\"\"\n        if isinstance(cluster, list):\n            self._plot_multiple_clusters(self.makeRateMap,\n                                         cluster,\n                                         channel,\n                                         **kwargs)\n        else:\n            ts = self.get_spike_times(channel, cluster)\n            self.makeRateMap(ts, **kwargs)\n        plt.show()\n\n    def hd_map(self, cluster: int | list, channel: int, **kwargs):\n        \"\"\"\n        Gets the head direction map for the specified cluster(s) and channel.\n\n        Args:\n            cluster (int | list): The cluster(s) to get the head direction map\n                for.\n            channel (int): The channel number.\n            **kwargs: Additional keyword arguments for the function.\n        \"\"\"\n        if isinstance(cluster, list):\n            self._plot_multiple_clusters(self.makeHDPlot,\n                                         cluster,\n                                         channel,\n                                         projection=\"polar\",\n                                         strip_axes=True,\n                                         **kwargs)\n        else:\n            ts = self.get_spike_times(channel, cluster)\n            self.makeHDPlot(ts, **kwargs)\n        plt.show()\n\n    def spike_path(self, cluster=None, channel=None, **kwargs):\n        \"\"\"\n        Gets the spike path for the specified cluster(s) and channel.\n\n        Args:\n            cluster (int | list | None): The cluster(s) to get the spike path\n                for.\n            channel (int | None): The channel number.\n            **kwargs: Additional keyword arguments for the function.\n        \"\"\"\n        if isinstance(cluster, list):\n            self._plot_multiple_clusters(self.makeSpikePathPlot,\n                                         cluster,\n                                         channel,\n                                         **kwargs)\n        else:\n            if channel is not None and cluster is not None:\n                ts = self.get_spike_times(channel, cluster)\n            else:\n                ts = None\n            self.makeSpikePathPlot(ts, **kwargs)\n        plt.show()\n\n    def eb_map(self, cluster: int | list, channel: int, **kwargs):\n        \"\"\"\n        Gets the ego-centric boundary map for the specified cluster(s) and\n        channel.\n\n        Args:\n            cluster (int | list): The cluster(s) to get the ego-centric\n                boundary map for.\n            channel (int): The channel number.\n            **kwargs: Additional keyword arguments for the function.\n        \"\"\"\n        if isinstance(cluster, list):\n            self._plot_multiple_clusters(self.makeEgoCentricBoundaryMap,\n                                         cluster,\n                                         channel,\n                                         projection='polar',\n                                         **kwargs)\n        else:\n            ts = self.get_spike_times(channel, cluster)\n            self.makeEgoCentricBoundaryMap(ts, **kwargs)\n        plt.show()\n\n    def eb_spikes(self, cluster: int | list, channel: int, **kwargs):\n        \"\"\"\n        Gets the ego-centric boundary spikes for the specified cluster(s)\n        and channel.\n\n        Args:\n            cluster (int | list): The cluster(s) to get the ego-centric\n                boundary spikes for.\n            channel (int): The channel number.\n            **kwargs: Additional keyword arguments for the function.\n        \"\"\"\n        if isinstance(cluster, list):\n            self._plot_multiple_clusters(self.makeEgoCentricBoundarySpikePlot,\n                                         cluster,\n                                         channel,\n                                         **kwargs)\n        else:\n            ts = self.get_spike_times(channel, cluster)\n            self.makeEgoCentricBoundarySpikePlot(ts, **kwargs)\n        plt.show()\n\n    def sac(self, cluster: int | list, channel: int, **kwargs):\n        \"\"\"\n        Gets the spatial autocorrelation for the specified cluster(s) and\n        channel.\n\n        Args:\n            cluster (int | list): The cluster(s) to get the spatial\n                autocorrelation for.\n            channel (int): The channel number.\n            **kwargs: Additional keyword arguments for the function.\n        \"\"\"\n        if isinstance(cluster, list):\n            self._plot_multiple_clusters(self.makeSAC,\n                                         cluster,\n                                         channel,\n                                         **kwargs)\n        else:\n            ts = self.get_spike_times(channel, cluster)\n            self.makeSAC(ts, **kwargs)\n        plt.show()\n\n    def speed_v_rate(self, cluster: int | list, channel: int, **kwargs):\n        \"\"\"\n        Gets the speed versus rate plot for the specified cluster(s) and\n        channel.\n\n        Args:\n            cluster (int | list): The cluster(s) to get the speed versus rate\n                plot for.\n            channel (int): The channel number.\n            **kwargs: Additional keyword arguments for the function.\n        \"\"\"\n        if isinstance(cluster, list):\n            self._plot_multiple_clusters(self.makeSpeedVsRatePlot,\n                                         cluster,\n                                         channel,\n                                         **kwargs)\n        else:\n            ts = self.get_spike_times(channel, cluster)\n            self.makeSpeedVsRatePlot(ts, **kwargs)\n        plt.show()\n\n    def speed_v_hd(self, cluster: int | list, channel: int, **kwargs):\n        \"\"\"\n        Gets the speed versus head direction plot for the specified cluster(s)\n        and channel.\n\n        Args:\n            cluster (int | list): The cluster(s) to get the speed versus head\n                direction plot for.\n            channel (int): The channel number.\n            **kwargs: Additional keyword arguments for the function.\n        \"\"\"\n        if isinstance(cluster, list):\n            self._plot_multiple_clusters(self.makeSpeedVsHeadDirectionPlot,\n                                         cluster,\n                                         channel,\n                                         **kwargs)\n        else:\n            ts = self.get_spike_times(channel, cluster)\n            self.makeSpeedVsHeadDirectionPlot(ts, **kwargs)\n        plt.show()\n\n    def power_spectrum(self, **kwargs):\n        \"\"\"\n        Gets the power spectrum.\n\n        Args:\n            **kwargs: Additional keyword arguments for the function.\n        \"\"\"\n        p = self.EEGCalcs.calcEEGPowerSpectrum()\n        self.makePowerSpectrum(p[0], p[1], p[2], p[3], p[4], **kwargs)\n        plt.show()\n\n    def getSpikePosIndices(self, spk_times: np.ndarray):\n        \"\"\"\n        Returns the indices into the position data at which some spike times\n        occurred.\n\n        Args:\n            spk_times (np.ndarray): The spike times in seconds.\n\n        Returns:\n            np.ndarray: The indices into the position data at which the spikes\n                occurred.\n        \"\"\"\n        pos_times = getattr(self.PosCalcs, \"xyTS\")\n        idx = np.searchsorted(pos_times, spk_times) - 1\n        return idx\n\n    def makeSummaryPlot(self, spk_times: np.ndarray):\n        \"\"\"\n        Creates a summary plot with spike path, rate map, head direction plot,\n        and spatial autocorrelation.\n\n        Args:\n            spk_times (np.ndarray): The spike times in seconds.\n\n        Returns:\n            matplotlib.figure.Figure: The created figure.\n        \"\"\"\n        fig = plt.figure()\n        ax = plt.subplot(221)\n        self.makeSpikePathPlot(spk_times, ax=ax, markersize=2)\n        ax = plt.subplot(222)\n        self.makeRateMap(spk_times, ax=ax)\n        ax = plt.subplot(223, projection=\"polar\")\n        self.makeHDPlot(spk_times, ax=ax)\n        ax = plt.subplot(224)\n        try:\n            self.makeSAC(spk_times, ax=ax)\n        except IndexError:\n            pass\n        return fig\n\n    def makeSpikePlot(self,\n                      mean_waveform: bool = True,\n                      ax: matplotlib.axes = None,\n                      **kwargs) -&gt; matplotlib.figure:\n        if not self.SpikeCalcs:\n            Warning(\"No spike data loaded\")\n            return\n        waves = self.SpikeCalcs.waveforms(range(4))\n        if ax is None:\n            fig = plt.figure()\n        spike_at = np.shape(waves)[2] // 2\n        if spike_at &gt; 25:  # OE data\n            # this should be equal to range(25, 75)\n            t = range(spike_at - self.SpikeCalcs.pre_spike_samples,\n                      spike_at + self.SpikeCalcs.post_spike_samples)\n        else:  # Axona data\n            t = range(50)\n        if mean_waveform:\n            for i in range(4):\n                ax = fig.add_subplot(2, 2, i+1)\n                ax = self._plotSpikes(np.mean(\n                    waves[:, :, t], 0)[i, :], ax=ax, **kwargs)\n                if spike_at &gt; 25:  # OE data\n                    ax.invert_yaxis()\n        else:\n            for i in range(4):\n                ax = fig.add_subplot(2, 2, i+1)\n                ax = self._plotSpikes(waves[:, i, t], ax=ax, **kwargs)\n                if spike_at &gt; 25:  # OE data\n                    ax.invert_yaxis()\n        return fig\n\n    @stripAxes\n    def _plotSpikes(self, waves: np.ndarray,\n                    ax: matplotlib.axes,\n                    **kwargs) -&gt; matplotlib.axes:\n        ax.plot(waves, c='k', **kwargs)\n        return ax\n\n    @stripAxes\n    def makeRateMap(self,\n                    spk_times: np.ndarray,\n                    ax: matplotlib.axes = None,\n                    **kwargs) -&gt; matplotlib.axes:\n        \"\"\"\n        Creates a rate map plot.\n\n        Args:\n            spk_times (np.ndarray): The spike times in seconds.\n            ax (matplotlib.axes, optional): The axes to plot on. If None,\n                new axes are created.\n            **kwargs: Additional keyword arguments for the function.\n\n        Returns:\n            matplotlib.axes: The axes with the plot.\n        \"\"\"\n        if not self.RateMap:\n            self.initialise()\n        spk_times_in_pos_samples = self.getSpikePosIndices(spk_times)\n        spk_weights = np.bincount(\n            spk_times_in_pos_samples, minlength=self.npos)\n        rmap = self.RateMap.getMap(spk_weights)\n        ratemap = np.ma.MaskedArray(rmap[0], np.isnan(rmap[0]), copy=True)\n        x, y = np.meshgrid(rmap[1][1][0:-1].data, rmap[1][0][0:-1].data)\n        vmax = np.nanmax(np.ravel(ratemap))\n        if ax is None:\n            fig = plt.figure()\n            ax = fig.add_subplot(111)\n        ax.pcolormesh(\n            x, y, ratemap,\n            cmap=jet_cmap,\n            edgecolors=\"face\",\n            vmax=vmax,\n            shading=\"auto\",\n            **kwargs\n        )\n        ax.set_aspect(\"equal\")\n        return ax\n\n    @stripAxes\n    def makeSpikePathPlot(self,\n                          spk_times: np.ndarray = None,\n                          ax: matplotlib.axes = None,\n                          **kwargs) -&gt; matplotlib.axes:\n        \"\"\"\n        Creates a spike path plot.\n\n        Args:\n            spk_times (np.ndarray, optional): The spike times in seconds.\n                If None, no spikes are plotted.\n            ax (matplotlib.axes, optional): The axes to plot on.\n                If None, new axes are created.\n            **kwargs: Additional keyword arguments for the function.\n\n        Returns:\n            matplotlib.axes: The axes with the plot.\n        \"\"\"\n        if not self.RateMap:\n            self.initialise()\n        if \"c\" in kwargs:\n            col = kwargs.pop(\"c\")\n        else:\n            col = tcols.colours[1]\n        if ax is None:\n            fig = plt.figure()\n            ax = fig.add_subplot(111)\n        ax.plot(\n            self.PosCalcs.xy[0, :],\n            self.PosCalcs.xy[1, :],\n            c=tcols.colours[0], zorder=1\n        )\n        ax.set_aspect(\"equal\")\n        if spk_times is not None:\n            idx = self.getSpikePosIndices(spk_times)\n            ax.plot(\n                self.PosCalcs.xy[0, idx],\n                self.PosCalcs.xy[1, idx],\n                \"s\", c=col, **kwargs\n            )\n        return ax\n\n    def makeEgoCentricBoundaryMap(self,\n                                  spk_times: np.ndarray,\n                                  ax: matplotlib.axes = None,\n                                  **kwargs) -&gt; matplotlib.axes:\n        \"\"\"\n        Creates an ego-centric boundary map plot.\n\n        Args:\n            spk_times (np.ndarray): The spike times in seconds.\n            ax (matplotlib.axes, optional): The axes to plot on. If None,\n                new axes are created.\n            **kwargs: Additional keyword arguments for the function.\n\n        Returns:\n            matplotlib.axes: The axes with the plot.\n        \"\"\"\n        if not self.RateMap:\n            self.initialise()\n\n        degs_per_bin = 3\n        xy_binsize = 2.5\n        arena_type = \"circle\"\n        # parse kwargs\n        if \"degs_per_bin\" in kwargs.keys():\n            degs_per_bin = kwargs[\"degs_per_bin\"]\n        if \"xy_binsize\" in kwargs.keys():\n            xy_binsize = kwargs[\"xy_binsize\"]\n        if \"arena_type\" in kwargs.keys():\n            arena_type = kwargs[\"arena_type\"]\n        if \"strip_axes\" in kwargs.keys():\n            strip_axes = kwargs.pop(\"strip_axes\")\n        else:\n            strip_axes = False\n        if 'return_ratemap' in kwargs.keys():\n            return_ratemap = kwargs.pop('return_ratemap')\n        else:\n            return_ratemap = False\n\n        idx = self.getSpikePosIndices(spk_times)\n        spk_weights = np.bincount(idx, minlength=len(self.RateMap.dir))\n        ego_map = self.RateMap.get_egocentric_boundary_map(spk_weights,\n                                                           degs_per_bin,\n                                                           xy_binsize,\n                                                           arena_type)\n        rmap = ego_map.rmap\n        if ax is None:\n            fig = plt.figure()\n            ax = fig.add_subplot(projection='polar')\n        theta = np.arange(0, 2*np.pi, 2*np.pi/rmap.shape[1])\n        phi = np.arange(0, rmap.shape[0]*2.5, 2.5)\n        X, Y = np.meshgrid(theta, phi)\n        ax.pcolormesh(X, Y, rmap, **kwargs)\n        ax.set_xticks(np.arange(0, 2*np.pi, np.pi/4))\n        # ax.set_xticklabels(np.arange(0, 2*np.pi, np.pi/4))\n        ax.set_yticks(np.arange(0, 50, 10))\n        ax.set_yticklabels(np.arange(0, 50, 10))\n        ax.set_xlabel('Angle (deg)')\n        ax.set_ylabel('Distance (cm)')\n        if strip_axes:\n            return stripAxes(ax)\n        if return_ratemap:\n            return ax, rmap\n        return ax\n\n    @stripAxes\n    def makeEgoCentricBoundarySpikePlot(self,\n                                        spk_times: np.ndarray,\n                                        add_colour_wheel: bool = False,\n                                        ax: matplotlib.axes = None,\n                                        **kwargs) -&gt; matplotlib.axes:\n        \"\"\"\n        Creates an ego-centric boundary spike plot.\n\n        Args:\n            spk_times (np.ndarray): The spike times in seconds.\n            add_colour_wheel (bool, optional): Whether to add a colour wheel\n                to the plot. Defaults to False.\n            ax (matplotlib.axes, optional): The axes to plot on. If None,\n                new axes are created.\n            **kwargs: Additional keyword arguments for the function.\n\n        Returns:\n            matplotlib.axes: The axes with the plot.\n        \"\"\"\n        if not self.RateMap:\n            self.initialise()\n        # get the index into a circular colormap based\n        # on directional heading, then create a LineCollection\n        num_dir_bins = 60\n        if \"dir_bins\" in kwargs.keys():\n            num_dir_bins = kwargs[\"num_dir_bins\"]\n        if \"strip_axes\" in kwargs.keys():\n            strip_axes = kwargs.pop(\"strip_axes\")\n        else:\n            strip_axes = False\n        if \"ms\" in kwargs.keys():\n            rect_size = kwargs.pop(\"ms\")\n        else:\n            rect_size = 1\n        dir_colours = sns.color_palette('hls', num_dir_bins)\n        # need to create line colours and line widths for the collection\n        idx = self.getSpikePosIndices(spk_times)\n        dir_spike_fired_at = self.RateMap.dir[idx]\n        idx_of_dir_to_colour = np.floor(\n            dir_spike_fired_at / (360 / num_dir_bins)).astype(int)\n        rects = [Rectangle(self.RateMap.xy[:, i],\n                           width=rect_size, height=rect_size)\n                 for i in idx]\n        if ax is None:\n            fig = plt.figure()\n            ax = fig.add_subplot()\n        # plot the path\n        ax.plot(self.RateMap.xy[0],\n                self.RateMap.xy[1],\n                c=tcols.colours[0],\n                zorder=1,\n                alpha=0.3)\n        ax.set_aspect('equal')\n        for col_idx, r in zip(idx_of_dir_to_colour, rects):\n            ax.add_artist(r)\n            r.set_clip_box(ax.bbox)\n            r.set_facecolor(dir_colours[col_idx])\n            r.set_rasterized(True)\n        if add_colour_wheel:\n            ax_col = ax.inset_axes(bounds=[0.75, 0.75, 0.15, 0.15],\n                                   projection='polar',\n                                   transform=fig.transFigure)\n            ax_col.set_theta_zero_location(\"N\")\n            theta = np.linspace(0, 2*np.pi, 1000)\n            phi = np.linspace(0, 1, 2)\n            X, Y = np.meshgrid(phi, theta)\n            norm = matplotlib.colors.Normalize(0, 2*np.pi)\n            col_map = sns.color_palette('hls', as_cmap=True)\n            ax_col.pcolormesh(theta, phi, Y.T, norm=norm, cmap=col_map)\n            ax_col.set_yticklabels([])\n            ax_col.spines['polar'].set_visible(False)\n            ax_col.set_thetagrids([0, 90])\n        if strip_axes:\n            return stripAxes(ax)\n        return ax\n\n    @stripAxes\n    def makeSAC(\n        self, spk_times: np.array = None, ax: matplotlib.axes = None, **kwargs\n    ) -&gt; matplotlib.axes:\n        \"\"\"\n        Creates a spatial autocorrelation plot.\n\n        Args:\n            spk_times (np.array, optional): The spike times in seconds. If\n                None, no spikes are plotted.\n            ax (matplotlib.axes, optional): The axes to plot on. If None,\n                new axes are created.\n            **kwargs: Additional keyword arguments for the function.\n\n        Returns:\n            matplotlib.axes: The axes with the plot.\n        \"\"\"\n        if not self.RateMap:\n            self.initialise()\n        spk_times_in_pos_samples = self.getSpikePosIndices(spk_times)\n        spk_weights = np.bincount(\n            spk_times_in_pos_samples, minlength=self.npos)\n        sac = self.RateMap.getSAC(spk_weights)\n        from ephysiopy.common.gridcell import SAC\n\n        S = SAC()\n        measures = S.getMeasures(sac)\n        if ax is None:\n            fig = plt.figure()\n            ax = fig.add_subplot(111)\n        ax = self.show_SAC(sac, measures, ax)\n        return ax\n\n    def makeHDPlot(\n        self, spk_times: np.array = None, ax: matplotlib.axes = None, **kwargs\n    ) -&gt; matplotlib.axes:\n        \"\"\"\n        Creates a head direction plot.\n\n        Args:\n            spk_times (np.array, optional): The spike times in seconds. If\n                None, no spikes are plotted.\n            ax (matplotlib.axes, optional): The axes to plot on. If None, new\n                axes are created.\n            **kwargs: Additional keyword arguments for the function.\n\n        Returns:\n            matplotlib.axes: The axes with the plot.\n        \"\"\"\n        if not self.RateMap:\n            self.initialise()\n        if \"strip_axes\" in kwargs.keys():\n            strip_axes = kwargs.pop(\"strip_axes\")\n        else:\n            strip_axes = True\n        spk_times_in_pos_samples = self.getSpikePosIndices(spk_times)\n        spk_weights = np.bincount(\n            spk_times_in_pos_samples, minlength=self.npos)\n        rmap = self.RateMap.getMap(spk_weights, varType=VariableToBin.DIR)\n        if ax is None:\n            fig = plt.figure()\n            ax = fig.add_subplot(111, **kwargs)\n        ax.set_theta_zero_location(\"N\")\n        # need to deal with the case where the axis is supplied but\n        # is not polar. deal with polar first\n        theta = np.deg2rad(rmap[1][0])\n        ax.clear()\n        r = rmap[0]  # in samples so * pos sample_rate\n        r = np.insert(r, -1, r[0])\n        if \"polar\" in ax.name:\n            ax.plot(theta, r)\n            if \"fill\" in kwargs:\n                ax.fill(theta, r, alpha=0.5)\n            ax.set_aspect(\"equal\")\n        else:\n            pass\n\n        # See if we should add the mean resultant vector (mrv)\n        if \"add_mrv\" in kwargs:\n            from ephysiopy.common.statscalcs import mean_resultant_vector\n\n            angles = self.PosCalcs.dir[spk_times_in_pos_samples]\n            r, th = mean_resultant_vector(np.deg2rad(angles))\n            ax.plot([th, th], [0, r * np.max(rmap[0])], \"r\")\n        if \"polar\" in ax.name:\n            ax.set_thetagrids([0, 90, 180, 270])\n        if strip_axes:\n            return stripAxes(ax)\n        return ax\n\n    def makeSpeedVsRatePlot(\n        self,\n        spk_times: np.array,\n        minSpeed: float = 0.0,\n        maxSpeed: float = 40.0,\n        sigma: float = 3.0,\n        ax: matplotlib.axes = None,\n        **kwargs\n    ) -&gt; matplotlib.axes:\n        \"\"\"\n        Plots the instantaneous firing rate of a cell against running speed.\n        Also outputs a couple of measures as with Kropff et al., 2015; the\n        Pearsons correlation and the depth of modulation (dom).\n\n        Args:\n            spk_times (np.array): The spike times in seconds.\n            minSpeed (float, optional): The minimum speed. Defaults to 0.0.\n            maxSpeed (float, optional): The maximum speed. Defaults to 40.0.\n            sigma (float, optional): The sigma value. Defaults to 3.0.\n            ax (matplotlib.axes, optional): The axes to plot on. If None, new\n                axes are created.\n            **kwargs: Additional keyword arguments for the function.\n\n        Returns:\n            matplotlib.axes: The axes with the plot.\n        \"\"\"\n        if \"strip_axes\" in kwargs.keys():\n            strip_axes = kwargs.pop(\"strip_axes\")\n        else:\n            strip_axes = False\n        if not self.RateMap:\n            self.initialise()\n        spk_times_in_pos_samples = self.getSpikePosIndices(spk_times)\n\n        speed = np.ravel(self.PosCalcs.speed)\n        if np.nanmax(speed) &lt; maxSpeed:\n            maxSpeed = np.nanmax(speed)\n        spd_bins = np.arange(minSpeed, maxSpeed, 1.0)\n        # Construct the mask\n        speed_filt = np.ma.MaskedArray(speed)\n        speed_filt = np.ma.masked_where(speed_filt &lt; minSpeed, speed_filt)\n        speed_filt = np.ma.masked_where(speed_filt &gt; maxSpeed, speed_filt)\n        from ephysiopy.common.spikecalcs import SpikeCalcsGeneric\n\n        x1 = spk_times_in_pos_samples\n        S = SpikeCalcsGeneric(x1)\n        spk_sm = S.smooth_spike_train(x1,\n                                      self.PosCalcs.xyTS.shape[0],\n                                      sigma, None)\n        spk_sm = np.ma.MaskedArray(spk_sm, mask=np.ma.getmask(speed_filt))\n        spd_dig = np.digitize(speed_filt, spd_bins, right=True)\n        mn_rate = np.array(\n            [np.ma.mean(spk_sm[spd_dig == i]) for i in range(0, len(spd_bins))]\n        )\n        var = np.array(\n            [np.ma.std(spk_sm[spd_dig == i]) for i in range(0, len(spd_bins))]\n        )\n        np.array([np.ma.sum(spk_sm[spd_dig == i]) for i in range(\n            0, len(spd_bins))])\n        if ax is None:\n            fig = plt.figure()\n            ax = fig.add_subplot(111)\n        ax.errorbar(spd_bins, mn_rate * self.PosCalcs.sample_rate,\n                    yerr=var, color=\"k\")\n        ax.set_xlim(spd_bins[0], spd_bins[-1])\n        ax.set_xticks(\n            [spd_bins[0], spd_bins[-1]],\n            labels=[\"0\", \"{:.2g}\".format(spd_bins[-1])],\n            fontweight=\"normal\",\n            size=6,\n        )\n        ax.set_yticks(\n            [0, np.nanmax(mn_rate) * self.PosCalcs.sample_rate],\n            labels=[\"0\", \"{:.2f}\".format(np.nanmax(mn_rate))],\n            fontweight=\"normal\",\n            size=6,\n        )\n        if strip_axes:\n            return stripAxes(ax)\n        return ax\n\n    def makeSpeedVsHeadDirectionPlot(\n        self, spk_times: np.array, ax: matplotlib.axes = None, **kwargs\n    ) -&gt; matplotlib.axes:\n        \"\"\"\n        Creates a speed versus head direction plot.\n\n        Args:\n            spk_times (np.array): The spike times in seconds.\n            ax (matplotlib.axes, optional): The axes to plot on. If None,\n                new axes are created.\n            **kwargs: Additional keyword arguments for the function.\n\n        Returns:\n            matplotlib.axes: The axes with the plot.\n        \"\"\"\n        if \"strip_axes\" in kwargs.keys():\n            strip_axes = kwargs.pop(\"strip_axes\")\n        else:\n            strip_axes = False\n        if not self.RateMap:\n            self.initialise()\n        spk_times_in_pos_samples = self.getSpikePosIndices(spk_times)\n        idx = np.array(spk_times_in_pos_samples, dtype=int)\n        w = np.bincount(idx, minlength=self.PosCalcs.speed.shape[0])\n        if np.ma.is_masked(self.PosCalcs.speed):\n            w[self.PosCalcs.speed.mask] = 0\n\n        dir_bins = np.arange(0, 360, 6)\n        spd_bins = np.arange(0, 30, 1)\n        h = np.histogram2d(self.PosCalcs.dir,\n                           self.PosCalcs.speed,\n                           [dir_bins, spd_bins], weights=w)\n        from ephysiopy.common.utils import blurImage\n\n        im = blurImage(h[0], 5, ftype=\"gaussian\")\n        im = np.ma.MaskedArray(im)\n        # mask low rates...\n        im = np.ma.masked_where(im &lt;= 1, im)\n        # ... and where less than 0.5% of data is accounted for\n        x, y = np.meshgrid(dir_bins, spd_bins)\n        vmax = np.max(np.ravel(im))\n        if ax is None:\n            fig = plt.figure()\n            ax = fig.add_subplot(111)\n        ax.pcolormesh(x, y, im.T,\n                      cmap=jet_cmap, edgecolors=\"face\",\n                      vmax=vmax, shading=\"auto\")\n        ax.set_xticks([90, 180, 270], labels=['90', '180', '270'],\n                      fontweight=\"normal\", size=6)\n        ax.set_yticks([10, 20], labels=['10', '20'],\n                      fontweight=\"normal\", size=6)\n        ax.set_xlabel(\"Heading\", fontweight=\"normal\", size=6)\n        if strip_axes:\n            stripAxes(ax)\n        return ax\n\n    def makePowerSpectrum(\n        self,\n        freqs: np.array,\n        power: np.array,\n        sm_power: np.array,\n        band_max_power: float,\n        freq_at_band_max_power: float,\n        max_freq: int = 50,\n        theta_range: tuple = [6, 12],\n        ax: matplotlib.axes = None,\n        **kwargs\n    ) -&gt; matplotlib.axes:\n        \"\"\"\n        Plots the power spectrum. The parameters can be obtained from\n        calcEEGPowerSpectrum() in the EEGCalcsGeneric class.\n\n        Args:\n            freqs (np.array): The frequencies.\n            power (np.array): The power values.\n            sm_power (np.array): The smoothed power values.\n            band_max_power (float): The maximum power in the band.\n            freq_at_band_max_power (float): The frequency at which the maximum\n                power in the band occurs.\n            max_freq (int, optional): The maximum frequency. Defaults to 50.\n            theta_range (tuple, optional): The theta range.\n                Defaults to [6, 12].\n            ax (matplotlib.axes, optional): The axes to plot on. If None, new\n                axes are created.\n            **kwargs: Additional keyword arguments for the function.\n\n        Returns:\n            matplotlib.axes: The axes with the plot.\n        \"\"\"\n        if \"strip_axes\" in kwargs.keys():\n            strip_axes = kwargs.pop(\"strip_axes\")\n        else:\n            strip_axes = False\n        # downsample frequencies and power\n        freqs = freqs[0::50]\n        power = power[0::50]\n        sm_power = sm_power[0::50]\n        if ax is None:\n            fig = plt.figure()\n            ax = fig.add_subplot(111)\n        ax.plot(freqs, power, alpha=0.5, color=[0.8627, 0.8627, 0.8627])\n        ax.plot(freqs, sm_power)\n        ax.set_xlim(0, max_freq)\n        ylim = [0, np.max(sm_power[freqs &lt; max_freq])]\n        if \"ylim\" in kwargs:\n            ylim = kwargs[\"ylim\"]\n        ax.set_ylim(ylim)\n        ax.set_ylabel(\"Power\")\n        ax.set_xlabel(\"Frequency\")\n        ax.text(\n            x=theta_range[1] / 0.9,\n            y=band_max_power,\n            s=str(freq_at_band_max_power)[0:4],\n            fontsize=20,\n        )\n        from matplotlib.patches import Rectangle\n\n        r = Rectangle(\n            (theta_range[0], 0),\n            width=np.diff(theta_range)[0],\n            height=np.diff(ax.get_ylim())[0],\n            alpha=0.25,\n            color=\"r\",\n            ec=\"none\",\n        )\n        ax.add_patch(r)\n        if strip_axes:\n            return stripAxes(ax)\n        return ax\n\n    def makeXCorr(\n        self, spk_times: np.array, ax: matplotlib.axes = None, **kwargs\n    ) -&gt; matplotlib.axes:\n        \"\"\"\n        Returns an axis containing the autocorrelogram of the spike\n        times provided over the range +/-500ms.\n\n        Args:\n            spk_times (np.array): Spike times in seconds.\n            ax (matplotlib.axes, optional): The axes to plot into. If None,\n                new axes are created.\n            **kwargs: Additional keyword arguments for the function.\n                binsize (int, optional): The size of the bins in ms. Gets\n                passed to SpikeCalcsGeneric.xcorr(). Defaults to 1.\n\n        Returns:\n            matplotlib.axes: The axes with the plot.\n        \"\"\"\n        if \"strip_axes\" in kwargs.keys():\n            strip_axes = kwargs.pop(\"strip_axes\")\n        else:\n            strip_axes = False\n        # spk_times in samples provided in seconds but convert to\n        # ms for a more display friendly scale\n        spk_times = spk_times\n        S = SpikeCalcsGeneric(spk_times)\n        c, b = S.acorr(spk_times, **kwargs)\n        if ax is None:\n            fig = plt.figure()\n            ax = fig.add_subplot(111)\n        if 'binsize' in kwargs.keys():\n            binsize = kwargs['binsize']\n        else:\n            binsize = 0.001\n        if \"Trange\" in kwargs.keys():\n            xrange = kwargs[\"Trange\"]\n        else:\n            xrange = [-0.5, 0.5]\n        ax.bar(b[:-1], c, width=binsize, color=\"k\", align=\"edge\")\n        ax.set_xlim(xrange)\n        ax.set_xticks((xrange[0], 0, xrange[1]))\n        ax.set_xticklabels(\"\")\n        ax.tick_params(axis=\"both\", which=\"both\", left=False, right=False,\n                       bottom=False, top=False)\n        ax.set_yticklabels(\"\")\n        ax.xaxis.set_ticks_position(\"bottom\")\n        if strip_axes:\n            return stripAxes(ax)\n        return ax\n\n    def makeRaster(\n        self,\n        spk_times: np.array,\n        dt=(-50, 100),\n        prc_max: float = 0.5,\n        ax: matplotlib.axes = None,\n        ms_per_bin: int = 1,\n        sample_rate: float = 3e4,  # OE=3e4, Axona=96000\n        **kwargs\n    ) -&gt; matplotlib.axes:\n        \"\"\"\n        Plots a raster plot for a specified tetrode/ cluster.\n\n        Args:\n            spk_times (np.array): The spike times in samples.\n            dt (tuple, optional): The window of time in ms to examine zeroed\n                on the event of interest i.e. the first value will probably\n                be negative as in the example. Defaults to (-50, 100).\n            prc_max (float, optional): The proportion of firing the cell has\n                to 'lose' to count as silent; a float between 0 and 1.\n                Defaults to 0.5.\n            ax (matplotlib.axes, optional): The axes to plot into.\n                If not provided a new figure is created. Defaults to None.\n            ms_per_bin (int, optional): The number of milliseconds in each bin\n                of the raster plot. Defaults to 1.\n            sample_rate (float, optional): The sample rate. Defaults to 3e4.\n            **kwargs: Additional keyword arguments for the function.\n\n        Returns:\n            matplotlib.axes: The axes with the plot.\n        \"\"\"\n        assert hasattr(self, \"ttl_data\")\n\n        if \"strip_axes\" in kwargs.keys():\n            strip_axes = kwargs.pop(\"strip_axes\")\n        else:\n            strip_axes = False\n        x1 = spk_times / sample_rate * 1000.0  # get into ms\n        x1.sort()\n        on_good = self.ttl_data[\"ttl_timestamps\"]\n        dt = np.array(dt)\n        irange = on_good[:, np.newaxis] + dt[np.newaxis, :]\n        dts = np.searchsorted(x1, irange)\n        y = []\n        x = []\n        for i, t in enumerate(dts):\n            tmp = x1[t[0]:t[1]] - on_good[i]\n            x.extend(tmp)\n            y.extend(np.repeat(i, len(tmp)))\n        if ax is None:\n            fig = plt.figure(figsize=(4.0, 7.0))\n            axScatter = fig.add_subplot(111)\n        else:\n            axScatter = ax\n        histColor = [1 / 255.0, 1 / 255.0, 1 / 255.0]\n        axScatter.scatter(x, y, marker=\".\", s=2,\n                          rasterized=False, color=histColor)\n        divider = make_axes_locatable(axScatter)\n        axScatter.set_xticks((dt[0], 0, dt[1]))\n        axScatter.set_xticklabels((str(dt[0]), \"0\", str(dt[1])))\n        axHistx = divider.append_axes(\"top\", 0.95, pad=0.2,\n                                      sharex=axScatter,\n                                      transform=axScatter.transAxes)\n        scattTrans = transforms.blended_transform_factory(\n            axScatter.transData, axScatter.transAxes\n        )\n        stim_pwidth = self.ttl_data[\"stim_duration\"]\n        if stim_pwidth is None:\n            raise ValueError(\"stim duration is None\")\n\n        axScatter.add_patch(\n            Rectangle(\n                (0, 0),\n                width=stim_pwidth,\n                height=1,\n                transform=scattTrans,\n                color=[0, 0, 1],\n                alpha=0.3,\n            )\n        )\n        histTrans = transforms.blended_transform_factory(\n            axHistx.transData, axHistx.transAxes\n        )\n        axHistx.add_patch(\n            Rectangle(\n                (0, 0),\n                width=stim_pwidth,\n                height=1,\n                transform=histTrans,\n                color=[0, 0, 1],\n                alpha=0.3,\n            )\n        )\n        axScatter.set_ylabel(\"Laser stimulation events\", labelpad=-2.5)\n        axScatter.set_xlabel(\"Time to stimulus onset(ms)\")\n        nStms = len(on_good)\n        axScatter.set_ylim(0, nStms)\n        # Label only the min and max of the y-axis\n        ylabels = axScatter.get_yticklabels()\n        for i in range(1, len(ylabels) - 1):\n            ylabels[i].set_visible(False)\n        yticks = axScatter.get_yticklines()\n        for i in range(1, len(yticks) - 1):\n            yticks[i].set_visible(False)\n\n        axHistx.hist(\n            x,\n            bins=np.arange(dt[0], dt[1] + ms_per_bin, ms_per_bin),\n            color=histColor,\n            range=dt,\n            rasterized=True,\n            histtype=\"stepfilled\",\n        )\n        axHistx.set_ylabel(\"Spike count\", labelpad=-2.5)\n        plt.setp(axHistx.get_xticklabels(), visible=False)\n        # Label only the min and max of the y-axis\n        ylabels = axHistx.get_yticklabels()\n        for i in range(1, len(ylabels) - 1):\n            ylabels[i].set_visible(False)\n        yticks = axHistx.get_yticklines()\n        for i in range(1, len(yticks) - 1):\n            yticks[i].set_visible(False)\n        axHistx.set_xlim(dt)\n        axScatter.set_xlim(dt)\n        if strip_axes:\n            return stripAxes(axScatter)\n        return axScatter\n\n    '''\n    def getRasterHist(\n            self, spike_ts: np.array,\n            sample_rate: int,\n            dt=(-50, 100), hist=True):\n        \"\"\"\n        MOVE TO SPIKECALCS\n\n        Calculates the histogram of the raster of spikes during a series of\n        events\n\n        Parameters\n        ----------\n        tetrode : int\n        cluster : int\n        dt : tuple\n            the window of time in ms to examine zeroed on the event of interest\n            i.e. the first value will probably be negative as in the example\n        hist : bool\n            not sure\n        \"\"\"\n        spike_ts = spike_ts * float(sample_rate)  # in ms\n        spike_ts.sort()\n        on_good = getattr(self, 'ttl_timestamps') / sample_rate / float(1000)\n        dt = np.array(dt)\n        irange = on_good[:, np.newaxis] + dt[np.newaxis, :]\n        dts = np.searchsorted(spike_ts, irange)\n        y = []\n        x = []\n        for i, t in enumerate(dts):\n            tmp = spike_ts[t[0]:t[1]] - on_good[i]\n            x.extend(tmp)\n            y.extend(np.repeat(i, len(tmp)))\n\n        if hist:\n            nEvents = int(self.STM[\"num_stm_samples\"])\n            return np.histogram2d(\n                x, y, bins=[np.arange(\n                    dt[0], dt[1]+1, 1), np.arange(0, nEvents+1, 1)])[0]\n        else:\n            return np.histogram(\n                x, bins=np.arange(\n                    dt[0], dt[1]+1, 1), range=dt)[0]\n    '''\n\n    @stripAxes\n    def show_SAC(\n        self, A: np.array, inDict: dict, ax: matplotlib.axes = None, **kwargs\n    ) -&gt; matplotlib.axes:\n        \"\"\"\n        Displays the result of performing a spatial autocorrelation (SAC)\n        on a grid cell.\n\n        Uses the dictionary containing measures of the grid cell SAC to\n        make a pretty picture\n\n        Args:\n            A (np.array): The spatial autocorrelogram.\n            inDict (dict): The dictionary calculated in getmeasures.\n            ax (matplotlib.axes, optional): If given the plot will get drawn\n                in these axes. Default None.\n            **kwargs: Additional keyword arguments for the function.\n\n        Returns:\n            matplotlib.axes: The axes with the plot.\n\n        See Also:\n            ephysiopy.common.binning.RateMap.autoCorr2D()\n            ephysiopy.common.ephys_generic.FieldCalcs.getMeaures()\n        \"\"\"\n        if ax is None:\n            fig = plt.figure()\n            ax = fig.add_subplot(111)\n        Am = A.copy()\n        Am[~inDict[\"dist_to_centre\"]] = np.nan\n        Am = np.ma.masked_invalid(np.atleast_2d(Am))\n        x, y = np.meshgrid(np.arange(0, np.shape(A)[1]),\n                           np.arange(0, np.shape(A)[0]))\n        vmax = np.nanmax(np.ravel(A))\n        ax.pcolormesh(x, y, A, cmap=grey_cmap, edgecolors=\"face\",\n                      vmax=vmax, shading=\"auto\")\n        import copy\n\n        cmap = copy.copy(jet_cmap)\n        cmap.set_bad(\"w\", 0)\n        ax.pcolormesh(x, y, Am, cmap=cmap,\n                      edgecolors=\"face\", vmax=vmax, shading=\"auto\")\n        # horizontal green line at 3 o'clock\n        _y = (np.shape(A)[0] / 2, np.shape(A)[0] / 2)\n        _x = (np.shape(A)[1] / 2, np.shape(A)[0])\n        ax.plot(_x, _y, c=\"g\")\n        mag = inDict[\"scale\"] * 0.5\n        th = np.linspace(0, inDict[\"orientation\"], 50)\n        from ephysiopy.common.utils import rect\n\n        [x, y] = rect(mag, th, deg=1)\n        # angle subtended by orientation\n        ax.plot(\n            x + (inDict[\"dist_to_centre\"].shape[1] / 2),\n            (inDict[\"dist_to_centre\"].shape[0] / 2) - y,\n            \"r\",\n            **kwargs\n        )\n        # plot lines from centre to peaks above middle\n        for p in inDict[\"closest_peak_coords\"]:\n            if p[0] &lt;= inDict[\"dist_to_centre\"].shape[0] / 2:\n                ax.plot(\n                    (inDict[\"dist_to_centre\"].shape[1] / 2, p[1]),\n                    (inDict[\"dist_to_centre\"].shape[0] / 2, p[0]),\n                    \"k\",\n                    **kwargs\n                )\n        ax.invert_yaxis()\n        all_ax = ax.axes\n        all_ax.set_aspect(\"equal\")\n        all_ax.set_xlim((0.5, inDict[\"dist_to_centre\"].shape[1] - 1.5))\n        all_ax.set_ylim((inDict[\"dist_to_centre\"].shape[0] - 0.5, -0.5))\n        return ax\n\n    def plotSpectrogramByDepth(\n        self,\n        nchannels: int = 384,\n        nseconds: int = 100,\n        maxFreq: int = 125,\n        channels: list = [],\n        frequencies: list = [],\n        frequencyIncrement: int = 1,\n        **kwargs\n    ):\n        \"\"\"\n        Plots a heat map spectrogram of the LFP for each channel.\n        Line plots of power per frequency band and power on a subset of\n        channels are also displayed to the right and above the main plot.\n\n        Args:\n            nchannels (int): The number of channels on the probe.\n            nseconds (int, optional): How long in seconds from the start of\n                the trial to do the spectrogram for (for speed).\n                Default is 100.\n            maxFreq (int): The maximum frequency in Hz to plot the spectrogram\n                out to. Maximum is 1250. Default is 125.\n            channels (list): The channels to plot separately on the top plot.\n            frequencies (list): The specific frequencies to examine across\n                all channels. The mean from frequency: \n                frequency+frequencyIncrement is calculated and plotted on\n                the left hand side of the plot.\n            frequencyIncrement (int): The amount to add to each value of\n                the frequencies list above.\n            **kwargs: Additional keyword arguments for the function.\n                Valid key value pairs:\n                    \"saveas\" - save the figure to this location, needs absolute\n                    path and filename.\n\n        Notes:\n            Should also allow kwargs to specify exactly which channels\n            and / or frequency bands to do the line plots for.\n        \"\"\"\n        if not self.path2LFPdata:\n            raise TypeError(\"Not a probe recording so not plotting\")\n        import os\n\n        lfp_file = os.path.join(self.path2LFPdata, \"continuous.dat\")\n        status = os.stat(lfp_file)\n        nsamples = int(status.st_size / 2 / nchannels)\n        mmap = np.memmap(lfp_file, np.int16, \"r\", 0,\n                         (nchannels, nsamples), order=\"F\")\n        # Load the channel map NB assumes this is in the AP data\n        # location and that kilosort was run there\n        channel_map = np.squeeze(\n            np.load(os.path.join(self.path2APdata, \"channel_map.npy\"))\n        )\n        lfp_sample_rate = 2500\n        data = np.array(mmap[channel_map, 0:nseconds * lfp_sample_rate])\n        from ephysiopy.common.ephys_generic import EEGCalcsGeneric\n\n        E = EEGCalcsGeneric(data[0, :], lfp_sample_rate)\n        E.calcEEGPowerSpectrum()\n        spec_data = np.zeros(shape=(data.shape[0], len(E.sm_power[0::50])))\n        for chan in range(data.shape[0]):\n            E = EEGCalcsGeneric(data[chan, :], lfp_sample_rate)\n            E.calcEEGPowerSpectrum()\n            spec_data[chan, :] = E.sm_power[0::50]\n\n        x, y = np.meshgrid(E.freqs[0::50], channel_map)\n        import matplotlib.colors as colors\n        from matplotlib.pyplot import cm\n        from mpl_toolkits.axes_grid1 import make_axes_locatable\n\n        _, spectoAx = plt.subplots()\n        spectoAx.pcolormesh(x, y, spec_data,\n                            edgecolors=\"face\", cmap=\"bone\",\n                            norm=colors.LogNorm())\n        spectoAx.set_xlim(0, maxFreq)\n        spectoAx.set_ylim(channel_map[0], channel_map[-1])\n        spectoAx.set_xlabel(\"Frequency (Hz)\")\n        spectoAx.set_ylabel(\"Channel\")\n        divider = make_axes_locatable(spectoAx)\n        channel_spectoAx = divider.append_axes(\"top\", 1.2, pad=0.1,\n                                               sharex=spectoAx)\n        meanfreq_powerAx = divider.append_axes(\"right\", 1.2, pad=0.1,\n                                               sharey=spectoAx)\n        plt.setp(channel_spectoAx.get_xticklabels()\n                 + meanfreq_powerAx.get_yticklabels(),\n                 visible=False)\n\n        # plot mean power across some channels\n        mn_power = np.mean(spec_data, 0)\n        if not channels:\n            channels = range(1, nchannels, 60)\n        cols = iter(cm.rainbow(np.linspace(0, 1, len(channels))))\n        for chan in channels:\n            c = next(cols)\n            channel_spectoAx.plot(\n                E.freqs[0::50],\n                10 * np.log10(spec_data[chan, :] / mn_power),\n                c=c,\n                label=str(chan),\n            )\n\n        channel_spectoAx.set_ylabel(\"Channel power(dB)\")\n        channel_spectoAx.legend(\n            bbox_to_anchor=(0.0, 1.02, 1.0, 0.102),\n            loc=\"lower left\",\n            mode=\"expand\",\n            fontsize=\"x-small\",\n            ncol=4,\n        )\n\n        # plot mean frequencies across all channels\n        if not frequencyIncrement:\n            freq_inc = 6\n        else:\n            freq_inc = frequencyIncrement\n        if not frequencies:\n            lower_freqs = np.arange(1, maxFreq - freq_inc, freq_inc)\n        else:\n            lower_freqs = frequencies\n        upper_freqs = [f + freq_inc for f in lower_freqs]\n        cols = iter(cm.nipy_spectral(np.linspace(0, 1, len(upper_freqs))))\n        mn_power = np.mean(spec_data, 1)\n        for freqs in zip(lower_freqs, upper_freqs):\n            freq_mask = np.logical_and(\n                E.freqs[0::50] &gt; freqs[0], E.freqs[0::50] &lt; freqs[1]\n            )\n            mean_power = 10 * np.log10(np.mean(\n                spec_data[:, freq_mask], 1) / mn_power)\n            c = next(cols)\n            meanfreq_powerAx.plot(\n                mean_power,\n                channel_map,\n                c=c,\n                label=str(freqs[0]) + \" - \" + str(freqs[1]),\n            )\n        meanfreq_powerAx.set_xlabel(\"Mean freq. band power(dB)\")\n        meanfreq_powerAx.legend(\n            bbox_to_anchor=(0.0, 1.02, 1.0, 0.102),\n            loc=\"lower left\",\n            mode=\"expand\",\n            fontsize=\"x-small\",\n            ncol=1,\n        )\n        if \"saveas\" in kwargs:\n            saveas = kwargs[\"saveas\"]\n            plt.savefig(saveas)\n        plt.show()\n\n    '''\n    def plotDirFilteredRmaps(self, tetrode, cluster, maptype='rmap', **kwargs):\n        \"\"\"\n        Plots out directionally filtered ratemaps for the tetrode/ cluster\n\n        Parameters\n        ----------\n        tetrode : int\n        cluster : int\n        maptype : str\n            Valid values include 'rmap', 'polar', 'xcorr'\n        \"\"\"\n        inc = 8.0\n        step = 360/inc\n        dirs_st = np.arange(-step/2, 360-(step/2), step)\n        dirs_en = np.arange(step/2, 360, step)\n        dirs_st[0] = dirs_en[-1]\n\n        if 'polar' in maptype:\n            _, axes = plt.subplots(\n                nrows=3, ncols=3, subplot_kw={'projection': 'polar'})\n        else:\n            _, axes = plt.subplots(nrows=3, ncols=3)\n        ax0 = axes[0][0]  # top-left\n        ax1 = axes[0][1]  # top-middle\n        ax2 = axes[0][2]  # top-right\n        ax3 = axes[1][0]  # middle-left\n        ax4 = axes[1][1]  # middle\n        ax5 = axes[1][2]  # middle-right\n        ax6 = axes[2][0]  # bottom-left\n        ax7 = axes[2][1]  # bottom-middle\n        ax8 = axes[2][2]  # bottom-right\n\n        max_rate = 0\n        for d in zip(dirs_st, dirs_en):\n            self.posFilter = {'dir': (d[0], d[1])}\n            if 'polar' in maptype:\n                rmap = self._getMap(\n                    tetrode=tetrode, cluster=cluster, var2bin='dir')[0]\n            elif 'xcorr' in maptype:\n                x1 = self.TETRODE[tetrode].getClustTS(cluster) / (96000/1000)\n                rmap = self.spikecalcs.acorr(\n                    x1, x1, Trange=np.array([-500, 500]))\n            else:\n                rmap = self._getMap(tetrode=tetrode, cluster=cluster)[0]\n            if np.nanmax(rmap) &gt; max_rate:\n                max_rate = np.nanmax(rmap)\n\n        from collections import OrderedDict\n        dir_rates = OrderedDict.fromkeys(dirs_st, None)\n\n        ax_collection = [ax5, ax2, ax1, ax0, ax3, ax6, ax7, ax8]\n        for d in zip(dirs_st, dirs_en, ax_collection):\n            self.posFilter = {'dir': (d[0], d[1])}\n            npos = np.count_nonzero(np.ma.compressed(~self.POS.dir.mask))\n            print(\"npos = {}\".format(npos))\n            nspikes = np.count_nonzero(\n                np.ma.compressed(\n                    ~self.TETRODE[tetrode].getClustSpks(\n                        cluster).mask[:, 0, 0]))\n            print(\"nspikes = {}\".format(nspikes))\n            dir_rates[d[0]] = nspikes  # / (npos/50.0)\n            if 'spikes' in maptype:\n                self.plotSpikesOnPath(\n                    tetrode, cluster, ax=d[2], markersize=4)\n            elif 'rmap' in maptype:\n                self._plotMap(\n                    tetrode, cluster, ax=d[2], vmax=max_rate)\n            elif 'polar' in maptype:\n                self._plotMap(\n                    tetrode, cluster, var2bin='dir', ax=d[2], vmax=max_rate)\n            elif 'xcorr' in maptype:\n                self.plotXCorr(\n                    tetrode, cluster, ax=d[2])\n                x1 = self.TETRODE[tetrode].getClustTS(cluster) / (96000/1000)\n                print(\"x1 len = {}\".format(len(x1)))\n                dir_rates[d[0]] = self.spikecalcs.theta_band_max_freq(x1)\n                d[2].set_xlabel('')\n                d[2].set_title('')\n                d[2].set_xticklabels('')\n            d[2].set_title(\"nspikes = {}\".format(nspikes))\n        self.posFilter = None\n        if 'spikes' in maptype:\n            self.plotSpikesOnPath(tetrode, cluster, ax=ax4)\n        elif 'rmap' in maptype:\n            self._plotMap(tetrode, cluster, ax=ax4)\n        elif 'polar' in maptype:\n            self._plotMap(tetrode, cluster, var2bin='dir', ax=ax4)\n        elif 'xcorr' in maptype:\n            self.plotXCorr(tetrode, cluster, ax=ax4)\n            ax4.set_xlabel('')\n            ax4.set_title('')\n            ax4.set_xticklabels('')\n        return dir_rates\n        '''\n</code></pre>"},{"location":"reference/#ephysiopy.visualise.plotting.FigureMaker.__init__","title":"<code>__init__()</code>","text":"<p>Initializes the FigureMaker object.</p> Source code in <code>ephysiopy/visualise/plotting.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    Initializes the FigureMaker object.\n    \"\"\"\n    self.PosCalcs = None\n</code></pre>"},{"location":"reference/#ephysiopy.visualise.plotting.FigureMaker.eb_map","title":"<code>eb_map(cluster, channel, **kwargs)</code>","text":"<p>Gets the ego-centric boundary map for the specified cluster(s) and channel.</p> <p>Parameters:</p> Name Type Description Default <code>cluster</code> <code>int | list</code> <p>The cluster(s) to get the ego-centric boundary map for.</p> required <code>channel</code> <code>int</code> <p>The channel number.</p> required <code>**kwargs</code> <p>Additional keyword arguments for the function.</p> <code>{}</code> Source code in <code>ephysiopy/visualise/plotting.py</code> <pre><code>def eb_map(self, cluster: int | list, channel: int, **kwargs):\n    \"\"\"\n    Gets the ego-centric boundary map for the specified cluster(s) and\n    channel.\n\n    Args:\n        cluster (int | list): The cluster(s) to get the ego-centric\n            boundary map for.\n        channel (int): The channel number.\n        **kwargs: Additional keyword arguments for the function.\n    \"\"\"\n    if isinstance(cluster, list):\n        self._plot_multiple_clusters(self.makeEgoCentricBoundaryMap,\n                                     cluster,\n                                     channel,\n                                     projection='polar',\n                                     **kwargs)\n    else:\n        ts = self.get_spike_times(channel, cluster)\n        self.makeEgoCentricBoundaryMap(ts, **kwargs)\n    plt.show()\n</code></pre>"},{"location":"reference/#ephysiopy.visualise.plotting.FigureMaker.eb_spikes","title":"<code>eb_spikes(cluster, channel, **kwargs)</code>","text":"<p>Gets the ego-centric boundary spikes for the specified cluster(s) and channel.</p> <p>Parameters:</p> Name Type Description Default <code>cluster</code> <code>int | list</code> <p>The cluster(s) to get the ego-centric boundary spikes for.</p> required <code>channel</code> <code>int</code> <p>The channel number.</p> required <code>**kwargs</code> <p>Additional keyword arguments for the function.</p> <code>{}</code> Source code in <code>ephysiopy/visualise/plotting.py</code> <pre><code>def eb_spikes(self, cluster: int | list, channel: int, **kwargs):\n    \"\"\"\n    Gets the ego-centric boundary spikes for the specified cluster(s)\n    and channel.\n\n    Args:\n        cluster (int | list): The cluster(s) to get the ego-centric\n            boundary spikes for.\n        channel (int): The channel number.\n        **kwargs: Additional keyword arguments for the function.\n    \"\"\"\n    if isinstance(cluster, list):\n        self._plot_multiple_clusters(self.makeEgoCentricBoundarySpikePlot,\n                                     cluster,\n                                     channel,\n                                     **kwargs)\n    else:\n        ts = self.get_spike_times(channel, cluster)\n        self.makeEgoCentricBoundarySpikePlot(ts, **kwargs)\n    plt.show()\n</code></pre>"},{"location":"reference/#ephysiopy.visualise.plotting.FigureMaker.getSpikePosIndices","title":"<code>getSpikePosIndices(spk_times)</code>","text":"<p>Returns the indices into the position data at which some spike times occurred.</p> <p>Parameters:</p> Name Type Description Default <code>spk_times</code> <code>ndarray</code> <p>The spike times in seconds.</p> required <p>Returns:</p> Type Description <p>np.ndarray: The indices into the position data at which the spikes occurred.</p> Source code in <code>ephysiopy/visualise/plotting.py</code> <pre><code>def getSpikePosIndices(self, spk_times: np.ndarray):\n    \"\"\"\n    Returns the indices into the position data at which some spike times\n    occurred.\n\n    Args:\n        spk_times (np.ndarray): The spike times in seconds.\n\n    Returns:\n        np.ndarray: The indices into the position data at which the spikes\n            occurred.\n    \"\"\"\n    pos_times = getattr(self.PosCalcs, \"xyTS\")\n    idx = np.searchsorted(pos_times, spk_times) - 1\n    return idx\n</code></pre>"},{"location":"reference/#ephysiopy.visualise.plotting.FigureMaker.hd_map","title":"<code>hd_map(cluster, channel, **kwargs)</code>","text":"<p>Gets the head direction map for the specified cluster(s) and channel.</p> <p>Parameters:</p> Name Type Description Default <code>cluster</code> <code>int | list</code> <p>The cluster(s) to get the head direction map for.</p> required <code>channel</code> <code>int</code> <p>The channel number.</p> required <code>**kwargs</code> <p>Additional keyword arguments for the function.</p> <code>{}</code> Source code in <code>ephysiopy/visualise/plotting.py</code> <pre><code>def hd_map(self, cluster: int | list, channel: int, **kwargs):\n    \"\"\"\n    Gets the head direction map for the specified cluster(s) and channel.\n\n    Args:\n        cluster (int | list): The cluster(s) to get the head direction map\n            for.\n        channel (int): The channel number.\n        **kwargs: Additional keyword arguments for the function.\n    \"\"\"\n    if isinstance(cluster, list):\n        self._plot_multiple_clusters(self.makeHDPlot,\n                                     cluster,\n                                     channel,\n                                     projection=\"polar\",\n                                     strip_axes=True,\n                                     **kwargs)\n    else:\n        ts = self.get_spike_times(channel, cluster)\n        self.makeHDPlot(ts, **kwargs)\n    plt.show()\n</code></pre>"},{"location":"reference/#ephysiopy.visualise.plotting.FigureMaker.initialise","title":"<code>initialise()</code>","text":"<p>Initializes the FigureMaker object with data from PosCalcs.</p> Source code in <code>ephysiopy/visualise/plotting.py</code> <pre><code>def initialise(self):\n    \"\"\"\n    Initializes the FigureMaker object with data from PosCalcs.\n    \"\"\"\n    self.RateMap = RateMap(self.PosCalcs.xy,\n                           self.PosCalcs.dir,\n                           self.PosCalcs.speed,)\n    self.npos = self.PosCalcs.xy.shape[1]\n</code></pre>"},{"location":"reference/#ephysiopy.visualise.plotting.FigureMaker.makeEgoCentricBoundaryMap","title":"<code>makeEgoCentricBoundaryMap(spk_times, ax=None, **kwargs)</code>","text":"<p>Creates an ego-centric boundary map plot.</p> <p>Parameters:</p> Name Type Description Default <code>spk_times</code> <code>ndarray</code> <p>The spike times in seconds.</p> required <code>ax</code> <code>axes</code> <p>The axes to plot on. If None, new axes are created.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments for the function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>axes</code> <p>matplotlib.axes: The axes with the plot.</p> Source code in <code>ephysiopy/visualise/plotting.py</code> <pre><code>def makeEgoCentricBoundaryMap(self,\n                              spk_times: np.ndarray,\n                              ax: matplotlib.axes = None,\n                              **kwargs) -&gt; matplotlib.axes:\n    \"\"\"\n    Creates an ego-centric boundary map plot.\n\n    Args:\n        spk_times (np.ndarray): The spike times in seconds.\n        ax (matplotlib.axes, optional): The axes to plot on. If None,\n            new axes are created.\n        **kwargs: Additional keyword arguments for the function.\n\n    Returns:\n        matplotlib.axes: The axes with the plot.\n    \"\"\"\n    if not self.RateMap:\n        self.initialise()\n\n    degs_per_bin = 3\n    xy_binsize = 2.5\n    arena_type = \"circle\"\n    # parse kwargs\n    if \"degs_per_bin\" in kwargs.keys():\n        degs_per_bin = kwargs[\"degs_per_bin\"]\n    if \"xy_binsize\" in kwargs.keys():\n        xy_binsize = kwargs[\"xy_binsize\"]\n    if \"arena_type\" in kwargs.keys():\n        arena_type = kwargs[\"arena_type\"]\n    if \"strip_axes\" in kwargs.keys():\n        strip_axes = kwargs.pop(\"strip_axes\")\n    else:\n        strip_axes = False\n    if 'return_ratemap' in kwargs.keys():\n        return_ratemap = kwargs.pop('return_ratemap')\n    else:\n        return_ratemap = False\n\n    idx = self.getSpikePosIndices(spk_times)\n    spk_weights = np.bincount(idx, minlength=len(self.RateMap.dir))\n    ego_map = self.RateMap.get_egocentric_boundary_map(spk_weights,\n                                                       degs_per_bin,\n                                                       xy_binsize,\n                                                       arena_type)\n    rmap = ego_map.rmap\n    if ax is None:\n        fig = plt.figure()\n        ax = fig.add_subplot(projection='polar')\n    theta = np.arange(0, 2*np.pi, 2*np.pi/rmap.shape[1])\n    phi = np.arange(0, rmap.shape[0]*2.5, 2.5)\n    X, Y = np.meshgrid(theta, phi)\n    ax.pcolormesh(X, Y, rmap, **kwargs)\n    ax.set_xticks(np.arange(0, 2*np.pi, np.pi/4))\n    # ax.set_xticklabels(np.arange(0, 2*np.pi, np.pi/4))\n    ax.set_yticks(np.arange(0, 50, 10))\n    ax.set_yticklabels(np.arange(0, 50, 10))\n    ax.set_xlabel('Angle (deg)')\n    ax.set_ylabel('Distance (cm)')\n    if strip_axes:\n        return stripAxes(ax)\n    if return_ratemap:\n        return ax, rmap\n    return ax\n</code></pre>"},{"location":"reference/#ephysiopy.visualise.plotting.FigureMaker.makeEgoCentricBoundarySpikePlot","title":"<code>makeEgoCentricBoundarySpikePlot(spk_times, add_colour_wheel=False, ax=None, **kwargs)</code>","text":"<p>Creates an ego-centric boundary spike plot.</p> <p>Parameters:</p> Name Type Description Default <code>spk_times</code> <code>ndarray</code> <p>The spike times in seconds.</p> required <code>add_colour_wheel</code> <code>bool</code> <p>Whether to add a colour wheel to the plot. Defaults to False.</p> <code>False</code> <code>ax</code> <code>axes</code> <p>The axes to plot on. If None, new axes are created.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments for the function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>axes</code> <p>matplotlib.axes: The axes with the plot.</p> Source code in <code>ephysiopy/visualise/plotting.py</code> <pre><code>@stripAxes\ndef makeEgoCentricBoundarySpikePlot(self,\n                                    spk_times: np.ndarray,\n                                    add_colour_wheel: bool = False,\n                                    ax: matplotlib.axes = None,\n                                    **kwargs) -&gt; matplotlib.axes:\n    \"\"\"\n    Creates an ego-centric boundary spike plot.\n\n    Args:\n        spk_times (np.ndarray): The spike times in seconds.\n        add_colour_wheel (bool, optional): Whether to add a colour wheel\n            to the plot. Defaults to False.\n        ax (matplotlib.axes, optional): The axes to plot on. If None,\n            new axes are created.\n        **kwargs: Additional keyword arguments for the function.\n\n    Returns:\n        matplotlib.axes: The axes with the plot.\n    \"\"\"\n    if not self.RateMap:\n        self.initialise()\n    # get the index into a circular colormap based\n    # on directional heading, then create a LineCollection\n    num_dir_bins = 60\n    if \"dir_bins\" in kwargs.keys():\n        num_dir_bins = kwargs[\"num_dir_bins\"]\n    if \"strip_axes\" in kwargs.keys():\n        strip_axes = kwargs.pop(\"strip_axes\")\n    else:\n        strip_axes = False\n    if \"ms\" in kwargs.keys():\n        rect_size = kwargs.pop(\"ms\")\n    else:\n        rect_size = 1\n    dir_colours = sns.color_palette('hls', num_dir_bins)\n    # need to create line colours and line widths for the collection\n    idx = self.getSpikePosIndices(spk_times)\n    dir_spike_fired_at = self.RateMap.dir[idx]\n    idx_of_dir_to_colour = np.floor(\n        dir_spike_fired_at / (360 / num_dir_bins)).astype(int)\n    rects = [Rectangle(self.RateMap.xy[:, i],\n                       width=rect_size, height=rect_size)\n             for i in idx]\n    if ax is None:\n        fig = plt.figure()\n        ax = fig.add_subplot()\n    # plot the path\n    ax.plot(self.RateMap.xy[0],\n            self.RateMap.xy[1],\n            c=tcols.colours[0],\n            zorder=1,\n            alpha=0.3)\n    ax.set_aspect('equal')\n    for col_idx, r in zip(idx_of_dir_to_colour, rects):\n        ax.add_artist(r)\n        r.set_clip_box(ax.bbox)\n        r.set_facecolor(dir_colours[col_idx])\n        r.set_rasterized(True)\n    if add_colour_wheel:\n        ax_col = ax.inset_axes(bounds=[0.75, 0.75, 0.15, 0.15],\n                               projection='polar',\n                               transform=fig.transFigure)\n        ax_col.set_theta_zero_location(\"N\")\n        theta = np.linspace(0, 2*np.pi, 1000)\n        phi = np.linspace(0, 1, 2)\n        X, Y = np.meshgrid(phi, theta)\n        norm = matplotlib.colors.Normalize(0, 2*np.pi)\n        col_map = sns.color_palette('hls', as_cmap=True)\n        ax_col.pcolormesh(theta, phi, Y.T, norm=norm, cmap=col_map)\n        ax_col.set_yticklabels([])\n        ax_col.spines['polar'].set_visible(False)\n        ax_col.set_thetagrids([0, 90])\n    if strip_axes:\n        return stripAxes(ax)\n    return ax\n</code></pre>"},{"location":"reference/#ephysiopy.visualise.plotting.FigureMaker.makeHDPlot","title":"<code>makeHDPlot(spk_times=None, ax=None, **kwargs)</code>","text":"<p>Creates a head direction plot.</p> <p>Parameters:</p> Name Type Description Default <code>spk_times</code> <code>array</code> <p>The spike times in seconds. If None, no spikes are plotted.</p> <code>None</code> <code>ax</code> <code>axes</code> <p>The axes to plot on. If None, new axes are created.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments for the function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>axes</code> <p>matplotlib.axes: The axes with the plot.</p> Source code in <code>ephysiopy/visualise/plotting.py</code> <pre><code>def makeHDPlot(\n    self, spk_times: np.array = None, ax: matplotlib.axes = None, **kwargs\n) -&gt; matplotlib.axes:\n    \"\"\"\n    Creates a head direction plot.\n\n    Args:\n        spk_times (np.array, optional): The spike times in seconds. If\n            None, no spikes are plotted.\n        ax (matplotlib.axes, optional): The axes to plot on. If None, new\n            axes are created.\n        **kwargs: Additional keyword arguments for the function.\n\n    Returns:\n        matplotlib.axes: The axes with the plot.\n    \"\"\"\n    if not self.RateMap:\n        self.initialise()\n    if \"strip_axes\" in kwargs.keys():\n        strip_axes = kwargs.pop(\"strip_axes\")\n    else:\n        strip_axes = True\n    spk_times_in_pos_samples = self.getSpikePosIndices(spk_times)\n    spk_weights = np.bincount(\n        spk_times_in_pos_samples, minlength=self.npos)\n    rmap = self.RateMap.getMap(spk_weights, varType=VariableToBin.DIR)\n    if ax is None:\n        fig = plt.figure()\n        ax = fig.add_subplot(111, **kwargs)\n    ax.set_theta_zero_location(\"N\")\n    # need to deal with the case where the axis is supplied but\n    # is not polar. deal with polar first\n    theta = np.deg2rad(rmap[1][0])\n    ax.clear()\n    r = rmap[0]  # in samples so * pos sample_rate\n    r = np.insert(r, -1, r[0])\n    if \"polar\" in ax.name:\n        ax.plot(theta, r)\n        if \"fill\" in kwargs:\n            ax.fill(theta, r, alpha=0.5)\n        ax.set_aspect(\"equal\")\n    else:\n        pass\n\n    # See if we should add the mean resultant vector (mrv)\n    if \"add_mrv\" in kwargs:\n        from ephysiopy.common.statscalcs import mean_resultant_vector\n\n        angles = self.PosCalcs.dir[spk_times_in_pos_samples]\n        r, th = mean_resultant_vector(np.deg2rad(angles))\n        ax.plot([th, th], [0, r * np.max(rmap[0])], \"r\")\n    if \"polar\" in ax.name:\n        ax.set_thetagrids([0, 90, 180, 270])\n    if strip_axes:\n        return stripAxes(ax)\n    return ax\n</code></pre>"},{"location":"reference/#ephysiopy.visualise.plotting.FigureMaker.makePowerSpectrum","title":"<code>makePowerSpectrum(freqs, power, sm_power, band_max_power, freq_at_band_max_power, max_freq=50, theta_range=[6, 12], ax=None, **kwargs)</code>","text":"<p>Plots the power spectrum. The parameters can be obtained from calcEEGPowerSpectrum() in the EEGCalcsGeneric class.</p> <p>Parameters:</p> Name Type Description Default <code>freqs</code> <code>array</code> <p>The frequencies.</p> required <code>power</code> <code>array</code> <p>The power values.</p> required <code>sm_power</code> <code>array</code> <p>The smoothed power values.</p> required <code>band_max_power</code> <code>float</code> <p>The maximum power in the band.</p> required <code>freq_at_band_max_power</code> <code>float</code> <p>The frequency at which the maximum power in the band occurs.</p> required <code>max_freq</code> <code>int</code> <p>The maximum frequency. Defaults to 50.</p> <code>50</code> <code>theta_range</code> <code>tuple</code> <p>The theta range. Defaults to [6, 12].</p> <code>[6, 12]</code> <code>ax</code> <code>axes</code> <p>The axes to plot on. If None, new axes are created.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments for the function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>axes</code> <p>matplotlib.axes: The axes with the plot.</p> Source code in <code>ephysiopy/visualise/plotting.py</code> <pre><code>def makePowerSpectrum(\n    self,\n    freqs: np.array,\n    power: np.array,\n    sm_power: np.array,\n    band_max_power: float,\n    freq_at_band_max_power: float,\n    max_freq: int = 50,\n    theta_range: tuple = [6, 12],\n    ax: matplotlib.axes = None,\n    **kwargs\n) -&gt; matplotlib.axes:\n    \"\"\"\n    Plots the power spectrum. The parameters can be obtained from\n    calcEEGPowerSpectrum() in the EEGCalcsGeneric class.\n\n    Args:\n        freqs (np.array): The frequencies.\n        power (np.array): The power values.\n        sm_power (np.array): The smoothed power values.\n        band_max_power (float): The maximum power in the band.\n        freq_at_band_max_power (float): The frequency at which the maximum\n            power in the band occurs.\n        max_freq (int, optional): The maximum frequency. Defaults to 50.\n        theta_range (tuple, optional): The theta range.\n            Defaults to [6, 12].\n        ax (matplotlib.axes, optional): The axes to plot on. If None, new\n            axes are created.\n        **kwargs: Additional keyword arguments for the function.\n\n    Returns:\n        matplotlib.axes: The axes with the plot.\n    \"\"\"\n    if \"strip_axes\" in kwargs.keys():\n        strip_axes = kwargs.pop(\"strip_axes\")\n    else:\n        strip_axes = False\n    # downsample frequencies and power\n    freqs = freqs[0::50]\n    power = power[0::50]\n    sm_power = sm_power[0::50]\n    if ax is None:\n        fig = plt.figure()\n        ax = fig.add_subplot(111)\n    ax.plot(freqs, power, alpha=0.5, color=[0.8627, 0.8627, 0.8627])\n    ax.plot(freqs, sm_power)\n    ax.set_xlim(0, max_freq)\n    ylim = [0, np.max(sm_power[freqs &lt; max_freq])]\n    if \"ylim\" in kwargs:\n        ylim = kwargs[\"ylim\"]\n    ax.set_ylim(ylim)\n    ax.set_ylabel(\"Power\")\n    ax.set_xlabel(\"Frequency\")\n    ax.text(\n        x=theta_range[1] / 0.9,\n        y=band_max_power,\n        s=str(freq_at_band_max_power)[0:4],\n        fontsize=20,\n    )\n    from matplotlib.patches import Rectangle\n\n    r = Rectangle(\n        (theta_range[0], 0),\n        width=np.diff(theta_range)[0],\n        height=np.diff(ax.get_ylim())[0],\n        alpha=0.25,\n        color=\"r\",\n        ec=\"none\",\n    )\n    ax.add_patch(r)\n    if strip_axes:\n        return stripAxes(ax)\n    return ax\n</code></pre>"},{"location":"reference/#ephysiopy.visualise.plotting.FigureMaker.makeRaster","title":"<code>makeRaster(spk_times, dt=(-50, 100), prc_max=0.5, ax=None, ms_per_bin=1, sample_rate=30000.0, **kwargs)</code>","text":"<p>Plots a raster plot for a specified tetrode/ cluster.</p> <p>Parameters:</p> Name Type Description Default <code>spk_times</code> <code>array</code> <p>The spike times in samples.</p> required <code>dt</code> <code>tuple</code> <p>The window of time in ms to examine zeroed on the event of interest i.e. the first value will probably be negative as in the example. Defaults to (-50, 100).</p> <code>(-50, 100)</code> <code>prc_max</code> <code>float</code> <p>The proportion of firing the cell has to 'lose' to count as silent; a float between 0 and 1. Defaults to 0.5.</p> <code>0.5</code> <code>ax</code> <code>axes</code> <p>The axes to plot into. If not provided a new figure is created. Defaults to None.</p> <code>None</code> <code>ms_per_bin</code> <code>int</code> <p>The number of milliseconds in each bin of the raster plot. Defaults to 1.</p> <code>1</code> <code>sample_rate</code> <code>float</code> <p>The sample rate. Defaults to 3e4.</p> <code>30000.0</code> <code>**kwargs</code> <p>Additional keyword arguments for the function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>axes</code> <p>matplotlib.axes: The axes with the plot.</p> Source code in <code>ephysiopy/visualise/plotting.py</code> <pre><code>def makeRaster(\n    self,\n    spk_times: np.array,\n    dt=(-50, 100),\n    prc_max: float = 0.5,\n    ax: matplotlib.axes = None,\n    ms_per_bin: int = 1,\n    sample_rate: float = 3e4,  # OE=3e4, Axona=96000\n    **kwargs\n) -&gt; matplotlib.axes:\n    \"\"\"\n    Plots a raster plot for a specified tetrode/ cluster.\n\n    Args:\n        spk_times (np.array): The spike times in samples.\n        dt (tuple, optional): The window of time in ms to examine zeroed\n            on the event of interest i.e. the first value will probably\n            be negative as in the example. Defaults to (-50, 100).\n        prc_max (float, optional): The proportion of firing the cell has\n            to 'lose' to count as silent; a float between 0 and 1.\n            Defaults to 0.5.\n        ax (matplotlib.axes, optional): The axes to plot into.\n            If not provided a new figure is created. Defaults to None.\n        ms_per_bin (int, optional): The number of milliseconds in each bin\n            of the raster plot. Defaults to 1.\n        sample_rate (float, optional): The sample rate. Defaults to 3e4.\n        **kwargs: Additional keyword arguments for the function.\n\n    Returns:\n        matplotlib.axes: The axes with the plot.\n    \"\"\"\n    assert hasattr(self, \"ttl_data\")\n\n    if \"strip_axes\" in kwargs.keys():\n        strip_axes = kwargs.pop(\"strip_axes\")\n    else:\n        strip_axes = False\n    x1 = spk_times / sample_rate * 1000.0  # get into ms\n    x1.sort()\n    on_good = self.ttl_data[\"ttl_timestamps\"]\n    dt = np.array(dt)\n    irange = on_good[:, np.newaxis] + dt[np.newaxis, :]\n    dts = np.searchsorted(x1, irange)\n    y = []\n    x = []\n    for i, t in enumerate(dts):\n        tmp = x1[t[0]:t[1]] - on_good[i]\n        x.extend(tmp)\n        y.extend(np.repeat(i, len(tmp)))\n    if ax is None:\n        fig = plt.figure(figsize=(4.0, 7.0))\n        axScatter = fig.add_subplot(111)\n    else:\n        axScatter = ax\n    histColor = [1 / 255.0, 1 / 255.0, 1 / 255.0]\n    axScatter.scatter(x, y, marker=\".\", s=2,\n                      rasterized=False, color=histColor)\n    divider = make_axes_locatable(axScatter)\n    axScatter.set_xticks((dt[0], 0, dt[1]))\n    axScatter.set_xticklabels((str(dt[0]), \"0\", str(dt[1])))\n    axHistx = divider.append_axes(\"top\", 0.95, pad=0.2,\n                                  sharex=axScatter,\n                                  transform=axScatter.transAxes)\n    scattTrans = transforms.blended_transform_factory(\n        axScatter.transData, axScatter.transAxes\n    )\n    stim_pwidth = self.ttl_data[\"stim_duration\"]\n    if stim_pwidth is None:\n        raise ValueError(\"stim duration is None\")\n\n    axScatter.add_patch(\n        Rectangle(\n            (0, 0),\n            width=stim_pwidth,\n            height=1,\n            transform=scattTrans,\n            color=[0, 0, 1],\n            alpha=0.3,\n        )\n    )\n    histTrans = transforms.blended_transform_factory(\n        axHistx.transData, axHistx.transAxes\n    )\n    axHistx.add_patch(\n        Rectangle(\n            (0, 0),\n            width=stim_pwidth,\n            height=1,\n            transform=histTrans,\n            color=[0, 0, 1],\n            alpha=0.3,\n        )\n    )\n    axScatter.set_ylabel(\"Laser stimulation events\", labelpad=-2.5)\n    axScatter.set_xlabel(\"Time to stimulus onset(ms)\")\n    nStms = len(on_good)\n    axScatter.set_ylim(0, nStms)\n    # Label only the min and max of the y-axis\n    ylabels = axScatter.get_yticklabels()\n    for i in range(1, len(ylabels) - 1):\n        ylabels[i].set_visible(False)\n    yticks = axScatter.get_yticklines()\n    for i in range(1, len(yticks) - 1):\n        yticks[i].set_visible(False)\n\n    axHistx.hist(\n        x,\n        bins=np.arange(dt[0], dt[1] + ms_per_bin, ms_per_bin),\n        color=histColor,\n        range=dt,\n        rasterized=True,\n        histtype=\"stepfilled\",\n    )\n    axHistx.set_ylabel(\"Spike count\", labelpad=-2.5)\n    plt.setp(axHistx.get_xticklabels(), visible=False)\n    # Label only the min and max of the y-axis\n    ylabels = axHistx.get_yticklabels()\n    for i in range(1, len(ylabels) - 1):\n        ylabels[i].set_visible(False)\n    yticks = axHistx.get_yticklines()\n    for i in range(1, len(yticks) - 1):\n        yticks[i].set_visible(False)\n    axHistx.set_xlim(dt)\n    axScatter.set_xlim(dt)\n    if strip_axes:\n        return stripAxes(axScatter)\n    return axScatter\n</code></pre>"},{"location":"reference/#ephysiopy.visualise.plotting.FigureMaker.makeRateMap","title":"<code>makeRateMap(spk_times, ax=None, **kwargs)</code>","text":"<p>Creates a rate map plot.</p> <p>Parameters:</p> Name Type Description Default <code>spk_times</code> <code>ndarray</code> <p>The spike times in seconds.</p> required <code>ax</code> <code>axes</code> <p>The axes to plot on. If None, new axes are created.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments for the function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>axes</code> <p>matplotlib.axes: The axes with the plot.</p> Source code in <code>ephysiopy/visualise/plotting.py</code> <pre><code>@stripAxes\ndef makeRateMap(self,\n                spk_times: np.ndarray,\n                ax: matplotlib.axes = None,\n                **kwargs) -&gt; matplotlib.axes:\n    \"\"\"\n    Creates a rate map plot.\n\n    Args:\n        spk_times (np.ndarray): The spike times in seconds.\n        ax (matplotlib.axes, optional): The axes to plot on. If None,\n            new axes are created.\n        **kwargs: Additional keyword arguments for the function.\n\n    Returns:\n        matplotlib.axes: The axes with the plot.\n    \"\"\"\n    if not self.RateMap:\n        self.initialise()\n    spk_times_in_pos_samples = self.getSpikePosIndices(spk_times)\n    spk_weights = np.bincount(\n        spk_times_in_pos_samples, minlength=self.npos)\n    rmap = self.RateMap.getMap(spk_weights)\n    ratemap = np.ma.MaskedArray(rmap[0], np.isnan(rmap[0]), copy=True)\n    x, y = np.meshgrid(rmap[1][1][0:-1].data, rmap[1][0][0:-1].data)\n    vmax = np.nanmax(np.ravel(ratemap))\n    if ax is None:\n        fig = plt.figure()\n        ax = fig.add_subplot(111)\n    ax.pcolormesh(\n        x, y, ratemap,\n        cmap=jet_cmap,\n        edgecolors=\"face\",\n        vmax=vmax,\n        shading=\"auto\",\n        **kwargs\n    )\n    ax.set_aspect(\"equal\")\n    return ax\n</code></pre>"},{"location":"reference/#ephysiopy.visualise.plotting.FigureMaker.makeSAC","title":"<code>makeSAC(spk_times=None, ax=None, **kwargs)</code>","text":"<p>Creates a spatial autocorrelation plot.</p> <p>Parameters:</p> Name Type Description Default <code>spk_times</code> <code>array</code> <p>The spike times in seconds. If None, no spikes are plotted.</p> <code>None</code> <code>ax</code> <code>axes</code> <p>The axes to plot on. If None, new axes are created.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments for the function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>axes</code> <p>matplotlib.axes: The axes with the plot.</p> Source code in <code>ephysiopy/visualise/plotting.py</code> <pre><code>@stripAxes\ndef makeSAC(\n    self, spk_times: np.array = None, ax: matplotlib.axes = None, **kwargs\n) -&gt; matplotlib.axes:\n    \"\"\"\n    Creates a spatial autocorrelation plot.\n\n    Args:\n        spk_times (np.array, optional): The spike times in seconds. If\n            None, no spikes are plotted.\n        ax (matplotlib.axes, optional): The axes to plot on. If None,\n            new axes are created.\n        **kwargs: Additional keyword arguments for the function.\n\n    Returns:\n        matplotlib.axes: The axes with the plot.\n    \"\"\"\n    if not self.RateMap:\n        self.initialise()\n    spk_times_in_pos_samples = self.getSpikePosIndices(spk_times)\n    spk_weights = np.bincount(\n        spk_times_in_pos_samples, minlength=self.npos)\n    sac = self.RateMap.getSAC(spk_weights)\n    from ephysiopy.common.gridcell import SAC\n\n    S = SAC()\n    measures = S.getMeasures(sac)\n    if ax is None:\n        fig = plt.figure()\n        ax = fig.add_subplot(111)\n    ax = self.show_SAC(sac, measures, ax)\n    return ax\n</code></pre>"},{"location":"reference/#ephysiopy.visualise.plotting.FigureMaker.makeSpeedVsHeadDirectionPlot","title":"<code>makeSpeedVsHeadDirectionPlot(spk_times, ax=None, **kwargs)</code>","text":"<p>Creates a speed versus head direction plot.</p> <p>Parameters:</p> Name Type Description Default <code>spk_times</code> <code>array</code> <p>The spike times in seconds.</p> required <code>ax</code> <code>axes</code> <p>The axes to plot on. If None, new axes are created.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments for the function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>axes</code> <p>matplotlib.axes: The axes with the plot.</p> Source code in <code>ephysiopy/visualise/plotting.py</code> <pre><code>def makeSpeedVsHeadDirectionPlot(\n    self, spk_times: np.array, ax: matplotlib.axes = None, **kwargs\n) -&gt; matplotlib.axes:\n    \"\"\"\n    Creates a speed versus head direction plot.\n\n    Args:\n        spk_times (np.array): The spike times in seconds.\n        ax (matplotlib.axes, optional): The axes to plot on. If None,\n            new axes are created.\n        **kwargs: Additional keyword arguments for the function.\n\n    Returns:\n        matplotlib.axes: The axes with the plot.\n    \"\"\"\n    if \"strip_axes\" in kwargs.keys():\n        strip_axes = kwargs.pop(\"strip_axes\")\n    else:\n        strip_axes = False\n    if not self.RateMap:\n        self.initialise()\n    spk_times_in_pos_samples = self.getSpikePosIndices(spk_times)\n    idx = np.array(spk_times_in_pos_samples, dtype=int)\n    w = np.bincount(idx, minlength=self.PosCalcs.speed.shape[0])\n    if np.ma.is_masked(self.PosCalcs.speed):\n        w[self.PosCalcs.speed.mask] = 0\n\n    dir_bins = np.arange(0, 360, 6)\n    spd_bins = np.arange(0, 30, 1)\n    h = np.histogram2d(self.PosCalcs.dir,\n                       self.PosCalcs.speed,\n                       [dir_bins, spd_bins], weights=w)\n    from ephysiopy.common.utils import blurImage\n\n    im = blurImage(h[0], 5, ftype=\"gaussian\")\n    im = np.ma.MaskedArray(im)\n    # mask low rates...\n    im = np.ma.masked_where(im &lt;= 1, im)\n    # ... and where less than 0.5% of data is accounted for\n    x, y = np.meshgrid(dir_bins, spd_bins)\n    vmax = np.max(np.ravel(im))\n    if ax is None:\n        fig = plt.figure()\n        ax = fig.add_subplot(111)\n    ax.pcolormesh(x, y, im.T,\n                  cmap=jet_cmap, edgecolors=\"face\",\n                  vmax=vmax, shading=\"auto\")\n    ax.set_xticks([90, 180, 270], labels=['90', '180', '270'],\n                  fontweight=\"normal\", size=6)\n    ax.set_yticks([10, 20], labels=['10', '20'],\n                  fontweight=\"normal\", size=6)\n    ax.set_xlabel(\"Heading\", fontweight=\"normal\", size=6)\n    if strip_axes:\n        stripAxes(ax)\n    return ax\n</code></pre>"},{"location":"reference/#ephysiopy.visualise.plotting.FigureMaker.makeSpeedVsRatePlot","title":"<code>makeSpeedVsRatePlot(spk_times, minSpeed=0.0, maxSpeed=40.0, sigma=3.0, ax=None, **kwargs)</code>","text":"<p>Plots the instantaneous firing rate of a cell against running speed. Also outputs a couple of measures as with Kropff et al., 2015; the Pearsons correlation and the depth of modulation (dom).</p> <p>Parameters:</p> Name Type Description Default <code>spk_times</code> <code>array</code> <p>The spike times in seconds.</p> required <code>minSpeed</code> <code>float</code> <p>The minimum speed. Defaults to 0.0.</p> <code>0.0</code> <code>maxSpeed</code> <code>float</code> <p>The maximum speed. Defaults to 40.0.</p> <code>40.0</code> <code>sigma</code> <code>float</code> <p>The sigma value. Defaults to 3.0.</p> <code>3.0</code> <code>ax</code> <code>axes</code> <p>The axes to plot on. If None, new axes are created.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments for the function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>axes</code> <p>matplotlib.axes: The axes with the plot.</p> Source code in <code>ephysiopy/visualise/plotting.py</code> <pre><code>def makeSpeedVsRatePlot(\n    self,\n    spk_times: np.array,\n    minSpeed: float = 0.0,\n    maxSpeed: float = 40.0,\n    sigma: float = 3.0,\n    ax: matplotlib.axes = None,\n    **kwargs\n) -&gt; matplotlib.axes:\n    \"\"\"\n    Plots the instantaneous firing rate of a cell against running speed.\n    Also outputs a couple of measures as with Kropff et al., 2015; the\n    Pearsons correlation and the depth of modulation (dom).\n\n    Args:\n        spk_times (np.array): The spike times in seconds.\n        minSpeed (float, optional): The minimum speed. Defaults to 0.0.\n        maxSpeed (float, optional): The maximum speed. Defaults to 40.0.\n        sigma (float, optional): The sigma value. Defaults to 3.0.\n        ax (matplotlib.axes, optional): The axes to plot on. If None, new\n            axes are created.\n        **kwargs: Additional keyword arguments for the function.\n\n    Returns:\n        matplotlib.axes: The axes with the plot.\n    \"\"\"\n    if \"strip_axes\" in kwargs.keys():\n        strip_axes = kwargs.pop(\"strip_axes\")\n    else:\n        strip_axes = False\n    if not self.RateMap:\n        self.initialise()\n    spk_times_in_pos_samples = self.getSpikePosIndices(spk_times)\n\n    speed = np.ravel(self.PosCalcs.speed)\n    if np.nanmax(speed) &lt; maxSpeed:\n        maxSpeed = np.nanmax(speed)\n    spd_bins = np.arange(minSpeed, maxSpeed, 1.0)\n    # Construct the mask\n    speed_filt = np.ma.MaskedArray(speed)\n    speed_filt = np.ma.masked_where(speed_filt &lt; minSpeed, speed_filt)\n    speed_filt = np.ma.masked_where(speed_filt &gt; maxSpeed, speed_filt)\n    from ephysiopy.common.spikecalcs import SpikeCalcsGeneric\n\n    x1 = spk_times_in_pos_samples\n    S = SpikeCalcsGeneric(x1)\n    spk_sm = S.smooth_spike_train(x1,\n                                  self.PosCalcs.xyTS.shape[0],\n                                  sigma, None)\n    spk_sm = np.ma.MaskedArray(spk_sm, mask=np.ma.getmask(speed_filt))\n    spd_dig = np.digitize(speed_filt, spd_bins, right=True)\n    mn_rate = np.array(\n        [np.ma.mean(spk_sm[spd_dig == i]) for i in range(0, len(spd_bins))]\n    )\n    var = np.array(\n        [np.ma.std(spk_sm[spd_dig == i]) for i in range(0, len(spd_bins))]\n    )\n    np.array([np.ma.sum(spk_sm[spd_dig == i]) for i in range(\n        0, len(spd_bins))])\n    if ax is None:\n        fig = plt.figure()\n        ax = fig.add_subplot(111)\n    ax.errorbar(spd_bins, mn_rate * self.PosCalcs.sample_rate,\n                yerr=var, color=\"k\")\n    ax.set_xlim(spd_bins[0], spd_bins[-1])\n    ax.set_xticks(\n        [spd_bins[0], spd_bins[-1]],\n        labels=[\"0\", \"{:.2g}\".format(spd_bins[-1])],\n        fontweight=\"normal\",\n        size=6,\n    )\n    ax.set_yticks(\n        [0, np.nanmax(mn_rate) * self.PosCalcs.sample_rate],\n        labels=[\"0\", \"{:.2f}\".format(np.nanmax(mn_rate))],\n        fontweight=\"normal\",\n        size=6,\n    )\n    if strip_axes:\n        return stripAxes(ax)\n    return ax\n</code></pre>"},{"location":"reference/#ephysiopy.visualise.plotting.FigureMaker.makeSpikePathPlot","title":"<code>makeSpikePathPlot(spk_times=None, ax=None, **kwargs)</code>","text":"<p>Creates a spike path plot.</p> <p>Parameters:</p> Name Type Description Default <code>spk_times</code> <code>ndarray</code> <p>The spike times in seconds. If None, no spikes are plotted.</p> <code>None</code> <code>ax</code> <code>axes</code> <p>The axes to plot on. If None, new axes are created.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments for the function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>axes</code> <p>matplotlib.axes: The axes with the plot.</p> Source code in <code>ephysiopy/visualise/plotting.py</code> <pre><code>@stripAxes\ndef makeSpikePathPlot(self,\n                      spk_times: np.ndarray = None,\n                      ax: matplotlib.axes = None,\n                      **kwargs) -&gt; matplotlib.axes:\n    \"\"\"\n    Creates a spike path plot.\n\n    Args:\n        spk_times (np.ndarray, optional): The spike times in seconds.\n            If None, no spikes are plotted.\n        ax (matplotlib.axes, optional): The axes to plot on.\n            If None, new axes are created.\n        **kwargs: Additional keyword arguments for the function.\n\n    Returns:\n        matplotlib.axes: The axes with the plot.\n    \"\"\"\n    if not self.RateMap:\n        self.initialise()\n    if \"c\" in kwargs:\n        col = kwargs.pop(\"c\")\n    else:\n        col = tcols.colours[1]\n    if ax is None:\n        fig = plt.figure()\n        ax = fig.add_subplot(111)\n    ax.plot(\n        self.PosCalcs.xy[0, :],\n        self.PosCalcs.xy[1, :],\n        c=tcols.colours[0], zorder=1\n    )\n    ax.set_aspect(\"equal\")\n    if spk_times is not None:\n        idx = self.getSpikePosIndices(spk_times)\n        ax.plot(\n            self.PosCalcs.xy[0, idx],\n            self.PosCalcs.xy[1, idx],\n            \"s\", c=col, **kwargs\n        )\n    return ax\n</code></pre>"},{"location":"reference/#ephysiopy.visualise.plotting.FigureMaker.makeSummaryPlot","title":"<code>makeSummaryPlot(spk_times)</code>","text":"<p>Creates a summary plot with spike path, rate map, head direction plot, and spatial autocorrelation.</p> <p>Parameters:</p> Name Type Description Default <code>spk_times</code> <code>ndarray</code> <p>The spike times in seconds.</p> required <p>Returns:</p> Type Description <p>matplotlib.figure.Figure: The created figure.</p> Source code in <code>ephysiopy/visualise/plotting.py</code> <pre><code>def makeSummaryPlot(self, spk_times: np.ndarray):\n    \"\"\"\n    Creates a summary plot with spike path, rate map, head direction plot,\n    and spatial autocorrelation.\n\n    Args:\n        spk_times (np.ndarray): The spike times in seconds.\n\n    Returns:\n        matplotlib.figure.Figure: The created figure.\n    \"\"\"\n    fig = plt.figure()\n    ax = plt.subplot(221)\n    self.makeSpikePathPlot(spk_times, ax=ax, markersize=2)\n    ax = plt.subplot(222)\n    self.makeRateMap(spk_times, ax=ax)\n    ax = plt.subplot(223, projection=\"polar\")\n    self.makeHDPlot(spk_times, ax=ax)\n    ax = plt.subplot(224)\n    try:\n        self.makeSAC(spk_times, ax=ax)\n    except IndexError:\n        pass\n    return fig\n</code></pre>"},{"location":"reference/#ephysiopy.visualise.plotting.FigureMaker.makeXCorr","title":"<code>makeXCorr(spk_times, ax=None, **kwargs)</code>","text":"<p>Returns an axis containing the autocorrelogram of the spike times provided over the range +/-500ms.</p> <p>Parameters:</p> Name Type Description Default <code>spk_times</code> <code>array</code> <p>Spike times in seconds.</p> required <code>ax</code> <code>axes</code> <p>The axes to plot into. If None, new axes are created.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments for the function. binsize (int, optional): The size of the bins in ms. Gets passed to SpikeCalcsGeneric.xcorr(). Defaults to 1.</p> <code>{}</code> <p>Returns:</p> Type Description <code>axes</code> <p>matplotlib.axes: The axes with the plot.</p> Source code in <code>ephysiopy/visualise/plotting.py</code> <pre><code>def makeXCorr(\n    self, spk_times: np.array, ax: matplotlib.axes = None, **kwargs\n) -&gt; matplotlib.axes:\n    \"\"\"\n    Returns an axis containing the autocorrelogram of the spike\n    times provided over the range +/-500ms.\n\n    Args:\n        spk_times (np.array): Spike times in seconds.\n        ax (matplotlib.axes, optional): The axes to plot into. If None,\n            new axes are created.\n        **kwargs: Additional keyword arguments for the function.\n            binsize (int, optional): The size of the bins in ms. Gets\n            passed to SpikeCalcsGeneric.xcorr(). Defaults to 1.\n\n    Returns:\n        matplotlib.axes: The axes with the plot.\n    \"\"\"\n    if \"strip_axes\" in kwargs.keys():\n        strip_axes = kwargs.pop(\"strip_axes\")\n    else:\n        strip_axes = False\n    # spk_times in samples provided in seconds but convert to\n    # ms for a more display friendly scale\n    spk_times = spk_times\n    S = SpikeCalcsGeneric(spk_times)\n    c, b = S.acorr(spk_times, **kwargs)\n    if ax is None:\n        fig = plt.figure()\n        ax = fig.add_subplot(111)\n    if 'binsize' in kwargs.keys():\n        binsize = kwargs['binsize']\n    else:\n        binsize = 0.001\n    if \"Trange\" in kwargs.keys():\n        xrange = kwargs[\"Trange\"]\n    else:\n        xrange = [-0.5, 0.5]\n    ax.bar(b[:-1], c, width=binsize, color=\"k\", align=\"edge\")\n    ax.set_xlim(xrange)\n    ax.set_xticks((xrange[0], 0, xrange[1]))\n    ax.set_xticklabels(\"\")\n    ax.tick_params(axis=\"both\", which=\"both\", left=False, right=False,\n                   bottom=False, top=False)\n    ax.set_yticklabels(\"\")\n    ax.xaxis.set_ticks_position(\"bottom\")\n    if strip_axes:\n        return stripAxes(ax)\n    return ax\n</code></pre>"},{"location":"reference/#ephysiopy.visualise.plotting.FigureMaker.plotSpectrogramByDepth","title":"<code>plotSpectrogramByDepth(nchannels=384, nseconds=100, maxFreq=125, channels=[], frequencies=[], frequencyIncrement=1, **kwargs)</code>","text":"<p>Plots a heat map spectrogram of the LFP for each channel. Line plots of power per frequency band and power on a subset of channels are also displayed to the right and above the main plot.</p> <p>Parameters:</p> Name Type Description Default <code>nchannels</code> <code>int</code> <p>The number of channels on the probe.</p> <code>384</code> <code>nseconds</code> <code>int</code> <p>How long in seconds from the start of the trial to do the spectrogram for (for speed). Default is 100.</p> <code>100</code> <code>maxFreq</code> <code>int</code> <p>The maximum frequency in Hz to plot the spectrogram out to. Maximum is 1250. Default is 125.</p> <code>125</code> <code>channels</code> <code>list</code> <p>The channels to plot separately on the top plot.</p> <code>[]</code> <code>frequencies</code> <code>list</code> <p>The specific frequencies to examine across all channels. The mean from frequency:  frequency+frequencyIncrement is calculated and plotted on the left hand side of the plot.</p> <code>[]</code> <code>frequencyIncrement</code> <code>int</code> <p>The amount to add to each value of the frequencies list above.</p> <code>1</code> <code>**kwargs</code> <p>Additional keyword arguments for the function. Valid key value pairs:     \"saveas\" - save the figure to this location, needs absolute     path and filename.</p> <code>{}</code> Notes <p>Should also allow kwargs to specify exactly which channels and / or frequency bands to do the line plots for.</p> Source code in <code>ephysiopy/visualise/plotting.py</code> <pre><code>def plotSpectrogramByDepth(\n    self,\n    nchannels: int = 384,\n    nseconds: int = 100,\n    maxFreq: int = 125,\n    channels: list = [],\n    frequencies: list = [],\n    frequencyIncrement: int = 1,\n    **kwargs\n):\n    \"\"\"\n    Plots a heat map spectrogram of the LFP for each channel.\n    Line plots of power per frequency band and power on a subset of\n    channels are also displayed to the right and above the main plot.\n\n    Args:\n        nchannels (int): The number of channels on the probe.\n        nseconds (int, optional): How long in seconds from the start of\n            the trial to do the spectrogram for (for speed).\n            Default is 100.\n        maxFreq (int): The maximum frequency in Hz to plot the spectrogram\n            out to. Maximum is 1250. Default is 125.\n        channels (list): The channels to plot separately on the top plot.\n        frequencies (list): The specific frequencies to examine across\n            all channels. The mean from frequency: \n            frequency+frequencyIncrement is calculated and plotted on\n            the left hand side of the plot.\n        frequencyIncrement (int): The amount to add to each value of\n            the frequencies list above.\n        **kwargs: Additional keyword arguments for the function.\n            Valid key value pairs:\n                \"saveas\" - save the figure to this location, needs absolute\n                path and filename.\n\n    Notes:\n        Should also allow kwargs to specify exactly which channels\n        and / or frequency bands to do the line plots for.\n    \"\"\"\n    if not self.path2LFPdata:\n        raise TypeError(\"Not a probe recording so not plotting\")\n    import os\n\n    lfp_file = os.path.join(self.path2LFPdata, \"continuous.dat\")\n    status = os.stat(lfp_file)\n    nsamples = int(status.st_size / 2 / nchannels)\n    mmap = np.memmap(lfp_file, np.int16, \"r\", 0,\n                     (nchannels, nsamples), order=\"F\")\n    # Load the channel map NB assumes this is in the AP data\n    # location and that kilosort was run there\n    channel_map = np.squeeze(\n        np.load(os.path.join(self.path2APdata, \"channel_map.npy\"))\n    )\n    lfp_sample_rate = 2500\n    data = np.array(mmap[channel_map, 0:nseconds * lfp_sample_rate])\n    from ephysiopy.common.ephys_generic import EEGCalcsGeneric\n\n    E = EEGCalcsGeneric(data[0, :], lfp_sample_rate)\n    E.calcEEGPowerSpectrum()\n    spec_data = np.zeros(shape=(data.shape[0], len(E.sm_power[0::50])))\n    for chan in range(data.shape[0]):\n        E = EEGCalcsGeneric(data[chan, :], lfp_sample_rate)\n        E.calcEEGPowerSpectrum()\n        spec_data[chan, :] = E.sm_power[0::50]\n\n    x, y = np.meshgrid(E.freqs[0::50], channel_map)\n    import matplotlib.colors as colors\n    from matplotlib.pyplot import cm\n    from mpl_toolkits.axes_grid1 import make_axes_locatable\n\n    _, spectoAx = plt.subplots()\n    spectoAx.pcolormesh(x, y, spec_data,\n                        edgecolors=\"face\", cmap=\"bone\",\n                        norm=colors.LogNorm())\n    spectoAx.set_xlim(0, maxFreq)\n    spectoAx.set_ylim(channel_map[0], channel_map[-1])\n    spectoAx.set_xlabel(\"Frequency (Hz)\")\n    spectoAx.set_ylabel(\"Channel\")\n    divider = make_axes_locatable(spectoAx)\n    channel_spectoAx = divider.append_axes(\"top\", 1.2, pad=0.1,\n                                           sharex=spectoAx)\n    meanfreq_powerAx = divider.append_axes(\"right\", 1.2, pad=0.1,\n                                           sharey=spectoAx)\n    plt.setp(channel_spectoAx.get_xticklabels()\n             + meanfreq_powerAx.get_yticklabels(),\n             visible=False)\n\n    # plot mean power across some channels\n    mn_power = np.mean(spec_data, 0)\n    if not channels:\n        channels = range(1, nchannels, 60)\n    cols = iter(cm.rainbow(np.linspace(0, 1, len(channels))))\n    for chan in channels:\n        c = next(cols)\n        channel_spectoAx.plot(\n            E.freqs[0::50],\n            10 * np.log10(spec_data[chan, :] / mn_power),\n            c=c,\n            label=str(chan),\n        )\n\n    channel_spectoAx.set_ylabel(\"Channel power(dB)\")\n    channel_spectoAx.legend(\n        bbox_to_anchor=(0.0, 1.02, 1.0, 0.102),\n        loc=\"lower left\",\n        mode=\"expand\",\n        fontsize=\"x-small\",\n        ncol=4,\n    )\n\n    # plot mean frequencies across all channels\n    if not frequencyIncrement:\n        freq_inc = 6\n    else:\n        freq_inc = frequencyIncrement\n    if not frequencies:\n        lower_freqs = np.arange(1, maxFreq - freq_inc, freq_inc)\n    else:\n        lower_freqs = frequencies\n    upper_freqs = [f + freq_inc for f in lower_freqs]\n    cols = iter(cm.nipy_spectral(np.linspace(0, 1, len(upper_freqs))))\n    mn_power = np.mean(spec_data, 1)\n    for freqs in zip(lower_freqs, upper_freqs):\n        freq_mask = np.logical_and(\n            E.freqs[0::50] &gt; freqs[0], E.freqs[0::50] &lt; freqs[1]\n        )\n        mean_power = 10 * np.log10(np.mean(\n            spec_data[:, freq_mask], 1) / mn_power)\n        c = next(cols)\n        meanfreq_powerAx.plot(\n            mean_power,\n            channel_map,\n            c=c,\n            label=str(freqs[0]) + \" - \" + str(freqs[1]),\n        )\n    meanfreq_powerAx.set_xlabel(\"Mean freq. band power(dB)\")\n    meanfreq_powerAx.legend(\n        bbox_to_anchor=(0.0, 1.02, 1.0, 0.102),\n        loc=\"lower left\",\n        mode=\"expand\",\n        fontsize=\"x-small\",\n        ncol=1,\n    )\n    if \"saveas\" in kwargs:\n        saveas = kwargs[\"saveas\"]\n        plt.savefig(saveas)\n    plt.show()\n</code></pre>"},{"location":"reference/#ephysiopy.visualise.plotting.FigureMaker.power_spectrum","title":"<code>power_spectrum(**kwargs)</code>","text":"<p>Gets the power spectrum.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments for the function.</p> <code>{}</code> Source code in <code>ephysiopy/visualise/plotting.py</code> <pre><code>def power_spectrum(self, **kwargs):\n    \"\"\"\n    Gets the power spectrum.\n\n    Args:\n        **kwargs: Additional keyword arguments for the function.\n    \"\"\"\n    p = self.EEGCalcs.calcEEGPowerSpectrum()\n    self.makePowerSpectrum(p[0], p[1], p[2], p[3], p[4], **kwargs)\n    plt.show()\n</code></pre>"},{"location":"reference/#ephysiopy.visualise.plotting.FigureMaker.rate_map","title":"<code>rate_map(cluster, channel, **kwargs)</code>","text":"<p>Gets the rate map for the specified cluster(s) and channel.</p> <p>Parameters:</p> Name Type Description Default <code>cluster</code> <code>int | list</code> <p>The cluster(s) to get the rate map for.</p> required <code>channel</code> <code>int</code> <p>The channel number.</p> required <code>**kwargs</code> <p>Additional keyword arguments for the function.</p> <code>{}</code> Source code in <code>ephysiopy/visualise/plotting.py</code> <pre><code>def rate_map(self, cluster: int | list, channel: int, **kwargs):\n    \"\"\"\n    Gets the rate map for the specified cluster(s) and channel.\n\n    Args:\n        cluster (int | list): The cluster(s) to get the rate map for.\n        channel (int): The channel number.\n        **kwargs: Additional keyword arguments for the function.\n    \"\"\"\n    if isinstance(cluster, list):\n        self._plot_multiple_clusters(self.makeRateMap,\n                                     cluster,\n                                     channel,\n                                     **kwargs)\n    else:\n        ts = self.get_spike_times(channel, cluster)\n        self.makeRateMap(ts, **kwargs)\n    plt.show()\n</code></pre>"},{"location":"reference/#ephysiopy.visualise.plotting.FigureMaker.sac","title":"<code>sac(cluster, channel, **kwargs)</code>","text":"<p>Gets the spatial autocorrelation for the specified cluster(s) and channel.</p> <p>Parameters:</p> Name Type Description Default <code>cluster</code> <code>int | list</code> <p>The cluster(s) to get the spatial autocorrelation for.</p> required <code>channel</code> <code>int</code> <p>The channel number.</p> required <code>**kwargs</code> <p>Additional keyword arguments for the function.</p> <code>{}</code> Source code in <code>ephysiopy/visualise/plotting.py</code> <pre><code>def sac(self, cluster: int | list, channel: int, **kwargs):\n    \"\"\"\n    Gets the spatial autocorrelation for the specified cluster(s) and\n    channel.\n\n    Args:\n        cluster (int | list): The cluster(s) to get the spatial\n            autocorrelation for.\n        channel (int): The channel number.\n        **kwargs: Additional keyword arguments for the function.\n    \"\"\"\n    if isinstance(cluster, list):\n        self._plot_multiple_clusters(self.makeSAC,\n                                     cluster,\n                                     channel,\n                                     **kwargs)\n    else:\n        ts = self.get_spike_times(channel, cluster)\n        self.makeSAC(ts, **kwargs)\n    plt.show()\n</code></pre>"},{"location":"reference/#ephysiopy.visualise.plotting.FigureMaker.show_SAC","title":"<code>show_SAC(A, inDict, ax=None, **kwargs)</code>","text":"<p>Displays the result of performing a spatial autocorrelation (SAC) on a grid cell.</p> <p>Uses the dictionary containing measures of the grid cell SAC to make a pretty picture</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>array</code> <p>The spatial autocorrelogram.</p> required <code>inDict</code> <code>dict</code> <p>The dictionary calculated in getmeasures.</p> required <code>ax</code> <code>axes</code> <p>If given the plot will get drawn in these axes. Default None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments for the function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>axes</code> <p>matplotlib.axes: The axes with the plot.</p> See Also <p>ephysiopy.common.binning.RateMap.autoCorr2D() ephysiopy.common.ephys_generic.FieldCalcs.getMeaures()</p> Source code in <code>ephysiopy/visualise/plotting.py</code> <pre><code>@stripAxes\ndef show_SAC(\n    self, A: np.array, inDict: dict, ax: matplotlib.axes = None, **kwargs\n) -&gt; matplotlib.axes:\n    \"\"\"\n    Displays the result of performing a spatial autocorrelation (SAC)\n    on a grid cell.\n\n    Uses the dictionary containing measures of the grid cell SAC to\n    make a pretty picture\n\n    Args:\n        A (np.array): The spatial autocorrelogram.\n        inDict (dict): The dictionary calculated in getmeasures.\n        ax (matplotlib.axes, optional): If given the plot will get drawn\n            in these axes. Default None.\n        **kwargs: Additional keyword arguments for the function.\n\n    Returns:\n        matplotlib.axes: The axes with the plot.\n\n    See Also:\n        ephysiopy.common.binning.RateMap.autoCorr2D()\n        ephysiopy.common.ephys_generic.FieldCalcs.getMeaures()\n    \"\"\"\n    if ax is None:\n        fig = plt.figure()\n        ax = fig.add_subplot(111)\n    Am = A.copy()\n    Am[~inDict[\"dist_to_centre\"]] = np.nan\n    Am = np.ma.masked_invalid(np.atleast_2d(Am))\n    x, y = np.meshgrid(np.arange(0, np.shape(A)[1]),\n                       np.arange(0, np.shape(A)[0]))\n    vmax = np.nanmax(np.ravel(A))\n    ax.pcolormesh(x, y, A, cmap=grey_cmap, edgecolors=\"face\",\n                  vmax=vmax, shading=\"auto\")\n    import copy\n\n    cmap = copy.copy(jet_cmap)\n    cmap.set_bad(\"w\", 0)\n    ax.pcolormesh(x, y, Am, cmap=cmap,\n                  edgecolors=\"face\", vmax=vmax, shading=\"auto\")\n    # horizontal green line at 3 o'clock\n    _y = (np.shape(A)[0] / 2, np.shape(A)[0] / 2)\n    _x = (np.shape(A)[1] / 2, np.shape(A)[0])\n    ax.plot(_x, _y, c=\"g\")\n    mag = inDict[\"scale\"] * 0.5\n    th = np.linspace(0, inDict[\"orientation\"], 50)\n    from ephysiopy.common.utils import rect\n\n    [x, y] = rect(mag, th, deg=1)\n    # angle subtended by orientation\n    ax.plot(\n        x + (inDict[\"dist_to_centre\"].shape[1] / 2),\n        (inDict[\"dist_to_centre\"].shape[0] / 2) - y,\n        \"r\",\n        **kwargs\n    )\n    # plot lines from centre to peaks above middle\n    for p in inDict[\"closest_peak_coords\"]:\n        if p[0] &lt;= inDict[\"dist_to_centre\"].shape[0] / 2:\n            ax.plot(\n                (inDict[\"dist_to_centre\"].shape[1] / 2, p[1]),\n                (inDict[\"dist_to_centre\"].shape[0] / 2, p[0]),\n                \"k\",\n                **kwargs\n            )\n    ax.invert_yaxis()\n    all_ax = ax.axes\n    all_ax.set_aspect(\"equal\")\n    all_ax.set_xlim((0.5, inDict[\"dist_to_centre\"].shape[1] - 1.5))\n    all_ax.set_ylim((inDict[\"dist_to_centre\"].shape[0] - 0.5, -0.5))\n    return ax\n</code></pre>"},{"location":"reference/#ephysiopy.visualise.plotting.FigureMaker.speed_v_hd","title":"<code>speed_v_hd(cluster, channel, **kwargs)</code>","text":"<p>Gets the speed versus head direction plot for the specified cluster(s) and channel.</p> <p>Parameters:</p> Name Type Description Default <code>cluster</code> <code>int | list</code> <p>The cluster(s) to get the speed versus head direction plot for.</p> required <code>channel</code> <code>int</code> <p>The channel number.</p> required <code>**kwargs</code> <p>Additional keyword arguments for the function.</p> <code>{}</code> Source code in <code>ephysiopy/visualise/plotting.py</code> <pre><code>def speed_v_hd(self, cluster: int | list, channel: int, **kwargs):\n    \"\"\"\n    Gets the speed versus head direction plot for the specified cluster(s)\n    and channel.\n\n    Args:\n        cluster (int | list): The cluster(s) to get the speed versus head\n            direction plot for.\n        channel (int): The channel number.\n        **kwargs: Additional keyword arguments for the function.\n    \"\"\"\n    if isinstance(cluster, list):\n        self._plot_multiple_clusters(self.makeSpeedVsHeadDirectionPlot,\n                                     cluster,\n                                     channel,\n                                     **kwargs)\n    else:\n        ts = self.get_spike_times(channel, cluster)\n        self.makeSpeedVsHeadDirectionPlot(ts, **kwargs)\n    plt.show()\n</code></pre>"},{"location":"reference/#ephysiopy.visualise.plotting.FigureMaker.speed_v_rate","title":"<code>speed_v_rate(cluster, channel, **kwargs)</code>","text":"<p>Gets the speed versus rate plot for the specified cluster(s) and channel.</p> <p>Parameters:</p> Name Type Description Default <code>cluster</code> <code>int | list</code> <p>The cluster(s) to get the speed versus rate plot for.</p> required <code>channel</code> <code>int</code> <p>The channel number.</p> required <code>**kwargs</code> <p>Additional keyword arguments for the function.</p> <code>{}</code> Source code in <code>ephysiopy/visualise/plotting.py</code> <pre><code>def speed_v_rate(self, cluster: int | list, channel: int, **kwargs):\n    \"\"\"\n    Gets the speed versus rate plot for the specified cluster(s) and\n    channel.\n\n    Args:\n        cluster (int | list): The cluster(s) to get the speed versus rate\n            plot for.\n        channel (int): The channel number.\n        **kwargs: Additional keyword arguments for the function.\n    \"\"\"\n    if isinstance(cluster, list):\n        self._plot_multiple_clusters(self.makeSpeedVsRatePlot,\n                                     cluster,\n                                     channel,\n                                     **kwargs)\n    else:\n        ts = self.get_spike_times(channel, cluster)\n        self.makeSpeedVsRatePlot(ts, **kwargs)\n    plt.show()\n</code></pre>"},{"location":"reference/#ephysiopy.visualise.plotting.FigureMaker.spike_path","title":"<code>spike_path(cluster=None, channel=None, **kwargs)</code>","text":"<p>Gets the spike path for the specified cluster(s) and channel.</p> <p>Parameters:</p> Name Type Description Default <code>cluster</code> <code>int | list | None</code> <p>The cluster(s) to get the spike path for.</p> <code>None</code> <code>channel</code> <code>int | None</code> <p>The channel number.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments for the function.</p> <code>{}</code> Source code in <code>ephysiopy/visualise/plotting.py</code> <pre><code>def spike_path(self, cluster=None, channel=None, **kwargs):\n    \"\"\"\n    Gets the spike path for the specified cluster(s) and channel.\n\n    Args:\n        cluster (int | list | None): The cluster(s) to get the spike path\n            for.\n        channel (int | None): The channel number.\n        **kwargs: Additional keyword arguments for the function.\n    \"\"\"\n    if isinstance(cluster, list):\n        self._plot_multiple_clusters(self.makeSpikePathPlot,\n                                     cluster,\n                                     channel,\n                                     **kwargs)\n    else:\n        if channel is not None and cluster is not None:\n            ts = self.get_spike_times(channel, cluster)\n        else:\n            ts = None\n        self.makeSpikePathPlot(ts, **kwargs)\n    plt.show()\n</code></pre>"},{"location":"reference/#binning-up-data","title":"Binning up data","text":""},{"location":"reference/#ephysiopy.common.binning.RateMap","title":"<code>RateMap</code>","text":"<p>               Bases: <code>object</code></p> <p>Bins up positional data (xy, head direction etc) and produces rate maps of the relevant kind. This is a generic class meant to be independent of any particular recording format.</p> <p>Parameters:</p> Name Type Description Default <code>xy</code> <code>ndarray</code> <p>The xy data, usually given as a 2 x n sample numpy array.</p> <code>None</code> <code>hdir</code> <code>ndarray</code> <p>The head direction data, usually a 1 x n sample numpy array.</p> <code>None</code> <code>speed</code> <code>ndarray</code> <p>Similar to hdir.</p> <code>None</code> <code>pos_weights</code> <code>ndarray</code> <p>A 1D numpy array n samples long which is used to weight a particular position sample when binning data. For example, if there were 5 positions recorded and a cell spiked once in position 2 and 5 times in position 3 and nothing anywhere else then pos_weights looks like: [0 0 1 5 0] In the case of binning up position this will be an array of mostly 1's unless there are some positions you want excluded for some reason.</p> <code>None</code> <code>ppm</code> <code>int</code> <p>Pixels per metre. Specifies how many camera pixels per metre so this, in combination with cmsPerBin, will determine how many bins there are in the rate map. Defaults to None.</p> <code>430</code> <code>xyInCms</code> <code>bool</code> <p>Whether the positional data is in cms. Defaults to False.</p> <code>False</code> <code>cmsPerBin</code> <code>int</code> <p>How many cms on a side each bin is in a rate map OR the number of degrees per bin in the case of directional binning. Defaults to 3.</p> required <code>smooth_sz</code> <code>int</code> <p>The width of the smoothing kernel for smoothing rate maps. Defaults to 5.</p> <code>5</code> Notes <p>There are several instance variables you can set, see below.</p> Source code in <code>ephysiopy/common/binning.py</code> <pre><code>class RateMap(object):\n    \"\"\"\n    Bins up positional data (xy, head direction etc) and produces rate maps\n    of the relevant kind. This is a generic class meant to be independent of\n    any particular recording format.\n\n    Args:\n        xy (ndarray): The xy data, usually given as a 2 x n sample numpy array.\n        hdir (ndarray): The head direction data, usually a 1 x n sample numpy array.\n        speed (ndarray): Similar to hdir.\n        pos_weights (ndarray): A 1D numpy array n samples long which is used to weight a particular\n            position sample when binning data. For example, if there were 5\n            positions recorded and a cell spiked once in position 2 and 5 times\n            in position 3 and nothing anywhere else then pos_weights looks like:\n            [0 0 1 5 0]\n            In the case of binning up position this will be an array of mostly 1's\n            unless there are some positions you want excluded for some reason.\n        ppm (int, optional): Pixels per metre. Specifies how many camera pixels per metre so this,\n            in combination with cmsPerBin, will determine how many bins there are\n            in the rate map. Defaults to None.\n        xyInCms (bool, optional): Whether the positional data is in cms. Defaults to False.\n        cmsPerBin (int, optional): How many cms on a side each bin is in a rate map OR the number of\n            degrees per bin in the case of directional binning. Defaults to 3.\n        smooth_sz (int, optional): The width of the smoothing kernel for smoothing rate maps. Defaults to 5.\n\n    Notes:\n        There are several instance variables you can set, see below.\n    \"\"\"\n\n    def __init__(\n        self,\n        xy: np.array = None,\n        hdir: np.array = None,\n        speed: np.array = None,\n        pos_weights: np.array = None,\n        pos_times: np.array = None,\n        ppm: int = 430,\n        xyInCms: bool = False,\n        binsize: int = 3,\n        smooth_sz: int = 5,\n    ):\n        self.xy = xy\n        self.dir = hdir\n        self.speed = speed\n        self._pos_weights = pos_weights\n        self._pos_times = pos_times\n        self._pos_time_splits = None\n        self._spike_weights = None\n        self._ppm = ppm  # pixels per metre\n        self._binsize = binsize\n        self._inCms = xyInCms\n        self._nBins = None\n        self._binedges = None  # has setter and getter - see below\n        self._x_lims = None\n        self._y_lims = None\n        self._smooth_sz = smooth_sz\n        self._smoothingType = \"gaussian\"  # 'boxcar' or 'gaussian'\n        self.whenToSmooth = \"before\"  # or 'after'\n        self._var2Bin = VariableToBin.XY\n        self._mapType = MapType.RATE\n        self._calcBinEdges()\n\n    @property\n    def inCms(self):\n        # Whether the units are in cms or not\n        return self._inCms\n\n    @inCms.setter\n    def inCms(self, value):\n        self._inCms = value\n\n    @property\n    def ppm(self):\n        # Get the current pixels per metre (ppm)\n        return self._ppm\n\n    @ppm.setter\n    def ppm(self, value):\n        self._ppm = value\n        # self._binedges = self._calcBinEdges(self.binsize)\n\n    @property\n    def var2Bin(self):\n        return self._var2Bin\n\n    @var2Bin.setter\n    def var2Bin(self, value):\n        self._var2Bin = value\n\n    @property\n    def mapType(self):\n        return self._mapType\n\n    @mapType.setter\n    def mapType(self, value):\n        self._mapType = value\n\n    @property\n    def nBins(self):\n        '''\n        The number of bins for each dim\n        '''\n        if self.binsize:\n            return len(self._binedges[0]), len(self._binedges[1])\n        else:\n            return None\n\n    @nBins.setter\n    def nBins(self, value):\n        '''\n        Sets the number of bins\n        '''\n        if self.var2Bin == VariableToBin.XY:\n            x_lims, y_lims = self._getXYLimits()\n            if isinstance(value, int):\n                value = [value]\n            if len(value) == 1:\n                _x, bs_x = np.linspace(x_lims[0],\n                                       x_lims[1],\n                                       int(value[0]),\n                                       retstep=True)\n                _y, bs_y = np.linspace(y_lims[0],\n                                       y_lims[1],\n                                       int(value[0]),\n                                       retstep=True)\n            elif len(value) == 2:\n                _x, bs_x = np.linspace(x_lims[0],\n                                       x_lims[1],\n                                       int(value[0]),\n                                       retstep=True)\n                _y, bs_y = np.linspace(y_lims[0],\n                                       y_lims[1],\n                                       int(value[1]),\n                                       retstep=True)\n            self._binedges = _y, _x\n            self.binsize = np.mean([bs_x, bs_y])\n        elif self.var2Bin == VariableToBin.DIR:\n            self._binedges, binsize = np.linspace(0,\n                                                  360 + self.binsize,\n                                                  value[0],\n                                                  retstep=True)\n            self.binsize = binsize\n        elif self.var2Bin == VariableToBin.SPEED:\n            maxspeed = np.max(self.speed)\n            self._binedges, binsize = np.linspace(0,\n                                                  maxspeed,\n                                                  value[0],\n                                                  retstep=True)\n            self.binsize = binsize\n\n    @property\n    def binedges(self):\n        return self._binedges\n\n    @binedges.setter\n    def binedges(self, value):\n        self._binedges = value\n\n    @property\n    def x_lims(self):\n        return self._x_lims\n\n    @x_lims.setter\n    def x_lims(self, value):\n        self._x_lims = value\n\n    @property\n    def y_lims(self):\n        return self._y_lims\n\n    @y_lims.setter\n    def y_lims(self, value):\n        self._y_lims = value\n\n    @property\n    def pos_weights(self):\n        \"\"\"\n        The 'weights' used as an argument to np.histogram* for binning up\n        position\n        Mostly this is just an array of 1's equal to the length of the pos\n        data, but usefully can be adjusted when masking data in the trial\n        by\n        \"\"\"\n        if self._pos_weights is None:\n            self._pos_weights = np.ones(self.xy.shape[1])\n        return self._pos_weights\n\n    @pos_weights.setter\n    def pos_weights(self, value):\n        self._pos_weights = value\n\n    @property\n    def pos_times(self):\n        return self._pos_times\n\n    @pos_times.setter\n    def pos_times(self, value):\n        self._pos_times = value\n\n    @property\n    def pos_time_splits(self):\n        return self._pos_times\n\n    @pos_time_splits.setter\n    def pos_time_splits(self, value):\n        self._pos_times = value\n\n    @property\n    def spike_weights(self):\n        return self._spike_weights\n\n    @spike_weights.setter\n    def spike_weights(self, value):\n        self._spike_weights = value\n\n    @property\n    def binsize(self):\n        # The number of cms per bin of the binned up map\n        return self._binsize\n\n    @binsize.setter\n    def binsize(self, value):\n        self._binsize = value\n        self._binedges = self._calcBinEdges(value)\n\n    @property\n    def smooth_sz(self):\n        # The size of the smoothing window applied to the binned data\n        return self._smooth_sz\n\n    @smooth_sz.setter\n    def smooth_sz(self, value):\n        self._smooth_sz = value\n\n    @property\n    def smoothingType(self):\n        # The type of smoothing to do - legal values are 'boxcar' or 'gaussian'\n        return self._smoothingType\n\n    @smoothingType.setter\n    def smoothingType(self, value):\n        self._smoothingType = value\n\n    def _getXYLimits(self):\n        '''\n        Gets the min/max of the x/y data\n        '''\n        x_lims = getattr(self, \"x_lims\", None)\n        y_lims = getattr(self, \"y_lims\", None)\n        if x_lims is None:\n            x_lims = (np.nanmin(self.xy[0]), np.nanmax(self.xy[0]))\n        if y_lims is None:\n            y_lims = (np.nanmin(self.xy[1]), np.nanmax(self.xy[1]))\n        self.x_lims = x_lims\n        self.y_lims = y_lims\n        return x_lims, y_lims\n\n    def _calcBinDims(self):\n        try:\n            self._binDims = [len(b) for b in self._binedges]\n        except TypeError:\n            self._binDims = len(self._binedges)\n\n    def _calcBinEdges(self, binsize: int = 3) -&gt; tuple:\n        \"\"\"\n        Aims to get the right number of bins for the variable to be binned\n\n        Args:\n            binsize (int, optional): The number of cms per bin for XY OR degrees for DIR OR cm/s for SPEED. Defaults to 3.\n\n        Returns:\n            tuple: each member an array of bin edges\n        \"\"\"\n        if self.var2Bin.value == VariableToBin.DIR.value:\n            self.binedges = np.arange(0, 360 + binsize, binsize)\n        elif self.var2Bin.value == VariableToBin.SPEED.value:\n            maxspeed = np.max(self.speed)\n            # assume min speed = 0\n            self.binedges = np.arange(0, maxspeed, binsize)\n        elif self.var2Bin == VariableToBin.XY:\n            x_lims, y_lims = self._getXYLimits()\n            nxbins = int(np.ceil((x_lims[1]-x_lims[0])/binsize))\n            nybins = int(np.ceil((y_lims[1]-y_lims[0])/binsize))\n            _x = np.linspace(x_lims[0], x_lims[1], nxbins)\n            _y = np.linspace(y_lims[0], y_lims[1], nybins)\n            self.binedges = _y, _x\n        elif self.var2Bin == VariableToBin.XY_TIME:\n            if self._pos_time_splits is None:\n                raise ValueError(\"Need pos times to bin up XY_TIME\")\n            x_lims, y_lims = self._getXYLimits()\n            nxbins = int(np.ceil((x_lims[1]-x_lims[0])/binsize))\n            nybins = int(np.ceil((y_lims[1]-y_lims[0])/binsize))\n            _x = np.linspace(x_lims[0], x_lims[1], nxbins)\n            _y = np.linspace(y_lims[0], y_lims[1], nybins)\n            be = [_y, _x]\n            be.append(self.pos_time_splits)\n            self.binedges = be\n        self._calcBinDims()\n        return self.binedges\n\n    def getSpatialSparsity(self,\n                           spkWeights,\n                           sample_rate=50,\n                           **kwargs):\n        \"\"\"\n        Gets the spatial sparsity measure - closer to 1 means\n        sparser firing field.\n\n        References:\n            Skaggs, W.E., McNaughton, B.L., Wilson, M.A. &amp; Barnes, C.A.\n            Theta phase precession in hippocampal neuronal populations\n            and the compression of temporal sequences.\n            Hippocampus 6, 149\u2013172 (1996).\n        \"\"\"\n        self.var2Bin = VariableToBin.XY\n        self._calcBinEdges()\n        sample = self.xy\n        keep_these = np.isfinite(sample[0])\n        pos, _ = self._binData(sample,\n                               self._binedges,\n                               self.pos_weights,\n                               keep_these)\n        npos = len(self.dir)\n        p_i = np.count_nonzero(pos) / npos / sample_rate\n        spk, _ = self._binData(sample,\n                               self._binedges,\n                               spkWeights,\n                               keep_these)\n        res = 1-(np.nansum(p_i*spk)**2) / np.nansum(p_i*spk**2)\n        return res\n\n    def getMap(self, spkWeights,\n               varType=VariableToBin.XY,\n               mapType=MapType.RATE,\n               smoothing=True,\n               **kwargs):\n        \"\"\"\n        Bins up the variable type varType and returns a tuple of\n        (rmap, binnedPositionDir) or\n        (rmap, binnedPostionX, binnedPositionY)\n\n        Args:\n            spkWeights (array_like): Shape equal to number of positions samples captured and consists of\n                position weights. For example, if there were 5 positions\n                recorded and a cell spiked once in position 2 and 5 times in\n                position 3 and nothing anywhere else then pos_weights looks\n                like: [0 0 1 5 0]\n            varType (Enum value - see Variable2Bin defined at top of this file): The variable to bin up. Legal values are: XY, DIR and SPEED\n            mapType (enum value - see MapType defined at top of this file): If RATE then the binned up spikes are divided by varType.\n                Otherwise return binned up position. Options are RATE or POS\n            smoothing (bool, optional): Whether to smooth the data or not. Defaults to True.\n\n        Returns:\n            binned_data, binned_pos (tuple): This is either a 2-tuple or a 3-tuple depening on whether binned\n                pos (mapType 'pos') or binned spikes (mapType 'rate') is asked\n                for respectively\n        \"\"\"\n        if varType.value == VariableToBin.DIR.value:\n            sample = self.dir\n            keep_these = np.isfinite(sample)\n        elif varType.value == VariableToBin.SPEED.value:\n            sample = self.speed\n            keep_these = np.isfinite(sample)\n        elif varType.value == VariableToBin.XY.value:\n            sample = self.xy\n            keep_these = np.isfinite(sample[0])\n        elif varType.value == VariableToBin.XY_TIME.value:\n            sample = np.concatenate((np.atleast_2d(self.xy),\n                                    np.atleast_2d(self.pos_times)))\n            keep_these = np.isfinite(self.xy[0])\n        else:\n            raise ValueError(\"Unrecognized variable to bin.\")\n        assert sample is not None\n\n        self.var2Bin = varType\n        self._spike_weights = spkWeights\n        self._calcBinEdges(self.binsize)\n\n        binned_pos, binned_pos_edges = self._binData(\n            sample,\n            self._binedges,\n            self.pos_weights,\n            keep_these)\n        nanIdx = binned_pos == 0\n\n        if mapType.value == MapType.POS.value:  # return binned up position\n            if smoothing:\n                if varType.value == VariableToBin.DIR.value:\n                    binned_pos = self._circPadSmooth(\n                        binned_pos, n=self.smooth_sz)\n                else:\n                    binned_pos = blurImage(binned_pos,\n                                           self.smooth_sz,\n                                           ftype=self.smoothingType,\n                                           **kwargs)\n            return binned_pos, binned_pos_edges\n\n        binned_spk, _ = self._binData(\n            sample, self._binedges, spkWeights, keep_these)\n        if mapType.value == MapType.SPK:\n            return binned_spk\n        # binned_spk is returned as a tuple of the binned data and the bin\n        # edges\n        if \"after\" in self.whenToSmooth:\n            rmap = binned_spk / binned_pos\n            if varType.value == VariableToBin.DIR.value:\n                rmap = self._circPadSmooth(rmap, self.smooth_sz)\n            else:\n                rmap = blurImage(rmap,\n                                 self.smooth_sz,\n                                 ftype=self.smoothingType,\n                                 **kwargs)\n        else:  # default case\n            if not smoothing:\n                return binned_spk / binned_pos, binned_pos_edges\n            if varType.value == VariableToBin.DIR.value:\n                binned_pos = self._circPadSmooth(binned_pos, self.smooth_sz)\n                binned_spk = self._circPadSmooth(binned_spk, self.smooth_sz)\n                rmap = binned_spk / binned_pos\n            else:\n                binned_pos = blurImage(binned_pos,\n                                       self.smooth_sz,\n                                       ftype=self.smoothingType,\n                                       **kwargs)\n                if binned_spk.ndim == 2:\n                    pass\n                elif binned_spk.ndim == 1:\n                    binned_spk_tmp = np.zeros(\n                        [binned_spk.shape[0], binned_spk.shape[0], 1]\n                    )\n                    for i in range(binned_spk.shape[0]):\n                        binned_spk_tmp[i, :, :] = binned_spk[i]\n                    binned_spk = binned_spk_tmp\n                binned_spk = blurImage(\n                    binned_spk,\n                    self.smooth_sz,\n                    ftype=self.smoothingType,\n                    **kwargs)\n                rmap = binned_spk / binned_pos\n                if rmap.ndim &lt;= 2:\n                    rmap[nanIdx] = np.nan\n\n        return rmap, binned_pos_edges\n\n    def getSAC(self, spkWeights, **kwargs):\n        '''\n        Returns the SAC - convenience function\n        '''\n        rmap = self.getMap(spkWeights=spkWeights, **kwargs)\n        nodwell = ~np.isfinite(rmap[0])\n        return self.autoCorr2D(rmap[0], nodwell)\n\n    def _binData(self, var, bin_edges, weights, good_indices=None):\n        \"\"\"\n        Bins data taking account of possible multi-dimensionality\n\n        Args:\n            var (array_like): The variable to bin\n            bin_edges (array_like): The edges of the data - see numpys histogramdd for more\n            weights (array_like): The weights attributed to the samples in var\n            good_indices (array_like): Valid indices (i.e. not nan and not infinite)\n\n        Returns:\n            ndhist (2-tuple): Think this always returns a two-tuple of the binned variable and\n                the bin edges - need to check to be sure...\n\n        Notes:\n            This breaks compatability with numpys histogramdd\n            In the 2d histogram case below I swap the axes around so that x and y\n            are binned in the 'normal' format i.e. so x appears horizontally and y\n            vertically.\n            Multi-binning issue is dealt with awkwardly through checking\n            the dimensionality of the weights array.\n            'normally' this would be 1 dim but when multiple clusters are being\n            binned it will be 2 dim.\n            In that case np.apply_along_axis functionality is applied.\n            The spike weights in that case might be created like so:\n\n            &gt;&gt;&gt; spk_W = np.zeros(shape=[len(trial.nClusters), trial.npos])\n            &gt;&gt;&gt; for i, cluster in enumerate(trial.clusters):\n            &gt;&gt;&gt;\t\tx1 = trial.getClusterIdx(cluster)\n            &gt;&gt;&gt;\t\tspk_W[i, :] = np.bincount(x1, minlength=trial.npos)\n\n            This can then be fed into this fcn something like so:\n\n            &gt;&gt;&gt; rng = np.array((np.ma.min(\n                trial.POS.xy, 1).data, np.ma.max(rial.POS.xy, 1).data))\n            &gt;&gt;&gt; h = _binData(\n                var=trial.POS.xy, bin_edges=np.array([64, 64]),\n                weights=spk_W, rng=rng)\n\n            Returned will be a tuple containing the binned up data and\n            the bin edges for x and y (obv this will be the same for all\n            entries of h)\n        \"\"\"\n        if weights is None:\n            weights = np.ones_like(var)\n        dims = weights.ndim\n        if dims == 1 and var.ndim == 1:\n            var = var[np.newaxis, :]\n            # if self.var2Bin != VariableToBin.XY and len(bin_edges) != 1:\n            #     bin_edges = self._calcBinEdges(self.binsize)\n            bin_edges = bin_edges[np.newaxis, :]\n        elif dims &gt; 1 and var.ndim == 1:\n            var = var[np.newaxis, :]\n            bin_edges = bin_edges[np.newaxis, :]\n        else:\n            var = np.flipud(var)\n        weights = np.atleast_2d(weights)  # needed for list comp below\n        var = np.array(var.data.T.tolist())\n        ndhist = [np.histogramdd(\n            sample=var[good_indices],\n            bins=bin_edges,\n            weights=np.ravel(w[good_indices])) for w in weights]\n        if np.shape(weights)[0] == 1:\n            return ndhist[0][0], ndhist[0][1]\n        else:\n            tmp = [d[0] for d in ndhist]\n            tmp = np.array(tmp)\n            return tmp, ndhist[1]\n\n    def _circPadSmooth(self, var, n=3, ny=None):\n        \"\"\"\n        Smooths a vector by convolving with a gaussian\n        Mirror reflects the start and end of the vector to\n        deal with edge effects\n\n        Args:\n            var (array_like): The vector to smooth\n            n, ny (int): Size of the smoothing (sigma in gaussian)\n\n        Returns:\n            array_like: The smoothed vector with shape the same as var\n        \"\"\"\n\n        tn = len(var)\n        t2 = int(np.floor(tn / 2))\n        var = np.concatenate((var[t2:tn], var, var[0:t2]))\n        if ny is None:\n            ny = n\n        x, y = np.mgrid[-n: n + 1, 0 - ny: ny + 1]\n        g = np.exp(-(x**2 / float(n) + y**2 / float(ny)))\n        if np.ndim(var) == 1:\n            g = g[n, :]\n        g = g / g.sum()\n        improc = signal.convolve(var, g, mode=\"same\")\n        improc = improc[tn - t2: tn - t2 + tn]\n        return improc\n\n    def _circularStructure(self, radius):\n        \"\"\"\n        Generates a circular binary structure for use with morphological\n        operations such as ndimage.binary_dilation etc\n\n        This is only used in this implementation for adaptively binning\n        ratemaps for use with information theoretic measures (Skaggs etc)\n\n        Args:\n            radius (int): the size of the circular structure\n\n        Returns:\n            res (array_like): Binary structure with shape [(radius*2) + 1,(radius*2) + 1]\n\n        See Also:\n            RateMap.__adpativeMap\n        \"\"\"\n        from skimage.morphology import disk\n\n        return disk(radius)\n\n    def getAdaptiveMap(self, pos_binned, spk_binned, alpha=200):\n        \"\"\"\n        Produces a ratemap that has been adaptively binned according to the\n        algorithm described in Skaggs et al., 1996) [1]_.\n\n        Args:\n            pos_binned (array_like): The binned positional data. For example that returned from getMap\n                above with mapType as 'pos'\n            spk_binned (array_like): The binned spikes\n            alpha (int, optional): A scaling parameter determing the amount of occupancy to aim at\n                in each bin. Defaults to 200.\n\n        Returns:\n            Returns adaptively binned spike and pos maps. Use to generate Skaggs\n            information measure\n\n        Notes:\n            Positions with high rates mean proportionately less error than those\n            with low rates, so this tries to even the playing field. This type\n            of binning should be used for calculations of spatial info\n            as with the skaggs_info method in the fieldcalcs class (see below)\n            alpha is a scaling parameter that might need tweaking for different\n            data sets.\n            From the paper:\n                The data [are] first binned\n                into a 64 X 64 grid of spatial locations, and then the firing rate\n                at each point in this grid was calculated by expanding a circle\n                around the point until the following criterion was met:\n                    Nspks &gt; alpha / (Nocc^2 * r^2)\n                where Nspks is the number of spikes emitted in a circle of radius\n                r (in bins), Nocc is the number of occupancy samples, alpha is the\n                scaling parameter\n                The firing rate in the given bin is then calculated as:\n                    sample_rate * (Nspks / Nocc)\n\n        References:\n            .. [1] W. E. Skaggs, B. L. McNaughton, K. M. Gothard &amp; E. J. Markus\n                \"An Information-Theoretic Approach to Deciphering the Hippocampal\n                Code\"\n                Neural Information Processing Systems, 1993.\n        \"\"\"\n        #  assign output arrays\n        smthdPos = np.zeros_like(pos_binned)\n        smthdSpk = np.zeros_like(spk_binned)\n        smthdRate = np.zeros_like(pos_binned)\n        idx = pos_binned == 0\n        pos_binned[idx] = np.nan\n        spk_binned[idx] = np.nan\n        visited = np.zeros_like(pos_binned)\n        visited[pos_binned &gt; 0] = 1\n        # array to check which bins have made it\n        binCheck = np.isnan(pos_binned)\n        r = 1\n        while np.any(~binCheck):\n            # create the filter kernel\n            h = self._circularStructure(r)\n            h[h &gt;= np.max(h) / 3.0] = 1\n            h[h != 1] = 0\n            if h.shape &gt;= pos_binned.shape:\n                break\n            # filter the arrays using astropys convolution\n            filtPos = convolution.convolve(pos_binned, h, boundary=None)\n            filtSpk = convolution.convolve(spk_binned, h, boundary=None)\n            filtVisited = convolution.convolve(visited, h, boundary=None)\n            # get the bins which made it through this iteration\n            trueBins = alpha / (np.sqrt(filtSpk) * filtPos) &lt;= r\n            trueBins = np.logical_and(trueBins, ~binCheck)\n            # insert values where true\n            smthdPos[trueBins] = filtPos[trueBins] / filtVisited[trueBins]\n            smthdSpk[trueBins] = filtSpk[trueBins] / filtVisited[trueBins]\n            binCheck[trueBins] = True\n            r += 1\n        smthdRate = smthdSpk / smthdPos\n        smthdRate[idx] = np.nan\n        smthdSpk[idx] = np.nan\n        smthdPos[idx] = np.nan\n        return smthdRate, smthdSpk, smthdPos\n\n    def autoCorr2D(self, A, nodwell, tol=1e-10):\n        \"\"\"\n        Performs a spatial autocorrelation on the array A\n\n        Args:\n            A (array_like): Either 2 or 3D. In the former it is simply the binned up ratemap\n                where the two dimensions correspond to x and y.\n                If 3D then the first two dimensions are x\n                and y and the third (last dimension) is 'stack' of ratemaps\n            nodwell (array_like): A boolean array corresponding the bins in the ratemap that\n                weren't visited. See Notes below.\n            tol (float, optional): Values below this are set to zero to deal with v small values\n                thrown up by the fft. Default 1e-10\n\n        Returns:\n            sac (array_like): The spatial autocorrelation in the relevant dimensionality\n\n        Notes:\n            The nodwell input can usually be generated by:\n\n            &gt;&gt;&gt; nodwell = ~np.isfinite(A)\n        \"\"\"\n\n        assert np.ndim(A) == 2\n        m, n = np.shape(A)\n        o = 1\n        x = np.reshape(A, (m, n, o))\n        nodwell = np.reshape(nodwell, (m, n, o))\n        x[nodwell] = 0\n        # [Step 1] Obtain FFTs of x, the sum of squares and bins visited\n        Fx = np.fft.fft(np.fft.fft(x, 2 * m - 1, axis=0), 2 * n - 1, axis=1)\n        FsumOfSquares_x = np.fft.fft(\n            np.fft.fft(np.power(x, 2), 2 * m - 1, axis=0), 2 * n - 1, axis=1\n        )\n        Fn = np.fft.fft(\n            np.fft.fft(np.invert(nodwell).astype(int), 2 * m - 1, axis=0),\n            2 * n - 1,\n            axis=1,\n        )\n        # [Step 2] Multiply the relevant transforms and invert to obtain the\n        # equivalent convolutions\n        rawCorr = np.fft.fftshift(\n            np.real(np.fft.ifft(\n                np.fft.ifft(Fx * np.conj(Fx), axis=1), axis=0)),\n            axes=(0, 1),\n        )\n        sums_x = np.fft.fftshift(\n            np.real(np.fft.ifft(\n                np.fft.ifft(np.conj(Fx) * Fn, axis=1), axis=0)),\n            axes=(0, 1),\n        )\n        sumOfSquares_x = np.fft.fftshift(\n            np.real(\n                np.fft.ifft(\n                    np.fft.ifft(Fn * np.conj(FsumOfSquares_x), axis=1), axis=0)\n            ),\n            axes=(0, 1),\n        )\n        N = np.fft.fftshift(\n            np.real(np.fft.ifft(\n                np.fft.ifft(Fn * np.conj(Fn), axis=1), axis=0)),\n            axes=(0, 1),\n        )\n        # [Step 3] Account for rounding errors.\n        rawCorr[np.abs(rawCorr) &lt; tol] = 0\n        sums_x[np.abs(sums_x) &lt; tol] = 0\n        sumOfSquares_x[np.abs(sumOfSquares_x) &lt; tol] = 0\n        N = np.round(N)\n        N[N &lt;= 1] = np.nan\n        # [Step 4] Compute correlation matrix\n        mapStd = np.sqrt((sumOfSquares_x * N) - sums_x**2)\n        mapCovar = (rawCorr * N) - sums_x * \\\n            sums_x[::-1, :, :][:, ::-1, :][:, :, :]\n\n        return np.squeeze(\n            mapCovar / mapStd / mapStd[::-1, :, :][:, ::-1, :][:, :, :])\n\n    def crossCorr2D(self, A, B, A_nodwell, B_nodwell, tol=1e-10):\n        \"\"\"\n        Performs a spatial crosscorrelation between the arrays A and B\n\n        Args:\n            A, B (array_like): Either 2 or 3D. In the former it is simply the binned up ratemap\n                where the two dimensions correspond to x and y.\n                If 3D then the first two dimensions are x\n                and y and the third (last dimension) is 'stack' of ratemaps\n            nodwell_A, nodwell_B (array_like): A boolean array corresponding the bins in the ratemap that\n                weren't visited. See Notes below.\n            tol (float, optional): Values below this are set to zero to deal with v small values\n                thrown up by the fft. Default 1e-10\n\n        Returns:\n            sac (array_like): The spatial crosscorrelation in the relevant dimensionality\n\n        Notes:\n            The nodwell input can usually be generated by:\n\n            &gt;&gt;&gt; nodwell = ~np.isfinite(A)\n        \"\"\"\n        if np.ndim(A) != np.ndim(B):\n            raise ValueError(\"Both arrays must have the same dimensionality\")\n        assert np.ndim(A) == 2\n        ma, na = np.shape(A)\n        mb, nb = np.shape(B)\n        oa = ob = 1\n        A = np.reshape(A, (ma, na, oa))\n        B = np.reshape(B, (mb, nb, ob))\n        A_nodwell = np.reshape(A_nodwell, (ma, na, oa))\n        B_nodwell = np.reshape(B_nodwell, (mb, nb, ob))\n        A[A_nodwell] = 0\n        B[B_nodwell] = 0\n        # [Step 1] Obtain FFTs of x, the sum of squares and bins visited\n        Fa = np.fft.fft(np.fft.fft(A, 2 * mb - 1, axis=0), 2 * nb - 1, axis=1)\n        FsumOfSquares_a = np.fft.fft(\n            np.fft.fft(np.power(A, 2), 2 * mb - 1, axis=0), 2 * nb - 1, axis=1\n        )\n        Fn_a = np.fft.fft(\n            np.fft.fft(np.invert(A_nodwell).astype(int), 2 * mb - 1, axis=0),\n            2 * nb - 1,\n            axis=1,\n        )\n        Fb = np.fft.fft(np.fft.fft(B, 2 * ma - 1, axis=0), 2 * na - 1, axis=1)\n        FsumOfSquares_b = np.fft.fft(\n            np.fft.fft(np.power(B, 2), 2 * ma - 1, axis=0), 2 * na - 1, axis=1\n        )\n        Fn_b = np.fft.fft(\n            np.fft.fft(np.invert(B_nodwell).astype(int), 2 * ma - 1, axis=0),\n            2 * na - 1,\n            axis=1,\n        )\n        # [Step 2] Multiply the relevant transforms and invert to obtain the\n        # equivalent convolutions\n        rawCorr = np.fft.fftshift(\n            np.real(np.fft.ifft(np.fft.ifft(Fa * np.conj(Fb), axis=1), axis=0))\n        )\n        sums_a = np.fft.fftshift(\n            np.real(np.fft.ifft(np.fft.ifft(\n                Fa * np.conj(Fn_b), axis=1), axis=0))\n        )\n        sums_b = np.fft.fftshift(\n            np.real(np.fft.ifft(np.fft.ifft(\n                Fn_a * np.conj(Fb), axis=1), axis=0))\n        )\n        sumOfSquares_a = np.fft.fftshift(\n            np.real(\n                np.fft.ifft(\n                    np.fft.ifft(\n                        FsumOfSquares_a * np.conj(Fn_b), axis=1), axis=0\n                )\n            )\n        )\n        sumOfSquares_b = np.fft.fftshift(\n            np.real(\n                np.fft.ifft(\n                    np.fft.ifft(\n                        Fn_a * np.conj(FsumOfSquares_b), axis=1), axis=0\n                )\n            )\n        )\n        N = np.fft.fftshift(\n            np.real(np.fft.ifft(np.fft.ifft(\n                Fn_a * np.conj(Fn_b), axis=1), axis=0))\n        )\n        # [Step 3] Account for rounding errors.\n        rawCorr[np.abs(rawCorr) &lt; tol] = 0\n        sums_a[np.abs(sums_a) &lt; tol] = 0\n        sums_b[np.abs(sums_b) &lt; tol] = 0\n        sumOfSquares_a[np.abs(sumOfSquares_a) &lt; tol] = 0\n        sumOfSquares_b[np.abs(sumOfSquares_b) &lt; tol] = 0\n        N = np.round(N)\n        N[N &lt;= 1] = np.nan\n        # [Step 4] Compute correlation matrix\n        mapStd_a = np.sqrt((sumOfSquares_a * N) - sums_a**2)\n        mapStd_b = np.sqrt((sumOfSquares_b * N) - sums_b**2)\n        mapCovar = (rawCorr * N) - sums_a * sums_b\n\n        return np.squeeze(mapCovar / (mapStd_a * mapStd_b))\n\n    def tWinSAC(\n        self,\n        xy,\n        spkIdx,\n        ppm=365,\n        winSize=10,\n        pos_sample_rate=50,\n        nbins=71,\n        boxcar=5,\n        Pthresh=100,\n        downsampfreq=50,\n        plot=False,\n    ):\n        \"\"\"\n        Temporal windowed spatial autocorrelation.\n\n        Args:\n            xy (array_like): The position data\n            spkIdx (array_like): The indices in xy where the cell fired\n            ppm (int, optional): The camera pixels per metre. Default 365\n            winSize (int, optional): The window size for the temporal search\n            pos_sample_rate (int, optional): The rate at which position was sampled. Default 50\n            nbins (int, optional): The number of bins for creating the resulting ratemap. Default 71\n            boxcar (int, optional): The size of the smoothing kernel to smooth ratemaps. Default 5\n            Pthresh (int, optional): The cut-off for values in the ratemap; values &lt; Pthresh become nans. Default 100\n            downsampfreq (int, optional): How much to downsample. Default 50\n            plot (bool, optional): Whether to show a plot of the result. Default False\n\n        Returns:\n            H (array_like): The temporal windowed SAC\n        \"\"\"\n        # [Stage 0] Get some numbers\n        xy = xy / ppm * 100\n        n_samps = xy.shape[1]\n        n_spks = len(spkIdx)\n        winSizeBins = np.min([winSize * pos_sample_rate, n_samps])\n        # factor by which positions are downsampled\n        downsample = np.ceil(pos_sample_rate / downsampfreq)\n        Pthresh = Pthresh / downsample  # take account of downsampling\n\n        # [Stage 1] Calculate number of spikes in the window for each spikeInd\n        # (ignoring spike itself)\n        # 1a. Loop preparation\n        nSpikesInWin = np.zeros(n_spks, dtype=int)\n\n        # 1b. Keep looping until we have dealt with all spikes\n        for i, s in enumerate(spkIdx):\n            t = np.searchsorted(spkIdx, (s, s + winSizeBins))\n            nSpikesInWin[i] = len(spkIdx[t[0]: t[1]]) - 1  # ignore ith spike\n\n        # [Stage 2] Prepare for main loop\n        # 2a. Work out offset inidices to be used when storing spike data\n        off_spike = np.cumsum([nSpikesInWin])\n        off_spike = np.pad(off_spike, (1, 0), \"constant\", constant_values=(0))\n\n        # 2b. Work out number of downsampled pos bins in window and\n        # offset indices for storing data\n        nPosInWindow = np.minimum(winSizeBins, n_samps - spkIdx)\n        nDownsampInWin = np.floor((nPosInWindow - 1) / downsample) + 1\n\n        off_dwell = np.cumsum(nDownsampInWin.astype(int))\n        off_dwell = np.pad(off_dwell, (1, 0), \"constant\", constant_values=(0))\n\n        # 2c. Pre-allocate dwell and spike arrays, singles for speed\n        dwell = np.zeros((2, off_dwell[-1]), dtype=np.single) * np.nan\n        spike = np.zeros((2, off_spike[-1]), dtype=np.single) * np.nan\n\n        filled_pvals = 0\n        filled_svals = 0\n\n        for i in range(n_spks):\n            # calculate dwell displacements\n            winInd_dwell = np.arange(\n                spkIdx[i] + 1,\n                np.minimum(spkIdx[i] + winSizeBins, n_samps),\n                downsample,\n                dtype=int,\n            )\n            WL = len(winInd_dwell)\n            dwell[:, filled_pvals: filled_pvals + WL] = np.rot90(\n                np.array(np.rot90(xy[:, winInd_dwell]) - xy[:, spkIdx[i]])\n            )\n            filled_pvals = filled_pvals + WL\n            # calculate spike displacements\n            winInd_spks = (\n                i + np.nonzero(spkIdx[i + 1: n_spks] &lt;\n                               spkIdx[i] + winSizeBins)[0]\n            )\n            WL = len(winInd_spks)\n            spike[:, filled_svals: filled_svals + WL] = np.rot90(\n                np.array(\n                    np.rot90(xy[:, spkIdx[winInd_spks]]) - xy[:, spkIdx[i]])\n            )\n            filled_svals = filled_svals + WL\n\n        dwell = np.delete(dwell, np.isnan(dwell).nonzero()[1], axis=1)\n        spike = np.delete(spike, np.isnan(spike).nonzero()[1], axis=1)\n\n        dwell = np.hstack((dwell, -dwell))\n        spike = np.hstack((spike, -spike))\n\n        dwell_min = np.min(dwell, axis=1)\n        dwell_max = np.max(dwell, axis=1)\n\n        binsize = (dwell_max[1] - dwell_min[1]) / nbins\n\n        dwell = np.round(\n            (dwell - np.ones_like(dwell) * dwell_min[:, np.newaxis]) / binsize\n        )\n        spike = np.round(\n            (spike - np.ones_like(spike) * dwell_min[:, np.newaxis]) / binsize\n        )\n\n        binsize = np.max(dwell, axis=1).astype(int)\n        binedges = np.array(((-0.5, -0.5), binsize + 0.5)).T\n        Hp = np.histogram2d(dwell[0, :], dwell[1, :],\n                            range=binedges, bins=binsize)[0]\n        Hs = np.histogram2d(spike[0, :], spike[1, :],\n                            range=binedges, bins=binsize)[0]\n\n        # reverse y,x order\n        Hp = np.swapaxes(Hp, 1, 0)\n        Hs = np.swapaxes(Hs, 1, 0)\n\n        fHp = blurImage(Hp, boxcar)\n        fHs = blurImage(Hs, boxcar)\n\n        H = fHs / fHp\n        H[Hp &lt; Pthresh] = np.nan\n\n        return H\n\n    @cache\n    def _create_boundary_distance_lookup(self,\n                                         arena_boundary: MultiLineString,\n                                         degs_per_bin: float,\n                                         xy_binsize: float,\n                                         **kwargs):\n        # Now we generate lines radiating out from a point as a\n        # multilinestring geometry collection - this looks\n        # like a 360/degs_per_bin\n        # star. We will move this to each valid location in the position map\n        # and then calculate the distance to the nearest intersection with the\n        # arena boundary.\n        # get the arena boundaries to figure out the radius of the arena,\n        # regardless of its actual shape\n        x1, y1, x2, y2 = arena_boundary.bounds\n        radius = max(x2-x1, y2-y1)/2\n        startpoint = Point((x1+radius, y1+radius))\n        endpoint = Point([x2, y1+radius])\n        angles = np.arange(0, 360, degs_per_bin)\n        lines = MultiLineString(\n            [rotate(LineString([startpoint, endpoint]), ang, origin=startpoint)\n             for ang in angles])\n        prepare(lines)\n        # arena centre\n        cx = x1 + radius\n        cy = y1 + radius\n        # get the position map and the valid locations within it\n        pos_map, (ybin_edges, xbin_edges) = self.getMap(np.ones_like(self.dir),\n                                                        varType=VariableToBin.XY,\n                                                        mapType=MapType.POS,\n                                                        smoothing=False)\n        yvalid, xvalid = np.nonzero(~np.isnan(pos_map))\n\n        # preallocate the array to hold distances\n        distances = np.full(\n            (len(xbin_edges), len(ybin_edges), len(angles)), np.nan)\n\n        # Now iterate through valid locations in the pos map and calculate the\n        # distances and the indices of the lines that intersect with the\n        # arena boundary. The indices are equivalent to the angle of the\n        # line in the lines geometry collection. This iteration is a bit slow\n        # but it will only need to be done once per session as it's creating\n        # a lookup table for the distances\n        for xi, yi in zip(xvalid, yvalid):\n            i_point = Point((xbin_edges[xi]+xy_binsize,\n                             ybin_edges[yi]+xy_binsize))\n            ipx, ipy = i_point.xy\n            new_point = Point(cx-ipx[0], cy-ipy[0])\n            t_arena = translate(arena_boundary, -new_point.x, -new_point.y)\n            prepare(t_arena)\n            di = [(startpoint.distance(t_arena.intersection(line)), idx)\n                  for idx, line in enumerate(lines.geoms) if\n                  t_arena.intersects(line)]\n            d, i = zip(*di)\n            distances[xi, yi, i] = d\n        return distances\n\n    def get_egocentric_boundary_map(self,\n                                    spk_weights,\n                                    degs_per_bin: float = 3,\n                                    xy_binsize: float = 2.5,\n                                    arena_type: str = \"circle\",\n                                    return_dists: bool = False,\n                                    return_raw_spk: bool = False,\n                                    return_raw_occ: bool = False) -&gt; namedtuple:\n        \"\"\"\n        Helps construct dwell time/spike counts maps with respect to boundaries at given egocentric directions and distances.\n\n        Note:\n            For the directional input, the 0 degree reference is horizontal pointing East and moves counter-clockwise.\n        \"\"\"\n        assert self.dir is not None, \"No direction data available\"\n        # initially do some binning to get valid locations\n        # (some might be nans due to\n        # arena shape and/or poor sampling) and then digitize\n        # the x and y positions\n        # and the angular positions\n        self.binsize = xy_binsize  # this will trigger a\n        # re-calculation of the bin edges\n\n        angles = np.arange(0, 360, degs_per_bin)\n\n        # Use the shaeply package to specify some geometry for the arena\n        # boundary and the lines radiating out\n        # from the current location of the animal. The geometry for the\n        # arena should be user specified but for now I'll just use a circle\n        if arena_type == \"circle\":\n            radius = 50\n            circle_centre = Point(\n                np.nanmin(self.xy[0])+radius, np.nanmin(self.xy[1])+radius)\n            arena_boundary = circle_centre.buffer(radius).boundary\n        # now we have a circle with its centre at the centre of the arena\n        # i.e. the circle defines the arena edges. Calling .boundary on the\n        # circle geometry actually gives us a 65-gon polygon\n        distances = self._create_boundary_distance_lookup(\n            arena_boundary, degs_per_bin, xy_binsize)\n        # iterate through the digitized locations (x/y and angular), using the\n        # lookup table to get the distances to the arena boundary and then\n        # increment the appropriate bin in the egocentric boundary map\n        good_idx = np.isfinite(self.xy[0])\n        xy_by_heading, _ = np.histogramdd([self.xy[0][good_idx],\n                                           self.xy[1][good_idx],\n                                           self.dir[good_idx]],\n                                          bins=distances.shape,\n                                          weights=self.pos_weights[good_idx])\n        spk_xy_by_hd, _ = np.histogramdd([self.xy[0][good_idx],\n                                          self.xy[1][good_idx],\n                                          self.dir[good_idx]],\n                                         bins=distances.shape,\n                                         weights=spk_weights[good_idx])\n        assert xy_by_heading.shape == distances.shape\n        distlist = []\n        anglist = []\n        spkdists = []\n        spkangs = []\n        for i_bin in np.ndindex(distances.shape[:2]):\n            i_dist = distances[i_bin]\n            valid_dist = np.isfinite(i_dist)\n            nonzero_bincounts = np.nonzero(xy_by_heading[i_bin])[0]\n            nonzero_spkbins = np.nonzero(spk_xy_by_hd[i_bin])[0]\n            for i_angle in nonzero_bincounts:\n                ego_angles = np.roll(angles, i_angle)[valid_dist]\n                n_repeats = xy_by_heading[i_bin][i_angle]\n                ego_angles_repeats = np.repeat(ego_angles, n_repeats)\n                dist_repeats = np.repeat(i_dist[valid_dist], n_repeats)\n                distlist.append(dist_repeats)\n                anglist.append(ego_angles_repeats)\n                if i_angle in nonzero_spkbins:\n                    n_repeats = spk_xy_by_hd[i_bin][i_angle]\n                    ego_angles_repeats = np.repeat(ego_angles, n_repeats)\n                    dist_repeats = np.repeat(i_dist[valid_dist], n_repeats)\n                    spkdists.append(dist_repeats)\n                    spkangs.append(ego_angles_repeats)\n        flat_angs = flatten_list(anglist)\n        flat_dists = flatten_list(distlist)\n        flat_spk_dists = flatten_list(spkdists)\n        flat_spk_angs = flatten_list(spkangs)\n        bins = [int(radius/xy_binsize), len(angles)]\n        ego_boundary_occ, _, _ = np.histogram2d(x=flat_dists, y=flat_angs,\n                                                bins=bins)\n        ego_boundary_spk, _, _ = np.histogram2d(x=flat_spk_dists,\n                                                y=flat_spk_angs,\n                                                bins=bins)\n        kernel = convolution.Gaussian2DKernel(5, x_size=3, y_size=5)\n        sm_occ = convolution.convolve(ego_boundary_occ,\n                                      kernel,\n                                      boundary='extend')\n        sm_spk = convolution.convolve(ego_boundary_spk,\n                                      kernel,\n                                      boundary='extend')\n        ego_boundary_map = sm_spk / sm_occ\n        EgoMap = namedtuple(\"EgoMap\", ['rmap', 'occ', 'spk', 'dists'],\n                            defaults=None)\n        em = EgoMap(None, None, None, None)\n        em = em._replace(rmap=ego_boundary_map)\n        if return_dists:\n            em = em._replace(dists=distances)\n        if return_raw_occ:\n            em = em._replace(occ=ego_boundary_occ)\n        if return_raw_spk:\n            em = em._replace(spk=ego_boundary_spk)\n        return em\n\n    def getAllSpikeWeights(self,\n                           spike_times: np.ndarray,\n                           spike_clusters: np.ndarray,\n                           pos_times: np.ndarray,\n                           **kwargs):\n        \"\"\"\n        Args:\n            spike_times (np.ndarray): Spike times in seconds\n            spike_clusters (np.ndarray): Cluster identity vector\n            pos_times (np.ndarray): The times at which position was captured in seconds\n\n        Returns:\n            np.ndarray: The bincounts with respect to position for each cluster. Shape of returned array will be nClusters x npos\n        \"\"\"\n        assert len(spike_clusters) == len(spike_times)\n        clusters = np.unique(spike_clusters)\n        npos = len(self.dir)\n        idx = np.searchsorted(pos_times, spike_times) - 1\n        weights = [np.bincount(idx[spike_clusters == c], minlength=npos)\n                   for c in clusters]\n        return np.array(weights)\n\n    def _splitStackedCorrelations(self, binned_data: list) -&gt; tuple:\n        '''\n        Takes in the result of doStackedCorrelations() and splits into\n        two arrays and returns these as a 2-tuple\n        '''\n        result = [(s[0][:, :, 0], s[0][:, :, 1]) for s in binned_data]\n        result = np.array(result)\n        return np.squeeze(result[:, 0, :, :]), np.squeeze(result[:, 1, :, :])\n\n    def doStackedCorrelations(self,\n                              spkW: np.ndarray,\n                              times: np.ndarray,\n                              splits: np.ndarray,\n                              var2bin: Enum = VariableToBin.XY,\n                              maptype: Enum = MapType.RATE,\n                              **kwargs):\n        \"\"\"\n        Returns a list of binned data where each item in the list\n        is the result of running np.histogramdd on a spatial\n        variable (xy, dir etc) and a temporal one at the same\n        time. The idea is to split the spatial variable into two\n        temporal halves based on the bin edges in 'splits' and\n        then to run correlations between the two halves and\n        furthermore to do this for all of the clusters that have\n        spike weights in 'spkW'. 'spkW' should be the result of\n        using getAllSpikeWeights().\n\n        Args:\n            spkW (np.ndarray): The result of calling getAllSpikeWeights()\n            times (np.ndarray): Position times in seconds\n            splits (np.ndarray): Where to split the data in seconds. Will\n                typically take the form (0, 100, 200) for\n                example which will give a split between 0-100\n                and 100-200 seconds\n            var2bin (Enum): The spatial variable to bin up\n            maptype (Enum): The type of map to produce\n        \"\"\"\n        if var2bin.value == VariableToBin.DIR.value:\n            sample = self.dir\n        elif var2bin.value == VariableToBin.SPEED.value:\n            sample = self.speed\n        elif var2bin.value == VariableToBin.XY.value:\n            sample = self.xy\n        else:\n            raise ValueError(\"Unrecognized variable to bin.\")\n        assert sample is not None\n        self.pos_time_splits = splits\n\n        sample = np.concatenate((np.atleast_2d(sample),\n                                np.atleast_2d(times)))\n        edges = [b for b in self._binedges][::-1]\n        edges.append(splits)\n        # bin pos\n        bp, bpe = np.histogramdd(sample.T, bins=edges)\n        map1_pos, map2_pos = np.squeeze(bp[:, :, 0]), np.squeeze(bp[:, :, 1])\n        # smooth position\n        map1_pos = blurImage(map1_pos, 7, ftype='gaussian')\n        map2_pos = blurImage(map2_pos, 7, ftype='gaussian')\n        # bin spk - ie the histogram is weighted by spike count\n        # in bin i\n        spk = [np.histogramdd(sample.T, bins=edges, weights=w)\n               for w in spkW]\n        map1_spk, map2_spk = self._splitStackedCorrelations(spk)\n        map1_sm_spk = np.array([blurImage(m, 7, ftype='gaussian')\n                                for m in map1_spk])\n        map2_sm_spk = np.array([blurImage(m, 7, ftype='gaussian')\n                                for m in map2_spk])\n        map1_rmaps = map1_sm_spk / map1_pos\n        map2_rmaps = map2_sm_spk / map2_pos\n        return map1_rmaps, map2_rmaps\n</code></pre>"},{"location":"reference/#ephysiopy.common.binning.RateMap.nBins","title":"<code>nBins</code>  <code>property</code> <code>writable</code>","text":"<p>The number of bins for each dim</p>"},{"location":"reference/#ephysiopy.common.binning.RateMap.pos_weights","title":"<code>pos_weights</code>  <code>property</code> <code>writable</code>","text":"<p>The 'weights' used as an argument to np.histogram* for binning up position Mostly this is just an array of 1's equal to the length of the pos data, but usefully can be adjusted when masking data in the trial by</p>"},{"location":"reference/#ephysiopy.common.binning.RateMap.autoCorr2D","title":"<code>autoCorr2D(A, nodwell, tol=1e-10)</code>","text":"<p>Performs a spatial autocorrelation on the array A</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>array_like</code> <p>Either 2 or 3D. In the former it is simply the binned up ratemap where the two dimensions correspond to x and y. If 3D then the first two dimensions are x and y and the third (last dimension) is 'stack' of ratemaps</p> required <code>nodwell</code> <code>array_like</code> <p>A boolean array corresponding the bins in the ratemap that weren't visited. See Notes below.</p> required <code>tol</code> <code>float</code> <p>Values below this are set to zero to deal with v small values thrown up by the fft. Default 1e-10</p> <code>1e-10</code> <p>Returns:</p> Name Type Description <code>sac</code> <code>array_like</code> <p>The spatial autocorrelation in the relevant dimensionality</p> Notes <p>The nodwell input can usually be generated by:</p> <p>nodwell = ~np.isfinite(A)</p> Source code in <code>ephysiopy/common/binning.py</code> <pre><code>def autoCorr2D(self, A, nodwell, tol=1e-10):\n    \"\"\"\n    Performs a spatial autocorrelation on the array A\n\n    Args:\n        A (array_like): Either 2 or 3D. In the former it is simply the binned up ratemap\n            where the two dimensions correspond to x and y.\n            If 3D then the first two dimensions are x\n            and y and the third (last dimension) is 'stack' of ratemaps\n        nodwell (array_like): A boolean array corresponding the bins in the ratemap that\n            weren't visited. See Notes below.\n        tol (float, optional): Values below this are set to zero to deal with v small values\n            thrown up by the fft. Default 1e-10\n\n    Returns:\n        sac (array_like): The spatial autocorrelation in the relevant dimensionality\n\n    Notes:\n        The nodwell input can usually be generated by:\n\n        &gt;&gt;&gt; nodwell = ~np.isfinite(A)\n    \"\"\"\n\n    assert np.ndim(A) == 2\n    m, n = np.shape(A)\n    o = 1\n    x = np.reshape(A, (m, n, o))\n    nodwell = np.reshape(nodwell, (m, n, o))\n    x[nodwell] = 0\n    # [Step 1] Obtain FFTs of x, the sum of squares and bins visited\n    Fx = np.fft.fft(np.fft.fft(x, 2 * m - 1, axis=0), 2 * n - 1, axis=1)\n    FsumOfSquares_x = np.fft.fft(\n        np.fft.fft(np.power(x, 2), 2 * m - 1, axis=0), 2 * n - 1, axis=1\n    )\n    Fn = np.fft.fft(\n        np.fft.fft(np.invert(nodwell).astype(int), 2 * m - 1, axis=0),\n        2 * n - 1,\n        axis=1,\n    )\n    # [Step 2] Multiply the relevant transforms and invert to obtain the\n    # equivalent convolutions\n    rawCorr = np.fft.fftshift(\n        np.real(np.fft.ifft(\n            np.fft.ifft(Fx * np.conj(Fx), axis=1), axis=0)),\n        axes=(0, 1),\n    )\n    sums_x = np.fft.fftshift(\n        np.real(np.fft.ifft(\n            np.fft.ifft(np.conj(Fx) * Fn, axis=1), axis=0)),\n        axes=(0, 1),\n    )\n    sumOfSquares_x = np.fft.fftshift(\n        np.real(\n            np.fft.ifft(\n                np.fft.ifft(Fn * np.conj(FsumOfSquares_x), axis=1), axis=0)\n        ),\n        axes=(0, 1),\n    )\n    N = np.fft.fftshift(\n        np.real(np.fft.ifft(\n            np.fft.ifft(Fn * np.conj(Fn), axis=1), axis=0)),\n        axes=(0, 1),\n    )\n    # [Step 3] Account for rounding errors.\n    rawCorr[np.abs(rawCorr) &lt; tol] = 0\n    sums_x[np.abs(sums_x) &lt; tol] = 0\n    sumOfSquares_x[np.abs(sumOfSquares_x) &lt; tol] = 0\n    N = np.round(N)\n    N[N &lt;= 1] = np.nan\n    # [Step 4] Compute correlation matrix\n    mapStd = np.sqrt((sumOfSquares_x * N) - sums_x**2)\n    mapCovar = (rawCorr * N) - sums_x * \\\n        sums_x[::-1, :, :][:, ::-1, :][:, :, :]\n\n    return np.squeeze(\n        mapCovar / mapStd / mapStd[::-1, :, :][:, ::-1, :][:, :, :])\n</code></pre>"},{"location":"reference/#ephysiopy.common.binning.RateMap.crossCorr2D","title":"<code>crossCorr2D(A, B, A_nodwell, B_nodwell, tol=1e-10)</code>","text":"<p>Performs a spatial crosscorrelation between the arrays A and B</p> <p>Parameters:</p> Name Type Description Default <code>A,</code> <code>B (array_like</code> <p>Either 2 or 3D. In the former it is simply the binned up ratemap where the two dimensions correspond to x and y. If 3D then the first two dimensions are x and y and the third (last dimension) is 'stack' of ratemaps</p> required <code>nodwell_A,</code> <code>nodwell_B (array_like</code> <p>A boolean array corresponding the bins in the ratemap that weren't visited. See Notes below.</p> required <code>tol</code> <code>float</code> <p>Values below this are set to zero to deal with v small values thrown up by the fft. Default 1e-10</p> <code>1e-10</code> <p>Returns:</p> Name Type Description <code>sac</code> <code>array_like</code> <p>The spatial crosscorrelation in the relevant dimensionality</p> Notes <p>The nodwell input can usually be generated by:</p> <p>nodwell = ~np.isfinite(A)</p> Source code in <code>ephysiopy/common/binning.py</code> <pre><code>def crossCorr2D(self, A, B, A_nodwell, B_nodwell, tol=1e-10):\n    \"\"\"\n    Performs a spatial crosscorrelation between the arrays A and B\n\n    Args:\n        A, B (array_like): Either 2 or 3D. In the former it is simply the binned up ratemap\n            where the two dimensions correspond to x and y.\n            If 3D then the first two dimensions are x\n            and y and the third (last dimension) is 'stack' of ratemaps\n        nodwell_A, nodwell_B (array_like): A boolean array corresponding the bins in the ratemap that\n            weren't visited. See Notes below.\n        tol (float, optional): Values below this are set to zero to deal with v small values\n            thrown up by the fft. Default 1e-10\n\n    Returns:\n        sac (array_like): The spatial crosscorrelation in the relevant dimensionality\n\n    Notes:\n        The nodwell input can usually be generated by:\n\n        &gt;&gt;&gt; nodwell = ~np.isfinite(A)\n    \"\"\"\n    if np.ndim(A) != np.ndim(B):\n        raise ValueError(\"Both arrays must have the same dimensionality\")\n    assert np.ndim(A) == 2\n    ma, na = np.shape(A)\n    mb, nb = np.shape(B)\n    oa = ob = 1\n    A = np.reshape(A, (ma, na, oa))\n    B = np.reshape(B, (mb, nb, ob))\n    A_nodwell = np.reshape(A_nodwell, (ma, na, oa))\n    B_nodwell = np.reshape(B_nodwell, (mb, nb, ob))\n    A[A_nodwell] = 0\n    B[B_nodwell] = 0\n    # [Step 1] Obtain FFTs of x, the sum of squares and bins visited\n    Fa = np.fft.fft(np.fft.fft(A, 2 * mb - 1, axis=0), 2 * nb - 1, axis=1)\n    FsumOfSquares_a = np.fft.fft(\n        np.fft.fft(np.power(A, 2), 2 * mb - 1, axis=0), 2 * nb - 1, axis=1\n    )\n    Fn_a = np.fft.fft(\n        np.fft.fft(np.invert(A_nodwell).astype(int), 2 * mb - 1, axis=0),\n        2 * nb - 1,\n        axis=1,\n    )\n    Fb = np.fft.fft(np.fft.fft(B, 2 * ma - 1, axis=0), 2 * na - 1, axis=1)\n    FsumOfSquares_b = np.fft.fft(\n        np.fft.fft(np.power(B, 2), 2 * ma - 1, axis=0), 2 * na - 1, axis=1\n    )\n    Fn_b = np.fft.fft(\n        np.fft.fft(np.invert(B_nodwell).astype(int), 2 * ma - 1, axis=0),\n        2 * na - 1,\n        axis=1,\n    )\n    # [Step 2] Multiply the relevant transforms and invert to obtain the\n    # equivalent convolutions\n    rawCorr = np.fft.fftshift(\n        np.real(np.fft.ifft(np.fft.ifft(Fa * np.conj(Fb), axis=1), axis=0))\n    )\n    sums_a = np.fft.fftshift(\n        np.real(np.fft.ifft(np.fft.ifft(\n            Fa * np.conj(Fn_b), axis=1), axis=0))\n    )\n    sums_b = np.fft.fftshift(\n        np.real(np.fft.ifft(np.fft.ifft(\n            Fn_a * np.conj(Fb), axis=1), axis=0))\n    )\n    sumOfSquares_a = np.fft.fftshift(\n        np.real(\n            np.fft.ifft(\n                np.fft.ifft(\n                    FsumOfSquares_a * np.conj(Fn_b), axis=1), axis=0\n            )\n        )\n    )\n    sumOfSquares_b = np.fft.fftshift(\n        np.real(\n            np.fft.ifft(\n                np.fft.ifft(\n                    Fn_a * np.conj(FsumOfSquares_b), axis=1), axis=0\n            )\n        )\n    )\n    N = np.fft.fftshift(\n        np.real(np.fft.ifft(np.fft.ifft(\n            Fn_a * np.conj(Fn_b), axis=1), axis=0))\n    )\n    # [Step 3] Account for rounding errors.\n    rawCorr[np.abs(rawCorr) &lt; tol] = 0\n    sums_a[np.abs(sums_a) &lt; tol] = 0\n    sums_b[np.abs(sums_b) &lt; tol] = 0\n    sumOfSquares_a[np.abs(sumOfSquares_a) &lt; tol] = 0\n    sumOfSquares_b[np.abs(sumOfSquares_b) &lt; tol] = 0\n    N = np.round(N)\n    N[N &lt;= 1] = np.nan\n    # [Step 4] Compute correlation matrix\n    mapStd_a = np.sqrt((sumOfSquares_a * N) - sums_a**2)\n    mapStd_b = np.sqrt((sumOfSquares_b * N) - sums_b**2)\n    mapCovar = (rawCorr * N) - sums_a * sums_b\n\n    return np.squeeze(mapCovar / (mapStd_a * mapStd_b))\n</code></pre>"},{"location":"reference/#ephysiopy.common.binning.RateMap.doStackedCorrelations","title":"<code>doStackedCorrelations(spkW, times, splits, var2bin=VariableToBin.XY, maptype=MapType.RATE, **kwargs)</code>","text":"<p>Returns a list of binned data where each item in the list is the result of running np.histogramdd on a spatial variable (xy, dir etc) and a temporal one at the same time. The idea is to split the spatial variable into two temporal halves based on the bin edges in 'splits' and then to run correlations between the two halves and furthermore to do this for all of the clusters that have spike weights in 'spkW'. 'spkW' should be the result of using getAllSpikeWeights().</p> <p>Parameters:</p> Name Type Description Default <code>spkW</code> <code>ndarray</code> <p>The result of calling getAllSpikeWeights()</p> required <code>times</code> <code>ndarray</code> <p>Position times in seconds</p> required <code>splits</code> <code>ndarray</code> <p>Where to split the data in seconds. Will typically take the form (0, 100, 200) for example which will give a split between 0-100 and 100-200 seconds</p> required <code>var2bin</code> <code>Enum</code> <p>The spatial variable to bin up</p> <code>XY</code> <code>maptype</code> <code>Enum</code> <p>The type of map to produce</p> <code>RATE</code> Source code in <code>ephysiopy/common/binning.py</code> <pre><code>def doStackedCorrelations(self,\n                          spkW: np.ndarray,\n                          times: np.ndarray,\n                          splits: np.ndarray,\n                          var2bin: Enum = VariableToBin.XY,\n                          maptype: Enum = MapType.RATE,\n                          **kwargs):\n    \"\"\"\n    Returns a list of binned data where each item in the list\n    is the result of running np.histogramdd on a spatial\n    variable (xy, dir etc) and a temporal one at the same\n    time. The idea is to split the spatial variable into two\n    temporal halves based on the bin edges in 'splits' and\n    then to run correlations between the two halves and\n    furthermore to do this for all of the clusters that have\n    spike weights in 'spkW'. 'spkW' should be the result of\n    using getAllSpikeWeights().\n\n    Args:\n        spkW (np.ndarray): The result of calling getAllSpikeWeights()\n        times (np.ndarray): Position times in seconds\n        splits (np.ndarray): Where to split the data in seconds. Will\n            typically take the form (0, 100, 200) for\n            example which will give a split between 0-100\n            and 100-200 seconds\n        var2bin (Enum): The spatial variable to bin up\n        maptype (Enum): The type of map to produce\n    \"\"\"\n    if var2bin.value == VariableToBin.DIR.value:\n        sample = self.dir\n    elif var2bin.value == VariableToBin.SPEED.value:\n        sample = self.speed\n    elif var2bin.value == VariableToBin.XY.value:\n        sample = self.xy\n    else:\n        raise ValueError(\"Unrecognized variable to bin.\")\n    assert sample is not None\n    self.pos_time_splits = splits\n\n    sample = np.concatenate((np.atleast_2d(sample),\n                            np.atleast_2d(times)))\n    edges = [b for b in self._binedges][::-1]\n    edges.append(splits)\n    # bin pos\n    bp, bpe = np.histogramdd(sample.T, bins=edges)\n    map1_pos, map2_pos = np.squeeze(bp[:, :, 0]), np.squeeze(bp[:, :, 1])\n    # smooth position\n    map1_pos = blurImage(map1_pos, 7, ftype='gaussian')\n    map2_pos = blurImage(map2_pos, 7, ftype='gaussian')\n    # bin spk - ie the histogram is weighted by spike count\n    # in bin i\n    spk = [np.histogramdd(sample.T, bins=edges, weights=w)\n           for w in spkW]\n    map1_spk, map2_spk = self._splitStackedCorrelations(spk)\n    map1_sm_spk = np.array([blurImage(m, 7, ftype='gaussian')\n                            for m in map1_spk])\n    map2_sm_spk = np.array([blurImage(m, 7, ftype='gaussian')\n                            for m in map2_spk])\n    map1_rmaps = map1_sm_spk / map1_pos\n    map2_rmaps = map2_sm_spk / map2_pos\n    return map1_rmaps, map2_rmaps\n</code></pre>"},{"location":"reference/#ephysiopy.common.binning.RateMap.getAdaptiveMap","title":"<code>getAdaptiveMap(pos_binned, spk_binned, alpha=200)</code>","text":"<p>Produces a ratemap that has been adaptively binned according to the algorithm described in Skaggs et al., 1996) [1]_.</p> <p>Parameters:</p> Name Type Description Default <code>pos_binned</code> <code>array_like</code> <p>The binned positional data. For example that returned from getMap above with mapType as 'pos'</p> required <code>spk_binned</code> <code>array_like</code> <p>The binned spikes</p> required <code>alpha</code> <code>int</code> <p>A scaling parameter determing the amount of occupancy to aim at in each bin. Defaults to 200.</p> <code>200</code> <p>Returns:</p> Type Description <p>Returns adaptively binned spike and pos maps. Use to generate Skaggs</p> <p>information measure</p> Notes <p>Positions with high rates mean proportionately less error than those with low rates, so this tries to even the playing field. This type of binning should be used for calculations of spatial info as with the skaggs_info method in the fieldcalcs class (see below) alpha is a scaling parameter that might need tweaking for different data sets. From the paper:     The data [are] first binned     into a 64 X 64 grid of spatial locations, and then the firing rate     at each point in this grid was calculated by expanding a circle     around the point until the following criterion was met:         Nspks &gt; alpha / (Nocc^2 * r^2)     where Nspks is the number of spikes emitted in a circle of radius     r (in bins), Nocc is the number of occupancy samples, alpha is the     scaling parameter     The firing rate in the given bin is then calculated as:         sample_rate * (Nspks / Nocc)</p> References <p>.. [1] W. E. Skaggs, B. L. McNaughton, K. M. Gothard &amp; E. J. Markus     \"An Information-Theoretic Approach to Deciphering the Hippocampal     Code\"     Neural Information Processing Systems, 1993.</p> Source code in <code>ephysiopy/common/binning.py</code> <pre><code>def getAdaptiveMap(self, pos_binned, spk_binned, alpha=200):\n    \"\"\"\n    Produces a ratemap that has been adaptively binned according to the\n    algorithm described in Skaggs et al., 1996) [1]_.\n\n    Args:\n        pos_binned (array_like): The binned positional data. For example that returned from getMap\n            above with mapType as 'pos'\n        spk_binned (array_like): The binned spikes\n        alpha (int, optional): A scaling parameter determing the amount of occupancy to aim at\n            in each bin. Defaults to 200.\n\n    Returns:\n        Returns adaptively binned spike and pos maps. Use to generate Skaggs\n        information measure\n\n    Notes:\n        Positions with high rates mean proportionately less error than those\n        with low rates, so this tries to even the playing field. This type\n        of binning should be used for calculations of spatial info\n        as with the skaggs_info method in the fieldcalcs class (see below)\n        alpha is a scaling parameter that might need tweaking for different\n        data sets.\n        From the paper:\n            The data [are] first binned\n            into a 64 X 64 grid of spatial locations, and then the firing rate\n            at each point in this grid was calculated by expanding a circle\n            around the point until the following criterion was met:\n                Nspks &gt; alpha / (Nocc^2 * r^2)\n            where Nspks is the number of spikes emitted in a circle of radius\n            r (in bins), Nocc is the number of occupancy samples, alpha is the\n            scaling parameter\n            The firing rate in the given bin is then calculated as:\n                sample_rate * (Nspks / Nocc)\n\n    References:\n        .. [1] W. E. Skaggs, B. L. McNaughton, K. M. Gothard &amp; E. J. Markus\n            \"An Information-Theoretic Approach to Deciphering the Hippocampal\n            Code\"\n            Neural Information Processing Systems, 1993.\n    \"\"\"\n    #  assign output arrays\n    smthdPos = np.zeros_like(pos_binned)\n    smthdSpk = np.zeros_like(spk_binned)\n    smthdRate = np.zeros_like(pos_binned)\n    idx = pos_binned == 0\n    pos_binned[idx] = np.nan\n    spk_binned[idx] = np.nan\n    visited = np.zeros_like(pos_binned)\n    visited[pos_binned &gt; 0] = 1\n    # array to check which bins have made it\n    binCheck = np.isnan(pos_binned)\n    r = 1\n    while np.any(~binCheck):\n        # create the filter kernel\n        h = self._circularStructure(r)\n        h[h &gt;= np.max(h) / 3.0] = 1\n        h[h != 1] = 0\n        if h.shape &gt;= pos_binned.shape:\n            break\n        # filter the arrays using astropys convolution\n        filtPos = convolution.convolve(pos_binned, h, boundary=None)\n        filtSpk = convolution.convolve(spk_binned, h, boundary=None)\n        filtVisited = convolution.convolve(visited, h, boundary=None)\n        # get the bins which made it through this iteration\n        trueBins = alpha / (np.sqrt(filtSpk) * filtPos) &lt;= r\n        trueBins = np.logical_and(trueBins, ~binCheck)\n        # insert values where true\n        smthdPos[trueBins] = filtPos[trueBins] / filtVisited[trueBins]\n        smthdSpk[trueBins] = filtSpk[trueBins] / filtVisited[trueBins]\n        binCheck[trueBins] = True\n        r += 1\n    smthdRate = smthdSpk / smthdPos\n    smthdRate[idx] = np.nan\n    smthdSpk[idx] = np.nan\n    smthdPos[idx] = np.nan\n    return smthdRate, smthdSpk, smthdPos\n</code></pre>"},{"location":"reference/#ephysiopy.common.binning.RateMap.getAllSpikeWeights","title":"<code>getAllSpikeWeights(spike_times, spike_clusters, pos_times, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>spike_times</code> <code>ndarray</code> <p>Spike times in seconds</p> required <code>spike_clusters</code> <code>ndarray</code> <p>Cluster identity vector</p> required <code>pos_times</code> <code>ndarray</code> <p>The times at which position was captured in seconds</p> required <p>Returns:</p> Type Description <p>np.ndarray: The bincounts with respect to position for each cluster. Shape of returned array will be nClusters x npos</p> Source code in <code>ephysiopy/common/binning.py</code> <pre><code>def getAllSpikeWeights(self,\n                       spike_times: np.ndarray,\n                       spike_clusters: np.ndarray,\n                       pos_times: np.ndarray,\n                       **kwargs):\n    \"\"\"\n    Args:\n        spike_times (np.ndarray): Spike times in seconds\n        spike_clusters (np.ndarray): Cluster identity vector\n        pos_times (np.ndarray): The times at which position was captured in seconds\n\n    Returns:\n        np.ndarray: The bincounts with respect to position for each cluster. Shape of returned array will be nClusters x npos\n    \"\"\"\n    assert len(spike_clusters) == len(spike_times)\n    clusters = np.unique(spike_clusters)\n    npos = len(self.dir)\n    idx = np.searchsorted(pos_times, spike_times) - 1\n    weights = [np.bincount(idx[spike_clusters == c], minlength=npos)\n               for c in clusters]\n    return np.array(weights)\n</code></pre>"},{"location":"reference/#ephysiopy.common.binning.RateMap.getMap","title":"<code>getMap(spkWeights, varType=VariableToBin.XY, mapType=MapType.RATE, smoothing=True, **kwargs)</code>","text":"<p>Bins up the variable type varType and returns a tuple of (rmap, binnedPositionDir) or (rmap, binnedPostionX, binnedPositionY)</p> <p>Parameters:</p> Name Type Description Default <code>spkWeights</code> <code>array_like</code> <p>Shape equal to number of positions samples captured and consists of position weights. For example, if there were 5 positions recorded and a cell spiked once in position 2 and 5 times in position 3 and nothing anywhere else then pos_weights looks like: [0 0 1 5 0]</p> required <code>varType</code> <code>Enum value - see Variable2Bin defined at top of this file</code> <p>The variable to bin up. Legal values are: XY, DIR and SPEED</p> <code>XY</code> <code>mapType</code> <code>enum value - see MapType defined at top of this file</code> <p>If RATE then the binned up spikes are divided by varType. Otherwise return binned up position. Options are RATE or POS</p> <code>RATE</code> <code>smoothing</code> <code>bool</code> <p>Whether to smooth the data or not. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <p>binned_data, binned_pos (tuple): This is either a 2-tuple or a 3-tuple depening on whether binned pos (mapType 'pos') or binned spikes (mapType 'rate') is asked for respectively</p> Source code in <code>ephysiopy/common/binning.py</code> <pre><code>def getMap(self, spkWeights,\n           varType=VariableToBin.XY,\n           mapType=MapType.RATE,\n           smoothing=True,\n           **kwargs):\n    \"\"\"\n    Bins up the variable type varType and returns a tuple of\n    (rmap, binnedPositionDir) or\n    (rmap, binnedPostionX, binnedPositionY)\n\n    Args:\n        spkWeights (array_like): Shape equal to number of positions samples captured and consists of\n            position weights. For example, if there were 5 positions\n            recorded and a cell spiked once in position 2 and 5 times in\n            position 3 and nothing anywhere else then pos_weights looks\n            like: [0 0 1 5 0]\n        varType (Enum value - see Variable2Bin defined at top of this file): The variable to bin up. Legal values are: XY, DIR and SPEED\n        mapType (enum value - see MapType defined at top of this file): If RATE then the binned up spikes are divided by varType.\n            Otherwise return binned up position. Options are RATE or POS\n        smoothing (bool, optional): Whether to smooth the data or not. Defaults to True.\n\n    Returns:\n        binned_data, binned_pos (tuple): This is either a 2-tuple or a 3-tuple depening on whether binned\n            pos (mapType 'pos') or binned spikes (mapType 'rate') is asked\n            for respectively\n    \"\"\"\n    if varType.value == VariableToBin.DIR.value:\n        sample = self.dir\n        keep_these = np.isfinite(sample)\n    elif varType.value == VariableToBin.SPEED.value:\n        sample = self.speed\n        keep_these = np.isfinite(sample)\n    elif varType.value == VariableToBin.XY.value:\n        sample = self.xy\n        keep_these = np.isfinite(sample[0])\n    elif varType.value == VariableToBin.XY_TIME.value:\n        sample = np.concatenate((np.atleast_2d(self.xy),\n                                np.atleast_2d(self.pos_times)))\n        keep_these = np.isfinite(self.xy[0])\n    else:\n        raise ValueError(\"Unrecognized variable to bin.\")\n    assert sample is not None\n\n    self.var2Bin = varType\n    self._spike_weights = spkWeights\n    self._calcBinEdges(self.binsize)\n\n    binned_pos, binned_pos_edges = self._binData(\n        sample,\n        self._binedges,\n        self.pos_weights,\n        keep_these)\n    nanIdx = binned_pos == 0\n\n    if mapType.value == MapType.POS.value:  # return binned up position\n        if smoothing:\n            if varType.value == VariableToBin.DIR.value:\n                binned_pos = self._circPadSmooth(\n                    binned_pos, n=self.smooth_sz)\n            else:\n                binned_pos = blurImage(binned_pos,\n                                       self.smooth_sz,\n                                       ftype=self.smoothingType,\n                                       **kwargs)\n        return binned_pos, binned_pos_edges\n\n    binned_spk, _ = self._binData(\n        sample, self._binedges, spkWeights, keep_these)\n    if mapType.value == MapType.SPK:\n        return binned_spk\n    # binned_spk is returned as a tuple of the binned data and the bin\n    # edges\n    if \"after\" in self.whenToSmooth:\n        rmap = binned_spk / binned_pos\n        if varType.value == VariableToBin.DIR.value:\n            rmap = self._circPadSmooth(rmap, self.smooth_sz)\n        else:\n            rmap = blurImage(rmap,\n                             self.smooth_sz,\n                             ftype=self.smoothingType,\n                             **kwargs)\n    else:  # default case\n        if not smoothing:\n            return binned_spk / binned_pos, binned_pos_edges\n        if varType.value == VariableToBin.DIR.value:\n            binned_pos = self._circPadSmooth(binned_pos, self.smooth_sz)\n            binned_spk = self._circPadSmooth(binned_spk, self.smooth_sz)\n            rmap = binned_spk / binned_pos\n        else:\n            binned_pos = blurImage(binned_pos,\n                                   self.smooth_sz,\n                                   ftype=self.smoothingType,\n                                   **kwargs)\n            if binned_spk.ndim == 2:\n                pass\n            elif binned_spk.ndim == 1:\n                binned_spk_tmp = np.zeros(\n                    [binned_spk.shape[0], binned_spk.shape[0], 1]\n                )\n                for i in range(binned_spk.shape[0]):\n                    binned_spk_tmp[i, :, :] = binned_spk[i]\n                binned_spk = binned_spk_tmp\n            binned_spk = blurImage(\n                binned_spk,\n                self.smooth_sz,\n                ftype=self.smoothingType,\n                **kwargs)\n            rmap = binned_spk / binned_pos\n            if rmap.ndim &lt;= 2:\n                rmap[nanIdx] = np.nan\n\n    return rmap, binned_pos_edges\n</code></pre>"},{"location":"reference/#ephysiopy.common.binning.RateMap.getSAC","title":"<code>getSAC(spkWeights, **kwargs)</code>","text":"<p>Returns the SAC - convenience function</p> Source code in <code>ephysiopy/common/binning.py</code> <pre><code>def getSAC(self, spkWeights, **kwargs):\n    '''\n    Returns the SAC - convenience function\n    '''\n    rmap = self.getMap(spkWeights=spkWeights, **kwargs)\n    nodwell = ~np.isfinite(rmap[0])\n    return self.autoCorr2D(rmap[0], nodwell)\n</code></pre>"},{"location":"reference/#ephysiopy.common.binning.RateMap.getSpatialSparsity","title":"<code>getSpatialSparsity(spkWeights, sample_rate=50, **kwargs)</code>","text":"<p>Gets the spatial sparsity measure - closer to 1 means sparser firing field.</p> References <p>Skaggs, W.E., McNaughton, B.L., Wilson, M.A. &amp; Barnes, C.A. Theta phase precession in hippocampal neuronal populations and the compression of temporal sequences. Hippocampus 6, 149\u2013172 (1996).</p> Source code in <code>ephysiopy/common/binning.py</code> <pre><code>def getSpatialSparsity(self,\n                       spkWeights,\n                       sample_rate=50,\n                       **kwargs):\n    \"\"\"\n    Gets the spatial sparsity measure - closer to 1 means\n    sparser firing field.\n\n    References:\n        Skaggs, W.E., McNaughton, B.L., Wilson, M.A. &amp; Barnes, C.A.\n        Theta phase precession in hippocampal neuronal populations\n        and the compression of temporal sequences.\n        Hippocampus 6, 149\u2013172 (1996).\n    \"\"\"\n    self.var2Bin = VariableToBin.XY\n    self._calcBinEdges()\n    sample = self.xy\n    keep_these = np.isfinite(sample[0])\n    pos, _ = self._binData(sample,\n                           self._binedges,\n                           self.pos_weights,\n                           keep_these)\n    npos = len(self.dir)\n    p_i = np.count_nonzero(pos) / npos / sample_rate\n    spk, _ = self._binData(sample,\n                           self._binedges,\n                           spkWeights,\n                           keep_these)\n    res = 1-(np.nansum(p_i*spk)**2) / np.nansum(p_i*spk**2)\n    return res\n</code></pre>"},{"location":"reference/#ephysiopy.common.binning.RateMap.get_egocentric_boundary_map","title":"<code>get_egocentric_boundary_map(spk_weights, degs_per_bin=3, xy_binsize=2.5, arena_type='circle', return_dists=False, return_raw_spk=False, return_raw_occ=False)</code>","text":"<p>Helps construct dwell time/spike counts maps with respect to boundaries at given egocentric directions and distances.</p> Note <p>For the directional input, the 0 degree reference is horizontal pointing East and moves counter-clockwise.</p> Source code in <code>ephysiopy/common/binning.py</code> <pre><code>def get_egocentric_boundary_map(self,\n                                spk_weights,\n                                degs_per_bin: float = 3,\n                                xy_binsize: float = 2.5,\n                                arena_type: str = \"circle\",\n                                return_dists: bool = False,\n                                return_raw_spk: bool = False,\n                                return_raw_occ: bool = False) -&gt; namedtuple:\n    \"\"\"\n    Helps construct dwell time/spike counts maps with respect to boundaries at given egocentric directions and distances.\n\n    Note:\n        For the directional input, the 0 degree reference is horizontal pointing East and moves counter-clockwise.\n    \"\"\"\n    assert self.dir is not None, \"No direction data available\"\n    # initially do some binning to get valid locations\n    # (some might be nans due to\n    # arena shape and/or poor sampling) and then digitize\n    # the x and y positions\n    # and the angular positions\n    self.binsize = xy_binsize  # this will trigger a\n    # re-calculation of the bin edges\n\n    angles = np.arange(0, 360, degs_per_bin)\n\n    # Use the shaeply package to specify some geometry for the arena\n    # boundary and the lines radiating out\n    # from the current location of the animal. The geometry for the\n    # arena should be user specified but for now I'll just use a circle\n    if arena_type == \"circle\":\n        radius = 50\n        circle_centre = Point(\n            np.nanmin(self.xy[0])+radius, np.nanmin(self.xy[1])+radius)\n        arena_boundary = circle_centre.buffer(radius).boundary\n    # now we have a circle with its centre at the centre of the arena\n    # i.e. the circle defines the arena edges. Calling .boundary on the\n    # circle geometry actually gives us a 65-gon polygon\n    distances = self._create_boundary_distance_lookup(\n        arena_boundary, degs_per_bin, xy_binsize)\n    # iterate through the digitized locations (x/y and angular), using the\n    # lookup table to get the distances to the arena boundary and then\n    # increment the appropriate bin in the egocentric boundary map\n    good_idx = np.isfinite(self.xy[0])\n    xy_by_heading, _ = np.histogramdd([self.xy[0][good_idx],\n                                       self.xy[1][good_idx],\n                                       self.dir[good_idx]],\n                                      bins=distances.shape,\n                                      weights=self.pos_weights[good_idx])\n    spk_xy_by_hd, _ = np.histogramdd([self.xy[0][good_idx],\n                                      self.xy[1][good_idx],\n                                      self.dir[good_idx]],\n                                     bins=distances.shape,\n                                     weights=spk_weights[good_idx])\n    assert xy_by_heading.shape == distances.shape\n    distlist = []\n    anglist = []\n    spkdists = []\n    spkangs = []\n    for i_bin in np.ndindex(distances.shape[:2]):\n        i_dist = distances[i_bin]\n        valid_dist = np.isfinite(i_dist)\n        nonzero_bincounts = np.nonzero(xy_by_heading[i_bin])[0]\n        nonzero_spkbins = np.nonzero(spk_xy_by_hd[i_bin])[0]\n        for i_angle in nonzero_bincounts:\n            ego_angles = np.roll(angles, i_angle)[valid_dist]\n            n_repeats = xy_by_heading[i_bin][i_angle]\n            ego_angles_repeats = np.repeat(ego_angles, n_repeats)\n            dist_repeats = np.repeat(i_dist[valid_dist], n_repeats)\n            distlist.append(dist_repeats)\n            anglist.append(ego_angles_repeats)\n            if i_angle in nonzero_spkbins:\n                n_repeats = spk_xy_by_hd[i_bin][i_angle]\n                ego_angles_repeats = np.repeat(ego_angles, n_repeats)\n                dist_repeats = np.repeat(i_dist[valid_dist], n_repeats)\n                spkdists.append(dist_repeats)\n                spkangs.append(ego_angles_repeats)\n    flat_angs = flatten_list(anglist)\n    flat_dists = flatten_list(distlist)\n    flat_spk_dists = flatten_list(spkdists)\n    flat_spk_angs = flatten_list(spkangs)\n    bins = [int(radius/xy_binsize), len(angles)]\n    ego_boundary_occ, _, _ = np.histogram2d(x=flat_dists, y=flat_angs,\n                                            bins=bins)\n    ego_boundary_spk, _, _ = np.histogram2d(x=flat_spk_dists,\n                                            y=flat_spk_angs,\n                                            bins=bins)\n    kernel = convolution.Gaussian2DKernel(5, x_size=3, y_size=5)\n    sm_occ = convolution.convolve(ego_boundary_occ,\n                                  kernel,\n                                  boundary='extend')\n    sm_spk = convolution.convolve(ego_boundary_spk,\n                                  kernel,\n                                  boundary='extend')\n    ego_boundary_map = sm_spk / sm_occ\n    EgoMap = namedtuple(\"EgoMap\", ['rmap', 'occ', 'spk', 'dists'],\n                        defaults=None)\n    em = EgoMap(None, None, None, None)\n    em = em._replace(rmap=ego_boundary_map)\n    if return_dists:\n        em = em._replace(dists=distances)\n    if return_raw_occ:\n        em = em._replace(occ=ego_boundary_occ)\n    if return_raw_spk:\n        em = em._replace(spk=ego_boundary_spk)\n    return em\n</code></pre>"},{"location":"reference/#ephysiopy.common.binning.RateMap.tWinSAC","title":"<code>tWinSAC(xy, spkIdx, ppm=365, winSize=10, pos_sample_rate=50, nbins=71, boxcar=5, Pthresh=100, downsampfreq=50, plot=False)</code>","text":"<p>Temporal windowed spatial autocorrelation.</p> <p>Parameters:</p> Name Type Description Default <code>xy</code> <code>array_like</code> <p>The position data</p> required <code>spkIdx</code> <code>array_like</code> <p>The indices in xy where the cell fired</p> required <code>ppm</code> <code>int</code> <p>The camera pixels per metre. Default 365</p> <code>365</code> <code>winSize</code> <code>int</code> <p>The window size for the temporal search</p> <code>10</code> <code>pos_sample_rate</code> <code>int</code> <p>The rate at which position was sampled. Default 50</p> <code>50</code> <code>nbins</code> <code>int</code> <p>The number of bins for creating the resulting ratemap. Default 71</p> <code>71</code> <code>boxcar</code> <code>int</code> <p>The size of the smoothing kernel to smooth ratemaps. Default 5</p> <code>5</code> <code>Pthresh</code> <code>int</code> <p>The cut-off for values in the ratemap; values &lt; Pthresh become nans. Default 100</p> <code>100</code> <code>downsampfreq</code> <code>int</code> <p>How much to downsample. Default 50</p> <code>50</code> <code>plot</code> <code>bool</code> <p>Whether to show a plot of the result. Default False</p> <code>False</code> <p>Returns:</p> Name Type Description <code>H</code> <code>array_like</code> <p>The temporal windowed SAC</p> Source code in <code>ephysiopy/common/binning.py</code> <pre><code>def tWinSAC(\n    self,\n    xy,\n    spkIdx,\n    ppm=365,\n    winSize=10,\n    pos_sample_rate=50,\n    nbins=71,\n    boxcar=5,\n    Pthresh=100,\n    downsampfreq=50,\n    plot=False,\n):\n    \"\"\"\n    Temporal windowed spatial autocorrelation.\n\n    Args:\n        xy (array_like): The position data\n        spkIdx (array_like): The indices in xy where the cell fired\n        ppm (int, optional): The camera pixels per metre. Default 365\n        winSize (int, optional): The window size for the temporal search\n        pos_sample_rate (int, optional): The rate at which position was sampled. Default 50\n        nbins (int, optional): The number of bins for creating the resulting ratemap. Default 71\n        boxcar (int, optional): The size of the smoothing kernel to smooth ratemaps. Default 5\n        Pthresh (int, optional): The cut-off for values in the ratemap; values &lt; Pthresh become nans. Default 100\n        downsampfreq (int, optional): How much to downsample. Default 50\n        plot (bool, optional): Whether to show a plot of the result. Default False\n\n    Returns:\n        H (array_like): The temporal windowed SAC\n    \"\"\"\n    # [Stage 0] Get some numbers\n    xy = xy / ppm * 100\n    n_samps = xy.shape[1]\n    n_spks = len(spkIdx)\n    winSizeBins = np.min([winSize * pos_sample_rate, n_samps])\n    # factor by which positions are downsampled\n    downsample = np.ceil(pos_sample_rate / downsampfreq)\n    Pthresh = Pthresh / downsample  # take account of downsampling\n\n    # [Stage 1] Calculate number of spikes in the window for each spikeInd\n    # (ignoring spike itself)\n    # 1a. Loop preparation\n    nSpikesInWin = np.zeros(n_spks, dtype=int)\n\n    # 1b. Keep looping until we have dealt with all spikes\n    for i, s in enumerate(spkIdx):\n        t = np.searchsorted(spkIdx, (s, s + winSizeBins))\n        nSpikesInWin[i] = len(spkIdx[t[0]: t[1]]) - 1  # ignore ith spike\n\n    # [Stage 2] Prepare for main loop\n    # 2a. Work out offset inidices to be used when storing spike data\n    off_spike = np.cumsum([nSpikesInWin])\n    off_spike = np.pad(off_spike, (1, 0), \"constant\", constant_values=(0))\n\n    # 2b. Work out number of downsampled pos bins in window and\n    # offset indices for storing data\n    nPosInWindow = np.minimum(winSizeBins, n_samps - spkIdx)\n    nDownsampInWin = np.floor((nPosInWindow - 1) / downsample) + 1\n\n    off_dwell = np.cumsum(nDownsampInWin.astype(int))\n    off_dwell = np.pad(off_dwell, (1, 0), \"constant\", constant_values=(0))\n\n    # 2c. Pre-allocate dwell and spike arrays, singles for speed\n    dwell = np.zeros((2, off_dwell[-1]), dtype=np.single) * np.nan\n    spike = np.zeros((2, off_spike[-1]), dtype=np.single) * np.nan\n\n    filled_pvals = 0\n    filled_svals = 0\n\n    for i in range(n_spks):\n        # calculate dwell displacements\n        winInd_dwell = np.arange(\n            spkIdx[i] + 1,\n            np.minimum(spkIdx[i] + winSizeBins, n_samps),\n            downsample,\n            dtype=int,\n        )\n        WL = len(winInd_dwell)\n        dwell[:, filled_pvals: filled_pvals + WL] = np.rot90(\n            np.array(np.rot90(xy[:, winInd_dwell]) - xy[:, spkIdx[i]])\n        )\n        filled_pvals = filled_pvals + WL\n        # calculate spike displacements\n        winInd_spks = (\n            i + np.nonzero(spkIdx[i + 1: n_spks] &lt;\n                           spkIdx[i] + winSizeBins)[0]\n        )\n        WL = len(winInd_spks)\n        spike[:, filled_svals: filled_svals + WL] = np.rot90(\n            np.array(\n                np.rot90(xy[:, spkIdx[winInd_spks]]) - xy[:, spkIdx[i]])\n        )\n        filled_svals = filled_svals + WL\n\n    dwell = np.delete(dwell, np.isnan(dwell).nonzero()[1], axis=1)\n    spike = np.delete(spike, np.isnan(spike).nonzero()[1], axis=1)\n\n    dwell = np.hstack((dwell, -dwell))\n    spike = np.hstack((spike, -spike))\n\n    dwell_min = np.min(dwell, axis=1)\n    dwell_max = np.max(dwell, axis=1)\n\n    binsize = (dwell_max[1] - dwell_min[1]) / nbins\n\n    dwell = np.round(\n        (dwell - np.ones_like(dwell) * dwell_min[:, np.newaxis]) / binsize\n    )\n    spike = np.round(\n        (spike - np.ones_like(spike) * dwell_min[:, np.newaxis]) / binsize\n    )\n\n    binsize = np.max(dwell, axis=1).astype(int)\n    binedges = np.array(((-0.5, -0.5), binsize + 0.5)).T\n    Hp = np.histogram2d(dwell[0, :], dwell[1, :],\n                        range=binedges, bins=binsize)[0]\n    Hs = np.histogram2d(spike[0, :], spike[1, :],\n                        range=binedges, bins=binsize)[0]\n\n    # reverse y,x order\n    Hp = np.swapaxes(Hp, 1, 0)\n    Hs = np.swapaxes(Hs, 1, 0)\n\n    fHp = blurImage(Hp, boxcar)\n    fHs = blurImage(Hs, boxcar)\n\n    H = fHs / fHp\n    H[Hp &lt; Pthresh] = np.nan\n\n    return H\n</code></pre>"},{"location":"reference/#field-calculations","title":"Field calculations","text":""},{"location":"reference/#ephysiopy.common.fieldcalcs.border_score","title":"<code>border_score(A, B=None, shape='square', fieldThresh=0.3, smthKernSig=3, circumPrc=0.2, binSize=3.0, minArea=200, debug=False)</code>","text":"<p>Calculates a border score totally dis-similar to that calculated in Solstad et al (2008)</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>array_like</code> <p>Should be the ratemap</p> required <code>B</code> <code>array_like</code> <p>This should be a boolean mask where True (1) is equivalent to the presence of a border and False (0) is equivalent to 'open space'. Naievely this will be the edges of the ratemap but could be used to take account of boundary insertions/ creations to check tuning to multiple environmental boundaries. Default None: when the mask is None then a mask is created that has 1's at the edges of the ratemap i.e. it is assumed that occupancy = environmental shape</p> <code>None</code> <code>shape</code> <code>str</code> <p>description of environment shape. Currently only 'square' or 'circle' accepted. Used to calculate the proportion of the environmental boundaries to examine for firing</p> <code>'square'</code> <code>fieldThresh</code> <code>float</code> <p>Between 0 and 1 this is the percentage amount of the maximum firing rate to remove from the ratemap (i.e. to remove noise)</p> <code>0.3</code> <code>smthKernSig</code> <code>float</code> <p>the sigma value used in smoothing the ratemap (again!) with a gaussian kernel</p> <code>3</code> <code>circumPrc</code> <code>float</code> <p>The percentage amount of the circumference of the environment that the field needs to be to count as long enough to make it through</p> <code>0.2</code> <code>binSize</code> <code>float</code> <p>bin size in cm</p> <code>3.0</code> <code>minArea</code> <code>float</code> <p>min area for a field to be considered</p> <code>200</code> <code>debug</code> <code>bool</code> <p>If True then some plots and text will be output</p> <code>False</code> <p>Returns:</p> Name Type Description <code>float</code> <p>the border score</p> Notes <p>If the cell is a border cell (BVC) then we know that it should fire at a fixed distance from a given boundary (possibly more than one). In essence this algorithm estimates the amount of variance in this distance i.e. if the cell is a border cell this number should be small. This is achieved by first doing a bunch of morphological operations to isolate individual fields in the ratemap (similar to the code used in phasePrecession.py - see the partitionFields method therein). These partitioned fields are then thinned out (using skimage's skeletonize) to a single pixel wide field which will lie more or less in the middle of the (highly smoothed) sub-field. It is the variance in distance from the nearest boundary along this pseudo-iso-line that is the boundary measure</p> <p>Other things to note are that the pixel-wide field has to have some minimum length. In the case of a circular environment this is set to 20% of the circumference; in the case of a square environment markers this is at least half the length of the longest side</p> Source code in <code>ephysiopy/common/fieldcalcs.py</code> <pre><code>def border_score(\n    A,\n    B=None,\n    shape=\"square\",\n    fieldThresh=0.3,\n    smthKernSig=3,\n    circumPrc=0.2,\n    binSize=3.0,\n    minArea=200,\n    debug=False,\n):\n    \"\"\"\n    Calculates a border score totally dis-similar to that calculated in\n    Solstad et al (2008)\n\n    Args:\n        A (array_like): Should be the ratemap\n        B (array_like): This should be a boolean mask where True (1)\n            is equivalent to the presence of a border and False (0)\n            is equivalent to 'open space'. Naievely this will be the\n            edges of the ratemap but could be used to take account of\n            boundary insertions/ creations to check tuning to multiple\n            environmental boundaries. Default None: when the mask is\n            None then a mask is created that has 1's at the edges of the\n            ratemap i.e. it is assumed that occupancy = environmental\n            shape\n        shape (str): description of environment shape. Currently\n            only 'square' or 'circle' accepted. Used to calculate the\n            proportion of the environmental boundaries to examine for\n            firing\n        fieldThresh (float): Between 0 and 1 this is the percentage\n            amount of the maximum firing rate\n            to remove from the ratemap (i.e. to remove noise)\n        smthKernSig (float): the sigma value used in smoothing the ratemap\n            (again!) with a gaussian kernel\n        circumPrc (float): The percentage amount of the circumference\n            of the environment that the field needs to be to count\n            as long enough to make it through\n        binSize (float): bin size in cm\n        minArea (float): min area for a field to be considered\n        debug (bool): If True then some plots and text will be output\n\n    Returns:\n        float: the border score\n\n    Notes:\n        If the cell is a border cell (BVC) then we know that it should\n        fire at a fixed distance from a given boundary (possibly more\n        than one). In essence this algorithm estimates the amount of\n        variance in this distance i.e. if the cell is a border cell this\n        number should be small. This is achieved by first doing a bunch of\n        morphological operations to isolate individual fields in the\n        ratemap (similar to the code used in phasePrecession.py - see\n        the partitionFields method therein). These partitioned fields are then\n        thinned out (using skimage's skeletonize) to a single pixel\n        wide field which will lie more or less in the middle of the\n        (highly smoothed) sub-field. It is the variance in distance from the\n        nearest boundary along this pseudo-iso-line that is the boundary\n        measure\n\n        Other things to note are that the pixel-wide field has to have some\n        minimum length. In the case of a circular environment this is set to\n        20% of the circumference; in the case of a square environment markers\n        this is at least half the length of the longest side\n    \"\"\"\n    # need to know borders of the environment so we can see if a field\n    # touches the edges, and the perimeter length of the environment\n    # deal with square or circles differently\n    borderMask = np.zeros_like(A)\n    A_rows, A_cols = np.shape(A)\n    if \"circle\" in shape:\n        radius = np.max(np.array(np.shape(A))) / 2.0\n        dist_mask = skimage.morphology.disk(radius)\n        if np.shape(dist_mask) &gt; np.shape(A):\n            dist_mask = dist_mask[1 : A_rows + 1, 1 : A_cols + 1]\n        tmp = np.zeros([A_rows + 2, A_cols + 2])\n        tmp[1:-1, 1:-1] = dist_mask\n        dists = ndimage.distance_transform_bf(tmp)\n        dists = dists[1:-1, 1:-1]\n        borderMask = np.logical_xor(dists &lt;= 0, dists &lt; 2)\n        # open up the border mask a little\n        borderMask = skimage.morphology.binary_dilation(\n            borderMask, skimage.morphology.disk(1)\n        )\n    elif \"square\" in shape:\n        borderMask[0:3, :] = 1\n        borderMask[-3:, :] = 1\n        borderMask[:, 0:3] = 1\n        borderMask[:, -3:] = 1\n        tmp = np.zeros([A_rows + 2, A_cols + 2])\n        dist_mask = np.ones_like(A)\n        tmp[1:-1, 1:-1] = dist_mask\n        dists = ndimage.distance_transform_bf(tmp)\n        # remove edges to make same shape as input ratemap\n        dists = dists[1:-1, 1:-1]\n    A[np.isnan(A)] = 0\n    # get some morphological info about the fields in the ratemap\n    # start image processing:\n    # get some markers\n    # NB I've tried a variety of techniques to optimise this part and the\n    # best seems to be the local adaptive thresholding technique which)\n    # smooths locally with a gaussian - see the skimage docs for more\n    idx = A &gt;= np.nanmax(np.ravel(A)) * fieldThresh\n    A_thresh = np.zeros_like(A)\n    A_thresh[idx] = A[idx]\n\n    # label these markers so each blob has a unique id\n    labels, nFields = ndimage.label(A_thresh)\n    # remove small objects\n    min_size = int(minArea / binSize) - 1\n    skimage.morphology.remove_small_objects(labels, min_size=min_size, connectivity=2)\n    labels = skimage.segmentation.relabel_sequential(labels)[0]\n    nFields = np.max(labels)\n    if nFields == 0:\n        return np.nan\n    # Iterate over the labelled parts of the array labels calculating\n    # how much of the total circumference of the environment edge it\n    # covers\n\n    fieldAngularCoverage = np.zeros([1, nFields]) * np.nan\n    fractionOfPixelsOnBorder = np.zeros([1, nFields]) * np.nan\n    fieldsToKeep = np.zeros_like(A).astype(bool)\n    for i in range(1, nFields + 1):\n        fieldMask = np.logical_and(labels == i, borderMask)\n\n        # check the angle subtended by the fieldMask\n        if np.sum(fieldMask.astype(int)) &gt; 0:\n            s = skimage.measure.regionprops(\n                fieldMask.astype(int), intensity_image=A_thresh\n            )[0]\n            x = s.coords[:, 0] - (A_cols / 2.0)\n            y = s.coords[:, 1] - (A_rows / 2.0)\n            subtended_angle = np.rad2deg(np.ptp(np.arctan2(x, y)))\n            if subtended_angle &gt; (360 * circumPrc):\n                pixelsOnBorder = np.count_nonzero(fieldMask) / float(\n                    np.count_nonzero(labels == i)\n                )\n                fractionOfPixelsOnBorder[:, i - 1] = pixelsOnBorder\n                if pixelsOnBorder &gt; 0.5:\n                    fieldAngularCoverage[0, i - 1] = subtended_angle\n\n            fieldsToKeep = np.logical_or(fieldsToKeep, labels == i)\n    fieldAngularCoverage = fieldAngularCoverage / 360.0\n    rateInField = A[fieldsToKeep]\n    # normalize firing rate in the field to sum to 1\n    rateInField = rateInField / np.nansum(rateInField)\n    dist2WallInField = dists[fieldsToKeep]\n    Dm = np.dot(dist2WallInField, rateInField)\n    if \"circle\" in shape:\n        Dm = Dm / radius\n    elif \"square\" in shape:\n        Dm = Dm / (np.max(np.shape(A)) / 2.0)\n    borderScore = (fractionOfPixelsOnBorder - Dm) / (fractionOfPixelsOnBorder + Dm)\n    return np.max(borderScore)\n</code></pre>"},{"location":"reference/#ephysiopy.common.fieldcalcs.calc_angs","title":"<code>calc_angs(points)</code>","text":"<p>Calculates the angles for all triangles in a delaunay tesselation of the peak points in the ratemap</p> Source code in <code>ephysiopy/common/fieldcalcs.py</code> <pre><code>def calc_angs(points):\n    \"\"\"\n    Calculates the angles for all triangles in a delaunay tesselation of\n    the peak points in the ratemap\n    \"\"\"\n\n    # calculate the lengths of the sides of the triangles\n    tri = spatial.Delaunay(points)\n    angs = []\n    for s in tri.simplices:\n        A = tri.points[s[1]] - tri.points[s[0]]\n        B = tri.points[s[2]] - tri.points[s[1]]\n        C = tri.points[s[0]] - tri.points[s[2]]\n        for e1, e2 in ((A, -B), (B, -C), (C, -A)):\n            num = np.dot(e1, e2)\n            denom = np.linalg.norm(e1) * np.linalg.norm(e2)\n            angs.append(np.arccos(num / denom) * 180 / np.pi)\n    return np.array(angs).T\n</code></pre>"},{"location":"reference/#ephysiopy.common.fieldcalcs.coherence","title":"<code>coherence(smthd_rate, unsmthd_rate)</code>","text":"<p>calculates coherence of receptive field via correlation of smoothed and unsmoothed ratemaps</p> Source code in <code>ephysiopy/common/fieldcalcs.py</code> <pre><code>def coherence(smthd_rate, unsmthd_rate):\n    \"\"\"calculates coherence of receptive field via correlation of smoothed\n    and unsmoothed ratemaps\n    \"\"\"\n    smthd = smthd_rate.ravel()\n    unsmthd = unsmthd_rate.ravel()\n    si = ~np.isnan(smthd)\n    ui = ~np.isnan(unsmthd)\n    idx = ~(~si | ~ui)\n    coherence = np.corrcoef(unsmthd[idx], smthd[idx])\n    return coherence[1, 0]\n</code></pre>"},{"location":"reference/#ephysiopy.common.fieldcalcs.corr_maps","title":"<code>corr_maps(map1, map2, maptype='normal')</code>","text":"<p>correlates two ratemaps together ignoring areas that have zero sampling</p> Source code in <code>ephysiopy/common/fieldcalcs.py</code> <pre><code>def corr_maps(map1, map2, maptype=\"normal\"):\n    \"\"\"\n    correlates two ratemaps together ignoring areas that have zero sampling\n    \"\"\"\n    if map1.shape &gt; map2.shape:\n        map2 = skimage.transform.resize(map2, map1.shape, mode=\"reflect\")\n    elif map1.shape &lt; map2.shape:\n        map1 = skimage.transform.resize(map1, map2.shape, mode=\"reflect\")\n    map1 = map1.flatten()\n    map2 = map2.flatten()\n    if \"normal\" in maptype:\n        valid_map1 = np.logical_or((map1 &gt; 0), ~np.isnan(map1))\n        valid_map2 = np.logical_or((map2 &gt; 0), ~np.isnan(map2))\n    elif \"grid\" in maptype:\n        valid_map1 = ~np.isnan(map1)\n        valid_map2 = ~np.isnan(map2)\n    valid = np.logical_and(valid_map1, valid_map2)\n    r = np.corrcoef(map1[valid], map2[valid])\n    return r[1][0]\n</code></pre>"},{"location":"reference/#ephysiopy.common.fieldcalcs.deform_SAC","title":"<code>deform_SAC(A, circleXY=None, ellipseXY=None)</code>","text":"<p>Deforms a SAC that is non-circular to be more circular</p> <p>Basically a blatant attempt to improve grid scores, possibly introduced in a paper by Matt Nolan...</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>array_like</code> <p>The SAC</p> required <code>circleXY</code> <code>array_like</code> <p>The xy coordinates defining a circle.</p> <code>None</code> <code>ellipseXY</code> <code>array_like</code> <p>The xy coordinates defining an</p> <code>None</code> <p>Returns:</p> Name Type Description <code>deformed_sac</code> <code>array_like</code> <p>The SAC deformed to be more circular</p> See Also <p>ephysiopy.common.ephys_generic.FieldCalcs.grid_field_props skimage.transform.AffineTransform skimage.transform.warp skimage.exposure.rescale_intensity</p> Source code in <code>ephysiopy/common/fieldcalcs.py</code> <pre><code>def deform_SAC(A, circleXY=None, ellipseXY=None):\n    \"\"\"\n    Deforms a SAC that is non-circular to be more circular\n\n    Basically a blatant attempt to improve grid scores, possibly\n    introduced in a paper by Matt Nolan...\n\n    Args:\n        A (array_like): The SAC\n        circleXY (array_like, optional): The xy coordinates defining a circle.\n        Default None.\n        ellipseXY (array_like, optional): The xy coordinates defining an\n        ellipse. Default None.\n\n    Returns:\n        deformed_sac (array_like): The SAC deformed to be more circular\n\n    See Also:\n        ephysiopy.common.ephys_generic.FieldCalcs.grid_field_props\n        skimage.transform.AffineTransform\n        skimage.transform.warp\n        skimage.exposure.rescale_intensity\n    \"\"\"\n    if circleXY is None or ellipseXY is None:\n        SAC_stats = grid_field_props(A)\n        circleXY = SAC_stats[\"circleXY\"]\n        ellipseXY = SAC_stats[\"ellipseXY\"]\n        # The ellipse detection stuff might have failed, if so\n        # return the original SAC\n        if circleXY is None:\n            warnings.warn(\"Ellipse detection failed. Returning original SAC\")\n            return A\n\n    tform = skimage.transform.AffineTransform()\n    tform.estimate(ellipseXY, circleXY)\n\n    \"\"\"\n    the transformation algorithms used here crop values &lt; 0 to 0. Need to\n    rescale the SAC values before doing the deformation and then rescale\n    again so the values assume the same range as in the unadulterated SAC\n    \"\"\"\n    A[np.isnan(A)] = 0\n    SACmin = np.nanmin(A.flatten())\n    SACmax = np.nanmax(A.flatten())  # should be 1 if autocorr\n    AA = A + 1\n    deformedSAC = skimage.transform.warp(\n        AA / np.nanmax(AA.flatten()), inverse_map=tform.inverse, cval=0\n    )\n    return skimage.exposure.rescale_intensity(deformedSAC, out_range=(SACmin, SACmax))\n</code></pre>"},{"location":"reference/#ephysiopy.common.fieldcalcs.field_lims","title":"<code>field_lims(A)</code>","text":"<p>Returns a labelled matrix of the ratemap A. Uses anything greater than the half peak rate to select as a field. Data is heavily smoothed.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>array</code> <p>The ratemap</p> required <p>Returns:</p> Name Type Description <code>label</code> <code>array</code> <p>The labelled ratemap</p> Source code in <code>ephysiopy/common/fieldcalcs.py</code> <pre><code>def field_lims(A):\n    \"\"\"\n    Returns a labelled matrix of the ratemap A.\n    Uses anything greater than the half peak rate to select as a field.\n    Data is heavily smoothed.\n\n    Args:\n        A (np.array): The ratemap\n\n    Returns:\n        label (np.array): The labelled ratemap\n    \"\"\"\n    nan_idx = np.isnan(A)\n    A[nan_idx] = 0\n    h = int(np.max(A.shape) / 2)\n    sm_rmap = blurImage(A, h, ftype=\"gaussian\")\n    thresh = np.max(sm_rmap.ravel()) * 0.2  # select area &gt; 20% of peak\n    distance = ndimage.distance_transform_edt(sm_rmap &gt; thresh)\n    peak_idx = skimage.feature.peak_local_max(\n        distance, exclude_border=False, labels=sm_rmap &gt; thresh\n    )\n    mask = np.zeros_like(distance, dtype=bool)\n    mask[tuple(peak_idx.T)] = True\n    label = ndimage.label(mask)[0]\n    w = watershed(image=-distance, markers=label, mask=sm_rmap &gt; thresh)\n    label = ndimage.label(w)[0]\n    return label\n</code></pre>"},{"location":"reference/#ephysiopy.common.fieldcalcs.field_props","title":"<code>field_props(A, min_dist=5, neighbours=2, prc=50, plot=False, ax=None, tri=False, verbose=True, **kwargs)</code>","text":"<p>Returns a dictionary of properties of the field(s) in a ratemap A</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>array_like</code> <p>a ratemap (but could be any image)</p> required <code>min_dist</code> <code>float</code> <p>the separation (in bins) between fields for measures such as field distance to make sense. Used to partition the image into separate fields in the call to feature.peak_local_max</p> <code>5</code> <code>neighbours</code> <code>int</code> <p>the number of fields to consider as neighbours to any given field. Defaults to 2</p> <code>2</code> <code>prc</code> <code>float</code> <p>percent of fields to consider</p> <code>50</code> <code>ax</code> <code>Axes</code> <p>user supplied axis. If None a new figure window</p> <code>None</code> <code>tri</code> <code>bool</code> <p>whether to do Delaunay triangulation between fields and add to plot</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>dumps the properties to the console</p> <code>True</code> <code>plot</code> <code>bool</code> <p>whether to plot some output - currently consists of the ratemap A, the fields of which are outline in a black contour. Default False</p> <code>False</code> <p>Returns:</p> Name Type Description <code>result</code> <code>dict</code> <p>The properties of the field(s) in the input ratemap A</p> Source code in <code>ephysiopy/common/fieldcalcs.py</code> <pre><code>def field_props(\n    A,\n    min_dist=5,\n    neighbours=2,\n    prc=50,\n    plot=False,\n    ax=None,\n    tri=False,\n    verbose=True,\n    **kwargs,\n):\n    \"\"\"\n    Returns a dictionary of properties of the field(s) in a ratemap A\n\n    Args:\n        A (array_like): a ratemap (but could be any image)\n        min_dist (float): the separation (in bins) between fields for measures\n            such as field distance to make sense. Used to\n            partition the image into separate fields in the call to\n            feature.peak_local_max\n        neighbours (int): the number of fields to consider as neighbours to\n            any given field. Defaults to 2\n        prc (float): percent of fields to consider\n        ax (matplotlib.Axes): user supplied axis. If None a new figure window\n        is created\n        tri (bool): whether to do Delaunay triangulation between fields\n            and add to plot\n        verbose (bool): dumps the properties to the console\n        plot (bool): whether to plot some output - currently consists of the\n            ratemap A, the fields of which are outline in a black\n            contour. Default False\n\n    Returns:\n        result (dict): The properties of the field(s) in the input ratemap A\n    \"\"\"\n\n    from skimage.measure import find_contours\n    from sklearn.neighbors import NearestNeighbors\n\n    nan_idx = np.isnan(A)\n    Ac = A.copy()\n    Ac[np.isnan(A)] = 0\n    # smooth Ac more to remove local irregularities\n    n = ny = 5\n    x, y = np.mgrid[-n : n + 1, -ny : ny + 1]\n    g = np.exp(-(x**2 / float(n) + y**2 / float(ny)))\n    g = g / g.sum()\n    Ac = signal.convolve(Ac, g, mode=\"same\")\n\n    peak_idx, field_labels = _get_field_labels(Ac, **kwargs)\n\n    nFields = np.max(field_labels)\n    if neighbours &gt; nFields:\n        print(\n            \"neighbours value of {0} &gt; the {1} peaks found\".format(neighbours, nFields)\n        )\n        print(\"Reducing neighbours to number of peaks found\")\n        neighbours = nFields\n    sub_field_mask = np.zeros((nFields, Ac.shape[0], Ac.shape[1]))\n    sub_field_props = skimage.measure.regionprops(field_labels, intensity_image=Ac)\n    sub_field_centroids = []\n    sub_field_size = []\n\n    for sub_field in sub_field_props:\n        tmp = np.zeros(Ac.shape).astype(bool)\n        tmp[sub_field.coords[:, 0], sub_field.coords[:, 1]] = True\n        tmp2 = Ac &gt; sub_field.max_intensity * (prc / float(100))\n        sub_field_mask[sub_field.label - 1, :, :] = np.logical_and(tmp2, tmp)\n        sub_field_centroids.append(sub_field.centroid)\n        sub_field_size.append(sub_field.area)  # in bins\n    sub_field_mask = np.sum(sub_field_mask, 0)\n    contours = skimage.measure.find_contours(sub_field_mask, 0.5)\n    # find the nearest neighbors to the peaks of each sub-field\n    nbrs = NearestNeighbors(n_neighbors=neighbours, algorithm=\"ball_tree\").fit(peak_idx)\n    distances, _ = nbrs.kneighbors(peak_idx)\n    mean_field_distance = np.mean(distances[:, 1:neighbours])\n\n    nValid_bins = np.sum(~nan_idx)\n    # calculate the amount of out of field firing\n    A_non_field = np.zeros_like(A) * np.nan\n    A_non_field[~sub_field_mask.astype(bool)] = A[~sub_field_mask.astype(bool)]\n    A_non_field[nan_idx] = np.nan\n    out_of_field_firing_prc = (\n        np.count_nonzero(A_non_field &gt; 0) / float(nValid_bins)\n    ) * 100\n    Ac[np.isnan(A)] = np.nan\n    \"\"\"\n    get some stats about the field ellipticity\n    \"\"\"\n    ellipse_ratio = np.nan\n    _, central_field, _ = limit_to_one(A, prc=50)\n\n    contour_coords = find_contours(central_field, 0.5)\n    from skimage.measure import EllipseModel\n\n    E = EllipseModel()\n    E.estimate(contour_coords[0])\n    ellipse_axes = E.params[2:4]\n    ellipse_ratio = np.min(ellipse_axes) / np.max(ellipse_axes)\n\n    \"\"\" using the peak_idx values calculate the angles of the triangles that\n    make up a delaunay tesselation of the space if the calc_angles arg is\n    in kwargs\n    \"\"\"\n    if \"calc_angs\" in kwargs.keys():\n        angs = calc_angs(peak_idx)\n    else:\n        angs = None\n\n    props = {\n        \"Ac\": Ac,\n        \"Peak_rate\": np.nanmax(A),\n        \"Mean_rate\": np.nanmean(A),\n        \"Field_size\": np.mean(sub_field_size),\n        \"Pct_bins_with_firing\": (np.sum(sub_field_mask) / nValid_bins) * 100,\n        \"Out_of_field_firing_prc\": out_of_field_firing_prc,\n        \"Dist_between_fields\": mean_field_distance,\n        \"Num_fields\": float(nFields),\n        \"Sub_field_mask\": sub_field_mask,\n        \"Smoothed_map\": Ac,\n        \"field_labels\": field_labels,\n        \"Peak_idx\": peak_idx,\n        \"angles\": angs,\n        \"contours\": contours,\n        \"ellipse_ratio\": ellipse_ratio,\n    }\n\n    if verbose:\n        print(\n            \"\\nPercentage of bins with firing: {:.2%}\".format(\n                np.sum(sub_field_mask) / nValid_bins\n            )\n        )\n        print(\n            \"Percentage out of field firing: {:.2%}\".format(\n                np.count_nonzero(A_non_field &gt; 0) / float(nValid_bins)\n            )\n        )\n        print(\"Peak firing rate: {:.3} Hz\".format(np.nanmax(A)))\n        print(\"Mean firing rate: {:.3} Hz\".format(np.nanmean(A)))\n        print(\"Number of fields: {0}\".format(nFields))\n        print(\"Mean field size: {:.5} cm\".format(np.mean(sub_field_size)))\n        print(\n            \"Mean inter-peak distance between \\\n            fields: {:.4} cm\".format(\n                mean_field_distance\n            )\n        )\n    return props\n</code></pre>"},{"location":"reference/#ephysiopy.common.fieldcalcs.get_circular_regions","title":"<code>get_circular_regions(A, **kwargs)</code>","text":"<p>Returns a list of images which are expanding circular regions centred on the middle of the image out to the image edge. Used for calculating the grid score of each image to find the one with the max grid score. Based on some Moser paper I can't recall.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>ndarray</code> <p>The SAC</p> required <p>Other Parameters:</p> Name Type Description <code>min_radius</code> <code>int</code> <p>The smallest radius circle to start with</p> Source code in <code>ephysiopy/common/fieldcalcs.py</code> <pre><code>def get_circular_regions(A: np.ndarray, **kwargs) -&gt; list:\n    \"\"\"\n    Returns a list of images which are expanding circular\n    regions centred on the middle of the image out to the\n    image edge. Used for calculating the grid score of each\n    image to find the one with the max grid score. Based on\n    some Moser paper I can't recall.\n\n    Args:\n        A (np.ndarray): The SAC\n\n    Keyword Args:\n        min_radius (int): The smallest radius circle to start with\n    \"\"\"\n    from skimage.measure import CircleModel, grid_points_in_poly\n\n    min_radius = 5\n    if \"min_radius\" in kwargs.keys():\n        min_radius = kwargs[\"min_radius\"]\n\n    centre = tuple([d // 2 for d in np.shape(A)])\n    max_radius = min(tuple(np.subtract(np.shape(A), centre)))\n    t = np.linspace(0, 2 * np.pi, 51)\n    circle = CircleModel()\n\n    result = []\n    for radius in range(min_radius, max_radius):\n        circle.params = [*centre, radius]\n        xy = circle.predict_xy(t)\n        mask = grid_points_in_poly(np.shape(A), xy)\n        im = A.copy()\n        im[~mask] = np.nan\n        result.append(im)\n    return result\n</code></pre>"},{"location":"reference/#ephysiopy.common.fieldcalcs.global_threshold","title":"<code>global_threshold(A, prc=50, min_dist=5)</code>","text":"<p>Globally thresholds a ratemap and counts number of fields found</p> Source code in <code>ephysiopy/common/fieldcalcs.py</code> <pre><code>def global_threshold(A, prc=50, min_dist=5):\n    \"\"\"\n    Globally thresholds a ratemap and counts number of fields found\n    \"\"\"\n    Ac = A.copy()\n    Ac[np.isnan(A)] = 0\n    n = ny = 5\n    x, y = np.mgrid[-n : n + 1, -ny : ny + 1]\n    g = np.exp(-(x**2 / float(n) + y**2 / float(ny)))\n    g = g / g.sum()\n    Ac = signal.convolve(Ac, g, mode=\"same\")\n    maxRate = np.nanmax(np.ravel(Ac))\n    Ac[Ac &lt; maxRate * (prc / float(100))] = 0\n    peak_idx = skimage.feature.peak_local_max(\n        Ac, min_distance=min_dist, exclude_border=False\n    )\n    peak_mask = np.zeros_like(Ac, dtype=bool)\n    peak_mask[tuple(peak_idx.T)] = True\n    peak_labels = skimage.measure.label(peak_mask, connectivity=2)\n    field_labels = watershed(image=-Ac, markers=peak_labels)\n    nFields = np.max(field_labels)\n    return nFields\n</code></pre>"},{"location":"reference/#ephysiopy.common.fieldcalcs.grid_field_props","title":"<code>grid_field_props(A, maxima='centroid', allProps=True, **kwargs)</code>","text":"<p>Extracts various measures from a spatial autocorrelogram</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>array_like</code> <p>The spatial autocorrelogram (SAC)</p> required <code>maxima</code> <code>str</code> <p>The method used to detect the peaks in the SAC. Legal values are 'single' and 'centroid'. Default 'centroid'</p> <code>'centroid'</code> <code>allProps</code> <code>bool</code> <p>Whether to return a dictionary that</p> <code>True</code> <p>Returns:</p> Name Type Description <code>props</code> <code>dict</code> <p>A dictionary containing measures of the SAC.</p> <p>Keys include: * gridness score * scale * orientation * coordinates of the peaks (nominally 6) closest to SAC centre * a binary mask around the extent of the 6 central fields * values of the rotation procedure used to calculate gridness * ellipse axes and angle (if allProps is True and the it worked)</p> Notes <p>The output from this method can be used as input to the show() method of this class. When it is the plot produced will display a lot more informative.</p> See Also <p>ephysiopy.common.binning.autoCorr2D()</p> Source code in <code>ephysiopy/common/fieldcalcs.py</code> <pre><code>def grid_field_props(A, maxima=\"centroid\", allProps=True, **kwargs):\n    \"\"\"\n    Extracts various measures from a spatial autocorrelogram\n\n    Args:\n        A (array_like): The spatial autocorrelogram (SAC)\n        maxima (str, optional): The method used to detect the peaks in the SAC.\n            Legal values are 'single' and 'centroid'. Default 'centroid'\n        allProps (bool, optional): Whether to return a dictionary that\n        contains the attempt to fit an ellipse around the edges of the\n        central size peaks. See below\n            Default True\n\n    Returns:\n        props (dict): A dictionary containing measures of the SAC.\n        Keys include:\n            * gridness score\n            * scale\n            * orientation\n            * coordinates of the peaks (nominally 6) closest to SAC centre\n            * a binary mask around the extent of the 6 central fields\n            * values of the rotation procedure used to calculate gridness\n            * ellipse axes and angle (if allProps is True and the it worked)\n\n    Notes:\n        The output from this method can be used as input to the show() method\n        of this class.\n        When it is the plot produced will display a lot more informative.\n\n    See Also:\n        ephysiopy.common.binning.autoCorr2D()\n    \"\"\"\n    A_tmp = A.copy()\n    A_tmp[~np.isfinite(A)] = -1\n    A_tmp[A_tmp &lt;= 0] = -1\n    A_sz = np.array(np.shape(A))\n    # [STAGE 1] find peaks &amp; identify 7 closest to centre\n    if \"min_distance\" in kwargs:\n        min_distance = kwargs.pop(\"min_distance\")\n    else:\n        min_distance = np.ceil(np.min(A_sz / 2) / 8.0).astype(int)\n\n    peak_idx, field_labels = _get_field_labels(A_tmp, neighbours=7, **kwargs)\n    # a fcn for the labeled_comprehension function that returns\n    # linear indices in A where the values in A for each label are\n    # greater than half the max in that labeled region\n\n    def fn(val, pos):\n        return pos[val &gt; (np.max(val) / 2)]\n\n    nLbls = np.max(field_labels)\n    indices = ndimage.labeled_comprehension(\n        A_tmp, field_labels, np.arange(0, nLbls), fn, np.ndarray, 0, True\n    )\n    # turn linear indices into coordinates\n    coords = [np.unravel_index(i, np.shape(A)) for i in indices]\n    half_peak_labels = np.zeros_like(A)\n    for peak_id, coord in enumerate(coords):\n        xc, yc = coord\n        half_peak_labels[xc, yc] = peak_id\n\n    # Get some statistics about the labeled regions\n    # fieldPerim = bwperim(half_peak_labels)\n    lbl_range = np.arange(0, nLbls)\n    # meanRInLabel = ndimage.mean(A, half_peak_labels, lbl_range)\n    # nPixelsInLabel = np.bincount(np.ravel(half_peak_labels.astype(int)))\n    # sumRInLabel = ndimage.sum_labels(A, half_peak_labels, lbl_range)\n    # maxRInLabel = ndimage.maximum(A, half_peak_labels, lbl_range)\n    peak_coords = ndimage.maximum_position(A, half_peak_labels, lbl_range)\n\n    # Get some distance and morphology measures\n    centre = np.floor(np.array(np.shape(A)) / 2)\n    centred_peak_coords = peak_coords - centre\n    peak_dist_to_centre = np.hypot(centred_peak_coords.T[0], centred_peak_coords.T[1])\n    closest_peak_idx = np.argsort(peak_dist_to_centre)\n    central_peak_label = closest_peak_idx[0]\n    closest_peak_idx = closest_peak_idx[1 : np.min((7, len(closest_peak_idx) - 1))]\n    # closest_peak_idx should now the indices of the labeled 6 peaks\n    # surrounding the central peak at the image centre\n    scale = np.median(peak_dist_to_centre[closest_peak_idx])\n    orientation = np.nan\n    orientation = grid_orientation(centred_peak_coords, closest_peak_idx)\n\n    central_pt = peak_coords[central_peak_label]\n    x = np.linspace(-central_pt[0], central_pt[0], A_sz[0])\n    y = np.linspace(-central_pt[1], central_pt[1], A_sz[1])\n    xv, yv = np.meshgrid(x, y, indexing=\"ij\")\n    dist_to_centre = np.hypot(xv, yv)\n    # get the max distance of the half-peak width labeled fields\n    # from the centre of the image\n    max_dist_from_centre = 0\n    for peak_id, _coords in enumerate(coords):\n        if peak_id in closest_peak_idx:\n            xc, yc = _coords\n            if np.any(xc) and np.any(yc):\n                xc = xc - np.floor(A_sz[0] / 2)\n                yc = yc - np.floor(A_sz[1] / 2)\n                d = np.max(np.hypot(xc, yc))\n                if d &gt; max_dist_from_centre:\n                    max_dist_from_centre = d\n\n    # Set the outer bits and the central region of the SAC to nans\n    # getting ready for the correlation procedure\n    dist_to_centre[np.abs(dist_to_centre) &gt; max_dist_from_centre] = 0\n    dist_to_centre[half_peak_labels == central_peak_label] = 0\n    dist_to_centre[dist_to_centre != 0] = 1\n    dist_to_centre = dist_to_centre.astype(bool)\n    sac_middle = A.copy()\n    sac_middle[~dist_to_centre] = np.nan\n\n    if \"step\" in kwargs.keys():\n        step = kwargs.pop(\"step\")\n    else:\n        step = 30\n    try:\n        gridscore, rotationCorrVals, rotationArr = gridness(sac_middle, step=step)\n    except Exception:\n        gridscore, rotationCorrVals, rotationArr = np.nan, np.nan, np.nan\n\n    im_centre = central_pt\n\n    if allProps:\n        # attempt to fit an ellipse around the outer edges of the nearest\n        # peaks to the centre of the SAC. First find the outer edges for\n        # the closest peaks using a ndimages labeled_comprehension\n        try:\n\n            def fn2(val, pos):\n                xc, yc = np.unravel_index(pos, A_sz)\n                xc = xc - np.floor(A_sz[0] / 2)\n                yc = yc - np.floor(A_sz[1] / 2)\n                idx = np.argmax(np.hypot(xc, yc))\n                return xc[idx], yc[idx]\n\n            ellipse_coords = ndimage.labeled_comprehension(\n                A, half_peak_labels, closest_peak_idx, fn2, tuple, 0, True\n            )\n\n            ellipse_fit_coords = np.array([(x, y) for x, y in ellipse_coords])\n            from skimage.measure import EllipseModel\n\n            E = EllipseModel()\n            E.estimate(ellipse_fit_coords)\n            im_centre = E.params[0:2]\n            ellipse_axes = E.params[2:4]\n            ellipse_angle = E.params[-1]\n            ellipseXY = E.predict_xy(np.linspace(0, 2 * np.pi, 50), E.params)\n\n            # get the min containing circle given the eliipse minor axis\n            from skimage.measure import CircleModel\n\n            _params = [im_centre, np.min(ellipse_axes)]\n            circleXY = CircleModel().predict_xy(\n                np.linspace(0, 2 * np.pi, 50), params=_params\n            )\n        except (TypeError, ValueError):  # non-iterable x and y\n            ellipse_axes = None\n            ellipse_angle = (None, None)\n            ellipseXY = None\n            circleXY = None\n\n    # collect all the following keywords into a dict for output\n    closest_peak_coords = np.array(peak_coords)[closest_peak_idx]\n    dictKeys = (\n        \"gridscore\",\n        \"scale\",\n        \"orientation\",\n        \"closest_peak_coords\",\n        \"dist_to_centre\",\n        \"ellipse_axes\",\n        \"ellipse_angle\",\n        \"ellipseXY\",\n        \"circleXY\",\n        \"im_centre\",\n        \"rotationArr\",\n        \"rotationCorrVals\",\n    )\n    outDict = dict.fromkeys(dictKeys, np.nan)\n    for thiskey in outDict.keys():\n        outDict[thiskey] = locals()[thiskey]\n        # neat trick: locals is a dict holding all locally scoped variables\n    return outDict\n</code></pre>"},{"location":"reference/#ephysiopy.common.fieldcalcs.grid_orientation","title":"<code>grid_orientation(peakCoords, closestPeakIdx)</code>","text":"<p>Calculates the orientation angle of a grid field.</p> <p>The orientation angle is the angle of the first peak working counter-clockwise from 3 o'clock</p> <p>Parameters:</p> Name Type Description Default <code>peakCoords</code> <code>array_like</code> <p>The peak coordinates as pairs of xy</p> required <code>closestPeakIdx</code> <code>array_like</code> <p>A 1D array of the indices in peakCoords</p> required <p>Returns:</p> Name Type Description <code>peak_orientation</code> <code>float</code> <p>The first value in an array of the angles of</p> <p>the peaks in the SAC working counter-clockwise from a line</p> <p>extending from the middle of the SAC to 3 o'clock.</p> Source code in <code>ephysiopy/common/fieldcalcs.py</code> <pre><code>def grid_orientation(peakCoords, closestPeakIdx):\n    \"\"\"\n    Calculates the orientation angle of a grid field.\n\n    The orientation angle is the angle of the first peak working\n    counter-clockwise from 3 o'clock\n\n    Args:\n        peakCoords (array_like): The peak coordinates as pairs of xy\n        closestPeakIdx (array_like): A 1D array of the indices in peakCoords\n        of the peaks closest to the centre of the SAC\n\n    Returns:\n        peak_orientation (float): The first value in an array of the angles of\n        the peaks in the SAC working counter-clockwise from a line\n        extending from the middle of the SAC to 3 o'clock.\n    \"\"\"\n    if len(peakCoords) &lt; 3 or closestPeakIdx.size == 0:\n        return np.nan\n    else:\n        from ephysiopy.common.utils import polar\n\n        # Assume that the first entry in peakCoords is\n        # the central peak of the SAC\n        peaks = peakCoords[closestPeakIdx]\n        peaks = peaks - peakCoords[closestPeakIdx[0]]\n        theta = polar(peaks[:, 1], -peaks[:, 0], deg=1)[1]\n        return np.sort(theta.compress(theta &gt;= 0))[0]\n</code></pre>"},{"location":"reference/#ephysiopy.common.fieldcalcs.gridness","title":"<code>gridness(image, step=30)</code>","text":"<p>Calculates the gridness score in a grid cell SAC.</p> <p>Briefly, the data in <code>image</code> is rotated in <code>step</code> amounts and each rotated array is correlated with the original. The maximum of the values at 30, 90 and 150 degrees is the subtracted from the minimum of the values at 60, 120 and 180 degrees to give the grid score.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>array_like</code> <p>The spatial autocorrelogram</p> required <code>step</code> <code>int</code> <p>The amount to rotate the SAC in each step of the</p> <code>30</code> <p>Returns:</p> Name Type Description <code>gridmeasures</code> <code>3 - tuple</code> <p>The gridscore, the correlation values at each</p> <p><code>step</code> and the rotational array</p> Notes <p>The correlation performed is a Pearsons R. Some rescaling of the values in <code>image</code> is performed following rotation.</p> See Also <p>skimage.transform.rotate : for how the rotation of <code>image</code> is done skimage.exposure.rescale_intensity : for the resscaling following rotation</p> Source code in <code>ephysiopy/common/fieldcalcs.py</code> <pre><code>def gridness(image, step=30):\n    \"\"\"\n    Calculates the gridness score in a grid cell SAC.\n\n    Briefly, the data in `image` is rotated in `step` amounts and\n    each rotated array is correlated with the original.\n    The maximum of the values at 30, 90 and 150 degrees\n    is the subtracted from the minimum of the values at 60, 120\n    and 180 degrees to give the grid score.\n\n    Args:\n        image (array_like): The spatial autocorrelogram\n        step (int, optional): The amount to rotate the SAC in each step of the\n        rotational correlation procedure\n\n    Returns:\n        gridmeasures (3-tuple): The gridscore, the correlation values at each\n        `step` and the rotational array\n\n    Notes:\n        The correlation performed is a Pearsons R. Some rescaling of the\n        values in `image` is performed following rotation.\n\n    See Also:\n        skimage.transform.rotate : for how the rotation of `image` is done\n        skimage.exposure.rescale_intensity : for the resscaling following\n        rotation\n    \"\"\"\n    # TODO: add options in here for whether the full range of correlations\n    # are wanted or whether a reduced set is wanted (i.e. at the 30-tuples)\n    from collections import OrderedDict\n\n    rotationalCorrVals = OrderedDict.fromkeys(np.arange(0, 181, step), np.nan)\n    rotationArr = np.zeros(len(rotationalCorrVals)) * np.nan\n    # autoCorrMiddle needs to be rescaled or the image rotation falls down\n    # as values are cropped to lie between 0 and 1.0\n    in_range = (np.nanmin(image), np.nanmax(image))\n    out_range = (0, 1)\n    import skimage\n\n    autoCorrMiddleRescaled = skimage.exposure.rescale_intensity(\n        image, in_range=in_range, out_range=out_range\n    )\n    origNanIdx = np.isnan(autoCorrMiddleRescaled.ravel())\n    for idx, angle in enumerate(rotationalCorrVals.keys()):\n        rotatedA = skimage.transform.rotate(\n            autoCorrMiddleRescaled, angle=angle, cval=np.nan, order=3\n        )\n        # ignore nans\n        rotatedNanIdx = np.isnan(rotatedA.ravel())\n        allNans = np.logical_or(origNanIdx, rotatedNanIdx)\n        # get the correlation between the original and rotated images\n        rotationalCorrVals[angle] = stats.pearsonr(\n            autoCorrMiddleRescaled.ravel()[~allNans], rotatedA.ravel()[~allNans]\n        )[0]\n        rotationArr[idx] = rotationalCorrVals[angle]\n    gridscore = np.min((rotationalCorrVals[60], rotationalCorrVals[120])) - np.max(\n        (rotationalCorrVals[150], rotationalCorrVals[30], rotationalCorrVals[90])\n    )\n    return gridscore, rotationalCorrVals, rotationArr\n</code></pre>"},{"location":"reference/#ephysiopy.common.fieldcalcs.kldiv","title":"<code>kldiv(X, pvect1, pvect2, variant=None)</code>","text":"<p>Calculates the Kullback-Leibler or Jensen-Shannon divergence between two distributions.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array_like</code> <p>Vector of M variable values</p> required <code>P1</code> <code>array_like</code> <p>Length-M vector of probabilities representing</p> required <code>P2</code> <code>array_like</code> <p>Length-M vector of probabilities representing</p> required <code>sym</code> <code>str</code> <p>If 'sym', returns a symmetric variant of the Kullback-Leibler divergence, given by [KL(P1,P2)+KL(P2,P1)]/2</p> required <code>js</code> <code>str</code> <p>If 'js', returns the Jensen-Shannon divergence,</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>The Kullback-Leibler divergence or Jensen-Shannon divergence</p> Notes <p>The Kullback-Leibler divergence is given by:</p> <p>.. math:: KL(P1(x),P2(x)) = sum_[P1(x).log(P1(x)/P2(x))]</p> <p>If X contains duplicate values, there will be an warning message, and these values will be treated as distinct values.  (I.e., the actual values do not enter into the computation, but the probabilities for the two duplicate values will be considered as probabilities corresponding to two unique values.). The elements of probability vectors P1 and P2 must each sum to 1 +/- .00001.</p> <p>This function is taken from one on the Mathworks file exchange</p> See Also <p>Cover, T.M. and J.A. Thomas. \"Elements of Information Theory,\" Wiley, 1991.</p> <p>https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence</p> Source code in <code>ephysiopy/common/fieldcalcs.py</code> <pre><code>def kldiv(X, pvect1, pvect2, variant=None):\n    \"\"\"\n    Calculates the Kullback-Leibler or Jensen-Shannon divergence between\n    two distributions.\n\n    Args:\n        X (array_like): Vector of M variable values\n        P1 (array_like): Length-M vector of probabilities representing\n        distribution 1\n        P2 (array_like): Length-M vector of probabilities representing\n        distribution 2\n        sym (str, optional): If 'sym', returns a symmetric variant of the\n            Kullback-Leibler divergence, given by [KL(P1,P2)+KL(P2,P1)]/2\n        js (str, optional): If 'js', returns the Jensen-Shannon divergence,\n        given by\n            [KL(P1,Q)+KL(P2,Q)]/2, where Q = (P1+P2)/2\n\n    Returns:\n        float: The Kullback-Leibler divergence or Jensen-Shannon divergence\n\n    Notes:\n        The Kullback-Leibler divergence is given by:\n\n        .. math:: KL(P1(x),P2(x)) = sum_[P1(x).log(P1(x)/P2(x))]\n\n        If X contains duplicate values, there will be an warning message,\n        and these values will be treated as distinct values.  (I.e., the\n        actual values do not enter into the computation, but the probabilities\n        for the two duplicate values will be considered as probabilities\n        corresponding to two unique values.).\n        The elements of probability vectors P1 and P2 must\n        each sum to 1 +/- .00001.\n\n        This function is taken from one on the Mathworks file exchange\n\n    See Also:\n        Cover, T.M. and J.A. Thomas. \"Elements of Information Theory,\" Wiley,\n        1991.\n\n        https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence\n    \"\"\"\n\n    if len(np.unique(X)) != len(np.sort(X)):\n        warnings.warn(\n            \"X contains duplicate values. Treated as distinct values.\", UserWarning\n        )\n    if (\n        not np.equal(np.shape(X), np.shape(pvect1)).all()\n        or not np.equal(np.shape(X), np.shape(pvect2)).all()\n    ):\n        raise ValueError(\"Inputs are not the same size\")\n    if (np.abs(np.sum(pvect1) - 1) &gt; 0.00001) or (np.abs(np.sum(pvect2) - 1) &gt; 0.00001):\n        print(f\"Probabilities sum to {np.abs(np.sum(pvect1))} for pvect1\")\n        print(f\"Probabilities sum to {np.abs(np.sum(pvect2))} for pvect2\")\n        warnings.warn(\"Probabilities don\" \"t sum to 1.\", UserWarning)\n    if variant:\n        if variant == \"js\":\n            logqvect = np.log2((pvect2 + pvect1) / 2)\n            KL = 0.5 * (\n                np.nansum(pvect1 * (np.log2(pvect1) - logqvect))\n                + np.sum(pvect2 * (np.log2(pvect2) - logqvect))\n            )\n            return KL\n        elif variant == \"sym\":\n            KL1 = np.nansum(pvect1 * (np.log2(pvect1) - np.log2(pvect2)))\n            KL2 = np.nansum(pvect2 * (np.log2(pvect2) - np.log2(pvect1)))\n            KL = (KL1 + KL2) / 2\n            return KL\n        else:\n            warnings.warn(\"Last argument not recognised\", UserWarning)\n    KL = np.nansum(pvect1 * (np.log2(pvect1) - np.log2(pvect2)))\n    return KL\n</code></pre>"},{"location":"reference/#ephysiopy.common.fieldcalcs.kldiv_dir","title":"<code>kldiv_dir(polarPlot)</code>","text":"<p>Returns a kl divergence for directional firing: measure of directionality. Calculates kl diveregence between a smoothed ratemap (probably should be smoothed otherwise information theoretic measures don't 'care' about position of bins relative to one another) and a pure circular distribution. The larger the divergence the more tendancy the cell has to fire when the animal faces a specific direction.</p> <p>Parameters:</p> Name Type Description Default <code>polarPlot</code> <code>1D-array</code> <p>The binned and smoothed directional ratemap</p> required <p>Returns:</p> Name Type Description <code>klDivergence</code> <code>float</code> <p>The divergence from circular of the 1D-array</p> <p>from a uniform circular distribution</p> Source code in <code>ephysiopy/common/fieldcalcs.py</code> <pre><code>def kldiv_dir(polarPlot):\n    \"\"\"\n    Returns a kl divergence for directional firing: measure of directionality.\n    Calculates kl diveregence between a smoothed ratemap (probably should be\n    smoothed otherwise information theoretic measures\n    don't 'care' about position of bins relative to one another) and a\n    pure circular distribution.\n    The larger the divergence the more tendancy the cell has to fire when the\n    animal faces a specific direction.\n\n    Args:\n        polarPlot (1D-array): The binned and smoothed directional ratemap\n\n    Returns:\n        klDivergence (float): The divergence from circular of the 1D-array\n        from a uniform circular distribution\n    \"\"\"\n\n    __inc = 0.00001\n    polarPlot = np.atleast_2d(polarPlot)\n    polarPlot[np.isnan(polarPlot)] = __inc\n    polarPlot[polarPlot == 0] = __inc\n    normdPolar = polarPlot / float(np.nansum(polarPlot))\n    nDirBins = polarPlot.shape[1]\n    compCirc = np.ones_like(polarPlot) / float(nDirBins)\n    X = np.arange(0, nDirBins)\n    kldivergence = kldiv(np.atleast_2d(X), normdPolar, compCirc)\n    return kldivergence\n</code></pre>"},{"location":"reference/#ephysiopy.common.fieldcalcs.limit_to_one","title":"<code>limit_to_one(A, prc=50, min_dist=5)</code>","text":"<p>Processes a multi-peaked ratemap (ie grid cell) and returns a matrix where the multi-peaked ratemap consist of a single peaked field that is a) not connected to the border and b) close to the middle of the ratemap</p> Source code in <code>ephysiopy/common/fieldcalcs.py</code> <pre><code>def limit_to_one(A, prc=50, min_dist=5):\n    \"\"\"\n    Processes a multi-peaked ratemap (ie grid cell) and returns a matrix\n    where the multi-peaked ratemap consist of a single peaked field that is\n    a) not connected to the border and b) close to the middle of the\n    ratemap\n    \"\"\"\n    Ac = A.copy()\n    Ac[np.isnan(A)] = 0\n    # smooth Ac more to remove local irregularities\n    n = ny = 5\n    x, y = np.mgrid[-n : n + 1, -ny : ny + 1]\n    g = np.exp(-(x**2 / float(n) + y**2 / float(ny)))\n    g = g / g.sum()\n    Ac = signal.convolve(Ac, g, mode=\"same\")\n    # remove really small values\n    Ac[Ac &lt; 1e-10] = 0\n    peak_idx = skimage.feature.peak_local_max(\n        Ac, min_distance=min_dist, exclude_border=False\n    )\n    peak_mask = np.zeros_like(Ac, dtype=bool)\n    peak_mask[tuple(peak_idx.T)] = True\n    peak_labels = skimage.measure.label(peak_mask, connectivity=2)\n    field_labels = watershed(image=-Ac, markers=peak_labels)\n    nFields = np.max(field_labels)\n    sub_field_mask = np.zeros((nFields, Ac.shape[0], Ac.shape[1]))\n    labelled_sub_field_mask = np.zeros_like(sub_field_mask)\n    sub_field_props = skimage.measure.regionprops(field_labels, intensity_image=Ac)\n    sub_field_centroids = []\n    sub_field_size = []\n\n    for sub_field in sub_field_props:\n        tmp = np.zeros(Ac.shape).astype(bool)\n        tmp[sub_field.coords[:, 0], sub_field.coords[:, 1]] = True\n        tmp2 = Ac &gt; sub_field.max_intensity * (prc / float(100))\n        sub_field_mask[sub_field.label - 1, :, :] = np.logical_and(tmp2, tmp)\n        labelled_sub_field_mask[sub_field.label - 1, np.logical_and(tmp2, tmp)] = (\n            sub_field.label\n        )\n        sub_field_centroids.append(sub_field.centroid)\n        sub_field_size.append(sub_field.area)  # in bins\n    sub_field_mask = np.sum(sub_field_mask, 0)\n    middle = np.round(np.array(A.shape) / 2)\n    normd_dists = sub_field_centroids - middle\n    field_dists_from_middle = np.hypot(normd_dists[:, 0], normd_dists[:, 1])\n    central_field_idx = np.argmin(field_dists_from_middle)\n    central_field = np.squeeze(labelled_sub_field_mask[central_field_idx, :, :])\n    # collapse the labelled mask down to an 2d array\n    labelled_sub_field_mask = np.sum(labelled_sub_field_mask, 0)\n    # clear the border\n    cleared_mask = skimage.segmentation.clear_border(central_field)\n    # check we've still got stuff in the matrix or fail\n    if ~np.any(cleared_mask):\n        print(\"No fields were detected away from edges so nothing returned\")\n        return None, None, None\n    else:\n        central_field_props = sub_field_props[central_field_idx]\n    return central_field_props, central_field, central_field_idx\n</code></pre>"},{"location":"reference/#ephysiopy.common.fieldcalcs.local_threshold","title":"<code>local_threshold(A, prc=50, min_dist=5)</code>","text":"<p>Locally thresholds a ratemap to take only the surrounding prc amount around any local peak</p> Source code in <code>ephysiopy/common/fieldcalcs.py</code> <pre><code>def local_threshold(A, prc=50, min_dist=5):\n    \"\"\"\n    Locally thresholds a ratemap to take only the surrounding prc amount\n    around any local peak\n    \"\"\"\n    Ac = A.copy()\n    nanidx = np.isnan(Ac)\n    Ac[nanidx] = 0\n    # smooth Ac more to remove local irregularities\n    n = ny = 5\n    x, y = np.mgrid[-n : n + 1, -ny : ny + 1]\n    g = np.exp(-(x**2 / float(n) + y**2 / float(ny)))\n    g = g / g.sum()\n    Ac = signal.convolve(Ac, g, mode=\"same\")\n    Ac_r = skimage.exposure.rescale_intensity(\n        Ac, in_range=\"image\", out_range=(0, 1000)\n    ).astype(np.int32)\n    peak_idx = skimage.feature.peak_local_max(\n        Ac_r, min_distance=min_dist, exclude_border=False\n    )\n    peak_mask = np.zeros_like(Ac, dtype=bool)\n    peak_mask[tuple(peak_idx.T)] = True\n    peak_labels = skimage.measure.label(peak_mask, connectivity=2)\n    field_labels = watershed(image=-Ac, markers=peak_labels)\n    nFields = np.max(field_labels)\n    sub_field_mask = np.zeros((nFields, Ac.shape[0], Ac.shape[1]))\n    sub_field_props = skimage.measure.regionprops(field_labels, intensity_image=Ac)\n    sub_field_centroids = []\n    sub_field_size = []\n\n    for sub_field in sub_field_props:\n        tmp = np.zeros(Ac.shape).astype(bool)\n        tmp[sub_field.coords[:, 0], sub_field.coords[:, 1]] = True\n        tmp2 = Ac &gt; sub_field.max_intensity * (prc / float(100))\n        sub_field_mask[sub_field.label - 1, :, :] = np.logical_and(tmp2, tmp)\n        sub_field_centroids.append(sub_field.centroid)\n        sub_field_size.append(sub_field.area)  # in bins\n    sub_field_mask = np.sum(sub_field_mask, 0)\n    A_out = np.zeros_like(A)\n    A_out[sub_field_mask.astype(bool)] = A[sub_field_mask.astype(bool)]\n    A_out[nanidx] = np.nan\n    return A_out\n</code></pre>"},{"location":"reference/#ephysiopy.common.fieldcalcs.skaggs_info","title":"<code>skaggs_info(ratemap, dwelltimes, **kwargs)</code>","text":"<p>Calculates Skaggs information measure</p> <p>Parameters:</p> Name Type Description Default <code>ratemap</code> <code>array_like</code> <p>The binned up ratemap</p> required <code>dwelltimes</code> <code>array_like</code> <p>Must be same size as ratemap</p> required <p>Returns:</p> Name Type Description <code>bits_per_spike</code> <code>float</code> <p>Skaggs information score</p> Notes <p>THIS DATA SHOULD UNDERGO ADAPTIVE BINNING See getAdaptiveMap() in binning class</p> <p>Returns Skaggs et al's estimate of spatial information in bits per spike:</p> <p>.. math:: I = sum_{x} p(x).r(x).log(r(x)/r)</p> Source code in <code>ephysiopy/common/fieldcalcs.py</code> <pre><code>def skaggs_info(ratemap, dwelltimes, **kwargs):\n    \"\"\"\n    Calculates Skaggs information measure\n\n    Args:\n        ratemap (array_like): The binned up ratemap\n        dwelltimes (array_like): Must be same size as ratemap\n\n    Returns:\n        bits_per_spike (float): Skaggs information score\n\n    Notes:\n        THIS DATA SHOULD UNDERGO ADAPTIVE BINNING\n        See getAdaptiveMap() in binning class\n\n        Returns Skaggs et al's estimate of spatial information\n        in bits per spike:\n\n        .. math:: I = sum_{x} p(x).r(x).log(r(x)/r)\n    \"\"\"\n    if \"sample_rate\" in kwargs:\n        sample_rate = kwargs[\"sample_rate\"]\n    else:\n        sample_rate = 50\n\n    dwelltimes = dwelltimes / sample_rate  # assumed sample rate of 50Hz\n    if ratemap.ndim &gt; 1:\n        ratemap = np.reshape(ratemap, (np.prod(np.shape(ratemap)), 1))\n        dwelltimes = np.reshape(dwelltimes, (np.prod(np.shape(dwelltimes)), 1))\n    duration = np.nansum(dwelltimes)\n    meanrate = np.nansum(ratemap * dwelltimes) / duration\n    if meanrate &lt;= 0.0:\n        bits_per_spike = np.nan\n        return bits_per_spike\n    p_x = dwelltimes / duration\n    p_r = ratemap / meanrate\n    dum = p_x * ratemap\n    ind = np.nonzero(dum)[0]\n    bits_per_spike = np.nansum(dum[ind] * np.log2(p_r[ind]))\n    bits_per_spike = bits_per_spike / meanrate\n    return bits_per_spike\n</code></pre>"},{"location":"reference/#grid-cells","title":"Grid cells","text":""},{"location":"reference/#ephysiopy.common.gridcell.SAC","title":"<code>SAC</code>","text":"<p>               Bases: <code>object</code></p> <p>Spatial AutoCorrelation (SAC) class</p> Source code in <code>ephysiopy/common/gridcell.py</code> <pre><code>class SAC(object):\n    \"\"\"\n    Spatial AutoCorrelation (SAC) class\n    \"\"\"\n\n    def getMeasures(\n            self, A, maxima='centroid',\n            allProps=True, **kwargs):\n        \"\"\"\n        Extracts various measures from a spatial autocorrelogram\n\n        Args:\n            A (array_like): The spatial autocorrelogram (SAC)\n            maxima (str, optional): The method used to detect the peaks in the\n            SAC.\n                Legal values are 'single' and 'centroid'. Default 'centroid'\n            field_extent_method (int, optional): The method used to delimit\n            the regions of interest in the SAC\n                Legal values:\n                * 1 - uses half height of the ROI peak to limit field extent\n                * 2 - uses a watershed method to limit field extent\n                Default 2\n            allProps (bool, optional): Whether to return a dictionary that\n            contains the attempt to fit an ellipse around the edges of the\n            central size peaks. See below. Default True\n\n        Returns:\n            props (dict): A dictionary containing measures of the SAC. \n            Keys include:\n                * gridness score\n                * scale\n                * orientation\n                * the coordinates of the peaks closest to  SAC centre\n                * a binary mask that defines the extent of the 6 central fields\n                * values of the rotation procedure used to calculate gridness\n                * ellipse axes and angle (if allProps is True and it worked)\n\n        Notes:\n            In order to maintain backward compatibility this is a wrapper for\n            ephysiopy.common.ephys_generic.FieldCalcs.grid_field_props()\n\n        See Also:\n            ephysiopy.common.ephys_generic.FieldCalcs.grid_field_props()\n        \"\"\"\n        return fieldcalcs.grid_field_props(\n            A, maxima, allProps, **kwargs)\n\n    def get_basic_gridscore(self, A: np.ndarray, step: int = 30, **kwargs):\n        '''\n        Rotates the image A in step amounts, correlated each rotated image\n        with the original. The maximum of the values at 30, 90 and 150 degrees\n        is the subtracted from the minimum of the values at 60, 120\n        and 180 degrees to give the grid score.\n        '''\n        from ephysiopy.common.fieldcalcs import gridness\n        return gridness(A, step)[0]\n\n    def get_expanding_circle_gridscore(self, A: np.ndarray, **kwargs):\n        '''\n        Calculates the gridscore for each circular sub-region of image A\n        where the circles are centred on the image centre and expanded to\n        the edge of the image. The maximum of the get_basic_gridscore() for\n        each of these circular sub-regions is returned as the gridscore\n        '''\n\n        from ephysiopy.common.fieldcalcs import get_circular_regions\n        images = get_circular_regions(A, **kwargs)\n        gridscores = [self.get_basic_gridscore(im) for im in images]\n        return max(gridscores)\n\n    def get_deformed_sac_gridscore(self, A: np.ndarray, **kwargs):\n        '''\n        Deforms a non-circular SAC into a circular SAC (circular meaning\n        the ellipse drawn around the edges of the 6 nearest peaks to the\n        SAC centre) and returns get_basic_griscore() calculated on the \n        deformed (or re-formed?!) SAC\n        '''\n        from ephysiopy.common.fieldcalcs import deform_SAC\n        deformed_SAC = deform_SAC(A)\n        return self.get_basic_gridscore(deformed_SAC)\n\n    def show(self, A, inDict, ax=None, **kwargs):\n        \"\"\"\n        Displays the result of performing a spatial autocorrelation (SAC)\n        on a grid cell.\n\n        Uses the dictionary containing measures of the grid cell SAC to\n        make a pretty picture\n\n        Args:\n            A (array_like): The spatial autocorrelogram\n            inDict (dict): The dictionary calculated in getmeasures\n            ax (matplotlib.axes._subplots.AxesSubplot, optional).\n            If given the plot will get drawn in these axes. Default None\n\n        Returns:\n            fig (matplotlib.Figure instance): The Figure on which the SAC is\n            shown\n\n        See Also:\n            ephysiopy.common.binning.RateMap.autoCorr2D()\n            ephysiopy.common.ephys_generic.FieldCalcs.getMeaures()\n        \"\"\"\n        from ephysiopy.visualise.plotting import FigureMaker\n        F = FigureMaker()\n        F.show_SAC(A, inDict, ax, **kwargs)\n</code></pre>"},{"location":"reference/#ephysiopy.common.gridcell.SAC.getMeasures","title":"<code>getMeasures(A, maxima='centroid', allProps=True, **kwargs)</code>","text":"<p>Extracts various measures from a spatial autocorrelogram</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>array_like</code> <p>The spatial autocorrelogram (SAC)</p> required <code>maxima</code> <code>str</code> <p>The method used to detect the peaks in the</p> <code>'centroid'</code> <code>field_extent_method</code> <code>int</code> <p>The method used to delimit</p> required <code>allProps</code> <code>bool</code> <p>Whether to return a dictionary that</p> <code>True</code> <p>Returns:</p> Name Type Description <code>props</code> <code>dict</code> <p>A dictionary containing measures of the SAC. </p> <p>Keys include: * gridness score * scale * orientation * the coordinates of the peaks closest to  SAC centre * a binary mask that defines the extent of the 6 central fields * values of the rotation procedure used to calculate gridness * ellipse axes and angle (if allProps is True and it worked)</p> Notes <p>In order to maintain backward compatibility this is a wrapper for ephysiopy.common.ephys_generic.FieldCalcs.grid_field_props()</p> See Also <p>ephysiopy.common.ephys_generic.FieldCalcs.grid_field_props()</p> Source code in <code>ephysiopy/common/gridcell.py</code> <pre><code>def getMeasures(\n        self, A, maxima='centroid',\n        allProps=True, **kwargs):\n    \"\"\"\n    Extracts various measures from a spatial autocorrelogram\n\n    Args:\n        A (array_like): The spatial autocorrelogram (SAC)\n        maxima (str, optional): The method used to detect the peaks in the\n        SAC.\n            Legal values are 'single' and 'centroid'. Default 'centroid'\n        field_extent_method (int, optional): The method used to delimit\n        the regions of interest in the SAC\n            Legal values:\n            * 1 - uses half height of the ROI peak to limit field extent\n            * 2 - uses a watershed method to limit field extent\n            Default 2\n        allProps (bool, optional): Whether to return a dictionary that\n        contains the attempt to fit an ellipse around the edges of the\n        central size peaks. See below. Default True\n\n    Returns:\n        props (dict): A dictionary containing measures of the SAC. \n        Keys include:\n            * gridness score\n            * scale\n            * orientation\n            * the coordinates of the peaks closest to  SAC centre\n            * a binary mask that defines the extent of the 6 central fields\n            * values of the rotation procedure used to calculate gridness\n            * ellipse axes and angle (if allProps is True and it worked)\n\n    Notes:\n        In order to maintain backward compatibility this is a wrapper for\n        ephysiopy.common.ephys_generic.FieldCalcs.grid_field_props()\n\n    See Also:\n        ephysiopy.common.ephys_generic.FieldCalcs.grid_field_props()\n    \"\"\"\n    return fieldcalcs.grid_field_props(\n        A, maxima, allProps, **kwargs)\n</code></pre>"},{"location":"reference/#ephysiopy.common.gridcell.SAC.get_basic_gridscore","title":"<code>get_basic_gridscore(A, step=30, **kwargs)</code>","text":"<p>Rotates the image A in step amounts, correlated each rotated image with the original. The maximum of the values at 30, 90 and 150 degrees is the subtracted from the minimum of the values at 60, 120 and 180 degrees to give the grid score.</p> Source code in <code>ephysiopy/common/gridcell.py</code> <pre><code>def get_basic_gridscore(self, A: np.ndarray, step: int = 30, **kwargs):\n    '''\n    Rotates the image A in step amounts, correlated each rotated image\n    with the original. The maximum of the values at 30, 90 and 150 degrees\n    is the subtracted from the minimum of the values at 60, 120\n    and 180 degrees to give the grid score.\n    '''\n    from ephysiopy.common.fieldcalcs import gridness\n    return gridness(A, step)[0]\n</code></pre>"},{"location":"reference/#ephysiopy.common.gridcell.SAC.get_deformed_sac_gridscore","title":"<code>get_deformed_sac_gridscore(A, **kwargs)</code>","text":"<p>Deforms a non-circular SAC into a circular SAC (circular meaning the ellipse drawn around the edges of the 6 nearest peaks to the SAC centre) and returns get_basic_griscore() calculated on the  deformed (or re-formed?!) SAC</p> Source code in <code>ephysiopy/common/gridcell.py</code> <pre><code>def get_deformed_sac_gridscore(self, A: np.ndarray, **kwargs):\n    '''\n    Deforms a non-circular SAC into a circular SAC (circular meaning\n    the ellipse drawn around the edges of the 6 nearest peaks to the\n    SAC centre) and returns get_basic_griscore() calculated on the \n    deformed (or re-formed?!) SAC\n    '''\n    from ephysiopy.common.fieldcalcs import deform_SAC\n    deformed_SAC = deform_SAC(A)\n    return self.get_basic_gridscore(deformed_SAC)\n</code></pre>"},{"location":"reference/#ephysiopy.common.gridcell.SAC.get_expanding_circle_gridscore","title":"<code>get_expanding_circle_gridscore(A, **kwargs)</code>","text":"<p>Calculates the gridscore for each circular sub-region of image A where the circles are centred on the image centre and expanded to the edge of the image. The maximum of the get_basic_gridscore() for each of these circular sub-regions is returned as the gridscore</p> Source code in <code>ephysiopy/common/gridcell.py</code> <pre><code>def get_expanding_circle_gridscore(self, A: np.ndarray, **kwargs):\n    '''\n    Calculates the gridscore for each circular sub-region of image A\n    where the circles are centred on the image centre and expanded to\n    the edge of the image. The maximum of the get_basic_gridscore() for\n    each of these circular sub-regions is returned as the gridscore\n    '''\n\n    from ephysiopy.common.fieldcalcs import get_circular_regions\n    images = get_circular_regions(A, **kwargs)\n    gridscores = [self.get_basic_gridscore(im) for im in images]\n    return max(gridscores)\n</code></pre>"},{"location":"reference/#ephysiopy.common.gridcell.SAC.show","title":"<code>show(A, inDict, ax=None, **kwargs)</code>","text":"<p>Displays the result of performing a spatial autocorrelation (SAC) on a grid cell.</p> <p>Uses the dictionary containing measures of the grid cell SAC to make a pretty picture</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>array_like</code> <p>The spatial autocorrelogram</p> required <code>inDict</code> <code>dict</code> <p>The dictionary calculated in getmeasures</p> required <p>Returns:</p> Name Type Description <code>fig</code> <code>matplotlib.Figure instance</code> <p>The Figure on which the SAC is</p> <p>shown</p> See Also <p>ephysiopy.common.binning.RateMap.autoCorr2D() ephysiopy.common.ephys_generic.FieldCalcs.getMeaures()</p> Source code in <code>ephysiopy/common/gridcell.py</code> <pre><code>def show(self, A, inDict, ax=None, **kwargs):\n    \"\"\"\n    Displays the result of performing a spatial autocorrelation (SAC)\n    on a grid cell.\n\n    Uses the dictionary containing measures of the grid cell SAC to\n    make a pretty picture\n\n    Args:\n        A (array_like): The spatial autocorrelogram\n        inDict (dict): The dictionary calculated in getmeasures\n        ax (matplotlib.axes._subplots.AxesSubplot, optional).\n        If given the plot will get drawn in these axes. Default None\n\n    Returns:\n        fig (matplotlib.Figure instance): The Figure on which the SAC is\n        shown\n\n    See Also:\n        ephysiopy.common.binning.RateMap.autoCorr2D()\n        ephysiopy.common.ephys_generic.FieldCalcs.getMeaures()\n    \"\"\"\n    from ephysiopy.visualise.plotting import FigureMaker\n    F = FigureMaker()\n    F.show_SAC(A, inDict, ax, **kwargs)\n</code></pre>"},{"location":"reference/#phase-coding","title":"Phase coding","text":""},{"location":"reference/#ephysiopy.common.phasecoding.phasePrecession2D","title":"<code>phasePrecession2D</code>","text":"<p>               Bases: <code>object</code></p> <p>Performs phase precession analysis for single unit data</p> <p>Mostly a total rip-off of code written by Ali Jeewajee for his paper on 2D phase precession in place and grid cells [1]_</p> <p>.. [1] Jeewajee A, Barry C, Douchamps V, Manson D, Lever C, Burgess N.     Theta phase precession of grid and place cell firing in open     environments.     Philos Trans R Soc Lond B Biol Sci. 2013 Dec 23;369(1635):20120532.     doi: 10.1098/rstb.2012.0532.</p> <p>Parameters:</p> Name Type Description Default <code>lfp_sig</code> <code>array</code> <p>The LFP signal against which cells might precess...</p> required <code>lfp_fs</code> <code>int</code> <p>The sampling frequency of the LFP signal</p> required <code>xy</code> <code>array</code> <p>The position data as 2 x num_position_samples</p> required <code>spike_ts</code> <code>array</code> <p>The times in samples at which the cell fired</p> required <code>pos_ts</code> <code>array</code> <p>The times in samples at which position was captured</p> required <code>pp_config</code> <code>dict</code> <p>Contains parameters for running the analysis. See phase_precession_config dict in ephysiopy.common.eegcalcs</p> <code>phase_precession_config</code> Source code in <code>ephysiopy/common/phasecoding.py</code> <pre><code>class phasePrecession2D(object):\n    \"\"\"\n    Performs phase precession analysis for single unit data\n\n    Mostly a total rip-off of code written by Ali Jeewajee for his paper on\n    2D phase precession in place and grid cells [1]_\n\n    .. [1] Jeewajee A, Barry C, Douchamps V, Manson D, Lever C, Burgess N.\n        Theta phase precession of grid and place cell firing in open\n        environments.\n        Philos Trans R Soc Lond B Biol Sci. 2013 Dec 23;369(1635):20120532.\n        doi: 10.1098/rstb.2012.0532.\n\n    Args:\n        lfp_sig (np.array): The LFP signal against which cells might precess...\n        lfp_fs (int): The sampling frequency of the LFP signal\n        xy (np.array): The position data as 2 x num_position_samples\n        spike_ts (np.array): The times in samples at which the cell fired\n        pos_ts (np.array): The times in samples at which position was captured\n        pp_config (dict): Contains parameters for running the analysis.\n            See phase_precession_config dict in ephysiopy.common.eegcalcs\n    \"\"\"\n\n    def __init__(\n        self,\n        lfp_sig: np.array,\n        lfp_fs: int,\n        xy: np.array,\n        spike_ts: np.array,\n        pos_ts: np.array,\n        pp_config: dict = phase_precession_config,\n    ):\n        # Set up the parameters\n        # this sets a bunch of member attributes from the pp_config dict\n        self.update_config(pp_config)\n        self._pos_ts = pos_ts\n\n        # Create a dict to hold the stats values\n        stats_dict = {\n            \"values\": None,\n            \"pha\": None,\n            \"slope\": None,\n            \"intercept\": None,\n            \"cor\": None,\n            \"p\": None,\n            \"cor_boot\": None,\n            \"p_shuffled\": None,\n            \"ci\": None,\n            \"reg\": None,\n        }\n        # Create a dict of regressors to hold stat values\n        # for each regressor\n        from collections import defaultdict\n\n        self.regressors = {}\n        self.regressors = defaultdict(lambda: stats_dict.copy(), self.regressors)\n        regressor_keys = [\n            \"spk_numWithinRun\",\n            \"pos_exptdRate_cum\",\n            \"pos_instFR\",\n            \"pos_timeInRun\",\n            \"pos_d_cum\",\n            \"pos_d_meanDir\",\n            \"pos_d_currentdir\",\n            \"spk_thetaBatchLabelInRun\",\n        ]\n        [self.regressors[k] for k in regressor_keys]\n        # each of the regressors in regressor_keys is a key with a value\n        # of stats_dict\n\n        self.k = 1000\n        self.alpha = 0.05\n        self.hyp = 0\n        self.conf = True\n\n        # Process the EEG data a bit...\n        self.eeg = lfp_sig\n        L = LFPOscillations(lfp_sig, lfp_fs)\n        filt_sig, phase, _, _ = L.getFreqPhase(lfp_sig, [6, 12], 2)\n        self.filteredEEG = filt_sig\n        self.phase = phase\n        self.phaseAdj = None\n\n        self.update_position(xy, self.ppm, cm=self.convert_xy_2_cm)\n        self.update_rate_map()\n\n        spk_times_in_pos_samples = self.getSpikePosIndices(spike_ts)\n        spk_weights = np.bincount(spk_times_in_pos_samples, minlength=len(self.pos_ts))\n        self.spk_times_in_pos_samples = spk_times_in_pos_samples\n        self.spk_weights = spk_weights\n\n        self.spike_ts = spike_ts\n\n    @property\n    def pos_ts(self):\n        return self._pos_ts\n\n    @pos_ts.setter\n    def pos_ts(self, value):\n        self._pos_ts = value\n\n    @property\n    def xy(self):\n        return self.PosData.xy\n\n    @xy.setter\n    def xy(self, value):\n        self.PosData.xy = value\n\n    def update_config(self, pp_config):\n        [setattr(self, k, pp_config[k]) for k in pp_config.keys()]\n\n    def update_position(self, xy, ppm: float, cm: bool):\n        P = PosCalcsGeneric(\n            xy[0, :],\n            xy[1, :],\n            ppm=ppm,\n            convert2cm=cm,\n        )\n        P.postprocesspos(tracker_params={\"AxonaBadValue\": 1023})\n        # ... do the ratemap creation here once\n        self.PosData = P\n\n    def update_rate_map(self):\n        R = RateMap(\n            self.PosData.xy,\n            self.PosData.dir,\n            self.PosData.speed,\n            xyInCms=self.convert_xy_2_cm,\n        )\n        R.binsize = self.cms_per_bin\n        R.smooth_sz = self.field_smoothing_kernel_len\n        R.ppm = self.ppm\n        self.RateMap = R  # this will be used a fair bit below\n\n    def getSpikePosIndices(self, spk_times: np.array):\n        pos_times = getattr(self, \"pos_ts\")\n        idx = np.searchsorted(pos_times, spk_times)\n        idx[idx == len(pos_times)] = idx[idx == len(pos_times)] - 1\n        return idx\n\n    def performRegression(self, laserEvents=None, **kwargs):\n        \"\"\"\n        Wrapper function for doing the actual regression which has multiple\n        stages.\n\n        Specifically here we partition fields into sub-fields, get a bunch of\n        information about the position, spiking and theta data and then\n        do the actual regression.\n\n        Args:\n            tetrode (int): The tetrode to examine\n            cluster (int): The cluster to examine\n            laserEvents (array_like, optional): The on times for laser events\n            if present. Default is None\n\n        See Also:\n            ephysiopy.common.eegcalcs.phasePrecession.partitionFields()\n            ephysiopy.common.eegcalcs.phasePrecession.getPosProps()\n            ephysiopy.common.eegcalcs.phasePrecession.getThetaProps()\n            ephysiopy.common.eegcalcs.phasePrecession.getSpikeProps()\n            ephysiopy.common.eegcalcs.phasePrecession._ppRegress()\n        \"\"\"\n\n        # Partition fields\n        peaksXY, _, labels, _ = self.partitionFields(plot=True)\n\n        # split into runs\n        posD, runD = self.getPosProps(\n            labels, peaksXY, laserEvents=laserEvents, plot=True\n        )\n\n        # get theta cycles, amplitudes, phase etc\n        self.getThetaProps()\n\n        # get the indices of spikes for various metrics such as\n        # theta cycle, run etc\n        spkD = self.getSpikeProps(\n            posD[\"runLabel\"], runD[\"meanDir\"], runD[\"runDurationInPosBins\"]\n        )\n\n        # Do the regressions\n        regress_dict = self._ppRegress(spkD, plot=True)\n\n        self.plotPPRegression(regress_dict)\n\n    def partitionFields(self, ftype=\"g\", plot=False, **kwargs):\n        \"\"\"\n        Partitions fields.\n\n        Partitions spikes into fields by finding the watersheds around the\n        peaks of a super-smoothed ratemap\n\n        Args:\n            spike_ts (np.array): The ratemap to partition\n            ftype (str): 'p' or 'g' denoting place or grid cells\n              - not implemented yet\n            plot (bool): Whether to produce a debugging plot or not\n\n        Returns:\n            peaksXY (array_like): The xy coordinates of the peak rates in\n            each field\n            peaksRate (array_like): The peak rates in peaksXY\n            labels (numpy.ndarray): An array of the labels corresponding to\n            each field (starting at 1)\n            rmap (numpy.ndarray): The ratemap of the tetrode / cluster\n        \"\"\"\n        rmap, (xe, ye) = self.RateMap.getMap(self.spk_weights)\n        nan_idx = np.isnan(rmap)\n        rmap[nan_idx] = 0\n        # start image processing:\n        # get some markers\n        from ephysiopy.common import fieldcalcs\n\n        markers = fieldcalcs.local_threshold(rmap, prc=self.field_threshold_percent)\n        # clear the edges / any invalid positions again\n        markers[nan_idx] = 0\n        # label these markers so each blob has a unique id\n        labels = ndimage.label(markers)[0]\n        # labels is now a labelled int array from 0 to however many fields have\n        # been detected\n        # get the number of spikes in each field - NB this is done against a\n        # flattened array so we need to figure out which count corresponds to\n        # which particular field id using np.unique\n        fieldId, _ = np.unique(labels, return_index=True)\n        fieldId = fieldId[1::]\n        # TODO: come back to this as may need to know field id ordering\n        peakCoords = np.array(\n            ndimage.maximum_position(rmap, labels=labels, index=fieldId)\n        ).astype(int)\n        # COMCoords = np.array(\n        #     ndimage.center_of_mass(\n        #         rmap, labels=labels, index=fieldId)\n        # ).astype(int)\n        peaksXY = np.vstack((xe[peakCoords[:, 0]], ye[peakCoords[:, 1]])).T\n        # find the peak rate at each of the centre of the detected fields to\n        # subsequently threshold the field at some fraction of the peak value\n        peakRates = rmap[peakCoords[:, 0], peakCoords[:, 1]]\n        fieldThresh = peakRates * self.field_threshold\n        rmFieldMask = np.zeros_like(rmap)\n        for fid in fieldId:\n            f = labels[peakCoords[fid - 1, 0], peakCoords[fid - 1, 1]]\n            rmFieldMask[labels == f] = rmap[labels == f] &gt; fieldThresh[f - 1]\n        labels[~rmFieldMask.astype(bool)] = 0\n        # peakBinInds = np.ceil(peakCoords)\n        # re-order some vars to get into same format as fieldLabels\n        peakLabels = labels[peakCoords[:, 0], peakCoords[:, 1]]\n        peaksXY = peaksXY[peakLabels - 1, :]\n        peaksRate = peakRates[peakLabels - 1]\n        # peakBinInds = peakBinInds[peakLabels-1, :]\n        # peaksXY = peakCoords - np.min(xy, 1)\n\n        # if ~np.isnan(self.area_threshold):\n        #     # TODO: this needs fixing so sensible values are used and the\n        #     # modified bool array is propagated correctly ie makes\n        #     # sense to have a function that applies a bool array to whatever\n        #     # arrays are used as output and call it in a couple of places\n        #     # areaInBins = self.area_threshold * self.binsPerCm\n        #     lb = ndimage.label(markers)[0]\n        #     rp = skimage.measure.regionprops(lb)\n        #     for reg in rp:\n        #         print(reg.filled_area)\n        #     markers = skimage.morphology.remove_small_objects(\n        #         lb, min_size=4000, connectivity=4, in_place=True)\n        if plot:\n            fig = plt.figure()\n            ax = fig.add_subplot(211)\n            ax.pcolormesh(\n                ye, xe, rmap, cmap=matplotlib.colormaps[\"jet\"], edgecolors=\"face\"\n            )\n            ax.set_title(\"Smoothed ratemap + peaks\")\n            ax.xaxis.set_visible(False)\n            ax.yaxis.set_visible(False)\n            ax.set_aspect(\"equal\")\n            xlim = ax.get_xlim()\n            ylim = ax.get_ylim()\n            ax.plot(peaksXY[:, 1], peaksXY[:, 0], \"ko\")\n            ax.set_ylim(ylim)\n            ax.set_xlim(xlim)\n\n            ax = fig.add_subplot(212)\n            ax.imshow(labels, interpolation=\"nearest\", origin=\"lower\")\n            ax.set_title(\"Labelled restricted fields\")\n            ax.xaxis.set_visible(False)\n            ax.yaxis.set_visible(False)\n            ax.set_aspect(\"equal\")\n\n        return peaksXY, peaksRate, labels, rmap\n\n    def getPosProps(self, labels, peaksXY, laserEvents=None, plot=False, **kwargs):\n        \"\"\"\n        Uses the output of partitionFields and returns vectors the same\n        length as pos.\n\n        Args:\n            tetrode, cluster (int): The tetrode / cluster to examine\n            peaksXY (array_like): The x-y coords of the peaks in the ratemap\n            laserEvents (array_like): The position indices of on events\n            (laser on)\n\n        Returns:\n            pos_dict, run_dict (dict): Contains a whole bunch of information\n            for the whole trial and also on a run-by-run basis (run_dict).\n            See the end of this function for all the key / value pairs.\n        \"\"\"\n\n        spikeTS = self.spike_ts  # in seconds\n        xy = self.RateMap.xy\n        xydir = self.RateMap.dir\n        spd = self.RateMap.speed\n        spkPosInd = np.ceil(spikeTS * self.pos_sample_rate).astype(int)\n        spkPosInd[spkPosInd &gt; len(xy.T)] = len(xy.T) - 1\n        nPos = xy.shape[1]\n        xy_old = xy.copy()\n        xydir = np.squeeze(xydir)\n        xydir_old = xydir.copy()\n\n        rmap, (x_bins_in_pixels, y_bins_in_pixels) = self.RateMap.getMap(\n            self.spk_weights\n        )\n        xe = x_bins_in_pixels\n        ye = y_bins_in_pixels\n\n        # The large number of bins combined with the super-smoothed ratemap\n        # will lead to fields labelled with lots of small holes in. Fill those\n        # gaps in here and calculate the perimeter of the fields based on that\n        # labelled image\n        labels, n_labels = ndimage.label(ndimage.binary_fill_holes(labels))\n\n        rmap[np.isnan(rmap)] = 0\n        xBins = np.digitize(xy[0], ye[:-1])\n        yBins = np.digitize(xy[1], xe[:-1])\n        fieldLabel = labels[yBins - 1, xBins - 1]\n        fl_counts, fl_bins = np.histogram(fieldLabel, bins=np.unique(labels))\n        for i, fl in enumerate(fl_bins[1::]):\n            print(\"Field {} has {} samples\".format(i, fl_counts[i]))\n\n        fieldPerimMask = bwperim(labels)\n        fieldPerimYBins, fieldPerimXBins = np.nonzero(fieldPerimMask)\n        fieldPerimX = ye[fieldPerimXBins]\n        fieldPerimY = xe[fieldPerimYBins]\n        fieldPerimXY = np.vstack((fieldPerimX, fieldPerimY))\n        peaksXYBins = np.array(\n            ndimage.maximum_position(rmap, labels=labels, index=np.unique(labels)[1::])\n        ).astype(int)\n        peakY = xe[peaksXYBins[:, 0]]\n        peakX = ye[peaksXYBins[:, 1]]\n        peaksXY = np.vstack((peakX, peakY)).T\n\n        posRUnsmthd = np.zeros((nPos)) * np.nan\n        posAngleFromPeak = np.zeros_like(posRUnsmthd) * np.nan\n        perimAngleFromPeak = np.zeros_like(fieldPerimMask) * np.nan\n        for i, peak in enumerate(peaksXY):\n            i = i + 1\n            # grab each fields perim coords and the pos samples within it\n            y_ind, x_ind = np.nonzero(fieldPerimMask == i)\n            thisFieldPerim = np.array([xe[x_ind], ye[y_ind]])\n            if thisFieldPerim.any():\n                this_xy = xy[:, fieldLabel == i]\n                # calculate angle from the field peak for each point on the\n                # perim and each pos sample that lies within the field\n                thisPerimAngle = np.arctan2(\n                    thisFieldPerim[1, :] - peak[1], thisFieldPerim[0, :] - peak[0]\n                )\n                thisPosAngle = np.arctan2(\n                    this_xy[1, :] - peak[1], this_xy[0, :] - peak[0]\n                )\n                posAngleFromPeak[fieldLabel == i] = thisPosAngle\n\n                perimAngleFromPeak[fieldPerimMask == i] = thisPerimAngle\n                # for each pos sample calculate which point on the perim is\n                # most colinear with the field centre - see _circ_abs for more\n                thisAngleDf = circ_abs(\n                    thisPerimAngle[:, np.newaxis] - thisPosAngle[np.newaxis, :]\n                )\n                thisPerimInd = np.argmin(thisAngleDf, 0)\n                # calculate the distance to the peak from pos and the min perim\n                # point and calculate the ratio (r - see OUtputs for method)\n                tmp = this_xy.T - peak.T\n                distFromPos2Peak = np.hypot(tmp[:, 0], tmp[:, 1])\n                tmp = thisFieldPerim[:, thisPerimInd].T - peak.T\n                distFromPerim2Peak = np.hypot(tmp[:, 0], tmp[:, 1])\n                posRUnsmthd[fieldLabel == i] = distFromPos2Peak / distFromPerim2Peak\n        # the skimage find_boundaries method combined with the labelled mask\n        # strive to make some of the values in thisDistFromPos2Peak larger than\n        # those in thisDistFromPerim2Peak which means that some of the vals in\n        # posRUnsmthd larger than 1 which means the values in xy_new later are\n        # wrong - so lets cap any value &gt; 1 to 1. The same cap is applied later\n        # to rho when calculating the angular values. Print out a warning\n        # message letting the user know how many values have been capped\n        print(\n            \"\\n\\n{:.2%} posRUnsmthd values have been capped to 1\\n\\n\".format(\n                np.sum(posRUnsmthd &gt;= 1) / posRUnsmthd.size\n            )\n        )\n        posRUnsmthd[posRUnsmthd &gt;= 1] = 1\n        # label non-zero contiguous runs with a unique id\n        runLabel = labelContigNonZeroRuns(fieldLabel)\n        isRun = runLabel &gt; 0\n        runStartIdx = getLabelStarts(runLabel)\n        runEndIdx = getLabelEnds(runLabel)\n        # find runs that are too short, have low speed or too few spikes\n        runsSansSpikes = np.ones(len(runStartIdx), dtype=bool)\n        spkRunLabels = runLabel[spkPosInd] - 1\n        runsSansSpikes[spkRunLabels[spkRunLabels &gt; 0]] = False\n        k = signal.windows.boxcar(self.speed_smoothing_window_len) / float(\n            self.speed_smoothing_window_len\n        )\n        spdSmthd = signal.convolve(np.squeeze(spd), k, mode=\"same\")\n        runDurationInPosBins = runEndIdx - runStartIdx + 1\n        runsMinSpeed = []\n        runId = np.unique(runLabel)[1::]\n        for run in runId:\n            runsMinSpeed.append(np.min(spdSmthd[runLabel == run]))\n        runsMinSpeed = np.array(runsMinSpeed)\n        badRuns = np.logical_or(\n            np.logical_or(\n                runsMinSpeed &lt; self.minimum_allowed_run_speed,\n                runDurationInPosBins &lt; self.minimum_allowed_run_duration,\n            ),\n            runsSansSpikes,\n        )\n        badRuns = np.squeeze(badRuns)\n        runLabel = applyFilter2Labels(~badRuns, runLabel)\n        runStartIdx = runStartIdx[~badRuns]\n        runEndIdx = runEndIdx[~badRuns]  # + 1\n        runsMinSpeed = runsMinSpeed[~badRuns]\n        runDurationInPosBins = runDurationInPosBins[~badRuns]\n        isRun = runLabel &gt; 0\n\n        # calculate mean and std direction for each run\n        runComplexMnDir = np.squeeze(np.zeros_like(runStartIdx))\n        np.add.at(\n            runComplexMnDir,\n            runLabel[isRun] - 1,\n            np.exp(1j * (xydir[isRun] * (np.pi / 180))),\n        )\n        meanDir = np.angle(runComplexMnDir)  # circ mean\n        tortuosity = 1 - np.abs(runComplexMnDir) / runDurationInPosBins\n\n        # caculate angular distance between the runs main direction and the\n        # pos's direction to the peak centre\n        posPhiUnSmthd = np.ones_like(fieldLabel) * np.nan\n        posPhiUnSmthd[isRun] = posAngleFromPeak[isRun] - meanDir[runLabel[isRun] - 1]\n\n        # smooth r and phi in cartesian space\n        # convert to cartesian coords first\n        posXUnSmthd, posYUnSmthd = pol2cart(posRUnsmthd, posPhiUnSmthd)\n        posXYUnSmthd = np.vstack((posXUnSmthd, posYUnSmthd))\n\n        # filter each run with filter of appropriate length\n        filtLen = np.squeeze(\n            np.floor((runEndIdx - runStartIdx + 1) * self.ifr_smoothing_constant)\n        )\n        xy_new = np.zeros_like(xy_old) * np.nan\n        for i in range(len(runStartIdx)):\n            if filtLen[i] &gt; 2:\n                filt = signal.firwin(\n                    int(filtLen[i] - 1),\n                    cutoff=self.spatial_lowpass_cutoff / self.pos_sample_rate * 2,\n                    window=\"blackman\",\n                )\n                xy_new[:, runStartIdx[i] : runEndIdx[i]] = signal.filtfilt(\n                    filt, [1], posXYUnSmthd[:, runStartIdx[i] : runEndIdx[i]], axis=1\n                )\n\n        r, phi = cart2pol(xy_new[0], xy_new[1])\n        r[r &gt; 1] = 1\n\n        # calculate the direction of the smoothed data\n        xydir_new = np.arctan2(np.diff(xy_new[1]), np.diff(xy_new[0]))\n        xydir_new = np.append(xydir_new, xydir_new[-1])\n        xydir_new[runEndIdx] = xydir_new[runEndIdx - 1]\n\n        # project the distance value onto the current direction\n        d_currentdir = r * np.cos(xydir_new - phi)\n\n        # calculate the cumulative distance travelled on each run\n        dr = np.sqrt(np.diff(np.power(r, 2), 1))\n        d_cumulative = labelledCumSum(np.insert(dr, 0, 0), runLabel)\n\n        # calculate cumulative sum of the expected normalised firing rate\n        exptdRate_cumulative = labelledCumSum(1 - r, runLabel)\n\n        # direction projected onto the run mean direction is just the x coord\n        d_meandir = xy_new[0]\n\n        # smooth binned spikes to get an instantaneous firing rate\n        # set up the smoothing kernel\n        kernLenInBins = np.round(self.ifr_kernel_len * self.bins_per_second)\n        kernSig = self.ifr_kernel_sigma * self.bins_per_second\n        k = signal.windows.gaussian(kernLenInBins, kernSig)\n        # get a count of spikes to smooth over\n        spkCount = np.bincount(spkPosInd, minlength=nPos)\n        # apply the smoothing kernel\n        instFiringRate = signal.convolve(spkCount, k, mode=\"same\")\n        instFiringRate[~isRun] = np.nan\n\n        # find time spent within run\n        time = np.ones(nPos)\n        time = labelledCumSum(time, runLabel)\n        timeInRun = time / self.pos_sample_rate\n\n        fieldNum = fieldLabel[runStartIdx]\n        mnSpd = np.squeeze(np.zeros_like(fieldNum))\n        np.add.at(mnSpd, runLabel[isRun] - 1, spd[isRun])\n        nPts = np.bincount(runLabel[isRun] - 1, minlength=len(mnSpd))\n        mnSpd = mnSpd / nPts\n        centralPeripheral = np.squeeze(np.zeros_like(fieldNum))\n        np.add.at(centralPeripheral, runLabel[isRun] - 1, xy_new[1, isRun])\n        centralPeripheral = centralPeripheral / nPts\n        if plot:\n            fig = plt.figure()\n            ax = fig.add_subplot(221)\n            ax.plot(xy_new[0], xy_new[1])\n            ax.set_title(\"Unit circle x-y\")\n            ax.set_aspect(\"equal\")\n            ax.set_xlim([-1, 1])\n            ax.set_ylim([-1, 1])\n            ax = fig.add_subplot(222)\n            ax.plot(fieldPerimX, fieldPerimY, \"k.\")\n            ax.set_title(\"Field perim and laser on events\")\n            ax.plot(xy[0, fieldLabel &gt; 0], xy[1, fieldLabel &gt; 0], \"y.\")\n            if laserEvents is not None:\n                validOns = np.setdiff1d(laserEvents, np.nonzero(~np.isnan(r))[0])\n                ax.plot(xy[0, validOns], xy[1, validOns], \"rx\")\n            ax.set_aspect(\"equal\")\n            angleCMInd = np.round(perimAngleFromPeak / np.pi * 180) + 180\n            angleCMInd[angleCMInd == 0] = 360\n            im = np.zeros_like(fieldPerimMask)\n            im[fieldPerimMask] = angleCMInd\n            imM = np.ma.MaskedArray(im, mask=~fieldPerimMask, copy=True)\n            #############################################\n            # create custom colormap\n            cmap = plt.colormaps[\"jet_r\"]\n            cmaplist = [cmap(i) for i in range(cmap.N)]\n            cmaplist[0] = (1, 1, 1, 1)\n            cmap = cmap.from_list(\"Runvals cmap\", cmaplist, cmap.N)\n            bounds = np.linspace(0, 1.0, 100)\n            norm = matplotlib.colors.BoundaryNorm(bounds, cmap.N)\n            # add the runs through the fields\n            runVals = np.zeros_like(im)\n            runVals[yBins[isRun] - 1, xBins[isRun] - 1] = r[isRun]\n            runVals = runVals\n            ax = fig.add_subplot(223)\n            imm = ax.imshow(\n                runVals, cmap=cmap, norm=norm, origin=\"lower\", interpolation=\"nearest\"\n            )\n            plt.colorbar(imm, orientation=\"horizontal\")\n            ax.set_aspect(\"equal\")\n            # add a custom colorbar for colors in runVals\n\n            # create a custom colormap for the plot\n            cmap = matplotlib.colormaps[\"hsv\"]\n            cmaplist = [cmap(i) for i in range(cmap.N)]\n            cmaplist[0] = (1, 1, 1, 1)\n            cmap = cmap.from_list(\"Perim cmap\", cmaplist, cmap.N)\n            bounds = np.linspace(0, 360, cmap.N)\n            norm = matplotlib.colors.BoundaryNorm(bounds, cmap.N)\n\n            imm = ax.imshow(\n                imM, cmap=cmap, norm=norm, origin=\"lower\", interpolation=\"nearest\"\n            )\n            plt.colorbar(imm)\n            ax.set_title(\"Runs by distance and angle\")\n            ax.plot(peaksXYBins[:, 1], peaksXYBins[:, 0], \"ko\")\n            ax.set_xlim(0, im.shape[1])\n            ax.set_ylim(0, im.shape[0])\n            #############################################\n            ax = fig.add_subplot(224)\n            ax.imshow(rmap, origin=\"lower\", interpolation=\"nearest\")\n            ax.set_aspect(\"equal\")\n            ax.set_title(\"Smoothed ratemap\")\n\n        # update the regressor dict from __init__ with relevant values\n        self.regressors[\"pos_exptdRate_cum\"][\"values\"] = exptdRate_cumulative\n        self.regressors[\"pos_instFR\"][\"values\"] = instFiringRate\n        self.regressors[\"pos_timeInRun\"][\"values\"] = timeInRun\n        self.regressors[\"pos_d_cum\"][\"values\"] = d_cumulative\n        self.regressors[\"pos_d_meanDir\"][\"values\"] = d_meandir\n        self.regressors[\"pos_d_currentdir\"][\"values\"] = d_currentdir\n        posKeys = (\n            \"xy\",\n            \"xydir\",\n            \"r\",\n            \"phi\",\n            \"xy_old\",\n            \"xydir_old\",\n            \"fieldLabel\",\n            \"runLabel\",\n            \"d_currentdir\",\n            \"d_cumulative\",\n            \"exptdRate_cumulative\",\n            \"d_meandir\",\n            \"instFiringRate\",\n            \"timeInRun\",\n            \"fieldPerimMask\",\n            \"perimAngleFromPeak\",\n            \"posAngleFromPeak\",\n        )\n        runsKeys = (\n            \"runStartIdx\",\n            \"runEndIdx\",\n            \"runDurationInPosBins\",\n            \"runsMinSpeed\",\n            \"meanDir\",\n            \"tortuosity\",\n            \"mnSpd\",\n            \"centralPeripheral\",\n        )\n        posDict = dict.fromkeys(posKeys, np.nan)\n        # neat trick: locals is a dict that holds all locally scoped variables\n        for thiskey in posDict.keys():\n            posDict[thiskey] = locals()[thiskey]\n        runsDict = dict.fromkeys(runsKeys, np.nan)\n        for thiskey in runsDict.keys():\n            runsDict[thiskey] = locals()[thiskey]\n        return posDict, runsDict\n\n    def getThetaProps(self, **kwargs):\n        spikeTS = self.spike_ts\n        phase = self.phase\n        filteredEEG = self.filteredEEG\n        oldAmplt = filteredEEG.copy()\n        # get indices of spikes into eeg\n        spkEEGIdx = np.ceil(\n            spikeTS * (self.lfp_sample_rate / self.pos_sample_rate)\n        ).astype(int)\n        spkEEGIdx[spkEEGIdx &gt; len(phase)] = len(phase) - 1\n        spkCount = np.bincount(spkEEGIdx, minlength=len(phase))\n        spkPhase = phase[spkEEGIdx]\n        minSpikingPhase = getPhaseOfMinSpiking(spkPhase)\n        phaseAdj = fixAngle(\n            phase - minSpikingPhase * (np.pi / 180) + self.allowed_min_spike_phase\n        )\n        isNegFreq = np.diff(np.unwrap(phaseAdj)) &lt; 0\n        isNegFreq = np.append(isNegFreq, isNegFreq[-1])\n        # get start of theta cycles as points where diff &gt; pi\n        phaseDf = np.diff(phaseAdj)\n        cycleStarts = phaseDf[1::] &lt; -np.pi\n        cycleStarts = np.append(cycleStarts, True)\n        cycleStarts = np.insert(cycleStarts, 0, True)\n        cycleStarts[isNegFreq] = False\n        cycleLabel = np.cumsum(cycleStarts)\n\n        # caculate power and find low power cycles\n        power = np.power(filteredEEG, 2)\n        cycleTotValidPow = np.bincount(\n            cycleLabel[~isNegFreq], weights=power[~isNegFreq]\n        )\n        cycleValidBinCount = np.bincount(cycleLabel[~isNegFreq])\n        cycleValidMnPow = cycleTotValidPow / cycleValidBinCount\n        powRejectThresh = np.percentile(\n            cycleValidMnPow, self.min_power_percent_threshold\n        )\n        cycleHasBadPow = cycleValidMnPow &lt; powRejectThresh\n\n        # find cycles too long or too short\n        cycleTotBinCount = np.bincount(cycleLabel)\n        cycleHasBadLen = np.logical_or(\n            cycleTotBinCount &gt; self.allowed_theta_len[1],\n            cycleTotBinCount &lt; self.allowed_theta_len[0],\n        )\n\n        # remove data calculated as 'bad'\n        isBadCycle = np.logical_or(cycleHasBadLen, cycleHasBadPow)\n        isInBadCycle = isBadCycle[cycleLabel]\n        isBad = np.logical_or(isInBadCycle, isNegFreq)\n        phaseAdj[isBad] = np.nan\n        self.phaseAdj = phaseAdj\n        ampAdj = filteredEEG.copy()\n        ampAdj[isBad] = np.nan\n        cycleLabel[isBad] = 0\n        self.cycleLabel = cycleLabel\n        out = {\n            \"phase\": phaseAdj,\n            \"amp\": ampAdj,\n            \"cycleLabel\": cycleLabel,\n            \"oldPhase\": phase.copy(),\n            \"oldAmplt\": oldAmplt,\n            \"spkCount\": spkCount,\n        }\n        return out\n\n    def getSpikeProps(self, runLabel, meanDir, durationInPosBins):\n\n        spikeTS = self.spike_ts\n        xy = self.RateMap.xy\n        phase = self.phaseAdj\n        cycleLabel = self.cycleLabel\n        spkEEGIdx = np.ceil(spikeTS * self.lfp_sample_rate)\n        spkEEGIdx[spkEEGIdx &gt; len(phase)] = len(phase) - 1\n        spkEEGIdx = spkEEGIdx.astype(int)\n        spkPosIdx = np.ceil(spikeTS * self.pos_sample_rate)\n        spkPosIdx[spkPosIdx &gt; xy.shape[1]] = xy.shape[1] - 1\n        spkRunLabel = runLabel[spkPosIdx.astype(int)]\n        thetaCycleLabel = cycleLabel[spkEEGIdx.astype(int)]\n\n        # build mask true for spikes in 1st half of cycle\n        firstInTheta = thetaCycleLabel[:-1] != thetaCycleLabel[1::]\n        firstInTheta = np.insert(firstInTheta, 0, True)\n        lastInTheta = firstInTheta[1::]\n        # calculate two kinds of numbering for spikes in a run\n        numWithinRun = labelledCumSum(np.ones_like(spkPosIdx), spkRunLabel)\n        thetaBatchLabelInRun = labelledCumSum(firstInTheta.astype(float), spkRunLabel)\n\n        spkCount = np.bincount(spkRunLabel[spkRunLabel &gt; 0], minlength=len(meanDir))\n        rateInPosBins = spkCount[1::] / durationInPosBins.astype(float)\n        # update the regressor dict from __init__ with relevant values\n        self.regressors[\"spk_numWithinRun\"][\"values\"] = numWithinRun\n        self.regressors[\"spk_thetaBatchLabelInRun\"][\"values\"] = thetaBatchLabelInRun\n        spkKeys = (\n            \"spikeTS\",\n            \"spkPosIdx\",\n            \"spkEEGIdx\",\n            \"spkRunLabel\",\n            \"thetaCycleLabel\",\n            \"firstInTheta\",\n            \"lastInTheta\",\n            \"numWithinRun\",\n            \"thetaBatchLabelInRun\",\n            \"spkCount\",\n            \"rateInPosBins\",\n        )\n        spkDict = dict.fromkeys(spkKeys, np.nan)\n        for thiskey in spkDict.keys():\n            spkDict[thiskey] = locals()[thiskey]\n        return spkDict\n\n    def _ppRegress(self, spkDict, whichSpk=\"first\", plot=False, **kwargs):\n\n        phase = self.phaseAdj\n        newSpkRunLabel = spkDict[\"spkRunLabel\"].copy()\n        # TODO: need code to deal with splitting the data based on a group of\n        # variables\n        spkUsed = newSpkRunLabel &gt; 0\n        if \"first\" in whichSpk:\n            spkUsed[~spkDict[\"firstInTheta\"]] = False\n        elif \"last\" in whichSpk:\n            if len(spkDict[\"lastInTheta\"]) &lt; len(spkDict[\"spkRunLabel\"]):\n                spkDict[\"lastInTheta\"] = np.insert(spkDict[\"lastInTheta\"], -1, False)\n            spkUsed[~spkDict[\"lastInTheta\"]] = False\n        spkPosIdxUsed = spkDict[\"spkPosIdx\"].astype(int)\n        # copy self.regressors and update with spk/ pos of interest\n        regressors = self.regressors.copy()\n        for k in regressors.keys():\n            if k.startswith(\"spk_\"):\n                regressors[k][\"values\"] = regressors[k][\"values\"][spkUsed]\n            elif k.startswith(\"pos_\"):\n                regressors[k][\"values\"] = regressors[k][\"values\"][\n                    spkPosIdxUsed[spkUsed]\n                ]\n        phase = phase[spkDict[\"spkEEGIdx\"][spkUsed]]\n        phase = phase.astype(np.double)\n        if \"mean\" in whichSpk:\n            goodPhase = ~np.isnan(phase)\n            cycleLabels = spkDict[\"thetaCycleLabel\"][spkUsed]\n            sz = np.max(cycleLabels)\n            cycleComplexPhase = np.squeeze(np.zeros(sz, dtype=np.complex))\n            np.add.at(\n                cycleComplexPhase,\n                cycleLabels[goodPhase] - 1,\n                np.exp(1j * phase[goodPhase]),\n            )\n            phase = np.angle(cycleComplexPhase)\n            spkCountPerCycle = np.bincount(cycleLabels[goodPhase], minlength=sz)\n            for k in regressors.keys():\n                regressors[k][\"values\"] = (\n                    np.bincount(\n                        cycleLabels[goodPhase],\n                        weights=regressors[k][\"values\"][goodPhase],\n                        minlength=sz,\n                    )\n                    / spkCountPerCycle\n                )\n\n        goodPhase = ~np.isnan(phase)\n        for k in regressors.keys():\n            print(f\"Doing regression: {k}\")\n            goodRegressor = ~np.isnan(regressors[k][\"values\"])\n            reg = regressors[k][\"values\"][np.logical_and(goodRegressor, goodPhase)]\n            pha = phase[np.logical_and(goodRegressor, goodPhase)]\n            regressors[k][\"slope\"],\n            regressors[k][\"intercept\"] = circRegress(reg, pha)\n            regressors[k][\"pha\"] = pha\n            mnx = np.mean(reg)\n            reg = reg - mnx\n            mxx = np.max(np.abs(reg)) + np.spacing(1)\n            reg = reg / mxx\n            # problem regressors = instFR, pos_d_cum\n            breakpoint()\n            theta = np.mod(np.abs(regressors[k][\"slope\"]) * reg, 2 * np.pi)\n            rho, p, rho_boot, p_shuff, ci = circCircCorrTLinear(\n                theta, pha, self.k, self.alpha, self.hyp, self.conf\n            )\n            regressors[k][\"reg\"] = reg\n            regressors[k][\"cor\"] = rho\n            regressors[k][\"p\"] = p\n            regressors[k][\"cor_boot\"] = rho_boot\n            regressors[k][\"p_shuffled\"] = p_shuff\n            regressors[k][\"ci\"] = ci\n\n        if plot:\n            fig = plt.figure()\n\n            ax = fig.add_subplot(2, 1, 1)\n            ax.plot(regressors[\"pos_d_currentdir\"][\"values\"], phase, \"k.\")\n            ax.plot(regressors[\"pos_d_currentdir\"][\"values\"], phase + 2 * np.pi, \"k.\")\n            slope = regressors[\"pos_d_currentdir\"][\"slope\"]\n            intercept = regressors[\"pos_d_currentdir\"][\"intercept\"]\n            mm = (0, -2 * np.pi, 2 * np.pi, 4 * np.pi)\n            for m in mm:\n                ax.plot(\n                    (-1, 1), (-slope + intercept + m, slope + intercept + m), \"r\", lw=3\n                )\n            ax.set_xlim(-1, 1)\n            ax.set_ylim(-np.pi, 3 * np.pi)\n            ax.set_title(\"pos_d_currentdir\")\n            ax.set_ylabel(\"Phase\")\n\n            ax = fig.add_subplot(2, 1, 2)\n            ax.plot(regressors[\"pos_d_meanDir\"][\"values\"], phase, \"k.\")\n            ax.plot(regressors[\"pos_d_meanDir\"][\"values\"], phase + 2 * np.pi, \"k.\")\n            slope = regressors[\"pos_d_meanDir\"][\"slope\"]\n            intercept = regressors[\"pos_d_meanDir\"][\"intercept\"]\n            mm = (0, -2 * np.pi, 2 * np.pi, 4 * np.pi)\n            for m in mm:\n                ax.plot(\n                    (-1, 1), (-slope + intercept + m, slope + intercept + m), \"r\", lw=3\n                )\n            ax.set_xlim(-1, 1)\n            ax.set_ylim(-np.pi, 3 * np.pi)\n            ax.set_title(\"pos_d_meanDir\")\n            ax.set_ylabel(\"Phase\")\n            ax.set_xlabel(\"Normalised position\")\n        self.reg_phase = phase\n        return regressors\n\n    def plotPPRegression(self, regressorDict, regressor2plot=\"pos_d_cum\", ax=None):\n\n        t = self.getLFPPhaseValsForSpikeTS()\n        x = self.RateMap.xy[0, self.spk_times_in_pos_samples]\n        from ephysiopy.common import fieldcalcs\n\n        rmap, (xe, _) = self.RateMap.getMap(self.spk_weights)\n        label = fieldcalcs.field_lims(rmap)\n        xInField = xe[label.nonzero()[1]]\n        mask = np.logical_and(x &gt; np.min(xInField), x &lt; np.max(xInField))\n        x = x[mask]\n        t = t[mask]\n        # keep x between -1 and +1\n        mnx = np.mean(x)\n        xn = x - mnx\n        mxx = np.max(np.abs(xn))\n        x = xn / mxx\n        # keep tn between 0 and 2pi\n        t = np.remainder(t, 2 * np.pi)\n        slope, intercept = circRegress(x, t)\n        rho, p, rho_boot, p_shuff, ci = circCircCorrTLinear(x, t)\n        plt.figure()\n        if ax is None:\n            fig = plt.figure()\n            ax = fig.add_subplot(111)\n        else:\n            ax = ax\n        ax.plot(x, t, \".\", color=\"k\")\n        ax.plot(x, t + 2 * np.pi, \".\", color=\"k\")\n        mm = (0, -2 * np.pi, 2 * np.pi, 4 * np.pi)\n        for m in mm:\n            ax.plot((-1, 1), (-slope + intercept + m, slope + intercept + m), \"r\", lw=3)\n        ax.set_xlim((-1, 1))\n        ax.set_ylim((-np.pi, 3 * np.pi))\n        return {\n            \"slope\": slope,\n            \"intercept\": intercept,\n            \"rho\": rho,\n            \"p\": p,\n            \"rho_boot\": rho_boot,\n            \"p_shuff\": p_shuff,\n            \"ci\": ci,\n        }\n\n    def getLFPPhaseValsForSpikeTS(self):\n        ts = self.spk_times_in_pos_samples * (\n            self.lfp_sample_rate / self.pos_sample_rate\n        )\n        ts_idx = np.array(np.floor(ts), dtype=int)\n        return self.phase[ts_idx]\n</code></pre>"},{"location":"reference/#ephysiopy.common.phasecoding.phasePrecession2D.getPosProps","title":"<code>getPosProps(labels, peaksXY, laserEvents=None, plot=False, **kwargs)</code>","text":"<p>Uses the output of partitionFields and returns vectors the same length as pos.</p> <p>Parameters:</p> Name Type Description Default <code>tetrode,</code> <code>cluster (int</code> <p>The tetrode / cluster to examine</p> required <code>peaksXY</code> <code>array_like</code> <p>The x-y coords of the peaks in the ratemap</p> required <code>laserEvents</code> <code>array_like</code> <p>The position indices of on events</p> <code>None</code> <p>Returns:</p> Type Description <p>pos_dict, run_dict (dict): Contains a whole bunch of information</p> <p>for the whole trial and also on a run-by-run basis (run_dict).</p> <p>See the end of this function for all the key / value pairs.</p> Source code in <code>ephysiopy/common/phasecoding.py</code> <pre><code>def getPosProps(self, labels, peaksXY, laserEvents=None, plot=False, **kwargs):\n    \"\"\"\n    Uses the output of partitionFields and returns vectors the same\n    length as pos.\n\n    Args:\n        tetrode, cluster (int): The tetrode / cluster to examine\n        peaksXY (array_like): The x-y coords of the peaks in the ratemap\n        laserEvents (array_like): The position indices of on events\n        (laser on)\n\n    Returns:\n        pos_dict, run_dict (dict): Contains a whole bunch of information\n        for the whole trial and also on a run-by-run basis (run_dict).\n        See the end of this function for all the key / value pairs.\n    \"\"\"\n\n    spikeTS = self.spike_ts  # in seconds\n    xy = self.RateMap.xy\n    xydir = self.RateMap.dir\n    spd = self.RateMap.speed\n    spkPosInd = np.ceil(spikeTS * self.pos_sample_rate).astype(int)\n    spkPosInd[spkPosInd &gt; len(xy.T)] = len(xy.T) - 1\n    nPos = xy.shape[1]\n    xy_old = xy.copy()\n    xydir = np.squeeze(xydir)\n    xydir_old = xydir.copy()\n\n    rmap, (x_bins_in_pixels, y_bins_in_pixels) = self.RateMap.getMap(\n        self.spk_weights\n    )\n    xe = x_bins_in_pixels\n    ye = y_bins_in_pixels\n\n    # The large number of bins combined with the super-smoothed ratemap\n    # will lead to fields labelled with lots of small holes in. Fill those\n    # gaps in here and calculate the perimeter of the fields based on that\n    # labelled image\n    labels, n_labels = ndimage.label(ndimage.binary_fill_holes(labels))\n\n    rmap[np.isnan(rmap)] = 0\n    xBins = np.digitize(xy[0], ye[:-1])\n    yBins = np.digitize(xy[1], xe[:-1])\n    fieldLabel = labels[yBins - 1, xBins - 1]\n    fl_counts, fl_bins = np.histogram(fieldLabel, bins=np.unique(labels))\n    for i, fl in enumerate(fl_bins[1::]):\n        print(\"Field {} has {} samples\".format(i, fl_counts[i]))\n\n    fieldPerimMask = bwperim(labels)\n    fieldPerimYBins, fieldPerimXBins = np.nonzero(fieldPerimMask)\n    fieldPerimX = ye[fieldPerimXBins]\n    fieldPerimY = xe[fieldPerimYBins]\n    fieldPerimXY = np.vstack((fieldPerimX, fieldPerimY))\n    peaksXYBins = np.array(\n        ndimage.maximum_position(rmap, labels=labels, index=np.unique(labels)[1::])\n    ).astype(int)\n    peakY = xe[peaksXYBins[:, 0]]\n    peakX = ye[peaksXYBins[:, 1]]\n    peaksXY = np.vstack((peakX, peakY)).T\n\n    posRUnsmthd = np.zeros((nPos)) * np.nan\n    posAngleFromPeak = np.zeros_like(posRUnsmthd) * np.nan\n    perimAngleFromPeak = np.zeros_like(fieldPerimMask) * np.nan\n    for i, peak in enumerate(peaksXY):\n        i = i + 1\n        # grab each fields perim coords and the pos samples within it\n        y_ind, x_ind = np.nonzero(fieldPerimMask == i)\n        thisFieldPerim = np.array([xe[x_ind], ye[y_ind]])\n        if thisFieldPerim.any():\n            this_xy = xy[:, fieldLabel == i]\n            # calculate angle from the field peak for each point on the\n            # perim and each pos sample that lies within the field\n            thisPerimAngle = np.arctan2(\n                thisFieldPerim[1, :] - peak[1], thisFieldPerim[0, :] - peak[0]\n            )\n            thisPosAngle = np.arctan2(\n                this_xy[1, :] - peak[1], this_xy[0, :] - peak[0]\n            )\n            posAngleFromPeak[fieldLabel == i] = thisPosAngle\n\n            perimAngleFromPeak[fieldPerimMask == i] = thisPerimAngle\n            # for each pos sample calculate which point on the perim is\n            # most colinear with the field centre - see _circ_abs for more\n            thisAngleDf = circ_abs(\n                thisPerimAngle[:, np.newaxis] - thisPosAngle[np.newaxis, :]\n            )\n            thisPerimInd = np.argmin(thisAngleDf, 0)\n            # calculate the distance to the peak from pos and the min perim\n            # point and calculate the ratio (r - see OUtputs for method)\n            tmp = this_xy.T - peak.T\n            distFromPos2Peak = np.hypot(tmp[:, 0], tmp[:, 1])\n            tmp = thisFieldPerim[:, thisPerimInd].T - peak.T\n            distFromPerim2Peak = np.hypot(tmp[:, 0], tmp[:, 1])\n            posRUnsmthd[fieldLabel == i] = distFromPos2Peak / distFromPerim2Peak\n    # the skimage find_boundaries method combined with the labelled mask\n    # strive to make some of the values in thisDistFromPos2Peak larger than\n    # those in thisDistFromPerim2Peak which means that some of the vals in\n    # posRUnsmthd larger than 1 which means the values in xy_new later are\n    # wrong - so lets cap any value &gt; 1 to 1. The same cap is applied later\n    # to rho when calculating the angular values. Print out a warning\n    # message letting the user know how many values have been capped\n    print(\n        \"\\n\\n{:.2%} posRUnsmthd values have been capped to 1\\n\\n\".format(\n            np.sum(posRUnsmthd &gt;= 1) / posRUnsmthd.size\n        )\n    )\n    posRUnsmthd[posRUnsmthd &gt;= 1] = 1\n    # label non-zero contiguous runs with a unique id\n    runLabel = labelContigNonZeroRuns(fieldLabel)\n    isRun = runLabel &gt; 0\n    runStartIdx = getLabelStarts(runLabel)\n    runEndIdx = getLabelEnds(runLabel)\n    # find runs that are too short, have low speed or too few spikes\n    runsSansSpikes = np.ones(len(runStartIdx), dtype=bool)\n    spkRunLabels = runLabel[spkPosInd] - 1\n    runsSansSpikes[spkRunLabels[spkRunLabels &gt; 0]] = False\n    k = signal.windows.boxcar(self.speed_smoothing_window_len) / float(\n        self.speed_smoothing_window_len\n    )\n    spdSmthd = signal.convolve(np.squeeze(spd), k, mode=\"same\")\n    runDurationInPosBins = runEndIdx - runStartIdx + 1\n    runsMinSpeed = []\n    runId = np.unique(runLabel)[1::]\n    for run in runId:\n        runsMinSpeed.append(np.min(spdSmthd[runLabel == run]))\n    runsMinSpeed = np.array(runsMinSpeed)\n    badRuns = np.logical_or(\n        np.logical_or(\n            runsMinSpeed &lt; self.minimum_allowed_run_speed,\n            runDurationInPosBins &lt; self.minimum_allowed_run_duration,\n        ),\n        runsSansSpikes,\n    )\n    badRuns = np.squeeze(badRuns)\n    runLabel = applyFilter2Labels(~badRuns, runLabel)\n    runStartIdx = runStartIdx[~badRuns]\n    runEndIdx = runEndIdx[~badRuns]  # + 1\n    runsMinSpeed = runsMinSpeed[~badRuns]\n    runDurationInPosBins = runDurationInPosBins[~badRuns]\n    isRun = runLabel &gt; 0\n\n    # calculate mean and std direction for each run\n    runComplexMnDir = np.squeeze(np.zeros_like(runStartIdx))\n    np.add.at(\n        runComplexMnDir,\n        runLabel[isRun] - 1,\n        np.exp(1j * (xydir[isRun] * (np.pi / 180))),\n    )\n    meanDir = np.angle(runComplexMnDir)  # circ mean\n    tortuosity = 1 - np.abs(runComplexMnDir) / runDurationInPosBins\n\n    # caculate angular distance between the runs main direction and the\n    # pos's direction to the peak centre\n    posPhiUnSmthd = np.ones_like(fieldLabel) * np.nan\n    posPhiUnSmthd[isRun] = posAngleFromPeak[isRun] - meanDir[runLabel[isRun] - 1]\n\n    # smooth r and phi in cartesian space\n    # convert to cartesian coords first\n    posXUnSmthd, posYUnSmthd = pol2cart(posRUnsmthd, posPhiUnSmthd)\n    posXYUnSmthd = np.vstack((posXUnSmthd, posYUnSmthd))\n\n    # filter each run with filter of appropriate length\n    filtLen = np.squeeze(\n        np.floor((runEndIdx - runStartIdx + 1) * self.ifr_smoothing_constant)\n    )\n    xy_new = np.zeros_like(xy_old) * np.nan\n    for i in range(len(runStartIdx)):\n        if filtLen[i] &gt; 2:\n            filt = signal.firwin(\n                int(filtLen[i] - 1),\n                cutoff=self.spatial_lowpass_cutoff / self.pos_sample_rate * 2,\n                window=\"blackman\",\n            )\n            xy_new[:, runStartIdx[i] : runEndIdx[i]] = signal.filtfilt(\n                filt, [1], posXYUnSmthd[:, runStartIdx[i] : runEndIdx[i]], axis=1\n            )\n\n    r, phi = cart2pol(xy_new[0], xy_new[1])\n    r[r &gt; 1] = 1\n\n    # calculate the direction of the smoothed data\n    xydir_new = np.arctan2(np.diff(xy_new[1]), np.diff(xy_new[0]))\n    xydir_new = np.append(xydir_new, xydir_new[-1])\n    xydir_new[runEndIdx] = xydir_new[runEndIdx - 1]\n\n    # project the distance value onto the current direction\n    d_currentdir = r * np.cos(xydir_new - phi)\n\n    # calculate the cumulative distance travelled on each run\n    dr = np.sqrt(np.diff(np.power(r, 2), 1))\n    d_cumulative = labelledCumSum(np.insert(dr, 0, 0), runLabel)\n\n    # calculate cumulative sum of the expected normalised firing rate\n    exptdRate_cumulative = labelledCumSum(1 - r, runLabel)\n\n    # direction projected onto the run mean direction is just the x coord\n    d_meandir = xy_new[0]\n\n    # smooth binned spikes to get an instantaneous firing rate\n    # set up the smoothing kernel\n    kernLenInBins = np.round(self.ifr_kernel_len * self.bins_per_second)\n    kernSig = self.ifr_kernel_sigma * self.bins_per_second\n    k = signal.windows.gaussian(kernLenInBins, kernSig)\n    # get a count of spikes to smooth over\n    spkCount = np.bincount(spkPosInd, minlength=nPos)\n    # apply the smoothing kernel\n    instFiringRate = signal.convolve(spkCount, k, mode=\"same\")\n    instFiringRate[~isRun] = np.nan\n\n    # find time spent within run\n    time = np.ones(nPos)\n    time = labelledCumSum(time, runLabel)\n    timeInRun = time / self.pos_sample_rate\n\n    fieldNum = fieldLabel[runStartIdx]\n    mnSpd = np.squeeze(np.zeros_like(fieldNum))\n    np.add.at(mnSpd, runLabel[isRun] - 1, spd[isRun])\n    nPts = np.bincount(runLabel[isRun] - 1, minlength=len(mnSpd))\n    mnSpd = mnSpd / nPts\n    centralPeripheral = np.squeeze(np.zeros_like(fieldNum))\n    np.add.at(centralPeripheral, runLabel[isRun] - 1, xy_new[1, isRun])\n    centralPeripheral = centralPeripheral / nPts\n    if plot:\n        fig = plt.figure()\n        ax = fig.add_subplot(221)\n        ax.plot(xy_new[0], xy_new[1])\n        ax.set_title(\"Unit circle x-y\")\n        ax.set_aspect(\"equal\")\n        ax.set_xlim([-1, 1])\n        ax.set_ylim([-1, 1])\n        ax = fig.add_subplot(222)\n        ax.plot(fieldPerimX, fieldPerimY, \"k.\")\n        ax.set_title(\"Field perim and laser on events\")\n        ax.plot(xy[0, fieldLabel &gt; 0], xy[1, fieldLabel &gt; 0], \"y.\")\n        if laserEvents is not None:\n            validOns = np.setdiff1d(laserEvents, np.nonzero(~np.isnan(r))[0])\n            ax.plot(xy[0, validOns], xy[1, validOns], \"rx\")\n        ax.set_aspect(\"equal\")\n        angleCMInd = np.round(perimAngleFromPeak / np.pi * 180) + 180\n        angleCMInd[angleCMInd == 0] = 360\n        im = np.zeros_like(fieldPerimMask)\n        im[fieldPerimMask] = angleCMInd\n        imM = np.ma.MaskedArray(im, mask=~fieldPerimMask, copy=True)\n        #############################################\n        # create custom colormap\n        cmap = plt.colormaps[\"jet_r\"]\n        cmaplist = [cmap(i) for i in range(cmap.N)]\n        cmaplist[0] = (1, 1, 1, 1)\n        cmap = cmap.from_list(\"Runvals cmap\", cmaplist, cmap.N)\n        bounds = np.linspace(0, 1.0, 100)\n        norm = matplotlib.colors.BoundaryNorm(bounds, cmap.N)\n        # add the runs through the fields\n        runVals = np.zeros_like(im)\n        runVals[yBins[isRun] - 1, xBins[isRun] - 1] = r[isRun]\n        runVals = runVals\n        ax = fig.add_subplot(223)\n        imm = ax.imshow(\n            runVals, cmap=cmap, norm=norm, origin=\"lower\", interpolation=\"nearest\"\n        )\n        plt.colorbar(imm, orientation=\"horizontal\")\n        ax.set_aspect(\"equal\")\n        # add a custom colorbar for colors in runVals\n\n        # create a custom colormap for the plot\n        cmap = matplotlib.colormaps[\"hsv\"]\n        cmaplist = [cmap(i) for i in range(cmap.N)]\n        cmaplist[0] = (1, 1, 1, 1)\n        cmap = cmap.from_list(\"Perim cmap\", cmaplist, cmap.N)\n        bounds = np.linspace(0, 360, cmap.N)\n        norm = matplotlib.colors.BoundaryNorm(bounds, cmap.N)\n\n        imm = ax.imshow(\n            imM, cmap=cmap, norm=norm, origin=\"lower\", interpolation=\"nearest\"\n        )\n        plt.colorbar(imm)\n        ax.set_title(\"Runs by distance and angle\")\n        ax.plot(peaksXYBins[:, 1], peaksXYBins[:, 0], \"ko\")\n        ax.set_xlim(0, im.shape[1])\n        ax.set_ylim(0, im.shape[0])\n        #############################################\n        ax = fig.add_subplot(224)\n        ax.imshow(rmap, origin=\"lower\", interpolation=\"nearest\")\n        ax.set_aspect(\"equal\")\n        ax.set_title(\"Smoothed ratemap\")\n\n    # update the regressor dict from __init__ with relevant values\n    self.regressors[\"pos_exptdRate_cum\"][\"values\"] = exptdRate_cumulative\n    self.regressors[\"pos_instFR\"][\"values\"] = instFiringRate\n    self.regressors[\"pos_timeInRun\"][\"values\"] = timeInRun\n    self.regressors[\"pos_d_cum\"][\"values\"] = d_cumulative\n    self.regressors[\"pos_d_meanDir\"][\"values\"] = d_meandir\n    self.regressors[\"pos_d_currentdir\"][\"values\"] = d_currentdir\n    posKeys = (\n        \"xy\",\n        \"xydir\",\n        \"r\",\n        \"phi\",\n        \"xy_old\",\n        \"xydir_old\",\n        \"fieldLabel\",\n        \"runLabel\",\n        \"d_currentdir\",\n        \"d_cumulative\",\n        \"exptdRate_cumulative\",\n        \"d_meandir\",\n        \"instFiringRate\",\n        \"timeInRun\",\n        \"fieldPerimMask\",\n        \"perimAngleFromPeak\",\n        \"posAngleFromPeak\",\n    )\n    runsKeys = (\n        \"runStartIdx\",\n        \"runEndIdx\",\n        \"runDurationInPosBins\",\n        \"runsMinSpeed\",\n        \"meanDir\",\n        \"tortuosity\",\n        \"mnSpd\",\n        \"centralPeripheral\",\n    )\n    posDict = dict.fromkeys(posKeys, np.nan)\n    # neat trick: locals is a dict that holds all locally scoped variables\n    for thiskey in posDict.keys():\n        posDict[thiskey] = locals()[thiskey]\n    runsDict = dict.fromkeys(runsKeys, np.nan)\n    for thiskey in runsDict.keys():\n        runsDict[thiskey] = locals()[thiskey]\n    return posDict, runsDict\n</code></pre>"},{"location":"reference/#ephysiopy.common.phasecoding.phasePrecession2D.partitionFields","title":"<code>partitionFields(ftype='g', plot=False, **kwargs)</code>","text":"<p>Partitions fields.</p> <p>Partitions spikes into fields by finding the watersheds around the peaks of a super-smoothed ratemap</p> <p>Parameters:</p> Name Type Description Default <code>spike_ts</code> <code>array</code> <p>The ratemap to partition</p> required <code>ftype</code> <code>str</code> <p>'p' or 'g' denoting place or grid cells - not implemented yet</p> <code>'g'</code> <code>plot</code> <code>bool</code> <p>Whether to produce a debugging plot or not</p> <code>False</code> <p>Returns:</p> Name Type Description <code>peaksXY</code> <code>array_like</code> <p>The xy coordinates of the peak rates in</p> <p>each field</p> <code>peaksRate</code> <code>array_like</code> <p>The peak rates in peaksXY</p> <code>labels</code> <code>ndarray</code> <p>An array of the labels corresponding to</p> <p>each field (starting at 1)</p> <code>rmap</code> <code>ndarray</code> <p>The ratemap of the tetrode / cluster</p> Source code in <code>ephysiopy/common/phasecoding.py</code> <pre><code>def partitionFields(self, ftype=\"g\", plot=False, **kwargs):\n    \"\"\"\n    Partitions fields.\n\n    Partitions spikes into fields by finding the watersheds around the\n    peaks of a super-smoothed ratemap\n\n    Args:\n        spike_ts (np.array): The ratemap to partition\n        ftype (str): 'p' or 'g' denoting place or grid cells\n          - not implemented yet\n        plot (bool): Whether to produce a debugging plot or not\n\n    Returns:\n        peaksXY (array_like): The xy coordinates of the peak rates in\n        each field\n        peaksRate (array_like): The peak rates in peaksXY\n        labels (numpy.ndarray): An array of the labels corresponding to\n        each field (starting at 1)\n        rmap (numpy.ndarray): The ratemap of the tetrode / cluster\n    \"\"\"\n    rmap, (xe, ye) = self.RateMap.getMap(self.spk_weights)\n    nan_idx = np.isnan(rmap)\n    rmap[nan_idx] = 0\n    # start image processing:\n    # get some markers\n    from ephysiopy.common import fieldcalcs\n\n    markers = fieldcalcs.local_threshold(rmap, prc=self.field_threshold_percent)\n    # clear the edges / any invalid positions again\n    markers[nan_idx] = 0\n    # label these markers so each blob has a unique id\n    labels = ndimage.label(markers)[0]\n    # labels is now a labelled int array from 0 to however many fields have\n    # been detected\n    # get the number of spikes in each field - NB this is done against a\n    # flattened array so we need to figure out which count corresponds to\n    # which particular field id using np.unique\n    fieldId, _ = np.unique(labels, return_index=True)\n    fieldId = fieldId[1::]\n    # TODO: come back to this as may need to know field id ordering\n    peakCoords = np.array(\n        ndimage.maximum_position(rmap, labels=labels, index=fieldId)\n    ).astype(int)\n    # COMCoords = np.array(\n    #     ndimage.center_of_mass(\n    #         rmap, labels=labels, index=fieldId)\n    # ).astype(int)\n    peaksXY = np.vstack((xe[peakCoords[:, 0]], ye[peakCoords[:, 1]])).T\n    # find the peak rate at each of the centre of the detected fields to\n    # subsequently threshold the field at some fraction of the peak value\n    peakRates = rmap[peakCoords[:, 0], peakCoords[:, 1]]\n    fieldThresh = peakRates * self.field_threshold\n    rmFieldMask = np.zeros_like(rmap)\n    for fid in fieldId:\n        f = labels[peakCoords[fid - 1, 0], peakCoords[fid - 1, 1]]\n        rmFieldMask[labels == f] = rmap[labels == f] &gt; fieldThresh[f - 1]\n    labels[~rmFieldMask.astype(bool)] = 0\n    # peakBinInds = np.ceil(peakCoords)\n    # re-order some vars to get into same format as fieldLabels\n    peakLabels = labels[peakCoords[:, 0], peakCoords[:, 1]]\n    peaksXY = peaksXY[peakLabels - 1, :]\n    peaksRate = peakRates[peakLabels - 1]\n    # peakBinInds = peakBinInds[peakLabels-1, :]\n    # peaksXY = peakCoords - np.min(xy, 1)\n\n    # if ~np.isnan(self.area_threshold):\n    #     # TODO: this needs fixing so sensible values are used and the\n    #     # modified bool array is propagated correctly ie makes\n    #     # sense to have a function that applies a bool array to whatever\n    #     # arrays are used as output and call it in a couple of places\n    #     # areaInBins = self.area_threshold * self.binsPerCm\n    #     lb = ndimage.label(markers)[0]\n    #     rp = skimage.measure.regionprops(lb)\n    #     for reg in rp:\n    #         print(reg.filled_area)\n    #     markers = skimage.morphology.remove_small_objects(\n    #         lb, min_size=4000, connectivity=4, in_place=True)\n    if plot:\n        fig = plt.figure()\n        ax = fig.add_subplot(211)\n        ax.pcolormesh(\n            ye, xe, rmap, cmap=matplotlib.colormaps[\"jet\"], edgecolors=\"face\"\n        )\n        ax.set_title(\"Smoothed ratemap + peaks\")\n        ax.xaxis.set_visible(False)\n        ax.yaxis.set_visible(False)\n        ax.set_aspect(\"equal\")\n        xlim = ax.get_xlim()\n        ylim = ax.get_ylim()\n        ax.plot(peaksXY[:, 1], peaksXY[:, 0], \"ko\")\n        ax.set_ylim(ylim)\n        ax.set_xlim(xlim)\n\n        ax = fig.add_subplot(212)\n        ax.imshow(labels, interpolation=\"nearest\", origin=\"lower\")\n        ax.set_title(\"Labelled restricted fields\")\n        ax.xaxis.set_visible(False)\n        ax.yaxis.set_visible(False)\n        ax.set_aspect(\"equal\")\n\n    return peaksXY, peaksRate, labels, rmap\n</code></pre>"},{"location":"reference/#ephysiopy.common.phasecoding.phasePrecession2D.performRegression","title":"<code>performRegression(laserEvents=None, **kwargs)</code>","text":"<p>Wrapper function for doing the actual regression which has multiple stages.</p> <p>Specifically here we partition fields into sub-fields, get a bunch of information about the position, spiking and theta data and then do the actual regression.</p> <p>Parameters:</p> Name Type Description Default <code>tetrode</code> <code>int</code> <p>The tetrode to examine</p> required <code>cluster</code> <code>int</code> <p>The cluster to examine</p> required <code>laserEvents</code> <code>array_like</code> <p>The on times for laser events</p> <code>None</code> See Also <p>ephysiopy.common.eegcalcs.phasePrecession.partitionFields() ephysiopy.common.eegcalcs.phasePrecession.getPosProps() ephysiopy.common.eegcalcs.phasePrecession.getThetaProps() ephysiopy.common.eegcalcs.phasePrecession.getSpikeProps() ephysiopy.common.eegcalcs.phasePrecession._ppRegress()</p> Source code in <code>ephysiopy/common/phasecoding.py</code> <pre><code>def performRegression(self, laserEvents=None, **kwargs):\n    \"\"\"\n    Wrapper function for doing the actual regression which has multiple\n    stages.\n\n    Specifically here we partition fields into sub-fields, get a bunch of\n    information about the position, spiking and theta data and then\n    do the actual regression.\n\n    Args:\n        tetrode (int): The tetrode to examine\n        cluster (int): The cluster to examine\n        laserEvents (array_like, optional): The on times for laser events\n        if present. Default is None\n\n    See Also:\n        ephysiopy.common.eegcalcs.phasePrecession.partitionFields()\n        ephysiopy.common.eegcalcs.phasePrecession.getPosProps()\n        ephysiopy.common.eegcalcs.phasePrecession.getThetaProps()\n        ephysiopy.common.eegcalcs.phasePrecession.getSpikeProps()\n        ephysiopy.common.eegcalcs.phasePrecession._ppRegress()\n    \"\"\"\n\n    # Partition fields\n    peaksXY, _, labels, _ = self.partitionFields(plot=True)\n\n    # split into runs\n    posD, runD = self.getPosProps(\n        labels, peaksXY, laserEvents=laserEvents, plot=True\n    )\n\n    # get theta cycles, amplitudes, phase etc\n    self.getThetaProps()\n\n    # get the indices of spikes for various metrics such as\n    # theta cycle, run etc\n    spkD = self.getSpikeProps(\n        posD[\"runLabel\"], runD[\"meanDir\"], runD[\"runDurationInPosBins\"]\n    )\n\n    # Do the regressions\n    regress_dict = self._ppRegress(spkD, plot=True)\n\n    self.plotPPRegression(regress_dict)\n</code></pre>"},{"location":"reference/#ephysiopy.common.phasecoding.applyFilter2Labels","title":"<code>applyFilter2Labels(M, x)</code>","text":"<p>M is a logical mask specifying which label numbers to keep x is an array of positive integer labels</p> <p>This method sets the undesired labels to 0 and renumbers the remaining labels 1 to n when n is the number of trues in M</p> Source code in <code>ephysiopy/common/phasecoding.py</code> <pre><code>def applyFilter2Labels(M, x):\n    \"\"\"\n    M is a logical mask specifying which label numbers to keep\n    x is an array of positive integer labels\n\n    This method sets the undesired labels to 0 and renumbers the remaining\n    labels 1 to n when n is the number of trues in M\n    \"\"\"\n    newVals = M * np.cumsum(M)\n    x[x &gt; 0] = newVals[x[x &gt; 0] - 1]\n    return x\n</code></pre>"},{"location":"reference/#ephysiopy.common.phasecoding.ccc","title":"<code>ccc(t, p)</code>","text":"<p>Calculates correlation between two random circular variables</p> Source code in <code>ephysiopy/common/phasecoding.py</code> <pre><code>def ccc(t, p):\n    \"\"\"\n    Calculates correlation between two random circular variables\n    \"\"\"\n    n = len(t)\n    A = np.sum(np.cos(t) * np.cos(p))\n    B = np.sum(np.sin(t) * np.sin(p))\n    C = np.sum(np.cos(t) * np.sin(p))\n    D = np.sum(np.sin(t) * np.cos(p))\n    E = np.sum(np.cos(2 * t))\n    F = np.sum(np.sin(2 * t))\n    G = np.sum(np.cos(2 * p))\n    H = np.sum(np.sin(2 * p))\n    rho = 4 * (A * B - C * D) / np.sqrt((n**2 - E**2 - F**2) * (n**2 - G**2 - H**2))\n    return rho\n</code></pre>"},{"location":"reference/#ephysiopy.common.phasecoding.ccc_jack","title":"<code>ccc_jack(t, p)</code>","text":"<p>Function used to calculate jackknife estimates of correlation</p> Source code in <code>ephysiopy/common/phasecoding.py</code> <pre><code>def ccc_jack(t, p):\n    \"\"\"\n    Function used to calculate jackknife estimates of correlation\n    \"\"\"\n    n = len(t) - 1\n    A = np.cos(t) * np.cos(p)\n    A = np.sum(A) - A\n    B = np.sin(t) * np.sin(p)\n    B = np.sum(B) - B\n    C = np.cos(t) * np.sin(p)\n    C = np.sum(C) - C\n    D = np.sin(t) * np.cos(p)\n    D = np.sum(D) - D\n    E = np.cos(2 * t)\n    E = np.sum(E) - E\n    F = np.sin(2 * t)\n    F = np.sum(F) - F\n    G = np.cos(2 * p)\n    G = np.sum(G) - G\n    H = np.sin(2 * p)\n    H = np.sum(H) - H\n    rho = 4 * (A * B - C * D) / np.sqrt((n**2 - E**2 - F**2) * (n**2 - G**2 - H**2))\n    return rho\n</code></pre>"},{"location":"reference/#ephysiopy.common.phasecoding.circCircCorrTLinear","title":"<code>circCircCorrTLinear(theta, phi, k=1000, alpha=0.05, hyp=0, conf=True)</code>","text":"<p>An almost direct copy from AJs Matlab fcn to perform correlation between 2 circular random variables.</p> <p>Returns the correlation value (rho), p-value, bootstrapped correlation values, shuffled p values and correlation values.</p> <p>Parameters:</p> Name Type Description Default <code>theta,</code> <code>phi (array_like</code> <p>mx1 array containing circular data (radians) whose correlation is to be measured</p> required <code>k</code> <code>int</code> <p>number of permutations to use to calculate p-value from randomisation and bootstrap estimation of confidence intervals. Leave empty to calculate p-value analytically (NB confidence intervals will not be calculated). Default is 1000.</p> <code>1000</code> <code>alpha</code> <code>float</code> <p>hypothesis test level e.g. 0.05, 0.01 etc. Default is 0.05.</p> <code>0.05</code> <code>hyp</code> <code>int</code> <p>hypothesis to test; -1/ 0 / 1 (-ve correlated / correlated in either direction / positively correlated). Default is 0.</p> <code>0</code> <code>conf</code> <code>bool</code> <p>True or False to calculate confidence intervals via jackknife or bootstrap. Default is True.</p> <code>True</code> References <p>Fisher (1993), Statistical Analysis of Circular Data,     Cambridge University Press, ISBN: 0 521 56890 0</p> Source code in <code>ephysiopy/common/phasecoding.py</code> <pre><code>def circCircCorrTLinear(theta, phi, k=1000, alpha=0.05, hyp=0, conf=True):\n    \"\"\"\n    An almost direct copy from AJs Matlab fcn to perform correlation\n    between 2 circular random variables.\n\n    Returns the correlation value (rho), p-value, bootstrapped correlation\n    values, shuffled p values and correlation values.\n\n    Args:\n        theta, phi (array_like): mx1 array containing circular data (radians)\n            whose correlation is to be measured\n        k (int, optional): number of permutations to use to calculate p-value\n            from randomisation and bootstrap estimation of confidence\n            intervals.\n            Leave empty to calculate p-value analytically (NB confidence\n            intervals will not be calculated). Default is 1000.\n        alpha (float, optional): hypothesis test level e.g. 0.05, 0.01 etc.\n            Default is 0.05.\n        hyp (int, optional): hypothesis to test; -1/ 0 / 1 (-ve correlated /\n            correlated in either direction / positively correlated).\n            Default is 0.\n        conf (bool, optional): True or False to calculate confidence intervals\n            via jackknife or bootstrap. Default is True.\n\n    References:\n        Fisher (1993), Statistical Analysis of Circular Data,\n            Cambridge University Press, ISBN: 0 521 56890 0\n    \"\"\"\n    theta = theta.ravel()\n    phi = phi.ravel()\n\n    if not len(theta) == len(phi):\n        print(\"theta and phi not same length - try again!\")\n        raise ValueError()\n\n    # estimate correlation\n    rho = ccc(theta, phi)\n    n = len(theta)\n\n    # derive p-values\n    if k:\n        p_shuff = shuffledPVal(theta, phi, rho, k, hyp)\n        p = np.nan\n\n    # estimtate ci's for correlation\n    if n &gt;= 25 and conf:\n        # obtain jackknife estimates of rho and its ci's\n        rho_jack = ccc_jack(theta, phi)\n        rho_jack = n * rho - (n - 1) * rho_jack\n        rho_boot = np.mean(rho_jack)\n        rho_jack_std = np.std(rho_jack)\n        ci = (\n            rho_boot - (1 / np.sqrt(n)) * rho_jack_std * norm.ppf(alpha / 2, (0, 1))[0],\n            rho_boot + (1 / np.sqrt(n)) * rho_jack_std * norm.ppf(alpha / 2, (0, 1))[0],\n        )\n    elif conf and k and n &lt; 25 and n &gt; 4:\n        from sklearn.utils import resample\n\n        # set up the bootstrapping parameters\n        boot_samples = []\n        for i in range(k):\n            theta_sample = resample(theta, replace=True)\n            phi_sample = resample(phi, replace=True)\n            boot_samples.append(ccc(theta_sample, phi_sample))\n        rho_boot = np.mean(boot_samples)\n        # confidence intervals\n        p = ((1.0 - alpha) / 2.0) * 100\n        lower = max(0.0, np.percentile(boot_samples, p))\n        p = (alpha + ((1.0 - alpha) / 2.0)) * 100\n        upper = min(1.0, np.percentile(boot_samples, p))\n\n        ci = (lower, upper)\n    else:\n        rho_boot = np.nan\n        ci = np.nan\n\n    return rho, p, rho_boot, p_shuff, ci\n</code></pre>"},{"location":"reference/#ephysiopy.common.phasecoding.circRegress","title":"<code>circRegress(x, t)</code>","text":"<p>Finds approximation to circular-linear regression for phase precession.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>list</code> <p>n-by-1 list of in-field positions (linear variable)</p> required <code>t</code> <code>list</code> <p>n-by-1 list of phases, in degrees (converted to radians)</p> required Note <p>Neither x nor t can contain NaNs, must be paired (of equal length).</p> Source code in <code>ephysiopy/common/phasecoding.py</code> <pre><code>def circRegress(x, t):\n    \"\"\"\n    Finds approximation to circular-linear regression for phase precession.\n\n    Args:\n        x (list): n-by-1 list of in-field positions (linear variable)\n        t (list): n-by-1 list of phases, in degrees (converted to radians)\n\n    Note:\n        Neither x nor t can contain NaNs, must be paired (of equal length).\n    \"\"\"\n    # transform the linear co-variate to the range -1 to 1\n    if not np.any(x) or not np.any(t):\n        return x, t\n    mnx = np.mean(x)\n    xn = x - mnx\n    mxx = np.max(np.fabs(xn))\n    xn = xn / mxx\n    # keep tn between 0 and 2pi\n    tn = np.remainder(t, 2 * np.pi)\n    # constrain max slope to give at most 720 degrees of phase precession\n    # over the field\n    max_slope = (2 * np.pi) / (np.max(xn) - np.min(xn))\n\n    # perform slope optimisation and find intercept\n    def _cost(m, x, t):\n        return -np.abs(np.sum(np.exp(1j * (t - m * x)))) / len(t - m * x)\n\n    slope = optimize.fminbound(_cost, -1 * max_slope, max_slope, args=(xn, tn))\n    intercept = np.arctan2(\n        np.sum(np.sin(tn - slope * xn)), np.sum(np.cos(tn - slope * xn))\n    )\n    intercept = intercept + ((0 - slope) * (mnx / mxx))\n    slope = slope / mxx\n    return slope, intercept\n</code></pre>"},{"location":"reference/#ephysiopy.common.phasecoding.fixAngle","title":"<code>fixAngle(a)</code>","text":"<p>Ensure angles lie between -pi and pi a must be in radians</p> Source code in <code>ephysiopy/common/phasecoding.py</code> <pre><code>def fixAngle(a):\n    \"\"\"\n    Ensure angles lie between -pi and pi\n    a must be in radians\n    \"\"\"\n    b = np.mod(a + np.pi, 2 * np.pi) - np.pi\n    return b\n</code></pre>"},{"location":"reference/#ephysiopy.common.phasecoding.shuffledPVal","title":"<code>shuffledPVal(theta, phi, rho, k, hyp)</code>","text":"<p>Calculates shuffled p-values for correlation</p> Source code in <code>ephysiopy/common/phasecoding.py</code> <pre><code>def shuffledPVal(theta, phi, rho, k, hyp):\n    \"\"\"\n    Calculates shuffled p-values for correlation\n    \"\"\"\n    n = len(theta)\n    idx = np.zeros((n, k))\n    for i in range(k):\n        idx[:, i] = np.random.permutation(np.arange(n))\n\n    thetaPerms = theta[idx.astype(int)]\n\n    A = np.dot(np.cos(phi), np.cos(thetaPerms))\n    B = np.dot(np.sin(phi), np.sin(thetaPerms))\n    C = np.dot(np.sin(phi), np.cos(thetaPerms))\n    D = np.dot(np.cos(phi), np.sin(thetaPerms))\n    E = np.sum(np.cos(2 * theta))\n    F = np.sum(np.sin(2 * theta))\n    G = np.sum(np.cos(2 * phi))\n    H = np.sum(np.sin(2 * phi))\n\n    rho_sim = 4 * (A * B - C * D) / np.sqrt((n**2 - E**2 - F**2) * (n**2 - G**2 - H**2))\n\n    if hyp == 1:\n        p_shuff = np.sum(rho_sim &gt;= rho) / float(k)\n    elif hyp == -1:\n        p_shuff = np.sum(rho_sim &lt;= rho) / float(k)\n    elif hyp == 0:\n        p_shuff = np.sum(np.fabs(rho_sim) &gt; np.fabs(rho)) / float(k)\n    else:\n        p_shuff = np.nan\n\n    return p_shuff\n</code></pre>"},{"location":"reference/#rhymicity","title":"Rhymicity","text":""},{"location":"reference/#ephysiopy.common.rhythmicity.CosineDirectionalTuning","title":"<code>CosineDirectionalTuning</code>","text":"<p>               Bases: <code>object</code></p> <p>Produces output to do with Welday et al (2011) like analysis of rhythmic firing a la oscialltory interference model</p> Source code in <code>ephysiopy/common/rhythmicity.py</code> <pre><code>class CosineDirectionalTuning(object):\n    \"\"\"\n    Produces output to do with Welday et al (2011) like analysis\n    of rhythmic firing a la oscialltory interference model\n    \"\"\"\n\n    def __init__(\n        self,\n        spike_times: np.array,\n        pos_times: np.array,\n        spk_clusters: np.array,\n        x: np.array,\n        y: np.array,\n        tracker_params={},\n    ):\n        \"\"\"\n        Args:\n            spike_times (1d np.array): Spike times\n            pos_times (1d np.array): Position times\n            spk_clusters (1d np.array): Spike clusters\n            x and y (1d np.array): Position coordinates\n            tracker_params (dict): From the PosTracker as created in\n                                    OESettings.Settings.parse\n\n        Note:\n            All timestamps should be given in sub-millisecond accurate seconds\n            and pos_xy in cms\n        \"\"\"\n        self.spike_times = spike_times\n        self.pos_times = pos_times\n        self.spk_clusters = spk_clusters\n        \"\"\"\n        There can be more spikes than pos samples in terms of sampling as the\n        open-ephys buffer probably needs to finish writing and the camera has\n        already stopped, so cut of any cluster indices and spike times\n        that exceed the length of the pos indices\n        \"\"\"\n        idx_to_keep = self.spike_times &lt; self.pos_times[-1]\n        self.spike_times = self.spike_times[idx_to_keep]\n        self.spk_clusters = self.spk_clusters[idx_to_keep]\n        self._pos_sample_rate = 30\n        self._spk_sample_rate = 3e4\n        self._pos_samples_for_spike = None\n        self._min_runlength = 0.4  # in seconds\n        self.posCalcs = PosCalcsGeneric(\n            x, y, 230, cm=True, jumpmax=100, tracker_params=tracker_params\n        )\n        self.spikeCalcs = SpikeCalcsGeneric(spike_times, spk_clusters[0])\n        self.spikeCalcs.spk_clusters = spk_clusters\n        self.posCalcs.postprocesspos(tracker_params)\n        xy = self.posCalcs.xy\n        hdir = self.posCalcs.dir\n        self.posCalcs.calcSpeed(xy)\n        self._xy = xy\n        self._hdir = hdir\n        self._speed = self.posCalcs.speed\n        # TEMPORARY FOR POWER SPECTRUM STUFF\n        self.smthKernelWidth = 2\n        self.smthKernelSigma = 0.1875\n        self.sn2Width = 2\n        self.thetaRange = [7, 11]\n        self.xmax = 11\n\n    @property\n    def spk_sample_rate(self):\n        return self._spk_sample_rate\n\n    @spk_sample_rate.setter\n    def spk_sample_rate(self, value):\n        self._spk_sample_rate = value\n\n    @property\n    def pos_sample_rate(self):\n        return self._pos_sample_rate\n\n    @pos_sample_rate.setter\n    def pos_sample_rate(self, value):\n        self._pos_sample_rate = value\n\n    @property\n    def min_runlength(self):\n        return self._min_runlength\n\n    @min_runlength.setter\n    def min_runlength(self, value):\n        self._min_runlength = value\n\n    @property\n    def xy(self):\n        return self._xy\n\n    @xy.setter\n    def xy(self, value):\n        self._xy = value\n\n    @property\n    def hdir(self):\n        return self._hdir\n\n    @hdir.setter\n    def hdir(self, value):\n        self._hdir = value\n\n    @property\n    def speed(self):\n        return self._speed\n\n    @speed.setter\n    def speed(self, value):\n        self._speed = value\n\n    @property\n    def pos_samples_for_spike(self):\n        return self._pos_samples_for_spike\n\n    @pos_samples_for_spike.setter\n    def pos_samples_for_spike(self, value):\n        self._pos_samples_for_spike = value\n\n    def _rolling_window(self, a: np.array, window: int):\n        \"\"\"\n        Totally nabbed from SO:\n        https://stackoverflow.com/questions/6811183/rolling-window-for-1d-arrays-in-numpy\n        \"\"\"\n        shape = a.shape[:-1] + (a.shape[-1] - window + 1, window)\n        strides = a.strides + (a.strides[-1],)\n        return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n\n    def getPosIndices(self):\n        self.pos_samples_for_spike = np.floor(\n            self.spike_times * self.pos_sample_rate\n        ).astype(int)\n\n    def getClusterPosIndices(self, clust: int) -&gt; np.array:\n        if self.pos_samples_for_spike is None:\n            self.getPosIndices()\n        clust_pos_idx = self.pos_samples_for_spike[self.spk_clusters == clust]\n        clust_pos_idx[clust_pos_idx &gt;= len(self.pos_times)] = (\n            len(self.pos_times) - 1\n        )\n        return clust_pos_idx\n\n    def getClusterSpikeTimes(self, cluster: int):\n        ts = self.spike_times[self.spk_clusters == cluster]\n        if self.pos_samples_for_spike is None:\n            self.getPosIndices()\n        return ts\n\n    def getDirectionalBinPerPosition(self, binwidth: int):\n        \"\"\"\n        Direction is in degrees as that what is created by me in some of the\n        other bits of this package.\n\n        Args:\n            binwidth (int): The bin width in degrees\n\n        Returns:\n            A digitization of which directional bin each pos sample belongs to\n        \"\"\"\n\n        bins = np.arange(0, 360, binwidth)\n        return np.digitize(self.hdir, bins)\n\n    def getDirectionalBinForCluster(self, cluster: int):\n        b = self.getDirectionalBinPerPosition(45)\n        cluster_pos = self.getClusterPosIndices(cluster)\n        # idx_to_keep = cluster_pos &lt; len(self.pos_times)\n        # cluster_pos = cluster_pos[idx_to_keep]\n        return b[cluster_pos]\n\n    def getRunsOfMinLength(self):\n        \"\"\"\n        Identifies runs of at least self.min_runlength seconds long,\n        which at 30Hz pos sampling rate equals 12 samples, and\n        returns the start and end indices at which\n        the run was occurred and the directional bin that run belongs to\n\n        Returns:\n            np.array: The start and end indices into pos samples of the run\n                      and the directional bin to which it belongs\n        \"\"\"\n\n        b = self.getDirectionalBinPerPosition(45)\n        # nabbed from SO\n        from itertools import groupby\n\n        grouped_runs = [(k, sum(1 for i in g)) for k, g in groupby(b)]\n        grouped_runs = np.array(grouped_runs)\n        run_start_indices = np.cumsum(grouped_runs[:, 1]) - grouped_runs[:, 1]\n        min_len_in_samples = int(self.pos_sample_rate * self.min_runlength)\n        min_len_runs_mask = grouped_runs[:, 1] &gt;= min_len_in_samples\n        ret = np.array(\n            [run_start_indices[min_len_runs_mask],\n                grouped_runs[min_len_runs_mask, 1]]\n        ).T\n        # ret contains run length as last column\n        ret = np.insert(ret, 1, np.sum(ret, 1), 1)\n        ret = np.insert(ret, 2, grouped_runs[min_len_runs_mask, 0], 1)\n        return ret[:, 0:3]\n\n    def speedFilterRuns(self, runs: np.array, minspeed=5.0):\n        \"\"\"\n        Given the runs identified in getRunsOfMinLength, filter for speed\n        and return runs that meet the min speed criteria.\n\n        The function goes over the runs with a moving window of length equal\n        to self.min_runlength in samples and sees if any of those segments\n        meet the speed criteria and splits them out into separate runs if true.\n\n        NB For now this means the same spikes might get included in the\n        autocorrelation procedure later as the\n        moving window will use overlapping periods - can be modified later.\n\n        Args:\n            runs (3 x nRuns np.array): Generated from getRunsOfMinLength\n            minspeed (float): Min running speed in cm/s for an epoch (minimum\n                              epoch length defined previously\n                              in getRunsOfMinLength as minlength, usually 0.4s)\n\n        Returns:\n            3 x nRuns np.array: A modified version of the \"runs\" input variable\n        \"\"\"\n        minlength_in_samples = int(self.pos_sample_rate * self.min_runlength)\n        run_list = runs.tolist()\n        all_speed = np.array(self.speed)\n        for start_idx, end_idx, dir_bin in run_list:\n            this_runs_speed = all_speed[start_idx:end_idx]\n            this_runs_runs = self._rolling_window(\n                this_runs_speed, minlength_in_samples)\n            run_mask = np.all(this_runs_runs &gt; minspeed, 1)\n            if np.any(run_mask):\n                print(\"got one\")\n\n    \"\"\"\n    def testing(self, cluster: int):\n        ts = self.getClusterSpikeTimes(cluster)\n        pos_idx = self.getClusterPosIndices(cluster)\n\n        dir_bins = self.getDirectionalBinPerPosition(45)\n        cluster_dir_bins = dir_bins[pos_idx.astype(int)]\n\n        from scipy.signal import periodogram, boxcar, filtfilt\n\n        acorrs = []\n        max_freqs = []\n        max_idx = []\n        isis = []\n\n        acorr_range = np.array([-500, 500])\n        for i in range(1, 9):\n            this_bin_indices = cluster_dir_bins == i\n            this_ts = ts[this_bin_indices]  # in seconds still so * 1000 for ms\n            y = self.spikeCalcs.xcorr(this_ts*1000, Trange=acorr_range)\n            isis.append(y)\n            corr, acorr_bins = np.histogram(\n                y[y != 0], bins=501, range=acorr_range)\n            freqs, power = periodogram(corr, fs=200, return_onesided=True)\n            # Smooth the power over +/- 1Hz\n            b = boxcar(3)\n            h = filtfilt(b, 3, power)\n            # Square the amplitude first\n            sqd_amp = h ** 2\n            # Then find the mean power in the +/-1Hz band either side of that\n            theta_band_max_idx = np.nonzero(\n                sqd_amp == np.max(\n                    sqd_amp[np.logical_and(freqs &gt; 6, freqs &lt; 11)]))[0][0]\n            max_freq = freqs[theta_band_max_idx]\n            acorrs.append(corr)\n            max_freqs.append(max_freq)\n            max_idx.append(theta_band_max_idx)\n        return isis, acorrs, max_freqs, max_idx, acorr_bins\n\n    def plotXCorrsByDirection(self, cluster: int):\n        acorr_range = np.array([-500, 500])\n        # plot_range = np.array([-400,400])\n        nbins = 501\n        isis, acorrs, max_freqs, max_idx, acorr_bins = self.testing(cluster)\n        bin_labels = np.arange(0, 360, 45)\n        fig, axs = plt.subplots(8)\n        pts = []\n        for i, a in enumerate(isis):\n            axs[i].hist(\n                a[a != 0], bins=nbins, range=acorr_range,\n                color='k', histtype='stepfilled')\n            # find the max of the first positive peak\n            corr, _ = np.histogram(a[a != 0], bins=nbins, range=acorr_range)\n            axs[i].set_xlim(acorr_range)\n            axs[i].set_ylabel(str(bin_labels[i]))\n            axs[i].set_yticklabels('')\n            if i &lt; 7:\n                axs[i].set_xticklabels('')\n            axs[i].spines['right'].set_visible(False)\n            axs[i].spines['top'].set_visible(False)\n            axs[i].spines['left'].set_visible(False)\n        plt.show()\n        return pts\n    \"\"\"\n\n    def intrinsic_freq_autoCorr(\n        self,\n        spkTimes=None,\n        posMask=None,\n        maxFreq=25,\n        acBinSize=0.002,\n        acWindow=0.5,\n        plot=True,\n        **kwargs,\n    ):\n        \"\"\"\n        This is taken and adapted from ephysiopy.common.eegcalcs.EEGCalcs\n\n        Args:\n            spkTimes (np.array): Times in seconds of the cells firing\n            posMask (np.array): Boolean array corresponding to the length of\n                                spkTimes where True is stuff to keep\n            maxFreq (float): The maximum frequency to do the power spectrum\n                                out to\n            acBinSize (float): The bin size of the autocorrelogram in seconds\n            acWindow (float): The range of the autocorr in seconds\n\n        Note:\n            Make sure all times are in seconds\n        \"\"\"\n        acBinsPerPos = 1.0 / self.pos_sample_rate / acBinSize\n        acWindowSizeBins = np.round(acWindow / acBinSize)\n        binCentres = np.arange(0.5, len(posMask) * acBinsPerPos) * acBinSize\n        spkTrHist, _ = np.histogram(spkTimes, bins=binCentres)\n\n        # split the single histogram into individual chunks\n        splitIdx = np.nonzero(np.diff(posMask.astype(int)))[0] + 1\n        splitMask = np.split(posMask, splitIdx)\n        splitSpkHist = np.split(\n            spkTrHist, (splitIdx * acBinsPerPos).astype(int))\n        histChunks = []\n        for i in range(len(splitSpkHist)):\n            if np.all(splitMask[i]):\n                if np.sum(splitSpkHist[i]) &gt; 2:\n                    if len(splitSpkHist[i]) &gt; int(acWindowSizeBins) * 2:\n                        histChunks.append(splitSpkHist[i])\n        autoCorrGrid = np.zeros((int(acWindowSizeBins) + 1, len(histChunks)))\n        chunkLens = []\n        from scipy import signal\n\n        print(f\"num chunks = {len(histChunks)}\")\n        for i in range(len(histChunks)):\n            lenThisChunk = len(histChunks[i])\n            chunkLens.append(lenThisChunk)\n            tmp = np.zeros(lenThisChunk * 2)\n            tmp[lenThisChunk // 2: lenThisChunk //\n                2 + lenThisChunk] = histChunks[i]\n            tmp2 = signal.fftconvolve(\n                tmp, histChunks[i][::-1], mode=\"valid\"\n            )  # the autocorrelation\n            autoCorrGrid[:, i] = (\n                tmp2[lenThisChunk // 2: lenThisChunk //\n                     2 + int(acWindowSizeBins) + 1]\n                / acBinsPerPos\n            )\n\n        totalLen = np.sum(chunkLens)\n        autoCorrSum = np.nansum(autoCorrGrid, 1) / totalLen\n        meanNormdAc = autoCorrSum[1::] - np.nanmean(autoCorrSum[1::])\n        # return meanNormdAc\n        out = self.power_spectrum(\n            eeg=meanNormdAc,\n            binWidthSecs=acBinSize,\n            maxFreq=maxFreq,\n            pad2pow=16,\n            **kwargs,\n        )\n        out.update({\"meanNormdAc\": meanNormdAc})\n        if plot:\n            fig = plt.gcf()\n            ax = fig.gca()\n            xlim = ax.get_xlim()\n            ylim = ax.get_ylim()\n            ax.imshow(\n                autoCorrGrid,\n                extent=[\n                    maxFreq * 0.6,\n                    maxFreq,\n                    np.max(out[\"Power\"]) * 0.6,\n                    ax.get_ylim()[1],\n                ],\n            )\n            ax.set_ylim(ylim)\n            ax.set_xlim(xlim)\n        return out\n\n    def power_spectrum(\n        self,\n        eeg,\n        plot=True,\n        binWidthSecs=None,\n        maxFreq=25,\n        pad2pow=None,\n        ymax=None,\n        **kwargs,\n    ):\n        \"\"\"\n        Method used by eeg_power_spectra and intrinsic_freq_autoCorr\n        Signal in must be mean normalised already\n        \"\"\"\n\n        # Get raw power spectrum\n        nqLim = 1 / binWidthSecs / 2.0\n        origLen = len(eeg)\n        # if pad2pow is None:\n        # \tfftLen = int(np.power(2, self._nextpow2(origLen)))\n        # else:\n        fftLen = int(np.power(2, pad2pow))\n        fftHalfLen = int(fftLen / float(2) + 1)\n\n        fftRes = np.fft.fft(eeg, fftLen)\n        # get power density from fft and discard second half of spectrum\n        _power = np.power(np.abs(fftRes), 2) / origLen\n        power = np.delete(_power, np.s_[fftHalfLen::])\n        power[1:-2] = power[1:-2] * 2\n\n        # calculate freqs and crop spectrum to requested range\n        freqs = nqLim * np.linspace(0, 1, fftHalfLen)\n        freqs = freqs[freqs &lt;= maxFreq].T\n        power = power[0: len(freqs)]\n\n        # smooth spectrum using gaussian kernel\n        binsPerHz = (fftHalfLen - 1) / nqLim\n        kernelLen = np.round(self.smthKernelWidth * binsPerHz)\n        kernelSig = self.smthKernelSigma * binsPerHz\n        from scipy import signal\n\n        k = signal.windows.gaussian(kernelLen, kernelSig) / (kernelLen / 2 / 2)\n        power_sm = signal.fftconvolve(power, k[::-1], mode=\"same\")\n\n        # calculate some metrics\n        # find max in theta band\n        spectrumMaskBand = np.logical_and(\n            freqs &gt; self.thetaRange[0], freqs &lt; self.thetaRange[1]\n        )\n        bandMaxPower = np.max(power_sm[spectrumMaskBand])\n        maxBinInBand = np.argmax(power_sm[spectrumMaskBand])\n        bandFreqs = freqs[spectrumMaskBand]\n        freqAtBandMaxPower = bandFreqs[maxBinInBand]\n        # self.maxBinInBand = maxBinInBand\n        # self.freqAtBandMaxPower = freqAtBandMaxPower\n        # self.bandMaxPower = bandMaxPower\n\n        # find power in small window around peak and divide by power in rest\n        # of spectrum to get snr\n        spectrumMaskPeak = np.logical_and(\n            freqs &gt; freqAtBandMaxPower - self.sn2Width / 2,\n            freqs &lt; freqAtBandMaxPower + self.sn2Width / 2,\n        )\n        s2n = np.nanmean(power_sm[spectrumMaskPeak]) / np.nanmean(\n            power_sm[~spectrumMaskPeak]\n        )\n        self.freqs = freqs\n        self.power_sm = power_sm\n        self.spectrumMaskPeak = spectrumMaskPeak\n        if plot:\n            fig = plt.figure()\n            ax = fig.add_subplot(111)\n            if ymax is None:\n                ymax = np.min([2 * np.max(power), np.max(power_sm)])\n                if ymax == 0:\n                    ymax = 1\n            ax.plot(freqs, power, c=[0.9, 0.9, 0.9])\n            # ax.hold(True)\n            ax.plot(freqs, power_sm, \"k\", lw=2)\n            ax.axvline(self.thetaRange[0], c=\"b\", ls=\"--\")\n            ax.axvline(self.thetaRange[1], c=\"b\", ls=\"--\")\n            _, stemlines, _ = ax.stem([freqAtBandMaxPower], [\n                                      bandMaxPower], linefmt=\"r\")\n            # plt.setp(stemlines, 'linewidth', 2)\n            ax.fill_between(\n                freqs,\n                0,\n                power_sm,\n                where=spectrumMaskPeak,\n                color=\"r\",\n                alpha=0.25,\n                zorder=25,\n            )\n            # ax.set_ylim(0, ymax)\n            # ax.set_xlim(0, self.xmax)\n            ax.set_xlabel(\"Frequency (Hz)\")\n            ax.set_ylabel(\"Power density (W/Hz)\")\n        out_dict = {\n            \"maxFreq\": freqAtBandMaxPower,\n            \"Power\": power_sm,\n            \"Freqs\": freqs,\n            \"s2n\": s2n,\n            \"Power_raw\": power,\n            \"k\": k,\n            \"kernelLen\": kernelLen,\n            \"kernelSig\": kernelSig,\n            \"binsPerHz\": binsPerHz,\n            \"kernelLen\": kernelLen,\n        }\n        return out_dict\n</code></pre>"},{"location":"reference/#ephysiopy.common.rhythmicity.CosineDirectionalTuning.spk_clusters","title":"<code>spk_clusters = self.spk_clusters[idx_to_keep]</code>  <code>instance-attribute</code>","text":"<p>There can be more spikes than pos samples in terms of sampling as the open-ephys buffer probably needs to finish writing and the camera has already stopped, so cut of any cluster indices and spike times that exceed the length of the pos indices</p>"},{"location":"reference/#ephysiopy.common.rhythmicity.CosineDirectionalTuning.__init__","title":"<code>__init__(spike_times, pos_times, spk_clusters, x, y, tracker_params={})</code>","text":"<p>Parameters:</p> Name Type Description Default <code>spike_times</code> <code>1d np.array</code> <p>Spike times</p> required <code>pos_times</code> <code>1d np.array</code> <p>Position times</p> required <code>spk_clusters</code> <code>1d np.array</code> <p>Spike clusters</p> required <code>x</code> <code>and y (1d np.array</code> <p>Position coordinates</p> required <code>tracker_params</code> <code>dict</code> <p>From the PosTracker as created in                     OESettings.Settings.parse</p> <code>{}</code> Note <p>All timestamps should be given in sub-millisecond accurate seconds and pos_xy in cms</p> Source code in <code>ephysiopy/common/rhythmicity.py</code> <pre><code>def __init__(\n    self,\n    spike_times: np.array,\n    pos_times: np.array,\n    spk_clusters: np.array,\n    x: np.array,\n    y: np.array,\n    tracker_params={},\n):\n    \"\"\"\n    Args:\n        spike_times (1d np.array): Spike times\n        pos_times (1d np.array): Position times\n        spk_clusters (1d np.array): Spike clusters\n        x and y (1d np.array): Position coordinates\n        tracker_params (dict): From the PosTracker as created in\n                                OESettings.Settings.parse\n\n    Note:\n        All timestamps should be given in sub-millisecond accurate seconds\n        and pos_xy in cms\n    \"\"\"\n    self.spike_times = spike_times\n    self.pos_times = pos_times\n    self.spk_clusters = spk_clusters\n    \"\"\"\n    There can be more spikes than pos samples in terms of sampling as the\n    open-ephys buffer probably needs to finish writing and the camera has\n    already stopped, so cut of any cluster indices and spike times\n    that exceed the length of the pos indices\n    \"\"\"\n    idx_to_keep = self.spike_times &lt; self.pos_times[-1]\n    self.spike_times = self.spike_times[idx_to_keep]\n    self.spk_clusters = self.spk_clusters[idx_to_keep]\n    self._pos_sample_rate = 30\n    self._spk_sample_rate = 3e4\n    self._pos_samples_for_spike = None\n    self._min_runlength = 0.4  # in seconds\n    self.posCalcs = PosCalcsGeneric(\n        x, y, 230, cm=True, jumpmax=100, tracker_params=tracker_params\n    )\n    self.spikeCalcs = SpikeCalcsGeneric(spike_times, spk_clusters[0])\n    self.spikeCalcs.spk_clusters = spk_clusters\n    self.posCalcs.postprocesspos(tracker_params)\n    xy = self.posCalcs.xy\n    hdir = self.posCalcs.dir\n    self.posCalcs.calcSpeed(xy)\n    self._xy = xy\n    self._hdir = hdir\n    self._speed = self.posCalcs.speed\n    # TEMPORARY FOR POWER SPECTRUM STUFF\n    self.smthKernelWidth = 2\n    self.smthKernelSigma = 0.1875\n    self.sn2Width = 2\n    self.thetaRange = [7, 11]\n    self.xmax = 11\n</code></pre>"},{"location":"reference/#ephysiopy.common.rhythmicity.CosineDirectionalTuning.getDirectionalBinPerPosition","title":"<code>getDirectionalBinPerPosition(binwidth)</code>","text":"<p>Direction is in degrees as that what is created by me in some of the other bits of this package.</p> <p>Parameters:</p> Name Type Description Default <code>binwidth</code> <code>int</code> <p>The bin width in degrees</p> required <p>Returns:</p> Type Description <p>A digitization of which directional bin each pos sample belongs to</p> Source code in <code>ephysiopy/common/rhythmicity.py</code> <pre><code>def getDirectionalBinPerPosition(self, binwidth: int):\n    \"\"\"\n    Direction is in degrees as that what is created by me in some of the\n    other bits of this package.\n\n    Args:\n        binwidth (int): The bin width in degrees\n\n    Returns:\n        A digitization of which directional bin each pos sample belongs to\n    \"\"\"\n\n    bins = np.arange(0, 360, binwidth)\n    return np.digitize(self.hdir, bins)\n</code></pre>"},{"location":"reference/#ephysiopy.common.rhythmicity.CosineDirectionalTuning.getRunsOfMinLength","title":"<code>getRunsOfMinLength()</code>","text":"<p>Identifies runs of at least self.min_runlength seconds long, which at 30Hz pos sampling rate equals 12 samples, and returns the start and end indices at which the run was occurred and the directional bin that run belongs to</p> <p>Returns:</p> Type Description <p>np.array: The start and end indices into pos samples of the run       and the directional bin to which it belongs</p> Source code in <code>ephysiopy/common/rhythmicity.py</code> <pre><code>def getRunsOfMinLength(self):\n    \"\"\"\n    Identifies runs of at least self.min_runlength seconds long,\n    which at 30Hz pos sampling rate equals 12 samples, and\n    returns the start and end indices at which\n    the run was occurred and the directional bin that run belongs to\n\n    Returns:\n        np.array: The start and end indices into pos samples of the run\n                  and the directional bin to which it belongs\n    \"\"\"\n\n    b = self.getDirectionalBinPerPosition(45)\n    # nabbed from SO\n    from itertools import groupby\n\n    grouped_runs = [(k, sum(1 for i in g)) for k, g in groupby(b)]\n    grouped_runs = np.array(grouped_runs)\n    run_start_indices = np.cumsum(grouped_runs[:, 1]) - grouped_runs[:, 1]\n    min_len_in_samples = int(self.pos_sample_rate * self.min_runlength)\n    min_len_runs_mask = grouped_runs[:, 1] &gt;= min_len_in_samples\n    ret = np.array(\n        [run_start_indices[min_len_runs_mask],\n            grouped_runs[min_len_runs_mask, 1]]\n    ).T\n    # ret contains run length as last column\n    ret = np.insert(ret, 1, np.sum(ret, 1), 1)\n    ret = np.insert(ret, 2, grouped_runs[min_len_runs_mask, 0], 1)\n    return ret[:, 0:3]\n</code></pre>"},{"location":"reference/#ephysiopy.common.rhythmicity.CosineDirectionalTuning.intrinsic_freq_autoCorr","title":"<code>intrinsic_freq_autoCorr(spkTimes=None, posMask=None, maxFreq=25, acBinSize=0.002, acWindow=0.5, plot=True, **kwargs)</code>","text":"<p>This is taken and adapted from ephysiopy.common.eegcalcs.EEGCalcs</p> <p>Parameters:</p> Name Type Description Default <code>spkTimes</code> <code>array</code> <p>Times in seconds of the cells firing</p> <code>None</code> <code>posMask</code> <code>array</code> <p>Boolean array corresponding to the length of                 spkTimes where True is stuff to keep</p> <code>None</code> <code>maxFreq</code> <code>float</code> <p>The maximum frequency to do the power spectrum                 out to</p> <code>25</code> <code>acBinSize</code> <code>float</code> <p>The bin size of the autocorrelogram in seconds</p> <code>0.002</code> <code>acWindow</code> <code>float</code> <p>The range of the autocorr in seconds</p> <code>0.5</code> Note <p>Make sure all times are in seconds</p> Source code in <code>ephysiopy/common/rhythmicity.py</code> <pre><code>def intrinsic_freq_autoCorr(\n    self,\n    spkTimes=None,\n    posMask=None,\n    maxFreq=25,\n    acBinSize=0.002,\n    acWindow=0.5,\n    plot=True,\n    **kwargs,\n):\n    \"\"\"\n    This is taken and adapted from ephysiopy.common.eegcalcs.EEGCalcs\n\n    Args:\n        spkTimes (np.array): Times in seconds of the cells firing\n        posMask (np.array): Boolean array corresponding to the length of\n                            spkTimes where True is stuff to keep\n        maxFreq (float): The maximum frequency to do the power spectrum\n                            out to\n        acBinSize (float): The bin size of the autocorrelogram in seconds\n        acWindow (float): The range of the autocorr in seconds\n\n    Note:\n        Make sure all times are in seconds\n    \"\"\"\n    acBinsPerPos = 1.0 / self.pos_sample_rate / acBinSize\n    acWindowSizeBins = np.round(acWindow / acBinSize)\n    binCentres = np.arange(0.5, len(posMask) * acBinsPerPos) * acBinSize\n    spkTrHist, _ = np.histogram(spkTimes, bins=binCentres)\n\n    # split the single histogram into individual chunks\n    splitIdx = np.nonzero(np.diff(posMask.astype(int)))[0] + 1\n    splitMask = np.split(posMask, splitIdx)\n    splitSpkHist = np.split(\n        spkTrHist, (splitIdx * acBinsPerPos).astype(int))\n    histChunks = []\n    for i in range(len(splitSpkHist)):\n        if np.all(splitMask[i]):\n            if np.sum(splitSpkHist[i]) &gt; 2:\n                if len(splitSpkHist[i]) &gt; int(acWindowSizeBins) * 2:\n                    histChunks.append(splitSpkHist[i])\n    autoCorrGrid = np.zeros((int(acWindowSizeBins) + 1, len(histChunks)))\n    chunkLens = []\n    from scipy import signal\n\n    print(f\"num chunks = {len(histChunks)}\")\n    for i in range(len(histChunks)):\n        lenThisChunk = len(histChunks[i])\n        chunkLens.append(lenThisChunk)\n        tmp = np.zeros(lenThisChunk * 2)\n        tmp[lenThisChunk // 2: lenThisChunk //\n            2 + lenThisChunk] = histChunks[i]\n        tmp2 = signal.fftconvolve(\n            tmp, histChunks[i][::-1], mode=\"valid\"\n        )  # the autocorrelation\n        autoCorrGrid[:, i] = (\n            tmp2[lenThisChunk // 2: lenThisChunk //\n                 2 + int(acWindowSizeBins) + 1]\n            / acBinsPerPos\n        )\n\n    totalLen = np.sum(chunkLens)\n    autoCorrSum = np.nansum(autoCorrGrid, 1) / totalLen\n    meanNormdAc = autoCorrSum[1::] - np.nanmean(autoCorrSum[1::])\n    # return meanNormdAc\n    out = self.power_spectrum(\n        eeg=meanNormdAc,\n        binWidthSecs=acBinSize,\n        maxFreq=maxFreq,\n        pad2pow=16,\n        **kwargs,\n    )\n    out.update({\"meanNormdAc\": meanNormdAc})\n    if plot:\n        fig = plt.gcf()\n        ax = fig.gca()\n        xlim = ax.get_xlim()\n        ylim = ax.get_ylim()\n        ax.imshow(\n            autoCorrGrid,\n            extent=[\n                maxFreq * 0.6,\n                maxFreq,\n                np.max(out[\"Power\"]) * 0.6,\n                ax.get_ylim()[1],\n            ],\n        )\n        ax.set_ylim(ylim)\n        ax.set_xlim(xlim)\n    return out\n</code></pre>"},{"location":"reference/#ephysiopy.common.rhythmicity.CosineDirectionalTuning.power_spectrum","title":"<code>power_spectrum(eeg, plot=True, binWidthSecs=None, maxFreq=25, pad2pow=None, ymax=None, **kwargs)</code>","text":"<p>Method used by eeg_power_spectra and intrinsic_freq_autoCorr Signal in must be mean normalised already</p> Source code in <code>ephysiopy/common/rhythmicity.py</code> <pre><code>def power_spectrum(\n    self,\n    eeg,\n    plot=True,\n    binWidthSecs=None,\n    maxFreq=25,\n    pad2pow=None,\n    ymax=None,\n    **kwargs,\n):\n    \"\"\"\n    Method used by eeg_power_spectra and intrinsic_freq_autoCorr\n    Signal in must be mean normalised already\n    \"\"\"\n\n    # Get raw power spectrum\n    nqLim = 1 / binWidthSecs / 2.0\n    origLen = len(eeg)\n    # if pad2pow is None:\n    # \tfftLen = int(np.power(2, self._nextpow2(origLen)))\n    # else:\n    fftLen = int(np.power(2, pad2pow))\n    fftHalfLen = int(fftLen / float(2) + 1)\n\n    fftRes = np.fft.fft(eeg, fftLen)\n    # get power density from fft and discard second half of spectrum\n    _power = np.power(np.abs(fftRes), 2) / origLen\n    power = np.delete(_power, np.s_[fftHalfLen::])\n    power[1:-2] = power[1:-2] * 2\n\n    # calculate freqs and crop spectrum to requested range\n    freqs = nqLim * np.linspace(0, 1, fftHalfLen)\n    freqs = freqs[freqs &lt;= maxFreq].T\n    power = power[0: len(freqs)]\n\n    # smooth spectrum using gaussian kernel\n    binsPerHz = (fftHalfLen - 1) / nqLim\n    kernelLen = np.round(self.smthKernelWidth * binsPerHz)\n    kernelSig = self.smthKernelSigma * binsPerHz\n    from scipy import signal\n\n    k = signal.windows.gaussian(kernelLen, kernelSig) / (kernelLen / 2 / 2)\n    power_sm = signal.fftconvolve(power, k[::-1], mode=\"same\")\n\n    # calculate some metrics\n    # find max in theta band\n    spectrumMaskBand = np.logical_and(\n        freqs &gt; self.thetaRange[0], freqs &lt; self.thetaRange[1]\n    )\n    bandMaxPower = np.max(power_sm[spectrumMaskBand])\n    maxBinInBand = np.argmax(power_sm[spectrumMaskBand])\n    bandFreqs = freqs[spectrumMaskBand]\n    freqAtBandMaxPower = bandFreqs[maxBinInBand]\n    # self.maxBinInBand = maxBinInBand\n    # self.freqAtBandMaxPower = freqAtBandMaxPower\n    # self.bandMaxPower = bandMaxPower\n\n    # find power in small window around peak and divide by power in rest\n    # of spectrum to get snr\n    spectrumMaskPeak = np.logical_and(\n        freqs &gt; freqAtBandMaxPower - self.sn2Width / 2,\n        freqs &lt; freqAtBandMaxPower + self.sn2Width / 2,\n    )\n    s2n = np.nanmean(power_sm[spectrumMaskPeak]) / np.nanmean(\n        power_sm[~spectrumMaskPeak]\n    )\n    self.freqs = freqs\n    self.power_sm = power_sm\n    self.spectrumMaskPeak = spectrumMaskPeak\n    if plot:\n        fig = plt.figure()\n        ax = fig.add_subplot(111)\n        if ymax is None:\n            ymax = np.min([2 * np.max(power), np.max(power_sm)])\n            if ymax == 0:\n                ymax = 1\n        ax.plot(freqs, power, c=[0.9, 0.9, 0.9])\n        # ax.hold(True)\n        ax.plot(freqs, power_sm, \"k\", lw=2)\n        ax.axvline(self.thetaRange[0], c=\"b\", ls=\"--\")\n        ax.axvline(self.thetaRange[1], c=\"b\", ls=\"--\")\n        _, stemlines, _ = ax.stem([freqAtBandMaxPower], [\n                                  bandMaxPower], linefmt=\"r\")\n        # plt.setp(stemlines, 'linewidth', 2)\n        ax.fill_between(\n            freqs,\n            0,\n            power_sm,\n            where=spectrumMaskPeak,\n            color=\"r\",\n            alpha=0.25,\n            zorder=25,\n        )\n        # ax.set_ylim(0, ymax)\n        # ax.set_xlim(0, self.xmax)\n        ax.set_xlabel(\"Frequency (Hz)\")\n        ax.set_ylabel(\"Power density (W/Hz)\")\n    out_dict = {\n        \"maxFreq\": freqAtBandMaxPower,\n        \"Power\": power_sm,\n        \"Freqs\": freqs,\n        \"s2n\": s2n,\n        \"Power_raw\": power,\n        \"k\": k,\n        \"kernelLen\": kernelLen,\n        \"kernelSig\": kernelSig,\n        \"binsPerHz\": binsPerHz,\n        \"kernelLen\": kernelLen,\n    }\n    return out_dict\n</code></pre>"},{"location":"reference/#ephysiopy.common.rhythmicity.CosineDirectionalTuning.speedFilterRuns","title":"<code>speedFilterRuns(runs, minspeed=5.0)</code>","text":"<p>Given the runs identified in getRunsOfMinLength, filter for speed and return runs that meet the min speed criteria.</p> <p>The function goes over the runs with a moving window of length equal to self.min_runlength in samples and sees if any of those segments meet the speed criteria and splits them out into separate runs if true.</p> <p>NB For now this means the same spikes might get included in the autocorrelation procedure later as the moving window will use overlapping periods - can be modified later.</p> <p>Parameters:</p> Name Type Description Default <code>runs</code> <code>3 x nRuns np.array</code> <p>Generated from getRunsOfMinLength</p> required <code>minspeed</code> <code>float</code> <p>Min running speed in cm/s for an epoch (minimum               epoch length defined previously               in getRunsOfMinLength as minlength, usually 0.4s)</p> <code>5.0</code> <p>Returns:</p> Type Description <p>3 x nRuns np.array: A modified version of the \"runs\" input variable</p> Source code in <code>ephysiopy/common/rhythmicity.py</code> <pre><code>def speedFilterRuns(self, runs: np.array, minspeed=5.0):\n    \"\"\"\n    Given the runs identified in getRunsOfMinLength, filter for speed\n    and return runs that meet the min speed criteria.\n\n    The function goes over the runs with a moving window of length equal\n    to self.min_runlength in samples and sees if any of those segments\n    meet the speed criteria and splits them out into separate runs if true.\n\n    NB For now this means the same spikes might get included in the\n    autocorrelation procedure later as the\n    moving window will use overlapping periods - can be modified later.\n\n    Args:\n        runs (3 x nRuns np.array): Generated from getRunsOfMinLength\n        minspeed (float): Min running speed in cm/s for an epoch (minimum\n                          epoch length defined previously\n                          in getRunsOfMinLength as minlength, usually 0.4s)\n\n    Returns:\n        3 x nRuns np.array: A modified version of the \"runs\" input variable\n    \"\"\"\n    minlength_in_samples = int(self.pos_sample_rate * self.min_runlength)\n    run_list = runs.tolist()\n    all_speed = np.array(self.speed)\n    for start_idx, end_idx, dir_bin in run_list:\n        this_runs_speed = all_speed[start_idx:end_idx]\n        this_runs_runs = self._rolling_window(\n            this_runs_speed, minlength_in_samples)\n        run_mask = np.all(this_runs_runs &gt; minspeed, 1)\n        if np.any(run_mask):\n            print(\"got one\")\n</code></pre>"},{"location":"reference/#ephysiopy.common.rhythmicity.LFPOscillations","title":"<code>LFPOscillations</code>","text":"<p>               Bases: <code>object</code></p> <p>Does stuff with the LFP such as looking at nested oscillations (theta/ gamma coupling), the modulation index of such phenomena, filtering out certain frequencies in the LFP, getting the instantaneous phase and amplitude and so on</p> Source code in <code>ephysiopy/common/rhythmicity.py</code> <pre><code>class LFPOscillations(object):\n    \"\"\"\n    Does stuff with the LFP such as looking at nested oscillations\n    (theta/ gamma coupling), the modulation index of such phenomena,\n    filtering out certain frequencies in the LFP, getting the instantaneous\n    phase and amplitude and so on\n\n    \"\"\"\n\n    def __init__(self, sig, fs, **kwargs):\n        self.sig = sig\n        self.fs = fs\n\n    def getFreqPhase(self, sig, band2filter: list, ford=3):\n        \"\"\"\n        Uses the Hilbert transform to calculate the instantaneous phase and\n        amplitude of the time series in sig.\n\n        Args:\n            sig (np.array): The signal to be analysed\n            ford (int): The order for the Butterworth filter\n            band2filter (list): The two frequencies to be filtered for\n        \"\"\"\n        if sig is None:\n            sig = self.sig\n        band2filter = np.array(band2filter, dtype=float)\n\n        b, a = signal.butter(ford, band2filter /\n                             (self.fs / 2), btype=\"bandpass\")\n\n        filt_sig = signal.filtfilt(b, a, sig, padtype=\"odd\")\n        phase = np.angle(signal.hilbert(filt_sig))\n        amplitude = np.abs(signal.hilbert(filt_sig))\n        amplitude_filtered = signal.filtfilt(b, a, amplitude, padtype=\"odd\")\n        return filt_sig, phase, amplitude, amplitude_filtered\n\n    def modulationindex(\n        self,\n        sig=None,\n        nbins=20,\n        forder=2,\n        thetaband=[4, 8],\n        gammaband=[30, 80],\n        plot=True,\n    ):\n        \"\"\"\n        Calculates the modulation index of theta and gamma oscillations.\n        Specifically this is the circular correlation between the phase of\n        theta and the power of theta.\n\n        Args:\n            sig (np.array): The LFP signal\n            nbins (int): The number of bins in the circular range 0 to 2*pi\n            forder (int): The order of the butterworth filter\n            thetaband (list): The lower/upper bands of the theta freq range\n            gammaband (list): The lower/upper bands of the gamma freq range\n            plot (bool): Show some pics or not\n        \"\"\"\n        if sig is None:\n            sig = self.sig\n        sig = sig - np.ma.mean(sig)\n        if np.ma.is_masked(sig):\n            sig = np.ma.compressed(sig)\n        _, lowphase, _, _ = self.getFreqPhase(sig, thetaband, forder)\n        _, _, highamp, _ = self.getFreqPhase(sig, gammaband, forder)\n        inc = 2 * np.pi / nbins\n        a = np.arange(-np.pi + inc / 2, np.pi, inc)\n        dt = np.array([-inc / 2, inc / 2])\n        pbins = a[:, np.newaxis] + dt[np.newaxis, :]\n        amp = np.zeros((nbins))\n        phaselen = np.arange(len(lowphase))\n        for i in range(nbins):\n            pts = np.nonzero(\n                (lowphase &gt;= pbins[i, 0]) * (lowphase &lt; pbins[i, 1]) * phaselen\n            )\n            amp[i] = np.mean(highamp[pts])\n        amp = amp / np.sum(amp)\n        from ephysiopy.common.statscalcs import circ_r\n\n        mi = circ_r(pbins[:, 1], amp)\n        if plot:\n            fig = plt.figure()\n            ax = fig.add_subplot(111, polar=True)\n            w = np.pi / (nbins / 2)\n            ax.bar(pbins[:, 1], amp, width=w)\n            ax.set_title(\"Modulation index={0:.5f}\".format(mi))\n        return mi\n\n    def plv(\n        self,\n        sig=None,\n        forder=2,\n        thetaband=[4, 8],\n        gammaband=[30, 80],\n        plot=True,\n        **kwargs,\n    ):\n        \"\"\"\n        Computes the phase-amplitude coupling (PAC) of nested oscillations.\n        More specifically this is the phase-locking value (PLV) between two\n        nested oscillations in EEG data, in this case theta (default 4-8Hz)\n        and gamma (defaults to 30-80Hz). A PLV of unity indicates perfect phase\n        locking (here PAC) and a value of zero indicates no locking (no PAC)\n\n        Args:\n            eeg (numpy array): The eeg data itself. This is a 1-d array which\n            can be masked or not\n            forder (int): The order of the filter(s) applied to the eeg data\n            thetaband, gammaband (list/array): The range of values to bandpass\n            filter for for the theta and gamma ranges\n            plot (bool, optional): Whether to plot the resulting binned up\n            polar plot which shows the amplitude of the gamma oscillation\n            found at different phases of the theta oscillation.\n            Default is True.\n\n        Returns:\n            plv (float): The value of the phase-amplitude coupling\n        \"\"\"\n\n        if sig is None:\n            sig = self.sig\n        sig = sig - np.ma.mean(sig)\n        if np.ma.is_masked(sig):\n            sig = np.ma.compressed(sig)\n\n        _, lowphase, _, _ = self.getFreqPhase(sig, thetaband, forder)\n        _, _, _, highamp_f = self.getFreqPhase(sig, gammaband, forder)\n\n        highampphase = np.angle(signal.hilbert(highamp_f))\n        phasedf = highampphase - lowphase\n        phasedf = np.exp(1j * phasedf)\n        phasedf = np.angle(phasedf)\n        from ephysiopy.common.statscalcs import circ_r\n\n        plv = circ_r(phasedf)\n        th = np.linspace(0.0, 2 * np.pi, 20, endpoint=False)\n        h, _ = np.histogram(phasedf, bins=20)\n        h = h / float(len(phasedf))\n\n        if plot:\n            fig = plt.figure()\n            ax = fig.add_subplot(111, polar=True)\n            w = np.pi / 10\n            ax.bar(th, h, width=w, bottom=0.0)\n        return plv, th, h\n\n    def filterForLaser(self, sig=None, width=0.125, dip=15.0, stimFreq=6.66):\n        \"\"\"\n        Attempts to filter out frequencies from optogenetic experiments where\n        the frequency of laser stimulation was at 6.66Hz.\n\n        Note:\n            This method may not work as expected for each trial and might\n            require tailoring. A potential improvement could be using mean\n            power or a similar metric.\n        \"\"\"\n        from scipy.signal import filtfilt, firwin, kaiserord\n\n        nyq = self.fs / 2.0\n        width = width / nyq\n        dip = dip\n        N, beta = kaiserord(dip, width)\n        print(\"N: {0}\\nbeta: {1}\".format(N, beta))\n        upper = np.ceil(nyq / stimFreq)\n        c = np.arange(stimFreq, upper * stimFreq, stimFreq)\n        dt = np.array([-0.125, 0.125])\n        cutoff_hz = dt[:, np.newaxis] + c[np.newaxis, :]\n        cutoff_hz = cutoff_hz.ravel()\n        cutoff_hz = np.append(cutoff_hz, nyq - 1)\n        cutoff_hz.sort()\n        cutoff_hz_nyq = cutoff_hz / nyq\n        taps = firwin(N, cutoff_hz_nyq, window=(\"kaiser\", beta))\n        if sig is None:\n            sig = self.sig\n        fx = filtfilt(taps, [1.0], sig)\n        return fx\n\n    def spike_phase_plot(self, cluster: int,\n                         pos_data: PosCalcsGeneric,\n                         KSdata: KiloSortSession,\n                         lfp_data: EEGCalcsGeneric) -&gt; None:\n        '''\n        Produces a plot of the phase of theta at which each spike was\n        emitted. Each spike is plotted according to the x-y location the\n        animal was in when it was fired and the colour of the marker \n        corresponds to the phase of theta at which it fired.\n        '''\n        _, phase, _, _ = self.getFreqPhase(\n            lfp_data.sig, [6, 12])\n        cluster_times = KSdata.spk_times[KSdata.spk_clusters == cluster]\n        # cluster_times in samples (@30000Hz)\n        # get indices into the phase vector\n        phase_idx = np.array(cluster_times/(3e4/self.fs), dtype=int)\n        # It's possible that there are indices higher than the length of\n        # the phase vector so lets set them to the last index\n        bad_idx = np.nonzero(phase_idx &gt; len(phase))[0]\n        phase_idx[bad_idx] = len(phase) - 1\n        # get indices into the position data\n        pos_idx = np.array(cluster_times/(3e4/pos_data.sample_rate), dtype=int)\n        bad_idx = np.nonzero(pos_idx &gt;= len(pos_data.xyTS))[0]\n        pos_idx[bad_idx] = len(pos_data.xyTS) - 1\n</code></pre>"},{"location":"reference/#ephysiopy.common.rhythmicity.LFPOscillations.filterForLaser","title":"<code>filterForLaser(sig=None, width=0.125, dip=15.0, stimFreq=6.66)</code>","text":"<p>Attempts to filter out frequencies from optogenetic experiments where the frequency of laser stimulation was at 6.66Hz.</p> Note <p>This method may not work as expected for each trial and might require tailoring. A potential improvement could be using mean power or a similar metric.</p> Source code in <code>ephysiopy/common/rhythmicity.py</code> <pre><code>def filterForLaser(self, sig=None, width=0.125, dip=15.0, stimFreq=6.66):\n    \"\"\"\n    Attempts to filter out frequencies from optogenetic experiments where\n    the frequency of laser stimulation was at 6.66Hz.\n\n    Note:\n        This method may not work as expected for each trial and might\n        require tailoring. A potential improvement could be using mean\n        power or a similar metric.\n    \"\"\"\n    from scipy.signal import filtfilt, firwin, kaiserord\n\n    nyq = self.fs / 2.0\n    width = width / nyq\n    dip = dip\n    N, beta = kaiserord(dip, width)\n    print(\"N: {0}\\nbeta: {1}\".format(N, beta))\n    upper = np.ceil(nyq / stimFreq)\n    c = np.arange(stimFreq, upper * stimFreq, stimFreq)\n    dt = np.array([-0.125, 0.125])\n    cutoff_hz = dt[:, np.newaxis] + c[np.newaxis, :]\n    cutoff_hz = cutoff_hz.ravel()\n    cutoff_hz = np.append(cutoff_hz, nyq - 1)\n    cutoff_hz.sort()\n    cutoff_hz_nyq = cutoff_hz / nyq\n    taps = firwin(N, cutoff_hz_nyq, window=(\"kaiser\", beta))\n    if sig is None:\n        sig = self.sig\n    fx = filtfilt(taps, [1.0], sig)\n    return fx\n</code></pre>"},{"location":"reference/#ephysiopy.common.rhythmicity.LFPOscillations.getFreqPhase","title":"<code>getFreqPhase(sig, band2filter, ford=3)</code>","text":"<p>Uses the Hilbert transform to calculate the instantaneous phase and amplitude of the time series in sig.</p> <p>Parameters:</p> Name Type Description Default <code>sig</code> <code>array</code> <p>The signal to be analysed</p> required <code>ford</code> <code>int</code> <p>The order for the Butterworth filter</p> <code>3</code> <code>band2filter</code> <code>list</code> <p>The two frequencies to be filtered for</p> required Source code in <code>ephysiopy/common/rhythmicity.py</code> <pre><code>def getFreqPhase(self, sig, band2filter: list, ford=3):\n    \"\"\"\n    Uses the Hilbert transform to calculate the instantaneous phase and\n    amplitude of the time series in sig.\n\n    Args:\n        sig (np.array): The signal to be analysed\n        ford (int): The order for the Butterworth filter\n        band2filter (list): The two frequencies to be filtered for\n    \"\"\"\n    if sig is None:\n        sig = self.sig\n    band2filter = np.array(band2filter, dtype=float)\n\n    b, a = signal.butter(ford, band2filter /\n                         (self.fs / 2), btype=\"bandpass\")\n\n    filt_sig = signal.filtfilt(b, a, sig, padtype=\"odd\")\n    phase = np.angle(signal.hilbert(filt_sig))\n    amplitude = np.abs(signal.hilbert(filt_sig))\n    amplitude_filtered = signal.filtfilt(b, a, amplitude, padtype=\"odd\")\n    return filt_sig, phase, amplitude, amplitude_filtered\n</code></pre>"},{"location":"reference/#ephysiopy.common.rhythmicity.LFPOscillations.modulationindex","title":"<code>modulationindex(sig=None, nbins=20, forder=2, thetaband=[4, 8], gammaband=[30, 80], plot=True)</code>","text":"<p>Calculates the modulation index of theta and gamma oscillations. Specifically this is the circular correlation between the phase of theta and the power of theta.</p> <p>Parameters:</p> Name Type Description Default <code>sig</code> <code>array</code> <p>The LFP signal</p> <code>None</code> <code>nbins</code> <code>int</code> <p>The number of bins in the circular range 0 to 2*pi</p> <code>20</code> <code>forder</code> <code>int</code> <p>The order of the butterworth filter</p> <code>2</code> <code>thetaband</code> <code>list</code> <p>The lower/upper bands of the theta freq range</p> <code>[4, 8]</code> <code>gammaband</code> <code>list</code> <p>The lower/upper bands of the gamma freq range</p> <code>[30, 80]</code> <code>plot</code> <code>bool</code> <p>Show some pics or not</p> <code>True</code> Source code in <code>ephysiopy/common/rhythmicity.py</code> <pre><code>def modulationindex(\n    self,\n    sig=None,\n    nbins=20,\n    forder=2,\n    thetaband=[4, 8],\n    gammaband=[30, 80],\n    plot=True,\n):\n    \"\"\"\n    Calculates the modulation index of theta and gamma oscillations.\n    Specifically this is the circular correlation between the phase of\n    theta and the power of theta.\n\n    Args:\n        sig (np.array): The LFP signal\n        nbins (int): The number of bins in the circular range 0 to 2*pi\n        forder (int): The order of the butterworth filter\n        thetaband (list): The lower/upper bands of the theta freq range\n        gammaband (list): The lower/upper bands of the gamma freq range\n        plot (bool): Show some pics or not\n    \"\"\"\n    if sig is None:\n        sig = self.sig\n    sig = sig - np.ma.mean(sig)\n    if np.ma.is_masked(sig):\n        sig = np.ma.compressed(sig)\n    _, lowphase, _, _ = self.getFreqPhase(sig, thetaband, forder)\n    _, _, highamp, _ = self.getFreqPhase(sig, gammaband, forder)\n    inc = 2 * np.pi / nbins\n    a = np.arange(-np.pi + inc / 2, np.pi, inc)\n    dt = np.array([-inc / 2, inc / 2])\n    pbins = a[:, np.newaxis] + dt[np.newaxis, :]\n    amp = np.zeros((nbins))\n    phaselen = np.arange(len(lowphase))\n    for i in range(nbins):\n        pts = np.nonzero(\n            (lowphase &gt;= pbins[i, 0]) * (lowphase &lt; pbins[i, 1]) * phaselen\n        )\n        amp[i] = np.mean(highamp[pts])\n    amp = amp / np.sum(amp)\n    from ephysiopy.common.statscalcs import circ_r\n\n    mi = circ_r(pbins[:, 1], amp)\n    if plot:\n        fig = plt.figure()\n        ax = fig.add_subplot(111, polar=True)\n        w = np.pi / (nbins / 2)\n        ax.bar(pbins[:, 1], amp, width=w)\n        ax.set_title(\"Modulation index={0:.5f}\".format(mi))\n    return mi\n</code></pre>"},{"location":"reference/#ephysiopy.common.rhythmicity.LFPOscillations.plv","title":"<code>plv(sig=None, forder=2, thetaband=[4, 8], gammaband=[30, 80], plot=True, **kwargs)</code>","text":"<p>Computes the phase-amplitude coupling (PAC) of nested oscillations. More specifically this is the phase-locking value (PLV) between two nested oscillations in EEG data, in this case theta (default 4-8Hz) and gamma (defaults to 30-80Hz). A PLV of unity indicates perfect phase locking (here PAC) and a value of zero indicates no locking (no PAC)</p> <p>Parameters:</p> Name Type Description Default <code>eeg</code> <code>numpy array</code> <p>The eeg data itself. This is a 1-d array which</p> required <code>forder</code> <code>int</code> <p>The order of the filter(s) applied to the eeg data</p> <code>2</code> <code>thetaband,</code> <code>gammaband (list/array</code> <p>The range of values to bandpass</p> required <code>plot</code> <code>bool</code> <p>Whether to plot the resulting binned up</p> <code>True</code> <p>Returns:</p> Name Type Description <code>plv</code> <code>float</code> <p>The value of the phase-amplitude coupling</p> Source code in <code>ephysiopy/common/rhythmicity.py</code> <pre><code>def plv(\n    self,\n    sig=None,\n    forder=2,\n    thetaband=[4, 8],\n    gammaband=[30, 80],\n    plot=True,\n    **kwargs,\n):\n    \"\"\"\n    Computes the phase-amplitude coupling (PAC) of nested oscillations.\n    More specifically this is the phase-locking value (PLV) between two\n    nested oscillations in EEG data, in this case theta (default 4-8Hz)\n    and gamma (defaults to 30-80Hz). A PLV of unity indicates perfect phase\n    locking (here PAC) and a value of zero indicates no locking (no PAC)\n\n    Args:\n        eeg (numpy array): The eeg data itself. This is a 1-d array which\n        can be masked or not\n        forder (int): The order of the filter(s) applied to the eeg data\n        thetaband, gammaband (list/array): The range of values to bandpass\n        filter for for the theta and gamma ranges\n        plot (bool, optional): Whether to plot the resulting binned up\n        polar plot which shows the amplitude of the gamma oscillation\n        found at different phases of the theta oscillation.\n        Default is True.\n\n    Returns:\n        plv (float): The value of the phase-amplitude coupling\n    \"\"\"\n\n    if sig is None:\n        sig = self.sig\n    sig = sig - np.ma.mean(sig)\n    if np.ma.is_masked(sig):\n        sig = np.ma.compressed(sig)\n\n    _, lowphase, _, _ = self.getFreqPhase(sig, thetaband, forder)\n    _, _, _, highamp_f = self.getFreqPhase(sig, gammaband, forder)\n\n    highampphase = np.angle(signal.hilbert(highamp_f))\n    phasedf = highampphase - lowphase\n    phasedf = np.exp(1j * phasedf)\n    phasedf = np.angle(phasedf)\n    from ephysiopy.common.statscalcs import circ_r\n\n    plv = circ_r(phasedf)\n    th = np.linspace(0.0, 2 * np.pi, 20, endpoint=False)\n    h, _ = np.histogram(phasedf, bins=20)\n    h = h / float(len(phasedf))\n\n    if plot:\n        fig = plt.figure()\n        ax = fig.add_subplot(111, polar=True)\n        w = np.pi / 10\n        ax.bar(th, h, width=w, bottom=0.0)\n    return plv, th, h\n</code></pre>"},{"location":"reference/#ephysiopy.common.rhythmicity.LFPOscillations.spike_phase_plot","title":"<code>spike_phase_plot(cluster, pos_data, KSdata, lfp_data)</code>","text":"<p>Produces a plot of the phase of theta at which each spike was emitted. Each spike is plotted according to the x-y location the animal was in when it was fired and the colour of the marker  corresponds to the phase of theta at which it fired.</p> Source code in <code>ephysiopy/common/rhythmicity.py</code> <pre><code>def spike_phase_plot(self, cluster: int,\n                     pos_data: PosCalcsGeneric,\n                     KSdata: KiloSortSession,\n                     lfp_data: EEGCalcsGeneric) -&gt; None:\n    '''\n    Produces a plot of the phase of theta at which each spike was\n    emitted. Each spike is plotted according to the x-y location the\n    animal was in when it was fired and the colour of the marker \n    corresponds to the phase of theta at which it fired.\n    '''\n    _, phase, _, _ = self.getFreqPhase(\n        lfp_data.sig, [6, 12])\n    cluster_times = KSdata.spk_times[KSdata.spk_clusters == cluster]\n    # cluster_times in samples (@30000Hz)\n    # get indices into the phase vector\n    phase_idx = np.array(cluster_times/(3e4/self.fs), dtype=int)\n    # It's possible that there are indices higher than the length of\n    # the phase vector so lets set them to the last index\n    bad_idx = np.nonzero(phase_idx &gt; len(phase))[0]\n    phase_idx[bad_idx] = len(phase) - 1\n    # get indices into the position data\n    pos_idx = np.array(cluster_times/(3e4/pos_data.sample_rate), dtype=int)\n    bad_idx = np.nonzero(pos_idx &gt;= len(pos_data.xyTS))[0]\n    pos_idx[bad_idx] = len(pos_data.xyTS) - 1\n</code></pre>"},{"location":"reference/#spike-calculations","title":"Spike calculations","text":""},{"location":"reference/#ephysiopy.common.spikecalcs.SpikeCalcsAxona","title":"<code>SpikeCalcsAxona</code>","text":"<p>               Bases: <code>SpikeCalcsGeneric</code></p> <p>Replaces SpikeCalcs from ephysiopy.axona.spikecalcs</p> Source code in <code>ephysiopy/common/spikecalcs.py</code> <pre><code>class SpikeCalcsAxona(SpikeCalcsGeneric):\n    \"\"\"\n    Replaces SpikeCalcs from ephysiopy.axona.spikecalcs\n    \"\"\"\n\n    def half_amp_dur(self, waveforms):\n        \"\"\"\n        Calculates the half amplitude duration of a spike.\n\n        Args:\n            A (ndarray): An nSpikes x nElectrodes x nSamples array.\n\n        Returns:\n            had (float): The half-amplitude duration for the channel\n                (electrode) that has the strongest (highest amplitude)\n                signal. Units are ms.\n        \"\"\"\n        from scipy import optimize\n\n        best_chan = np.argmax(np.max(np.mean(waveforms, 0), 1))\n        mn_wvs = np.mean(waveforms, 0)\n        wvs = mn_wvs[best_chan, :]\n        half_amp = np.max(wvs) / 2\n        half_amp = np.zeros_like(wvs) + half_amp\n        t = np.linspace(0, 1 / 1000.0, 50)\n        # create functions from the data using PiecewisePolynomial\n        from scipy.interpolate import BPoly\n\n        p1 = BPoly.from_derivatives(t, wvs[:, np.newaxis])\n        p2 = BPoly.from_derivatives(t, half_amp[:, np.newaxis])\n        xs = np.r_[t, t]\n        xs.sort()\n        x_min = xs.min()\n        x_max = xs.max()\n        x_mid = xs[:-1] + np.diff(xs) / 2\n        roots = set()\n        for val in x_mid:\n            root, infodict, ier, mesg = optimize.fsolve(\n                lambda x: p1(x) - p2(x), val, full_output=True\n            )\n            if ier == 1 and x_min &lt; root &lt; x_max:\n                roots.add(root[0])\n        roots = list(roots)\n        if len(roots) &gt; 1:\n            r = np.abs(np.diff(roots[0:2]))[0]\n        else:\n            r = np.nan\n        return r\n\n    def p2t_time(self, waveforms):\n        \"\"\"\n        The peak to trough time of a spike in ms\n\n        Args:\n            cluster (int): The cluster whose waveforms are to be analysed\n\n        Returns:\n            p2t (float): The mean peak-to-trough time for the channel\n                (electrode) that has the strongest (highest amplitude) signal.\n                Units are ms.\n        \"\"\"\n        best_chan = np.argmax(np.max(np.mean(waveforms, 0), 1))\n        tP = get_param(waveforms, param=\"tP\")\n        tT = get_param(waveforms, param=\"tT\")\n        mn_tP = np.mean(tP, 0)\n        mn_tT = np.mean(tT, 0)\n        p2t = np.abs(mn_tP[best_chan] - mn_tT[best_chan])\n        return p2t * 1000\n\n    def plotClusterSpace(self, waveforms, param=\"Amp\", clusts=None, bins=256, **kwargs):\n        \"\"\"\n        Assumes the waveform data is signed 8-bit ints\n        TODO: aspect of plot boxes in ImageGrid not right as scaled by range of\n        values now\n        \"\"\"\n        from itertools import combinations\n\n        import matplotlib.colors as colors\n        from mpl_toolkits.axes_grid1 import ImageGrid\n\n        from ephysiopy.axona.tintcolours import colours as tcols\n\n        self.scaling = np.full(4, 15)\n\n        amps = get_param(waveforms, param=param)\n        cmap = np.tile(tcols[0], (bins, 1))\n        cmap[0] = (1, 1, 1)\n        cmap = colors.ListedColormap(cmap)\n        cmap._init()\n        alpha_vals = np.ones(cmap.N + 3)\n        alpha_vals[0] = 0\n        cmap._lut[:, -1] = alpha_vals\n        cmb = combinations(range(4), 2)\n        if \"fig\" in kwargs:\n            fig = kwargs[\"fig\"]\n        else:\n            fig = plt.figure(figsize=(8, 6))\n        grid = ImageGrid(fig, 111, nrows_ncols=(2, 3), axes_pad=0.1, aspect=False)\n        clustCMap0 = np.tile(tcols[0], (bins, 1))\n        clustCMap0[0] = (1, 1, 1)\n        clustCMap0 = colors.ListedColormap(clustCMap0)\n        clustCMap0._init()\n        clustCMap0._lut[:, -1] = alpha_vals\n        for i, c in enumerate(cmb):\n            h, ye, xe = np.histogram2d(\n                amps[:, c[0]],\n                amps[:, c[1]],\n                range=((-128, 127), (-128, 127)),\n                bins=bins,\n            )\n            x, y = np.meshgrid(xe[0:-1], ye[0:-1])\n            grid[i].pcolormesh(\n                x, y, h, cmap=clustCMap0, shading=\"nearest\", edgecolors=\"face\"\n            )\n            h, ye, xe = np.histogram2d(\n                amps[:, c[0]],\n                amps[:, c[1]],\n                range=((-128, 127), (-128, 127)),\n                bins=bins,\n            )\n            clustCMap = np.tile(tcols[1], (bins, 1))\n            clustCMap[0] = (1, 1, 1)\n            clustCMap = colors.ListedColormap(clustCMap)\n            clustCMap._init()\n            clustCMap._lut[:, -1] = alpha_vals\n            grid[i].pcolormesh(\n                x, y, h, cmap=clustCMap, shading=\"nearest\", edgecolors=\"face\"\n            )\n            s = str(c[0] + 1) + \" v \" + str(c[1] + 1)\n            grid[i].text(\n                0.05,\n                0.95,\n                s,\n                va=\"top\",\n                ha=\"left\",\n                size=\"small\",\n                color=\"k\",\n                transform=grid[i].transAxes,\n            )\n            grid[i].set_xlim(xe.min(), xe.max())\n            grid[i].set_ylim(ye.min(), ye.max())\n        plt.setp([a.get_xticklabels() for a in grid], visible=False)\n        plt.setp([a.get_yticklabels() for a in grid], visible=False)\n        return fig\n</code></pre>"},{"location":"reference/#ephysiopy.common.spikecalcs.SpikeCalcsAxona.half_amp_dur","title":"<code>half_amp_dur(waveforms)</code>","text":"<p>Calculates the half amplitude duration of a spike.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>ndarray</code> <p>An nSpikes x nElectrodes x nSamples array.</p> required <p>Returns:</p> Name Type Description <code>had</code> <code>float</code> <p>The half-amplitude duration for the channel (electrode) that has the strongest (highest amplitude) signal. Units are ms.</p> Source code in <code>ephysiopy/common/spikecalcs.py</code> <pre><code>def half_amp_dur(self, waveforms):\n    \"\"\"\n    Calculates the half amplitude duration of a spike.\n\n    Args:\n        A (ndarray): An nSpikes x nElectrodes x nSamples array.\n\n    Returns:\n        had (float): The half-amplitude duration for the channel\n            (electrode) that has the strongest (highest amplitude)\n            signal. Units are ms.\n    \"\"\"\n    from scipy import optimize\n\n    best_chan = np.argmax(np.max(np.mean(waveforms, 0), 1))\n    mn_wvs = np.mean(waveforms, 0)\n    wvs = mn_wvs[best_chan, :]\n    half_amp = np.max(wvs) / 2\n    half_amp = np.zeros_like(wvs) + half_amp\n    t = np.linspace(0, 1 / 1000.0, 50)\n    # create functions from the data using PiecewisePolynomial\n    from scipy.interpolate import BPoly\n\n    p1 = BPoly.from_derivatives(t, wvs[:, np.newaxis])\n    p2 = BPoly.from_derivatives(t, half_amp[:, np.newaxis])\n    xs = np.r_[t, t]\n    xs.sort()\n    x_min = xs.min()\n    x_max = xs.max()\n    x_mid = xs[:-1] + np.diff(xs) / 2\n    roots = set()\n    for val in x_mid:\n        root, infodict, ier, mesg = optimize.fsolve(\n            lambda x: p1(x) - p2(x), val, full_output=True\n        )\n        if ier == 1 and x_min &lt; root &lt; x_max:\n            roots.add(root[0])\n    roots = list(roots)\n    if len(roots) &gt; 1:\n        r = np.abs(np.diff(roots[0:2]))[0]\n    else:\n        r = np.nan\n    return r\n</code></pre>"},{"location":"reference/#ephysiopy.common.spikecalcs.SpikeCalcsAxona.p2t_time","title":"<code>p2t_time(waveforms)</code>","text":"<p>The peak to trough time of a spike in ms</p> <p>Parameters:</p> Name Type Description Default <code>cluster</code> <code>int</code> <p>The cluster whose waveforms are to be analysed</p> required <p>Returns:</p> Name Type Description <code>p2t</code> <code>float</code> <p>The mean peak-to-trough time for the channel (electrode) that has the strongest (highest amplitude) signal. Units are ms.</p> Source code in <code>ephysiopy/common/spikecalcs.py</code> <pre><code>def p2t_time(self, waveforms):\n    \"\"\"\n    The peak to trough time of a spike in ms\n\n    Args:\n        cluster (int): The cluster whose waveforms are to be analysed\n\n    Returns:\n        p2t (float): The mean peak-to-trough time for the channel\n            (electrode) that has the strongest (highest amplitude) signal.\n            Units are ms.\n    \"\"\"\n    best_chan = np.argmax(np.max(np.mean(waveforms, 0), 1))\n    tP = get_param(waveforms, param=\"tP\")\n    tT = get_param(waveforms, param=\"tT\")\n    mn_tP = np.mean(tP, 0)\n    mn_tT = np.mean(tT, 0)\n    p2t = np.abs(mn_tP[best_chan] - mn_tT[best_chan])\n    return p2t * 1000\n</code></pre>"},{"location":"reference/#ephysiopy.common.spikecalcs.SpikeCalcsAxona.plotClusterSpace","title":"<code>plotClusterSpace(waveforms, param='Amp', clusts=None, bins=256, **kwargs)</code>","text":"<p>Assumes the waveform data is signed 8-bit ints TODO: aspect of plot boxes in ImageGrid not right as scaled by range of values now</p> Source code in <code>ephysiopy/common/spikecalcs.py</code> <pre><code>def plotClusterSpace(self, waveforms, param=\"Amp\", clusts=None, bins=256, **kwargs):\n    \"\"\"\n    Assumes the waveform data is signed 8-bit ints\n    TODO: aspect of plot boxes in ImageGrid not right as scaled by range of\n    values now\n    \"\"\"\n    from itertools import combinations\n\n    import matplotlib.colors as colors\n    from mpl_toolkits.axes_grid1 import ImageGrid\n\n    from ephysiopy.axona.tintcolours import colours as tcols\n\n    self.scaling = np.full(4, 15)\n\n    amps = get_param(waveforms, param=param)\n    cmap = np.tile(tcols[0], (bins, 1))\n    cmap[0] = (1, 1, 1)\n    cmap = colors.ListedColormap(cmap)\n    cmap._init()\n    alpha_vals = np.ones(cmap.N + 3)\n    alpha_vals[0] = 0\n    cmap._lut[:, -1] = alpha_vals\n    cmb = combinations(range(4), 2)\n    if \"fig\" in kwargs:\n        fig = kwargs[\"fig\"]\n    else:\n        fig = plt.figure(figsize=(8, 6))\n    grid = ImageGrid(fig, 111, nrows_ncols=(2, 3), axes_pad=0.1, aspect=False)\n    clustCMap0 = np.tile(tcols[0], (bins, 1))\n    clustCMap0[0] = (1, 1, 1)\n    clustCMap0 = colors.ListedColormap(clustCMap0)\n    clustCMap0._init()\n    clustCMap0._lut[:, -1] = alpha_vals\n    for i, c in enumerate(cmb):\n        h, ye, xe = np.histogram2d(\n            amps[:, c[0]],\n            amps[:, c[1]],\n            range=((-128, 127), (-128, 127)),\n            bins=bins,\n        )\n        x, y = np.meshgrid(xe[0:-1], ye[0:-1])\n        grid[i].pcolormesh(\n            x, y, h, cmap=clustCMap0, shading=\"nearest\", edgecolors=\"face\"\n        )\n        h, ye, xe = np.histogram2d(\n            amps[:, c[0]],\n            amps[:, c[1]],\n            range=((-128, 127), (-128, 127)),\n            bins=bins,\n        )\n        clustCMap = np.tile(tcols[1], (bins, 1))\n        clustCMap[0] = (1, 1, 1)\n        clustCMap = colors.ListedColormap(clustCMap)\n        clustCMap._init()\n        clustCMap._lut[:, -1] = alpha_vals\n        grid[i].pcolormesh(\n            x, y, h, cmap=clustCMap, shading=\"nearest\", edgecolors=\"face\"\n        )\n        s = str(c[0] + 1) + \" v \" + str(c[1] + 1)\n        grid[i].text(\n            0.05,\n            0.95,\n            s,\n            va=\"top\",\n            ha=\"left\",\n            size=\"small\",\n            color=\"k\",\n            transform=grid[i].transAxes,\n        )\n        grid[i].set_xlim(xe.min(), xe.max())\n        grid[i].set_ylim(ye.min(), ye.max())\n    plt.setp([a.get_xticklabels() for a in grid], visible=False)\n    plt.setp([a.get_yticklabels() for a in grid], visible=False)\n    return fig\n</code></pre>"},{"location":"reference/#ephysiopy.common.spikecalcs.SpikeCalcsGeneric","title":"<code>SpikeCalcsGeneric</code>","text":"<p>               Bases: <code>object</code></p> <p>Deals with the processing and analysis of spike data. There should be one instance of this class per cluster in the recording session. NB this differs from previous versions of this class where there was one instance per recording session and clusters were selected by passing in the cluster id to the methods.</p> <p>Parameters:</p> Name Type Description Default <code>spike_times</code> <code>array_like</code> <p>The times of spikes in the trial in seconds</p> required <code>waveforms</code> <code>array</code> <p>An nSpikes x nChannels x nSamples array</p> <code>None</code> Source code in <code>ephysiopy/common/spikecalcs.py</code> <pre><code>class SpikeCalcsGeneric(object):\n    \"\"\"\n    Deals with the processing and analysis of spike data.\n    There should be one instance of this class per cluster in the\n    recording session. NB this differs from previous versions of this\n    class where there was one instance per recording session and clusters\n    were selected by passing in the cluster id to the methods.\n\n    Args:\n        spike_times (array_like): The times of spikes in the trial in seconds\n        waveforms (np.array, optional): An nSpikes x nChannels x nSamples array\n\n    \"\"\"\n\n    def __init__(\n        self,\n        spike_times: np.ndarray,\n        cluster: int,\n        waveforms: np.ndarray = None,\n        **kwargs\n    ):\n        self.spike_times = spike_times  # IN SECONDS\n        self._waves = waveforms\n        self.cluster = cluster\n        self._event_ts = None  # the times that events occured IN SECONDS\n        # window, in seconds, either side of the stimulus, to examine\n        self._event_window = np.array((-0.050, 0.100))\n        self._stim_width = None  # the width, in ms, of the stimulus\n        # used to increase / decrease size of bins in psth\n        self._secs_per_bin = 0.001\n        self._sample_rate = 30000\n        self._duration = None\n        # these values should be specific to OE data\n        self._pre_spike_samples = 16\n        self._post_spike_samples = 34\n        # values from running KS\n        self._ksmeta = KSMetaTuple(None, None, None, None)\n        # update the __dict__ attribute with the kwargs\n        self.__dict__.update(kwargs)\n\n    @property\n    def sample_rate(self):\n        return self._sample_rate\n\n    @sample_rate.setter\n    def sample_rate(self, value):\n        self._sample_rate = value\n\n    @property\n    def pre_spike_samples(self):\n        return self._pre_spike_samples\n\n    @pre_spike_samples.setter\n    def pre_spike_samples(self, value):\n        self._pre_spike_samples = int(self._pre_spike_samples)\n\n    @property\n    def post_spike_samples(self):\n        return self._post_spike_samples\n\n    @post_spike_samples.setter\n    def post_spike_samples(self, value):\n        self._post_spike_samples = int(self._post_spike_samples)\n\n    def waveforms(self, channel_id: Sequence = None):\n        if self._waves is not None:\n            if channel_id is None:\n                return self._waves[:, :, :]\n            else:\n                if isinstance(channel_id, int):\n                    channel_id = [channel_id]\n                return self._waves[:, channel_id, :]\n        else:\n            return None\n\n    @property\n    def n_spikes(self):\n        \"\"\"\n        Returns the number of spikes in the cluster\n\n        Returns:\n            int: The number of spikes in the cluster\n        \"\"\"\n        return len(self.spike_times)\n\n    @property\n    def event_ts(self):\n        return self._event_ts\n\n    @event_ts.setter\n    def event_ts(self, value):\n        self._event_ts = value\n\n    @property\n    def duration(self):\n        return self._duration\n\n    @duration.setter\n    def duration(self, value):\n        self._duration = value\n\n    @property\n    def KSMeta(self):\n        return self._ksmeta\n\n    def update_KSMeta(self, value: dict):\n        \"\"\"\n        Takes in a TemplateModel instance from a phy session and\n        parses out the relevant metrics for the cluster and places\n        into the namedtuple KSMeta\n        \"\"\"\n        metavals = []\n        for f in KSMetaTuple._fields:\n            if f in value.keys():\n                metavals.append(value[f][self.cluster])\n            else:\n                metavals.append(None)\n        self._ksmeta = KSMetaTuple(*metavals)\n\n    @property\n    def event_window(self):\n        return self._event_window\n\n    @event_window.setter\n    def event_window(self, value):\n        self._event_window = value\n\n    @property\n    def stim_width(self):\n        return self._stim_width\n\n    @stim_width.setter\n    def stim_width(self, value):\n        self._stim_width = value\n\n    @property\n    def secs_per_bin(self):\n        return self._secs_per_bin\n\n    @secs_per_bin.setter\n    def secs_per_bin(self, value):\n        self._secs_per_bin = value\n\n    def acorr(self, Trange: np.ndarray = None) -&gt; tuple:\n        \"\"\"\n        Calculates the autocorrelogram of a spike train\n\n        Args:\n            ts (np.ndarray): The spike times\n            Trange (np.ndarray): The range of times to calculate the\n                autocorrelogram over\n\n        Returns:\n            counts (np.ndarray): The autocorrelogram\n            bins (np.ndarray): The bins used to calculate the\n                autocorrelogram\n        \"\"\"\n        return xcorr(self.spike_times, Trange=Trange)\n\n    def trial_mean_fr(self) -&gt; float:\n        # Returns the trial mean firing rate for the cluster\n        if self.duration is None:\n            raise IndexError(\"No duration provided, give me one!\")\n        return self.n_spikes / self.duration\n\n    def mean_isi_range(self, isi_range: int) -&gt; float:\n        \"\"\"\n        Calculates the mean of the autocorrelation from 0 to n milliseconds\n        Used to help classify a neurons type (principal, interneuron etc)\n\n        Args:\n            isi_range (int): The range in ms to calculate the mean over\n\n        Returns:\n            float: The mean of the autocorrelogram between 0 and n milliseconds\n        \"\"\"\n        bins = 201\n        trange = np.array((-500, 500))\n        counts, bins = self.acorr(Trange=trange)\n        mask = np.logical_and(bins &gt; 0, bins &lt; isi_range)\n        return np.mean(counts[mask[1:]])\n\n    def mean_waveform(self, channel_id: Sequence = None):\n        \"\"\"\n        Returns the mean waveform and sem for a given spike train on a\n        particular channel\n\n        Args:\n            cluster_id (int): The cluster to get the mean waveform for\n\n        Returns:\n            mn_wvs (ndarray): The mean waveforms, usually 4x50 for tetrode\n                                recordings\n            std_wvs (ndarray): The standard deviations of the waveforms,\n                                usually 4x50 for tetrode recordings\n        \"\"\"\n        x = self.waveforms(channel_id)\n        if x is not None:\n            return np.mean(x, axis=0), np.std(x, axis=0)\n        else:\n            return None\n\n    def psth(self, **kwargs):\n        \"\"\"\n        Calculate the PSTH of event_ts against the spiking of a cell\n\n        Args:\n            cluster_id (int): The cluster for which to calculate the psth\n\n        Returns:\n            x, y (list): The list of time differences between the spikes of\n                            the cluster and the events (x) and the trials (y)\n        \"\"\"\n        if self._event_ts is None:\n            raise Exception(\"Need some event timestamps! Aborting\")\n        event_ts = self.event_ts\n        event_ts.sort()\n        if isinstance(event_ts, list):\n            event_ts = np.array(event_ts)\n\n        irange = event_ts[:, np.newaxis] + self.event_window[np.newaxis, :]\n        dts = np.searchsorted(self.spike_times, irange)\n        x = []\n        y = []\n        for i, t in enumerate(dts):\n            tmp = self.spike_times[t[0] : t[1]] - event_ts[i]\n            x.extend(tmp)\n            y.extend(np.repeat(i, len(tmp)))\n        return x, y\n\n    def psch(self, bin_width_secs: float) -&gt; np.ndarray:\n        \"\"\"\n        Calculate the peri-stimulus *count* histogram of a cell's spiking\n        against event times.\n\n        Args:\n            cluster_id (int): The cluster for which to calculate the psth.\n            bin_width_secs (float): The width of each bin in seconds.\n\n        Returns:\n            result (np.ndarray): Rows are counts of spikes per bin_width_secs.\n            Size of columns ranges from self.event_window[0] to\n            self.event_window[1] with bin_width_secs steps;\n            so x is count, y is \"event\".\n        \"\"\"\n        if self._event_ts is None:\n            raise Exception(\"Need some event timestamps! Aborting\")\n        event_ts = self.event_ts\n        event_ts.sort()\n        if isinstance(event_ts, list):\n            event_ts = np.array(event_ts)\n\n        irange = event_ts[:, np.newaxis] + self.event_window[np.newaxis, :]\n        dts = np.searchsorted(self.spike_times, irange)\n        bins = np.arange(self.event_window[0], self.event_window[1], bin_width_secs)\n        result = np.zeros(shape=(len(bins) - 1, len(event_ts)))\n        for i, t in enumerate(dts):\n            tmp = self.spike_times[t[0] : t[1]] - event_ts[i]\n            indices = np.digitize(tmp, bins=bins)\n            counts = np.bincount(indices, minlength=len(bins))\n            result[:, i] = counts[1:]\n        return result\n\n    def responds_to_stimulus(\n        self,\n        threshold: float,\n        min_contiguous: int,\n        return_activity: bool = False,\n        return_magnitude: bool = False,\n        **kwargs\n    ) -&gt; tuple:\n        \"\"\"\n        Checks whether a cluster responds to a laser stimulus.\n\n        Args:\n            cluster (int): The cluster to check.\n            threshold (float): The amount of activity the cluster needs to go\n                beyond to be classified as a responder (1.5 = 50% more or less\n                than the baseline activity).\n            min_contiguous (int): The number of contiguous samples in the\n                post-stimulus period for which the cluster needs to be active\n                beyond the threshold value to be classed as a responder.\n            return_activity (bool): Whether to return the mean reponse curve.\n            return_magnitude (int): Whether to return the magnitude of the\n                response. NB this is either +1 for excited or -1 for inhibited.\n\n        Returns:\n            responds (bool): Whether the cell responds or not.\n            OR\n            tuple: responds (bool), normed_response_curve (np.ndarray).\n            OR\n            tuple: responds (bool), normed_response_curve (np.ndarray),\n                response_magnitude (np.ndarray).\n        \"\"\"\n        spk_count_by_trial = self.psch(self._secs_per_bin)\n        firing_rate_by_trial = spk_count_by_trial / self.secs_per_bin\n        mean_firing_rate = np.mean(firing_rate_by_trial, 1)\n        # smooth with a moving average\n        # check nothing in kwargs first\n        if \"window_len\" in kwargs.keys():\n            window_len = kwargs[\"window_len\"]\n        else:\n            window_len = 5\n        if \"window\" in kwargs.keys():\n            window = kwargs[\"window\"]\n        else:\n            window = \"flat\"\n        if \"flat\" in window:\n            kernel = Box1DKernel(window_len)\n        if \"gauss\" in window:\n            kernel = Gaussian1DKernel(1, window_len)\n        if \"do_smooth\" in kwargs.keys():\n            do_smooth = kwargs.get(\"do_smooth\")\n        else:\n            do_smooth = True\n\n        if do_smooth:\n            smoothed_binned_spikes = convolve(mean_firing_rate, kernel, boundary=\"wrap\")\n        else:\n            smoothed_binned_spikes = mean_firing_rate\n        nbins = np.floor(np.sum(np.abs(self.event_window)) / self.secs_per_bin)\n        bins = np.linspace(self.event_window[0], self.event_window[1], int(nbins))\n        # normalize all activity by activity in the time before\n        # the laser onset\n        idx = bins &lt; 0\n        normd = min_max_norm(\n            smoothed_binned_spikes,\n            np.min(smoothed_binned_spikes[idx]),\n            np.max(smoothed_binned_spikes[idx]),\n        )\n        # mask the array outside of a threshold value so that\n        # only True values in the masked array are those that\n        # exceed the threshold (positively or negatively)\n        # the threshold provided to this function is expressed\n        # as a % above / below unit normality so adjust that now\n        # so it is expressed as a pre-stimulus firing rate mean\n        # pre_stim_mean = np.mean(smoothed_binned_spikes[idx])\n        # pre_stim_max = pre_stim_mean * threshold\n        # pre_stim_min = pre_stim_mean * (threshold-1.0)\n        # updated so threshold is double (+ or -) the pre-stim\n        # norm (lies between )\n        normd_masked = np.ma.masked_inside(normd, -threshold, 1 + threshold)\n        # find the contiguous runs in the masked array\n        # that are at least as long as the min_contiguous value\n        # and classify this as a True response\n        slices = np.ma.notmasked_contiguous(normd_masked)\n        if slices and np.any(np.isfinite(normd)):\n            # make sure that slices are within the first 25ms post-stim\n            if ~np.any([s.start &gt; 50 and s.start &lt; 75 for s in slices]):\n                if not return_activity:\n                    return False\n                else:\n                    if return_magnitude:\n                        return False, normd, 0\n                    return False, normd\n            max_runlength = max([len(normd_masked[s]) for s in slices])\n            if max_runlength &gt;= min_contiguous:\n                if not return_activity:\n                    return True\n                else:\n                    if return_magnitude:\n                        sl = [\n                            slc\n                            for slc in slices\n                            if (slc.stop - slc.start) == max_runlength\n                        ]\n                        mag = [-1 if np.mean(normd[sl[0]]) &lt; 0 else 1][0]\n                        return True, normd, mag\n                    else:\n                        return True, normd\n        if not return_activity:\n            return False\n        else:\n            if return_magnitude:\n                return False, normd, 0\n            return False, normd\n\n    def theta_mod_idx(self):\n        \"\"\"\n        Calculates a theta modulation index of a spike train based on the cells\n        autocorrelogram.\n\n        Args:\n            x1 (np.array): The spike time-series.\n\n        Returns:\n            thetaMod (float): The difference of the values at the first peak\n            and trough of the autocorrelogram.\n        \"\"\"\n        corr, _ = self.acorr()\n        # Take the fft of the spike train autocorr (from -500 to +500ms)\n        from scipy.signal import periodogram\n\n        freqs, power = periodogram(corr, fs=200, return_onesided=True)\n        # Smooth the power over +/- 1Hz\n        b = signal.windows.boxcar(3)\n        h = signal.filtfilt(b, 3, power)\n\n        # Square the amplitude first\n        sqd_amp = h**2\n        # Then find the mean power in the +/-1Hz band either side of that\n        theta_band_max_idx = np.nonzero(\n            sqd_amp == np.max(sqd_amp[np.logical_and(freqs &gt; 6, freqs &lt; 11)])\n        )[0][0]\n        # Get the mean theta band power - mtbp\n        mtbp = np.mean(sqd_amp[theta_band_max_idx - 1 : theta_band_max_idx + 1])\n        # Find the mean amplitude in the 2-50Hz range\n        other_band_idx = np.logical_and(freqs &gt; 2, freqs &lt; 50)\n        # Get the mean in the other band - mobp\n        mobp = np.mean(sqd_amp[other_band_idx])\n        # Find the ratio of these two - this is the theta modulation index\n        return (mtbp - mobp) / (mtbp + mobp)\n\n    def theta_mod_idxV2(self):\n        \"\"\"\n        This is a simpler alternative to the theta_mod_idx method in that it\n        calculates the difference between the normalized temporal\n        autocorrelogram at the trough between 50-70ms and the\n        peak between 100-140ms over their sum (data is binned into 5ms bins)\n\n        Measure used in Cacucci et al., 2004 and Kropff et al 2015\n        \"\"\"\n        corr, bins = self.acorr()\n        # 'close' the right-hand bin\n        bins = bins[0:-1]\n        # normalise corr so max is 1.0\n        corr = corr / float(np.max(corr))\n        thetaAntiPhase = np.min(\n            corr[np.logical_and(bins &gt; 50 / 1000.0, bins &lt; 70 / 1000.0)]\n        )\n        thetaPhase = np.max(\n            corr[np.logical_and(bins &gt; 100 / 1000.0, bins &lt; 140 / 1000.0)]\n        )\n        return (thetaPhase - thetaAntiPhase) / (thetaPhase + thetaAntiPhase)\n\n    def theta_band_max_freq(self):\n        \"\"\"\n        Calculates the frequency with the maximum power in the theta band (6-12Hz)\n        of a spike train's autocorrelogram.\n\n        This function is used to look for differences in theta frequency in\n        different running directions as per Blair.\n        See Welday paper - https://doi.org/10.1523/jneurosci.0712-11.2011\n\n        Args:\n            x1 (np.ndarray): The spike train for which the autocorrelogram will be\n                calculated.\n\n        Returns:\n            float: The frequency with the maximum power in the theta band.\n\n        Raises:\n            ValueError: If the input spike train is not valid.\n        \"\"\"\n        corr, _ = self.acorr()\n        # Take the fft of the spike train autocorr (from -500 to +500ms)\n        from scipy.signal import periodogram\n\n        freqs, power = periodogram(corr, fs=200, return_onesided=True)\n        power_masked = np.ma.MaskedArray(power, np.logical_or(freqs &lt; 6, freqs &gt; 12))\n        return freqs[np.argmax(power_masked)]\n\n    def smooth_spike_train(self, npos, sigma=3.0, shuffle=None):\n        \"\"\"\n        Returns a spike train the same length as num pos samples that has been\n        smoothed in time with a gaussian kernel M in width and standard\n        deviation equal to sigma.\n\n        Args:\n            x1 (np.array): The pos indices the spikes occurred at.\n            npos (int): The number of position samples captured.\n            sigma (float): The standard deviation of the gaussian used to\n                smooth the spike train.\n            shuffle (int, optional): The number of seconds to shift the spike\n                train by. Default is None.\n\n        Returns:\n            smoothed_spikes (np.array): The smoothed spike train.\n        \"\"\"\n        spk_hist = np.bincount(self.spike_times, minlength=npos)\n        if shuffle is not None:\n            spk_hist = np.roll(spk_hist, int(shuffle * 50))\n        # smooth the spk_hist (which is a temporal histogram) with a 250ms\n        # gaussian as with Kropff et al., 2015\n        h = signal.windows.gaussian(13, sigma)\n        h = h / float(np.sum(h))\n        return signal.filtfilt(h.ravel(), 1, spk_hist)\n\n    def contamination_percent(self, **kwargs) -&gt; tuple:\n\n        c, Qi, Q00, Q01, Ri = contamination_percent(self.spike_times, **kwargs)\n        Q = min(Qi / (max(Q00, Q01)))  # this is a measure of refractoriness\n        # this is a second measure of refractoriness (kicks in for very low\n        # firing rates)\n        R = min(Ri)\n        return Q, R\n</code></pre>"},{"location":"reference/#ephysiopy.common.spikecalcs.SpikeCalcsGeneric.n_spikes","title":"<code>n_spikes</code>  <code>property</code>","text":"<p>Returns the number of spikes in the cluster</p> <p>Returns:</p> Name Type Description <code>int</code> <p>The number of spikes in the cluster</p>"},{"location":"reference/#ephysiopy.common.spikecalcs.SpikeCalcsGeneric.acorr","title":"<code>acorr(Trange=None)</code>","text":"<p>Calculates the autocorrelogram of a spike train</p> <p>Parameters:</p> Name Type Description Default <code>ts</code> <code>ndarray</code> <p>The spike times</p> required <code>Trange</code> <code>ndarray</code> <p>The range of times to calculate the autocorrelogram over</p> <code>None</code> <p>Returns:</p> Name Type Description <code>counts</code> <code>ndarray</code> <p>The autocorrelogram</p> <code>bins</code> <code>ndarray</code> <p>The bins used to calculate the autocorrelogram</p> Source code in <code>ephysiopy/common/spikecalcs.py</code> <pre><code>def acorr(self, Trange: np.ndarray = None) -&gt; tuple:\n    \"\"\"\n    Calculates the autocorrelogram of a spike train\n\n    Args:\n        ts (np.ndarray): The spike times\n        Trange (np.ndarray): The range of times to calculate the\n            autocorrelogram over\n\n    Returns:\n        counts (np.ndarray): The autocorrelogram\n        bins (np.ndarray): The bins used to calculate the\n            autocorrelogram\n    \"\"\"\n    return xcorr(self.spike_times, Trange=Trange)\n</code></pre>"},{"location":"reference/#ephysiopy.common.spikecalcs.SpikeCalcsGeneric.mean_isi_range","title":"<code>mean_isi_range(isi_range)</code>","text":"<p>Calculates the mean of the autocorrelation from 0 to n milliseconds Used to help classify a neurons type (principal, interneuron etc)</p> <p>Parameters:</p> Name Type Description Default <code>isi_range</code> <code>int</code> <p>The range in ms to calculate the mean over</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The mean of the autocorrelogram between 0 and n milliseconds</p> Source code in <code>ephysiopy/common/spikecalcs.py</code> <pre><code>def mean_isi_range(self, isi_range: int) -&gt; float:\n    \"\"\"\n    Calculates the mean of the autocorrelation from 0 to n milliseconds\n    Used to help classify a neurons type (principal, interneuron etc)\n\n    Args:\n        isi_range (int): The range in ms to calculate the mean over\n\n    Returns:\n        float: The mean of the autocorrelogram between 0 and n milliseconds\n    \"\"\"\n    bins = 201\n    trange = np.array((-500, 500))\n    counts, bins = self.acorr(Trange=trange)\n    mask = np.logical_and(bins &gt; 0, bins &lt; isi_range)\n    return np.mean(counts[mask[1:]])\n</code></pre>"},{"location":"reference/#ephysiopy.common.spikecalcs.SpikeCalcsGeneric.mean_waveform","title":"<code>mean_waveform(channel_id=None)</code>","text":"<p>Returns the mean waveform and sem for a given spike train on a particular channel</p> <p>Parameters:</p> Name Type Description Default <code>cluster_id</code> <code>int</code> <p>The cluster to get the mean waveform for</p> required <p>Returns:</p> Name Type Description <code>mn_wvs</code> <code>ndarray</code> <p>The mean waveforms, usually 4x50 for tetrode                 recordings</p> <code>std_wvs</code> <code>ndarray</code> <p>The standard deviations of the waveforms,                 usually 4x50 for tetrode recordings</p> Source code in <code>ephysiopy/common/spikecalcs.py</code> <pre><code>def mean_waveform(self, channel_id: Sequence = None):\n    \"\"\"\n    Returns the mean waveform and sem for a given spike train on a\n    particular channel\n\n    Args:\n        cluster_id (int): The cluster to get the mean waveform for\n\n    Returns:\n        mn_wvs (ndarray): The mean waveforms, usually 4x50 for tetrode\n                            recordings\n        std_wvs (ndarray): The standard deviations of the waveforms,\n                            usually 4x50 for tetrode recordings\n    \"\"\"\n    x = self.waveforms(channel_id)\n    if x is not None:\n        return np.mean(x, axis=0), np.std(x, axis=0)\n    else:\n        return None\n</code></pre>"},{"location":"reference/#ephysiopy.common.spikecalcs.SpikeCalcsGeneric.psch","title":"<code>psch(bin_width_secs)</code>","text":"<p>Calculate the peri-stimulus count histogram of a cell's spiking against event times.</p> <p>Parameters:</p> Name Type Description Default <code>cluster_id</code> <code>int</code> <p>The cluster for which to calculate the psth.</p> required <code>bin_width_secs</code> <code>float</code> <p>The width of each bin in seconds.</p> required <p>Returns:</p> Name Type Description <code>result</code> <code>ndarray</code> <p>Rows are counts of spikes per bin_width_secs.</p> <code>ndarray</code> <p>Size of columns ranges from self.event_window[0] to</p> <code>ndarray</code> <p>self.event_window[1] with bin_width_secs steps;</p> <code>ndarray</code> <p>so x is count, y is \"event\".</p> Source code in <code>ephysiopy/common/spikecalcs.py</code> <pre><code>def psch(self, bin_width_secs: float) -&gt; np.ndarray:\n    \"\"\"\n    Calculate the peri-stimulus *count* histogram of a cell's spiking\n    against event times.\n\n    Args:\n        cluster_id (int): The cluster for which to calculate the psth.\n        bin_width_secs (float): The width of each bin in seconds.\n\n    Returns:\n        result (np.ndarray): Rows are counts of spikes per bin_width_secs.\n        Size of columns ranges from self.event_window[0] to\n        self.event_window[1] with bin_width_secs steps;\n        so x is count, y is \"event\".\n    \"\"\"\n    if self._event_ts is None:\n        raise Exception(\"Need some event timestamps! Aborting\")\n    event_ts = self.event_ts\n    event_ts.sort()\n    if isinstance(event_ts, list):\n        event_ts = np.array(event_ts)\n\n    irange = event_ts[:, np.newaxis] + self.event_window[np.newaxis, :]\n    dts = np.searchsorted(self.spike_times, irange)\n    bins = np.arange(self.event_window[0], self.event_window[1], bin_width_secs)\n    result = np.zeros(shape=(len(bins) - 1, len(event_ts)))\n    for i, t in enumerate(dts):\n        tmp = self.spike_times[t[0] : t[1]] - event_ts[i]\n        indices = np.digitize(tmp, bins=bins)\n        counts = np.bincount(indices, minlength=len(bins))\n        result[:, i] = counts[1:]\n    return result\n</code></pre>"},{"location":"reference/#ephysiopy.common.spikecalcs.SpikeCalcsGeneric.psth","title":"<code>psth(**kwargs)</code>","text":"<p>Calculate the PSTH of event_ts against the spiking of a cell</p> <p>Parameters:</p> Name Type Description Default <code>cluster_id</code> <code>int</code> <p>The cluster for which to calculate the psth</p> required <p>Returns:</p> Type Description <p>x, y (list): The list of time differences between the spikes of             the cluster and the events (x) and the trials (y)</p> Source code in <code>ephysiopy/common/spikecalcs.py</code> <pre><code>def psth(self, **kwargs):\n    \"\"\"\n    Calculate the PSTH of event_ts against the spiking of a cell\n\n    Args:\n        cluster_id (int): The cluster for which to calculate the psth\n\n    Returns:\n        x, y (list): The list of time differences between the spikes of\n                        the cluster and the events (x) and the trials (y)\n    \"\"\"\n    if self._event_ts is None:\n        raise Exception(\"Need some event timestamps! Aborting\")\n    event_ts = self.event_ts\n    event_ts.sort()\n    if isinstance(event_ts, list):\n        event_ts = np.array(event_ts)\n\n    irange = event_ts[:, np.newaxis] + self.event_window[np.newaxis, :]\n    dts = np.searchsorted(self.spike_times, irange)\n    x = []\n    y = []\n    for i, t in enumerate(dts):\n        tmp = self.spike_times[t[0] : t[1]] - event_ts[i]\n        x.extend(tmp)\n        y.extend(np.repeat(i, len(tmp)))\n    return x, y\n</code></pre>"},{"location":"reference/#ephysiopy.common.spikecalcs.SpikeCalcsGeneric.responds_to_stimulus","title":"<code>responds_to_stimulus(threshold, min_contiguous, return_activity=False, return_magnitude=False, **kwargs)</code>","text":"<p>Checks whether a cluster responds to a laser stimulus.</p> <p>Parameters:</p> Name Type Description Default <code>cluster</code> <code>int</code> <p>The cluster to check.</p> required <code>threshold</code> <code>float</code> <p>The amount of activity the cluster needs to go beyond to be classified as a responder (1.5 = 50% more or less than the baseline activity).</p> required <code>min_contiguous</code> <code>int</code> <p>The number of contiguous samples in the post-stimulus period for which the cluster needs to be active beyond the threshold value to be classed as a responder.</p> required <code>return_activity</code> <code>bool</code> <p>Whether to return the mean reponse curve.</p> <code>False</code> <code>return_magnitude</code> <code>int</code> <p>Whether to return the magnitude of the response. NB this is either +1 for excited or -1 for inhibited.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>responds</code> <code>bool</code> <p>Whether the cell responds or not.</p> <code>tuple</code> <p>OR</p> <code>tuple</code> <code>tuple</code> <p>responds (bool), normed_response_curve (np.ndarray).</p> <code>tuple</code> <p>OR</p> <code>tuple</code> <code>tuple</code> <p>responds (bool), normed_response_curve (np.ndarray), response_magnitude (np.ndarray).</p> Source code in <code>ephysiopy/common/spikecalcs.py</code> <pre><code>def responds_to_stimulus(\n    self,\n    threshold: float,\n    min_contiguous: int,\n    return_activity: bool = False,\n    return_magnitude: bool = False,\n    **kwargs\n) -&gt; tuple:\n    \"\"\"\n    Checks whether a cluster responds to a laser stimulus.\n\n    Args:\n        cluster (int): The cluster to check.\n        threshold (float): The amount of activity the cluster needs to go\n            beyond to be classified as a responder (1.5 = 50% more or less\n            than the baseline activity).\n        min_contiguous (int): The number of contiguous samples in the\n            post-stimulus period for which the cluster needs to be active\n            beyond the threshold value to be classed as a responder.\n        return_activity (bool): Whether to return the mean reponse curve.\n        return_magnitude (int): Whether to return the magnitude of the\n            response. NB this is either +1 for excited or -1 for inhibited.\n\n    Returns:\n        responds (bool): Whether the cell responds or not.\n        OR\n        tuple: responds (bool), normed_response_curve (np.ndarray).\n        OR\n        tuple: responds (bool), normed_response_curve (np.ndarray),\n            response_magnitude (np.ndarray).\n    \"\"\"\n    spk_count_by_trial = self.psch(self._secs_per_bin)\n    firing_rate_by_trial = spk_count_by_trial / self.secs_per_bin\n    mean_firing_rate = np.mean(firing_rate_by_trial, 1)\n    # smooth with a moving average\n    # check nothing in kwargs first\n    if \"window_len\" in kwargs.keys():\n        window_len = kwargs[\"window_len\"]\n    else:\n        window_len = 5\n    if \"window\" in kwargs.keys():\n        window = kwargs[\"window\"]\n    else:\n        window = \"flat\"\n    if \"flat\" in window:\n        kernel = Box1DKernel(window_len)\n    if \"gauss\" in window:\n        kernel = Gaussian1DKernel(1, window_len)\n    if \"do_smooth\" in kwargs.keys():\n        do_smooth = kwargs.get(\"do_smooth\")\n    else:\n        do_smooth = True\n\n    if do_smooth:\n        smoothed_binned_spikes = convolve(mean_firing_rate, kernel, boundary=\"wrap\")\n    else:\n        smoothed_binned_spikes = mean_firing_rate\n    nbins = np.floor(np.sum(np.abs(self.event_window)) / self.secs_per_bin)\n    bins = np.linspace(self.event_window[0], self.event_window[1], int(nbins))\n    # normalize all activity by activity in the time before\n    # the laser onset\n    idx = bins &lt; 0\n    normd = min_max_norm(\n        smoothed_binned_spikes,\n        np.min(smoothed_binned_spikes[idx]),\n        np.max(smoothed_binned_spikes[idx]),\n    )\n    # mask the array outside of a threshold value so that\n    # only True values in the masked array are those that\n    # exceed the threshold (positively or negatively)\n    # the threshold provided to this function is expressed\n    # as a % above / below unit normality so adjust that now\n    # so it is expressed as a pre-stimulus firing rate mean\n    # pre_stim_mean = np.mean(smoothed_binned_spikes[idx])\n    # pre_stim_max = pre_stim_mean * threshold\n    # pre_stim_min = pre_stim_mean * (threshold-1.0)\n    # updated so threshold is double (+ or -) the pre-stim\n    # norm (lies between )\n    normd_masked = np.ma.masked_inside(normd, -threshold, 1 + threshold)\n    # find the contiguous runs in the masked array\n    # that are at least as long as the min_contiguous value\n    # and classify this as a True response\n    slices = np.ma.notmasked_contiguous(normd_masked)\n    if slices and np.any(np.isfinite(normd)):\n        # make sure that slices are within the first 25ms post-stim\n        if ~np.any([s.start &gt; 50 and s.start &lt; 75 for s in slices]):\n            if not return_activity:\n                return False\n            else:\n                if return_magnitude:\n                    return False, normd, 0\n                return False, normd\n        max_runlength = max([len(normd_masked[s]) for s in slices])\n        if max_runlength &gt;= min_contiguous:\n            if not return_activity:\n                return True\n            else:\n                if return_magnitude:\n                    sl = [\n                        slc\n                        for slc in slices\n                        if (slc.stop - slc.start) == max_runlength\n                    ]\n                    mag = [-1 if np.mean(normd[sl[0]]) &lt; 0 else 1][0]\n                    return True, normd, mag\n                else:\n                    return True, normd\n    if not return_activity:\n        return False\n    else:\n        if return_magnitude:\n            return False, normd, 0\n        return False, normd\n</code></pre>"},{"location":"reference/#ephysiopy.common.spikecalcs.SpikeCalcsGeneric.smooth_spike_train","title":"<code>smooth_spike_train(npos, sigma=3.0, shuffle=None)</code>","text":"<p>Returns a spike train the same length as num pos samples that has been smoothed in time with a gaussian kernel M in width and standard deviation equal to sigma.</p> <p>Parameters:</p> Name Type Description Default <code>x1</code> <code>array</code> <p>The pos indices the spikes occurred at.</p> required <code>npos</code> <code>int</code> <p>The number of position samples captured.</p> required <code>sigma</code> <code>float</code> <p>The standard deviation of the gaussian used to smooth the spike train.</p> <code>3.0</code> <code>shuffle</code> <code>int</code> <p>The number of seconds to shift the spike train by. Default is None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>smoothed_spikes</code> <code>array</code> <p>The smoothed spike train.</p> Source code in <code>ephysiopy/common/spikecalcs.py</code> <pre><code>def smooth_spike_train(self, npos, sigma=3.0, shuffle=None):\n    \"\"\"\n    Returns a spike train the same length as num pos samples that has been\n    smoothed in time with a gaussian kernel M in width and standard\n    deviation equal to sigma.\n\n    Args:\n        x1 (np.array): The pos indices the spikes occurred at.\n        npos (int): The number of position samples captured.\n        sigma (float): The standard deviation of the gaussian used to\n            smooth the spike train.\n        shuffle (int, optional): The number of seconds to shift the spike\n            train by. Default is None.\n\n    Returns:\n        smoothed_spikes (np.array): The smoothed spike train.\n    \"\"\"\n    spk_hist = np.bincount(self.spike_times, minlength=npos)\n    if shuffle is not None:\n        spk_hist = np.roll(spk_hist, int(shuffle * 50))\n    # smooth the spk_hist (which is a temporal histogram) with a 250ms\n    # gaussian as with Kropff et al., 2015\n    h = signal.windows.gaussian(13, sigma)\n    h = h / float(np.sum(h))\n    return signal.filtfilt(h.ravel(), 1, spk_hist)\n</code></pre>"},{"location":"reference/#ephysiopy.common.spikecalcs.SpikeCalcsGeneric.theta_band_max_freq","title":"<code>theta_band_max_freq()</code>","text":"<p>Calculates the frequency with the maximum power in the theta band (6-12Hz) of a spike train's autocorrelogram.</p> <p>This function is used to look for differences in theta frequency in different running directions as per Blair. See Welday paper - https://doi.org/10.1523/jneurosci.0712-11.2011</p> <p>Parameters:</p> Name Type Description Default <code>x1</code> <code>ndarray</code> <p>The spike train for which the autocorrelogram will be calculated.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>The frequency with the maximum power in the theta band.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input spike train is not valid.</p> Source code in <code>ephysiopy/common/spikecalcs.py</code> <pre><code>def theta_band_max_freq(self):\n    \"\"\"\n    Calculates the frequency with the maximum power in the theta band (6-12Hz)\n    of a spike train's autocorrelogram.\n\n    This function is used to look for differences in theta frequency in\n    different running directions as per Blair.\n    See Welday paper - https://doi.org/10.1523/jneurosci.0712-11.2011\n\n    Args:\n        x1 (np.ndarray): The spike train for which the autocorrelogram will be\n            calculated.\n\n    Returns:\n        float: The frequency with the maximum power in the theta band.\n\n    Raises:\n        ValueError: If the input spike train is not valid.\n    \"\"\"\n    corr, _ = self.acorr()\n    # Take the fft of the spike train autocorr (from -500 to +500ms)\n    from scipy.signal import periodogram\n\n    freqs, power = periodogram(corr, fs=200, return_onesided=True)\n    power_masked = np.ma.MaskedArray(power, np.logical_or(freqs &lt; 6, freqs &gt; 12))\n    return freqs[np.argmax(power_masked)]\n</code></pre>"},{"location":"reference/#ephysiopy.common.spikecalcs.SpikeCalcsGeneric.theta_mod_idx","title":"<code>theta_mod_idx()</code>","text":"<p>Calculates a theta modulation index of a spike train based on the cells autocorrelogram.</p> <p>Parameters:</p> Name Type Description Default <code>x1</code> <code>array</code> <p>The spike time-series.</p> required <p>Returns:</p> Name Type Description <code>thetaMod</code> <code>float</code> <p>The difference of the values at the first peak</p> <p>and trough of the autocorrelogram.</p> Source code in <code>ephysiopy/common/spikecalcs.py</code> <pre><code>def theta_mod_idx(self):\n    \"\"\"\n    Calculates a theta modulation index of a spike train based on the cells\n    autocorrelogram.\n\n    Args:\n        x1 (np.array): The spike time-series.\n\n    Returns:\n        thetaMod (float): The difference of the values at the first peak\n        and trough of the autocorrelogram.\n    \"\"\"\n    corr, _ = self.acorr()\n    # Take the fft of the spike train autocorr (from -500 to +500ms)\n    from scipy.signal import periodogram\n\n    freqs, power = periodogram(corr, fs=200, return_onesided=True)\n    # Smooth the power over +/- 1Hz\n    b = signal.windows.boxcar(3)\n    h = signal.filtfilt(b, 3, power)\n\n    # Square the amplitude first\n    sqd_amp = h**2\n    # Then find the mean power in the +/-1Hz band either side of that\n    theta_band_max_idx = np.nonzero(\n        sqd_amp == np.max(sqd_amp[np.logical_and(freqs &gt; 6, freqs &lt; 11)])\n    )[0][0]\n    # Get the mean theta band power - mtbp\n    mtbp = np.mean(sqd_amp[theta_band_max_idx - 1 : theta_band_max_idx + 1])\n    # Find the mean amplitude in the 2-50Hz range\n    other_band_idx = np.logical_and(freqs &gt; 2, freqs &lt; 50)\n    # Get the mean in the other band - mobp\n    mobp = np.mean(sqd_amp[other_band_idx])\n    # Find the ratio of these two - this is the theta modulation index\n    return (mtbp - mobp) / (mtbp + mobp)\n</code></pre>"},{"location":"reference/#ephysiopy.common.spikecalcs.SpikeCalcsGeneric.theta_mod_idxV2","title":"<code>theta_mod_idxV2()</code>","text":"<p>This is a simpler alternative to the theta_mod_idx method in that it calculates the difference between the normalized temporal autocorrelogram at the trough between 50-70ms and the peak between 100-140ms over their sum (data is binned into 5ms bins)</p> <p>Measure used in Cacucci et al., 2004 and Kropff et al 2015</p> Source code in <code>ephysiopy/common/spikecalcs.py</code> <pre><code>def theta_mod_idxV2(self):\n    \"\"\"\n    This is a simpler alternative to the theta_mod_idx method in that it\n    calculates the difference between the normalized temporal\n    autocorrelogram at the trough between 50-70ms and the\n    peak between 100-140ms over their sum (data is binned into 5ms bins)\n\n    Measure used in Cacucci et al., 2004 and Kropff et al 2015\n    \"\"\"\n    corr, bins = self.acorr()\n    # 'close' the right-hand bin\n    bins = bins[0:-1]\n    # normalise corr so max is 1.0\n    corr = corr / float(np.max(corr))\n    thetaAntiPhase = np.min(\n        corr[np.logical_and(bins &gt; 50 / 1000.0, bins &lt; 70 / 1000.0)]\n    )\n    thetaPhase = np.max(\n        corr[np.logical_and(bins &gt; 100 / 1000.0, bins &lt; 140 / 1000.0)]\n    )\n    return (thetaPhase - thetaAntiPhase) / (thetaPhase + thetaAntiPhase)\n</code></pre>"},{"location":"reference/#ephysiopy.common.spikecalcs.SpikeCalcsGeneric.update_KSMeta","title":"<code>update_KSMeta(value)</code>","text":"<p>Takes in a TemplateModel instance from a phy session and parses out the relevant metrics for the cluster and places into the namedtuple KSMeta</p> Source code in <code>ephysiopy/common/spikecalcs.py</code> <pre><code>def update_KSMeta(self, value: dict):\n    \"\"\"\n    Takes in a TemplateModel instance from a phy session and\n    parses out the relevant metrics for the cluster and places\n    into the namedtuple KSMeta\n    \"\"\"\n    metavals = []\n    for f in KSMetaTuple._fields:\n        if f in value.keys():\n            metavals.append(value[f][self.cluster])\n        else:\n            metavals.append(None)\n    self._ksmeta = KSMetaTuple(*metavals)\n</code></pre>"},{"location":"reference/#ephysiopy.common.spikecalcs.SpikeCalcsOpenEphys","title":"<code>SpikeCalcsOpenEphys</code>","text":"<p>               Bases: <code>SpikeCalcsGeneric</code></p> Source code in <code>ephysiopy/common/spikecalcs.py</code> <pre><code>class SpikeCalcsOpenEphys(SpikeCalcsGeneric):\n    def __init__(self, spike_times, cluster, waveforms=None, **kwargs):\n        super().__init__(spike_times, cluster, waveforms, **kwargs)\n        self.n_samples = [-40, 41]\n        self.TemplateModel = None\n\n    def get_waveforms(\n        self,\n        cluster: int,\n        cluster_data: KiloSortSession,\n        n_waveforms: int = 2000,\n        n_channels: int = 64,\n        channel_range=None,\n        **kwargs\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Returns waveforms for a cluster.\n\n        Args:\n            cluster (int): The cluster to return the waveforms for.\n            cluster_data (KiloSortSession): The KiloSortSession object for the\n                session that contains the cluster.\n            n_waveforms (int, optional): The number of waveforms to return.\n                Defaults to 2000.\n            n_channels (int, optional): The number of channels in the\n                recording. Defaults to 64.\n        \"\"\"\n        # instantiate the TemplateModel - this is used to get the waveforms\n        # for the cluster. TemplateModel encapsulates the results of KiloSort\n        if self.TemplateModel is None:\n            self.TemplateModel = TemplateModel(\n                dir_path=os.path.join(cluster_data.fname_root),\n                sample_rate=3e4,\n                dat_path=os.path.join(cluster_data.fname_root, \"continuous.dat\"),\n                n_channels_dat=n_channels,\n            )\n        # get the waveforms for the given cluster on the best channel only\n        waveforms = self.TemplateModel.get_cluster_spike_waveforms(cluster)\n        # get a random subset of the waveforms\n        rng = np.random.default_rng()\n        total_waves = waveforms.shape[0]\n        n_waveforms = n_waveforms if n_waveforms &lt; total_waves else total_waves\n        waveforms_subset = rng.choice(waveforms, n_waveforms)\n        # return the waveforms\n        if channel_range is None:\n            return np.squeeze(waveforms_subset[:, :, 0])\n        else:\n            if isinstance(channel_range, Sequence):\n                return np.squeeze(waveforms_subset[:, :, channel_range])\n            else:\n                warnings.warn(\"Invalid channel_range sequence\")\n\n    def get_channel_depth_from_templates(self, pname: Path):\n        \"\"\"\n        Determine depth of template as well as closest channel. Adopted from\n        'templatePositionsAmplitudes' by N. Steinmetz\n        (https://github.com/cortex-lab/spikes)\n        \"\"\"\n        # Load inverse whitening matrix\n        Winv = np.load(os.path.join(pname, \"whitening_mat_inv.npy\"))\n        # Load templates\n        templates = np.load(os.path.join(pname, \"templates.npy\"))\n        # Load channel_map and positions\n        channel_map = np.load(os.path.join(pname, \"channel_map.npy\"))\n        channel_positions = np.load(os.path.join(pname, \"channel_positions.npy\"))\n        map_and_pos = np.array([np.squeeze(channel_map), channel_positions[:, 1]])\n        # unwhiten all the templates\n        tempsUnW = np.zeros(np.shape(templates))\n        for i in np.shape(templates)[0]:\n            tempsUnW[i, :, :] = np.squeeze(templates[i, :, :]) @ Winv\n\n        tempAmp = np.squeeze(np.max(tempsUnW, 1)) - np.squeeze(np.min(tempsUnW, 1))\n        tempAmpsUnscaled = np.max(tempAmp, 1)\n        # need to zero-out the potentially-many low values on distant channels\n        threshVals = tempAmpsUnscaled * 0.3\n        tempAmp[tempAmp &lt; threshVals[:, None]] = 0\n        # Compute the depth as a centre of mass\n        templateDepths = np.sum(tempAmp * map_and_pos[1, :], -1) / np.sum(tempAmp, 1)\n        maxChanIdx = np.argmin(\n            np.abs((templateDepths[:, None] - map_and_pos[1, :].T)), 1\n        )\n        return templateDepths, maxChanIdx\n\n    def get_template_id_for_cluster(self, pname: Path, cluster: int):\n        \"\"\"\n        Determine the best channel (one with highest amplitude spikes)\n        for a given cluster.\n        \"\"\"\n        spike_templates = np.load(os.path.join(pname, \"spike_templates.npy\"))\n        spike_times = np.load(os.path.join(pname, \"spike_times.npy\"))\n        spike_clusters = np.load(os.path.join(pname, \"spike_clusters.npy\"))\n        cluster_times = spike_times[spike_clusters == cluster]\n        rez_mat = h5py.File(os.path.join(pname, \"rez.mat\"), \"r\")\n        st3 = rez_mat[\"rez\"][\"st3\"]\n        st_spike_times = st3[0, :]\n        idx = np.searchsorted(st_spike_times, cluster_times)\n        template_idx, counts = np.unique(spike_templates[idx], return_counts=True)\n        ind = np.argmax(counts)\n        return template_idx[ind]\n</code></pre>"},{"location":"reference/#ephysiopy.common.spikecalcs.SpikeCalcsOpenEphys.get_channel_depth_from_templates","title":"<code>get_channel_depth_from_templates(pname)</code>","text":"<p>Determine depth of template as well as closest channel. Adopted from 'templatePositionsAmplitudes' by N. Steinmetz (https://github.com/cortex-lab/spikes)</p> Source code in <code>ephysiopy/common/spikecalcs.py</code> <pre><code>def get_channel_depth_from_templates(self, pname: Path):\n    \"\"\"\n    Determine depth of template as well as closest channel. Adopted from\n    'templatePositionsAmplitudes' by N. Steinmetz\n    (https://github.com/cortex-lab/spikes)\n    \"\"\"\n    # Load inverse whitening matrix\n    Winv = np.load(os.path.join(pname, \"whitening_mat_inv.npy\"))\n    # Load templates\n    templates = np.load(os.path.join(pname, \"templates.npy\"))\n    # Load channel_map and positions\n    channel_map = np.load(os.path.join(pname, \"channel_map.npy\"))\n    channel_positions = np.load(os.path.join(pname, \"channel_positions.npy\"))\n    map_and_pos = np.array([np.squeeze(channel_map), channel_positions[:, 1]])\n    # unwhiten all the templates\n    tempsUnW = np.zeros(np.shape(templates))\n    for i in np.shape(templates)[0]:\n        tempsUnW[i, :, :] = np.squeeze(templates[i, :, :]) @ Winv\n\n    tempAmp = np.squeeze(np.max(tempsUnW, 1)) - np.squeeze(np.min(tempsUnW, 1))\n    tempAmpsUnscaled = np.max(tempAmp, 1)\n    # need to zero-out the potentially-many low values on distant channels\n    threshVals = tempAmpsUnscaled * 0.3\n    tempAmp[tempAmp &lt; threshVals[:, None]] = 0\n    # Compute the depth as a centre of mass\n    templateDepths = np.sum(tempAmp * map_and_pos[1, :], -1) / np.sum(tempAmp, 1)\n    maxChanIdx = np.argmin(\n        np.abs((templateDepths[:, None] - map_and_pos[1, :].T)), 1\n    )\n    return templateDepths, maxChanIdx\n</code></pre>"},{"location":"reference/#ephysiopy.common.spikecalcs.SpikeCalcsOpenEphys.get_template_id_for_cluster","title":"<code>get_template_id_for_cluster(pname, cluster)</code>","text":"<p>Determine the best channel (one with highest amplitude spikes) for a given cluster.</p> Source code in <code>ephysiopy/common/spikecalcs.py</code> <pre><code>def get_template_id_for_cluster(self, pname: Path, cluster: int):\n    \"\"\"\n    Determine the best channel (one with highest amplitude spikes)\n    for a given cluster.\n    \"\"\"\n    spike_templates = np.load(os.path.join(pname, \"spike_templates.npy\"))\n    spike_times = np.load(os.path.join(pname, \"spike_times.npy\"))\n    spike_clusters = np.load(os.path.join(pname, \"spike_clusters.npy\"))\n    cluster_times = spike_times[spike_clusters == cluster]\n    rez_mat = h5py.File(os.path.join(pname, \"rez.mat\"), \"r\")\n    st3 = rez_mat[\"rez\"][\"st3\"]\n    st_spike_times = st3[0, :]\n    idx = np.searchsorted(st_spike_times, cluster_times)\n    template_idx, counts = np.unique(spike_templates[idx], return_counts=True)\n    ind = np.argmax(counts)\n    return template_idx[ind]\n</code></pre>"},{"location":"reference/#ephysiopy.common.spikecalcs.SpikeCalcsOpenEphys.get_waveforms","title":"<code>get_waveforms(cluster, cluster_data, n_waveforms=2000, n_channels=64, channel_range=None, **kwargs)</code>","text":"<p>Returns waveforms for a cluster.</p> <p>Parameters:</p> Name Type Description Default <code>cluster</code> <code>int</code> <p>The cluster to return the waveforms for.</p> required <code>cluster_data</code> <code>KiloSortSession</code> <p>The KiloSortSession object for the session that contains the cluster.</p> required <code>n_waveforms</code> <code>int</code> <p>The number of waveforms to return. Defaults to 2000.</p> <code>2000</code> <code>n_channels</code> <code>int</code> <p>The number of channels in the recording. Defaults to 64.</p> <code>64</code> Source code in <code>ephysiopy/common/spikecalcs.py</code> <pre><code>def get_waveforms(\n    self,\n    cluster: int,\n    cluster_data: KiloSortSession,\n    n_waveforms: int = 2000,\n    n_channels: int = 64,\n    channel_range=None,\n    **kwargs\n) -&gt; np.ndarray:\n    \"\"\"\n    Returns waveforms for a cluster.\n\n    Args:\n        cluster (int): The cluster to return the waveforms for.\n        cluster_data (KiloSortSession): The KiloSortSession object for the\n            session that contains the cluster.\n        n_waveforms (int, optional): The number of waveforms to return.\n            Defaults to 2000.\n        n_channels (int, optional): The number of channels in the\n            recording. Defaults to 64.\n    \"\"\"\n    # instantiate the TemplateModel - this is used to get the waveforms\n    # for the cluster. TemplateModel encapsulates the results of KiloSort\n    if self.TemplateModel is None:\n        self.TemplateModel = TemplateModel(\n            dir_path=os.path.join(cluster_data.fname_root),\n            sample_rate=3e4,\n            dat_path=os.path.join(cluster_data.fname_root, \"continuous.dat\"),\n            n_channels_dat=n_channels,\n        )\n    # get the waveforms for the given cluster on the best channel only\n    waveforms = self.TemplateModel.get_cluster_spike_waveforms(cluster)\n    # get a random subset of the waveforms\n    rng = np.random.default_rng()\n    total_waves = waveforms.shape[0]\n    n_waveforms = n_waveforms if n_waveforms &lt; total_waves else total_waves\n    waveforms_subset = rng.choice(waveforms, n_waveforms)\n    # return the waveforms\n    if channel_range is None:\n        return np.squeeze(waveforms_subset[:, :, 0])\n    else:\n        if isinstance(channel_range, Sequence):\n            return np.squeeze(waveforms_subset[:, :, channel_range])\n        else:\n            warnings.warn(\"Invalid channel_range sequence\")\n</code></pre>"},{"location":"reference/#ephysiopy.common.spikecalcs.SpikeCalcsProbe","title":"<code>SpikeCalcsProbe</code>","text":"<p>               Bases: <code>SpikeCalcsGeneric</code></p> <p>Encapsulates methods specific to probe-based recordings</p> Source code in <code>ephysiopy/common/spikecalcs.py</code> <pre><code>class SpikeCalcsProbe(SpikeCalcsGeneric):\n    \"\"\"\n    Encapsulates methods specific to probe-based recordings\n    \"\"\"\n\n    def __init__(self):\n        pass\n</code></pre>"},{"location":"reference/#ephysiopy.common.spikecalcs.SpikeCalcsTetrode","title":"<code>SpikeCalcsTetrode</code>","text":"<p>               Bases: <code>SpikeCalcsGeneric</code></p> <p>Encapsulates methods specific to the geometry inherent in tetrode-based recordings</p> Source code in <code>ephysiopy/common/spikecalcs.py</code> <pre><code>class SpikeCalcsTetrode(SpikeCalcsGeneric):\n    \"\"\"\n    Encapsulates methods specific to the geometry inherent in tetrode-based\n    recordings\n    \"\"\"\n\n    def __init__(self, spike_times, cluster, waveforms=None, **kwargs):\n        super().__init__(spike_times, cluster, waveforms, **kwargs)\n\n    def ifr_sp_corr(\n        self,\n        x1,\n        speed,\n        minSpeed=2.0,\n        maxSpeed=40.0,\n        sigma=3,\n        shuffle=False,\n        nShuffles=100,\n        minTime=30,\n        plot=False,\n    ):\n        \"\"\"\n        Calculates the correlation between the instantaneous firing rate and\n        speed.\n\n        Args:\n            x1 (np.array): The indices of pos at which the cluster fired.\n            speed (np.array): Instantaneous speed (1 x nSamples).\n            minSpeed (float, optional): Speeds below this value are ignored.\n                Defaults to 2.0 cm/s as with Kropff et al., 2015.\n            maxSpeed (float, optional): Speeds above this value are ignored.\n                Defaults to 40.0 cm/s.\n            sigma (int, optional): The standard deviation of the gaussian used\n                to smooth the spike train. Defaults to 3.\n            shuffle (bool, optional): Whether to shuffle the spike train.\n                Defaults to False.\n            nShuffles (int, optional): The number of times to shuffle the\n                spike train. Defaults to 100.\n            minTime (int, optional): The minimum time for which the spike\n                train should be considered. Defaults to 30.\n            plot (bool, optional): Whether to plot the result.\n                Defaults to False.\n        \"\"\"\n        speed = speed.ravel()\n        posSampRate = 50\n        nSamples = len(speed)\n        # position is sampled at 50Hz and so is 'automatically' binned into\n        # 20ms bins\n        spk_hist = np.bincount(x1, minlength=nSamples)\n        # smooth the spk_hist (which is a temporal histogram) with a 250ms\n        # gaussian as with Kropff et al., 2015\n        h = signal.windows.gaussian(13, sigma)\n        h = h / float(np.sum(h))\n        # filter for low speeds\n        lowSpeedIdx = speed &lt; minSpeed\n        highSpeedIdx = speed &gt; maxSpeed\n        speed_filt = speed[~np.logical_or(lowSpeedIdx, highSpeedIdx)]\n        spk_hist_filt = spk_hist[~np.logical_or(lowSpeedIdx, highSpeedIdx)]\n        spk_sm = signal.filtfilt(h.ravel(), 1, spk_hist_filt)\n        sm_spk_rate = spk_sm * posSampRate\n        res = stats.pearsonr(sm_spk_rate, speed_filt)\n        if plot:\n            # do some fancy plotting stuff\n            _, sp_bin_edges = np.histogram(speed_filt, bins=50)\n            sp_dig = np.digitize(speed_filt, sp_bin_edges, right=True)\n            spks_per_sp_bin = [\n                spk_hist_filt[sp_dig == i] for i in range(len(sp_bin_edges))\n            ]\n            rate_per_sp_bin = []\n            for x in spks_per_sp_bin:\n                rate_per_sp_bin.append(np.mean(x) * posSampRate)\n            rate_filter = signal.windows.gaussian(5, 1.0)\n            rate_filter = rate_filter / np.sum(rate_filter)\n            binned_spk_rate = signal.filtfilt(rate_filter, 1, rate_per_sp_bin)\n            # instead of plotting a scatter plot of the firing rate at each\n            # speed bin, plot a log normalised heatmap and overlay results\n\n            spk_binning_edges = np.linspace(\n                np.min(sm_spk_rate), np.max(sm_spk_rate), len(sp_bin_edges)\n            )\n            speed_mesh, spk_mesh = np.meshgrid(sp_bin_edges, spk_binning_edges)\n            binned_rate, _, _ = np.histogram2d(\n                speed_filt, sm_spk_rate, bins=[sp_bin_edges, spk_binning_edges]\n            )\n            # blur the binned rate a bit to make it look nicer\n            from ephysiopy.common.utils import blurImage\n\n            sm_binned_rate = blurImage(binned_rate, 5)\n            fig = plt.figure()\n            ax = fig.add_subplot(111)\n            from matplotlib.colors import LogNorm\n\n            speed_mesh = speed_mesh[:-1, :-1]\n            spk_mesh = spk_mesh[:-1, :-1]\n            ax.pcolormesh(\n                speed_mesh,\n                spk_mesh,\n                sm_binned_rate,\n                norm=LogNorm(),\n                alpha=0.5,\n                shading=\"nearest\",\n                edgecolors=\"None\",\n            )\n            # overlay the smoothed binned rate against speed\n            ax.plot(sp_bin_edges, binned_spk_rate, \"r\")\n            # do the linear regression and plot the fit too\n            # TODO: linear regression is broken ie not regressing the correct\n            # variables\n            lr = stats.linregress(speed_filt, sm_spk_rate)\n            end_point = lr.intercept + ((sp_bin_edges[-1] - sp_bin_edges[0]) * lr.slope)\n            ax.plot(\n                [np.min(sp_bin_edges), np.max(sp_bin_edges)],\n                [lr.intercept, end_point],\n                \"r--\",\n            )\n            ax.set_xlim(np.min(sp_bin_edges), np.max(sp_bin_edges[-2]))\n            ax.set_ylim(0, np.nanmax(binned_spk_rate) * 1.1)\n            ax.set_ylabel(\"Firing rate(Hz)\")\n            ax.set_xlabel(\"Running speed(cm/s)\")\n            ax.set_title(\n                \"Intercept: {0:.3f}   Slope: {1:.5f}\\nPearson: {2:.5f}\".format(\n                    lr.intercept, lr.slope, lr.rvalue\n                )\n            )\n        # do some shuffling of the data to see if the result is signficant\n        if shuffle:\n            # shift spikes by at least 30 seconds after trial start and\n            # 30 seconds before trial end\n            timeSteps = np.random.randint(\n                30 * posSampRate, nSamples - (30 * posSampRate), nShuffles\n            )\n            shuffled_results = []\n            for t in timeSteps:\n                spk_count = np.roll(spk_hist, t)\n                spk_count_filt = spk_count[~lowSpeedIdx]\n                spk_count_sm = signal.filtfilt(h.ravel(), 1, spk_count_filt)\n                shuffled_results.append(stats.pearsonr(spk_count_sm, speed_filt)[0])\n            if plot:\n                fig = plt.figure()\n                ax = fig.add_subplot(1, 1, 1)\n                ax.hist(np.abs(shuffled_results), 20)\n                ylims = ax.get_ylim()\n                ax.vlines(res, ylims[0], ylims[1], \"r\")\n        if isinstance(fig, plt.Figure):\n            return fig\n</code></pre>"},{"location":"reference/#ephysiopy.common.spikecalcs.SpikeCalcsTetrode.ifr_sp_corr","title":"<code>ifr_sp_corr(x1, speed, minSpeed=2.0, maxSpeed=40.0, sigma=3, shuffle=False, nShuffles=100, minTime=30, plot=False)</code>","text":"<p>Calculates the correlation between the instantaneous firing rate and speed.</p> <p>Parameters:</p> Name Type Description Default <code>x1</code> <code>array</code> <p>The indices of pos at which the cluster fired.</p> required <code>speed</code> <code>array</code> <p>Instantaneous speed (1 x nSamples).</p> required <code>minSpeed</code> <code>float</code> <p>Speeds below this value are ignored. Defaults to 2.0 cm/s as with Kropff et al., 2015.</p> <code>2.0</code> <code>maxSpeed</code> <code>float</code> <p>Speeds above this value are ignored. Defaults to 40.0 cm/s.</p> <code>40.0</code> <code>sigma</code> <code>int</code> <p>The standard deviation of the gaussian used to smooth the spike train. Defaults to 3.</p> <code>3</code> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the spike train. Defaults to False.</p> <code>False</code> <code>nShuffles</code> <code>int</code> <p>The number of times to shuffle the spike train. Defaults to 100.</p> <code>100</code> <code>minTime</code> <code>int</code> <p>The minimum time for which the spike train should be considered. Defaults to 30.</p> <code>30</code> <code>plot</code> <code>bool</code> <p>Whether to plot the result. Defaults to False.</p> <code>False</code> Source code in <code>ephysiopy/common/spikecalcs.py</code> <pre><code>def ifr_sp_corr(\n    self,\n    x1,\n    speed,\n    minSpeed=2.0,\n    maxSpeed=40.0,\n    sigma=3,\n    shuffle=False,\n    nShuffles=100,\n    minTime=30,\n    plot=False,\n):\n    \"\"\"\n    Calculates the correlation between the instantaneous firing rate and\n    speed.\n\n    Args:\n        x1 (np.array): The indices of pos at which the cluster fired.\n        speed (np.array): Instantaneous speed (1 x nSamples).\n        minSpeed (float, optional): Speeds below this value are ignored.\n            Defaults to 2.0 cm/s as with Kropff et al., 2015.\n        maxSpeed (float, optional): Speeds above this value are ignored.\n            Defaults to 40.0 cm/s.\n        sigma (int, optional): The standard deviation of the gaussian used\n            to smooth the spike train. Defaults to 3.\n        shuffle (bool, optional): Whether to shuffle the spike train.\n            Defaults to False.\n        nShuffles (int, optional): The number of times to shuffle the\n            spike train. Defaults to 100.\n        minTime (int, optional): The minimum time for which the spike\n            train should be considered. Defaults to 30.\n        plot (bool, optional): Whether to plot the result.\n            Defaults to False.\n    \"\"\"\n    speed = speed.ravel()\n    posSampRate = 50\n    nSamples = len(speed)\n    # position is sampled at 50Hz and so is 'automatically' binned into\n    # 20ms bins\n    spk_hist = np.bincount(x1, minlength=nSamples)\n    # smooth the spk_hist (which is a temporal histogram) with a 250ms\n    # gaussian as with Kropff et al., 2015\n    h = signal.windows.gaussian(13, sigma)\n    h = h / float(np.sum(h))\n    # filter for low speeds\n    lowSpeedIdx = speed &lt; minSpeed\n    highSpeedIdx = speed &gt; maxSpeed\n    speed_filt = speed[~np.logical_or(lowSpeedIdx, highSpeedIdx)]\n    spk_hist_filt = spk_hist[~np.logical_or(lowSpeedIdx, highSpeedIdx)]\n    spk_sm = signal.filtfilt(h.ravel(), 1, spk_hist_filt)\n    sm_spk_rate = spk_sm * posSampRate\n    res = stats.pearsonr(sm_spk_rate, speed_filt)\n    if plot:\n        # do some fancy plotting stuff\n        _, sp_bin_edges = np.histogram(speed_filt, bins=50)\n        sp_dig = np.digitize(speed_filt, sp_bin_edges, right=True)\n        spks_per_sp_bin = [\n            spk_hist_filt[sp_dig == i] for i in range(len(sp_bin_edges))\n        ]\n        rate_per_sp_bin = []\n        for x in spks_per_sp_bin:\n            rate_per_sp_bin.append(np.mean(x) * posSampRate)\n        rate_filter = signal.windows.gaussian(5, 1.0)\n        rate_filter = rate_filter / np.sum(rate_filter)\n        binned_spk_rate = signal.filtfilt(rate_filter, 1, rate_per_sp_bin)\n        # instead of plotting a scatter plot of the firing rate at each\n        # speed bin, plot a log normalised heatmap and overlay results\n\n        spk_binning_edges = np.linspace(\n            np.min(sm_spk_rate), np.max(sm_spk_rate), len(sp_bin_edges)\n        )\n        speed_mesh, spk_mesh = np.meshgrid(sp_bin_edges, spk_binning_edges)\n        binned_rate, _, _ = np.histogram2d(\n            speed_filt, sm_spk_rate, bins=[sp_bin_edges, spk_binning_edges]\n        )\n        # blur the binned rate a bit to make it look nicer\n        from ephysiopy.common.utils import blurImage\n\n        sm_binned_rate = blurImage(binned_rate, 5)\n        fig = plt.figure()\n        ax = fig.add_subplot(111)\n        from matplotlib.colors import LogNorm\n\n        speed_mesh = speed_mesh[:-1, :-1]\n        spk_mesh = spk_mesh[:-1, :-1]\n        ax.pcolormesh(\n            speed_mesh,\n            spk_mesh,\n            sm_binned_rate,\n            norm=LogNorm(),\n            alpha=0.5,\n            shading=\"nearest\",\n            edgecolors=\"None\",\n        )\n        # overlay the smoothed binned rate against speed\n        ax.plot(sp_bin_edges, binned_spk_rate, \"r\")\n        # do the linear regression and plot the fit too\n        # TODO: linear regression is broken ie not regressing the correct\n        # variables\n        lr = stats.linregress(speed_filt, sm_spk_rate)\n        end_point = lr.intercept + ((sp_bin_edges[-1] - sp_bin_edges[0]) * lr.slope)\n        ax.plot(\n            [np.min(sp_bin_edges), np.max(sp_bin_edges)],\n            [lr.intercept, end_point],\n            \"r--\",\n        )\n        ax.set_xlim(np.min(sp_bin_edges), np.max(sp_bin_edges[-2]))\n        ax.set_ylim(0, np.nanmax(binned_spk_rate) * 1.1)\n        ax.set_ylabel(\"Firing rate(Hz)\")\n        ax.set_xlabel(\"Running speed(cm/s)\")\n        ax.set_title(\n            \"Intercept: {0:.3f}   Slope: {1:.5f}\\nPearson: {2:.5f}\".format(\n                lr.intercept, lr.slope, lr.rvalue\n            )\n        )\n    # do some shuffling of the data to see if the result is signficant\n    if shuffle:\n        # shift spikes by at least 30 seconds after trial start and\n        # 30 seconds before trial end\n        timeSteps = np.random.randint(\n            30 * posSampRate, nSamples - (30 * posSampRate), nShuffles\n        )\n        shuffled_results = []\n        for t in timeSteps:\n            spk_count = np.roll(spk_hist, t)\n            spk_count_filt = spk_count[~lowSpeedIdx]\n            spk_count_sm = signal.filtfilt(h.ravel(), 1, spk_count_filt)\n            shuffled_results.append(stats.pearsonr(spk_count_sm, speed_filt)[0])\n        if plot:\n            fig = plt.figure()\n            ax = fig.add_subplot(1, 1, 1)\n            ax.hist(np.abs(shuffled_results), 20)\n            ylims = ax.get_ylim()\n            ax.vlines(res, ylims[0], ylims[1], \"r\")\n    if isinstance(fig, plt.Figure):\n        return fig\n</code></pre>"},{"location":"reference/#ephysiopy.common.spikecalcs.cluster_quality","title":"<code>cluster_quality(waveforms=None, spike_clusters=None, cluster_id=None, fet=1)</code>","text":"<p>Returns the L-ratio and Isolation Distance measures calculated on the principal components of the energy in a spike matrix.</p> <p>Parameters:</p> Name Type Description Default <code>waveforms</code> <code>ndarray</code> <p>The waveforms to be processed. If None, the function will return None.</p> <code>None</code> <code>spike_clusters</code> <code>ndarray</code> <p>The spike clusters to be processed.</p> <code>None</code> <code>cluster_id</code> <code>int</code> <p>The ID of the cluster to be processed.</p> <code>None</code> <code>fet</code> <code>int, default=1</code> <p>The feature to be used in the PCA calculation.</p> <code>1</code> <p>Returns:</p> Name Type Description <code>tuple</code> <p>A tuple containing the L-ratio and Isolation Distance of the cluster.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If an error occurs during the calculation of the L-ratio or Isolation Distance.</p> Source code in <code>ephysiopy/common/spikecalcs.py</code> <pre><code>def cluster_quality(\n    waveforms: np.ndarray = None,\n    spike_clusters: np.ndarray = None,\n    cluster_id: int = None,\n    fet: int = 1,\n):\n    \"\"\"\n    Returns the L-ratio and Isolation Distance measures calculated\n    on the principal components of the energy in a spike matrix.\n\n    Args:\n        waveforms (np.ndarray, optional): The waveforms to be processed.\n            If None, the function will return None.\n        spike_clusters (np.ndarray, optional): The spike clusters to be\n            processed.\n        cluster_id (int, optional): The ID of the cluster to be processed.\n        fet (int, default=1): The feature to be used in the PCA calculation.\n\n    Returns:\n        tuple: A tuple containing the L-ratio and Isolation Distance of the\n            cluster.\n\n    Raises:\n        Exception: If an error occurs during the calculation of the L-ratio or\n            Isolation Distance.\n    \"\"\"\n    if waveforms is None:\n        return None\n    nSpikes, nElectrodes, _ = waveforms.shape\n    wvs = waveforms.copy()\n    E = np.sqrt(np.nansum(waveforms**2, axis=2))\n    zeroIdx = np.sum(E, 0) == [0, 0, 0, 0]\n    E = E[:, ~zeroIdx]\n    wvs = wvs[:, ~zeroIdx, :]\n    normdWaves = (wvs.T / E.T).T\n    PCA_m = get_param(normdWaves, \"PCA\", fet=fet)\n    badIdx = np.sum(PCA_m, axis=0) == 0\n    PCA_m = PCA_m[:, ~badIdx]\n    # get mahalanobis distance\n    idx = spike_clusters == cluster_id\n    nClustSpikes = np.count_nonzero(idx)\n    try:\n        d = mahal(PCA_m, PCA_m[idx, :])\n        # get the indices of the spikes not in the cluster\n        M_noise = d[~idx]\n        df = np.prod((fet, nElectrodes))\n        from scipy import stats\n\n        L = np.sum(1 - stats.chi2.cdf(M_noise, df))\n        L_ratio = L / nClustSpikes\n        # calculate isolation distance\n        if nClustSpikes &lt; nSpikes / 2:\n            M_noise.sort()\n            isolation_dist = M_noise[nClustSpikes]\n        else:\n            isolation_dist = np.nan\n    except Exception:\n        isolation_dist = L_ratio = np.nan\n    return L_ratio, isolation_dist\n</code></pre>"},{"location":"reference/#ephysiopy.common.spikecalcs.contamination_percent","title":"<code>contamination_percent(x1, x2=None, **kwargs)</code>","text":"<p>Computes the cross-correlogram between two sets of spikes and estimates how refractory the cross-correlogram is.</p> <p>Parameters:</p> Name Type Description Default <code>st1</code> <code>array</code> <p>The first set of spikes.</p> required <code>st2</code> <code>array</code> <p>The second set of spikes.</p> required kwargs <p>Anything that can be fed into xcorr above</p> <p>Returns:</p> Name Type Description <code>Q</code> <code>float</code> <p>a measure of refractoriness</p> <code>R</code> <code>float</code> <p>a second measure of refractoriness     (kicks in for very low firing rates)</p> Notes <p>Taken from KiloSorts ccg.m</p> <p>The contamination metrics are calculated based on an analysis of the 'shoulders' of the cross-correlogram. Specifically, the spike counts in the ranges +/-5-25ms and +/-250-500ms are compared for refractoriness</p> Source code in <code>ephysiopy/common/spikecalcs.py</code> <pre><code>def contamination_percent(x1: np.ndarray, x2: np.ndarray = None, **kwargs) -&gt; tuple:\n    \"\"\"\n    Computes the cross-correlogram between two sets of spikes and\n    estimates how refractory the cross-correlogram is.\n\n    Args:\n        st1 (np.array): The first set of spikes.\n        st2 (np.array): The second set of spikes.\n\n    kwargs:\n        Anything that can be fed into xcorr above\n\n    Returns:\n        Q (float): a measure of refractoriness\n        R (float): a second measure of refractoriness\n                (kicks in for very low firing rates)\n\n    Notes:\n        Taken from KiloSorts ccg.m\n\n        The contamination metrics are calculated based on\n        an analysis of the 'shoulders' of the cross-correlogram.\n        Specifically, the spike counts in the ranges +/-5-25ms and\n        +/-250-500ms are compared for refractoriness\n    \"\"\"\n    if x2 is None:\n        x2 = x1.copy()\n    c, b = xcorr(x1, x2, **kwargs)\n    left = [[-0.05, -0.01]]\n    right = [[0.01, 0.051]]\n    far = [[-0.5, -0.249], [0.25, 0.501]]\n\n    def get_shoulder(bins, vals):\n        all = np.array([np.logical_and(bins &gt;= i[0], bins &lt; i[1]) for i in vals])\n        return np.any(all, 0)\n\n    inner_left = get_shoulder(b, left)\n    inner_right = get_shoulder(b, right)\n    outer = get_shoulder(b, far)\n\n    tbin = 1000\n    Tr = max(np.concatenate([x1, x2])) - min(np.concatenate([x1, x2]))\n\n    def get_normd_shoulder(idx):\n        return np.sum(c[idx[:-1]]) / (\n            len(np.nonzero(idx)[0]) * tbin * len(x1) * len(x2) / Tr\n        )\n\n    Q00 = get_normd_shoulder(outer)\n    Q01 = max(get_normd_shoulder(inner_left), get_normd_shoulder(inner_right))\n\n    R00 = max(\n        np.mean(c[outer[:-1]]), np.mean(c[inner_left[:-1]]), np.mean(c[inner_right[1:]])\n    )\n\n    middle_idx = np.nonzero(b == 0)[0]\n    a = c[middle_idx]\n    c[middle_idx] = 0\n    Qi = np.zeros(10)\n    Ri = np.zeros(10)\n    # enumerate through the central range of the xcorr\n    # saving the same calculation as done above\n    for i, t in enumerate(np.linspace(0.001, 0.01, 10)):\n        irange = [[-t, t]]\n        chunk = get_shoulder(b, irange)\n        # compute the same normalized ratio as above;\n        # this should be 1 if there is no refractoriness\n        Qi[i] = get_normd_shoulder(chunk)  # save the normd prob\n        n = np.sum(c[chunk[:-1]]) / 2\n        lam = R00 * i\n        # this is tricky: we approximate the Poisson likelihood with a\n        # gaussian of equal mean and variance\n        # that allows us to integrate the probability that we would see &lt;N\n        # spikes in the center of the\n        # cross-correlogram from a distribution with mean R00*i spikes\n        p = 1 / 2 * (1 + erf((n - lam) / np.sqrt(2 * lam)))\n\n        Ri[i] = p  # keep track of p for each bin size i\n\n    c[middle_idx] = a  # restore the center value of the cross-correlogram\n    return c, Qi, Q00, Q01, Ri\n</code></pre>"},{"location":"reference/#ephysiopy.common.spikecalcs.get_param","title":"<code>get_param(waveforms, param='Amp', t=200, fet=1)</code>","text":"<p>Returns the requested parameter from a spike train as a numpy array</p> <p>Parameters:</p> Name Type Description Default <code>waveforms</code> <code>numpy array</code> <p>Shape of array can be nSpikes x nSamples OR a nSpikes x nElectrodes x nSamples</p> required <code>param</code> <code>str</code> <p>Valid values are: 'Amp' - peak-to-trough amplitude (default) 'P' - height of peak 'T' - depth of trough 'Vt' height at time t 'tP' - time of peak (in seconds) 'tT' - time of trough (in seconds) 'PCA' - first n fet principal components (defaults to 1)</p> <code>'Amp'</code> <code>t</code> <code>int</code> <p>The time used for Vt</p> <code>200</code> <code>fet</code> <code>int</code> <p>The number of principal components (use with param 'PCA')</p> <code>1</code> Source code in <code>ephysiopy/common/spikecalcs.py</code> <pre><code>def get_param(waveforms, param=\"Amp\", t=200, fet=1):\n    \"\"\"\n    Returns the requested parameter from a spike train as a numpy array\n\n    Args:\n        waveforms (numpy array): Shape of array can be nSpikes x nSamples\n            OR\n            a nSpikes x nElectrodes x nSamples\n        param (str): Valid values are:\n            'Amp' - peak-to-trough amplitude (default)\n            'P' - height of peak\n            'T' - depth of trough\n            'Vt' height at time t\n            'tP' - time of peak (in seconds)\n            'tT' - time of trough (in seconds)\n            'PCA' - first n fet principal components (defaults to 1)\n        t (int): The time used for Vt\n        fet (int): The number of principal components\n            (use with param 'PCA')\n    \"\"\"\n    from scipy import interpolate\n    from sklearn.decomposition import PCA\n\n    if param == \"Amp\":\n        return np.ptp(waveforms, axis=-1)\n    elif param == \"P\":\n        return np.max(waveforms, axis=-1)\n    elif param == \"T\":\n        return np.min(waveforms, axis=-1)\n    elif param == \"Vt\":\n        times = np.arange(0, 1000, 20)\n        f = interpolate.interp1d(times, range(50), \"nearest\")\n        if waveforms.ndim == 2:\n            return waveforms[:, int(f(t))]\n        elif waveforms.ndim == 3:\n            return waveforms[:, :, int(f(t))]\n    elif param == \"tP\":\n        idx = np.argmax(waveforms, axis=-1)\n        m = interpolate.interp1d([0, waveforms.shape[-1] - 1], [0, 1 / 1000.0])\n        return m(idx)\n    elif param == \"tT\":\n        idx = np.argmin(waveforms, axis=-1)\n        m = interpolate.interp1d([0, waveforms.shape[-1] - 1], [0, 1 / 1000.0])\n        return m(idx)\n    elif param == \"PCA\":\n        pca = PCA(n_components=fet)\n        if waveforms.ndim == 2:\n            return pca.fit(waveforms).transform(waveforms).squeeze()\n        elif waveforms.ndim == 3:\n            out = np.zeros((waveforms.shape[0], waveforms.shape[1] * fet))\n            st = np.arange(0, waveforms.shape[1] * fet, fet)\n            en = np.arange(fet, fet + (waveforms.shape[1] * fet), fet)\n            rng = np.vstack((st, en))\n            for i in range(waveforms.shape[1]):\n                if ~np.any(np.isnan(waveforms[:, i, :])):\n                    A = np.squeeze(\n                        pca.fit(waveforms[:, i, :].squeeze()).transform(\n                            waveforms[:, i, :].squeeze()\n                        )\n                    )\n                    if A.ndim &lt; 2:\n                        out[:, rng[0, i] : rng[1, i]] = np.atleast_2d(A).T\n                    else:\n                        out[:, rng[0, i] : rng[1, i]] = A\n            return out\n</code></pre>"},{"location":"reference/#ephysiopy.common.spikecalcs.mahal","title":"<code>mahal(u, v)</code>","text":"<p>Returns the L-ratio and Isolation Distance measures calculated on the principal components of the energy in a spike matrix.</p> <p>Parameters:</p> Name Type Description Default <code>waveforms</code> <code>ndarray</code> <p>The waveforms to be processed. If None, the function will return None.</p> required <code>spike_clusters</code> <code>ndarray</code> <p>The spike clusters to be processed.</p> required <code>cluster_id</code> <code>int</code> <p>The ID of the cluster to be processed.</p> required <code>fet</code> <code>int, default=1</code> <p>The feature to be used in the PCA calculation.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>A tuple containing the L-ratio and Isolation Distance of the cluster.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If an error occurs during the calculation of the L-ratio or Isolation Distance.</p> Source code in <code>ephysiopy/common/spikecalcs.py</code> <pre><code>def mahal(u, v):\n    \"\"\"\n    Returns the L-ratio and Isolation Distance measures calculated on the\n    principal components of the energy in a spike matrix.\n\n    Args:\n        waveforms (np.ndarray, optional): The waveforms to be processed. If\n            None, the function will return None.\n        spike_clusters (np.ndarray, optional): The spike clusters to be\n            processed.\n        cluster_id (int, optional): The ID of the cluster to be processed.\n        fet (int, default=1): The feature to be used in the PCA calculation.\n\n    Returns:\n        tuple: A tuple containing the L-ratio and Isolation Distance of the\n            cluster.\n\n    Raises:\n        Exception: If an error occurs during the calculation of the L-ratio or\n            Isolation Distance.\n    \"\"\"\n    u_sz = u.shape\n    v_sz = v.shape\n    if u_sz[1] != v_sz[1]:\n        warnings.warn(\n            \"Input size mismatch: \\\n                        matrices must have same num of columns\"\n        )\n    if v_sz[0] &lt; v_sz[1]:\n        warnings.warn(\"Too few rows: v must have more rows than columns\")\n    if np.any(np.imag(u)) or np.any(np.imag(v)):\n        warnings.warn(\"No complex inputs are allowed\")\n    m = np.nanmean(v, axis=0)\n    M = np.tile(m, reps=(u_sz[0], 1))\n    C = v - np.tile(m, reps=(v_sz[0], 1))\n    _, R = np.linalg.qr(C)\n    ri = np.linalg.solve(R.T, (u - M).T)\n    d = np.sum(ri * ri, 0).T * (v_sz[0] - 1)\n    return d\n</code></pre>"},{"location":"reference/#ephysiopy.common.spikecalcs.xcorr","title":"<code>xcorr(x1, x2=None, Trange=None, binsize=0.001, **kwargs)</code>","text":"<p>Calculates the ISIs in x1 or x1 vs x2 within a given range</p> <p>Parameters:</p> Name Type Description Default <code>x1,</code> <code>x2 (array_like</code> <p>The times of the spikes emitted by the                 cluster(s) in seconds</p> required <code>Trange</code> <code>array_like</code> <p>Range of times to bin up in seconds                     Defaults to [-0.5, +0.5]</p> <code>None</code> <code>binsize</code> <code>float</code> <p>The size of the bins in seconds</p> <code>0.001</code> <p>Returns:</p> Name Type Description <code>counts</code> <code>ndarray</code> <p>The cross-correlogram of the spike trains x1 and x2</p> <code>bins</code> <code>ndarray</code> <p>The bins used to calculate the cross-correlogram</p> Source code in <code>ephysiopy/common/spikecalcs.py</code> <pre><code>def xcorr(x1: np.ndarray, x2=None, Trange=None, binsize=0.001, **kwargs) -&gt; tuple:\n    \"\"\"\n    Calculates the ISIs in x1 or x1 vs x2 within a given range\n\n    Args:\n        x1, x2 (array_like): The times of the spikes emitted by the\n                            cluster(s) in seconds\n        Trange (array_like): Range of times to bin up in seconds\n                                Defaults to [-0.5, +0.5]\n        binsize (float): The size of the bins in seconds\n\n    Returns:\n        counts (np.ndarray): The cross-correlogram of the spike trains\n            x1 and x2\n        bins (np.ndarray): The bins used to calculate the cross-correlogram\n    \"\"\"\n    if x2 is None:\n        x2 = x1.copy()\n    if Trange is None:\n        Trange = np.array([-0.5, 0.5])\n    if isinstance(Trange, list):\n        Trange = np.array(Trange)\n    y = []\n    irange = x1[:, np.newaxis] + Trange[np.newaxis, :]\n    dts = np.searchsorted(x2, irange)\n    for i, t in enumerate(dts):\n        y.extend((x2[t[0] : t[1]] - x1[i]))\n    y = np.array(y, dtype=float)\n    counts, bins = np.histogram(\n        y[y != 0], bins=int(np.ptp(Trange) / binsize) + 1, range=Trange\n    )\n    return counts, bins\n</code></pre>"},{"location":"reference/#statistics","title":"Statistics","text":""},{"location":"reference/#ephysiopy.common.statscalcs.V_test","title":"<code>V_test(angles, test_direction)</code>","text":"<p>The Watson U2 tests whether the observed angles have a tendency to cluster around a given angle indicating a lack of randomness in the distribution. Also known as the modified Rayleigh test.</p> <p>Parameters:</p> Name Type Description Default <code>angles</code> <code>array_like</code> <p>Vector of angular values in degrees.</p> required <code>test_direction</code> <code>int</code> <p>A single angular value in degrees.</p> required Notes <p>For grouped data the length of the mean vector must be adjusted, and for axial data all angles must be doubled.</p> Source code in <code>ephysiopy/common/statscalcs.py</code> <pre><code>def V_test(angles, test_direction):\n    \"\"\"\n    The Watson U2 tests whether the observed angles have a tendency to\n    cluster around a given angle indicating a lack of randomness in the\n    distribution. Also known as the modified Rayleigh test.\n\n    Args:\n        angles (array_like): Vector of angular values in degrees.\n        test_direction (int): A single angular value in degrees.\n\n    Notes:\n        For grouped data the length of the mean vector must be adjusted,\n        and for axial data all angles must be doubled.\n    \"\"\"\n    n = len(angles)\n    x_hat = np.sum(np.cos(np.radians(angles))) / float(n)\n    y_hat = np.sum(np.sin(np.radians(angles))) / float(n)\n    r = np.sqrt(x_hat**2 + y_hat**2)\n    theta_hat = np.degrees(np.arctan(y_hat / x_hat))\n    v_squiggle = r * np.cos(\n        np.radians(theta_hat) - np.radians(test_direction))\n    V = np.sqrt(2 * n) * v_squiggle\n    return V\n</code></pre>"},{"location":"reference/#ephysiopy.common.statscalcs.circ_r","title":"<code>circ_r(alpha, w=None, d=0, axis=0)</code>","text":"<p>Computes the mean resultant vector length for circular data.</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>array or list</code> <p>Sample of angles in radians.</p> required <code>w</code> <code>array or list</code> <p>Counts in the case of binned data. Must be same length as alpha.</p> <code>None</code> <code>d</code> <code>array or list</code> <p>Spacing of bin centres for binned data; if supplied, correction factor is used to correct for bias in estimation of r, in radians.</p> <code>0</code> <code>axis</code> <code>int</code> <p>The dimension along which to compute. Default is 0.</p> <code>0</code> <p>Returns:</p> Name Type Description <code>r</code> <code>float</code> <p>The mean resultant vector length.</p> Source code in <code>ephysiopy/common/statscalcs.py</code> <pre><code>def circ_r(alpha, w=None, d=0, axis=0):\n    \"\"\"\n    Computes the mean resultant vector length for circular data.\n\n    Args:\n        alpha (array or list): Sample of angles in radians.\n        w (array or list): Counts in the case of binned data.\n            Must be same length as alpha.\n        d (array or list, optional): Spacing of bin centres for binned data; if\n            supplied, correction factor is used to correct for bias in\n            estimation of r, in radians.\n        axis (int, optional): The dimension along which to compute.\n            Default is 0.\n\n    Returns:\n        r (float): The mean resultant vector length.\n    \"\"\"\n\n    if w is None:\n        w = np.ones_like(alpha, dtype=float)\n    # TODO: error check for size constancy\n    r = np.sum(w * np.exp(1j * alpha))\n    r = np.abs(r) / np.sum(w)\n    if d != 0:\n        c = d/2./np.sin(d/2.)\n        r = c * r\n    return r\n</code></pre>"},{"location":"reference/#ephysiopy.common.statscalcs.duplicates_as_complex","title":"<code>duplicates_as_complex(x, already_sorted=False)</code>","text":"<p>Finds duplicates in x</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array_like</code> <p>The list to find duplicates in.</p> required <code>already_sorted</code> <code>bool</code> <p>Whether x is already sorted. Default False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>x</code> <code>array_like</code> <p>A complex array where the complex part is the count of the number of duplicates of the real value.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt;     x = [9.9, 9.9, 12.3, 15.2, 15.2, 15.2]\n&gt;&gt;&gt; ret = duplicates_as_complex(x)\n&gt;&gt;&gt;     print(ret)\n[9.9+0j, 9.9+1j,  12.3+0j, 15.2+0j, 15.2+1j, 15.2+2j]\n</code></pre> Source code in <code>ephysiopy/common/statscalcs.py</code> <pre><code>def duplicates_as_complex(x, already_sorted=False):\n    \"\"\"\n    Finds duplicates in x\n\n    Args:\n        x (array_like): The list to find duplicates in.\n        already_sorted (bool, optional): Whether x is already sorted.\n            Default False.\n\n    Returns:\n        x (array_like): A complex array where the complex part is the count of\n            the number of duplicates of the real value.\n\n    Examples:\n        &gt;&gt;&gt;\tx = [9.9, 9.9, 12.3, 15.2, 15.2, 15.2]\n        &gt;&gt;&gt; ret = duplicates_as_complex(x)\n        &gt;&gt;&gt;\tprint(ret)\n        [9.9+0j, 9.9+1j,  12.3+0j, 15.2+0j, 15.2+1j, 15.2+2j]\n    \"\"\"\n\n    if not already_sorted:\n        x = np.sort(x)\n    is_start = np.empty(len(x), dtype=bool)\n    is_start[0], is_start[1:] = True, x[:-1] != x[1:]\n    labels = np.cumsum(is_start)-1\n    sub_idx = np.arange(len(x)) - np.nonzero(is_start)[0][labels]\n    return x + 1j*sub_idx\n</code></pre>"},{"location":"reference/#ephysiopy.common.statscalcs.mean_resultant_vector","title":"<code>mean_resultant_vector(angles)</code>","text":"<p>Calculate the mean resultant length and direction for angles.</p> <p>Parameters:</p> Name Type Description Default <code>angles</code> <code>array</code> <p>Sample of angles in radians.</p> required <p>Returns:</p> Name Type Description <code>r</code> <code>float</code> <p>The mean resultant vector length.</p> <code>th</code> <code>float</code> <p>The mean resultant vector direction.</p> Source code in <code>ephysiopy/common/statscalcs.py</code> <pre><code>def mean_resultant_vector(angles):\n    \"\"\"\n    Calculate the mean resultant length and direction for angles.\n\n    Args:\n        angles (np.array): Sample of angles in radians.\n\n    Returns:\n        r (float): The mean resultant vector length.\n        th (float): The mean resultant vector direction.\n    \"\"\"\n    S = np.sum(np.sin(angles)) * (1/float(len(angles)))\n    C = np.sum(np.cos(angles)) * (1/float(len(angles)))\n    r = np.hypot(S, C)\n    th = np.arctan(S / C)\n    if (C &lt; 0):\n        th = np.pi + th\n    return r, th\n</code></pre>"},{"location":"reference/#ephysiopy.common.statscalcs.watsonWilliams","title":"<code>watsonWilliams(a, b)</code>","text":"<p>The Watson-Williams F test tests whether a set of mean directions are equal given that the concentrations are unknown, but equal, given that the groups each follow a von Mises distribution.</p> <p>Parameters:</p> Name Type Description Default <code>a,</code> <code>b (array_like</code> <p>The directional samples</p> required <p>Returns:</p> Name Type Description <code>F_stat</code> <code>float</code> <p>The F-statistic</p> Source code in <code>ephysiopy/common/statscalcs.py</code> <pre><code>def watsonWilliams(a, b):\n    \"\"\"\n    The Watson-Williams F test tests whether a set of mean directions are\n    equal given that the concentrations are unknown, but equal, given that\n    the groups each follow a von Mises distribution.\n\n    Args:\n        a, b (array_like): The directional samples\n\n    Returns:\n        F_stat (float): The F-statistic\n    \"\"\"\n\n    n = len(a)\n    m = len(b)\n    N = n + m\n    # v_1 = 1 # needed to do p-value lookup in table of critical values\n    #  of F distribution\n    # v_2 = N - 2 # needed to do p-value lookup in table of critical values\n    # of F distribution\n    C_1 = np.sum(np.cos(np.radians(a)))\n    S_1 = np.sum(np.sin(np.radians(a)))\n    C_2 = np.sum(np.cos(np.radians(b)))\n    S_2 = np.sum(np.sin(np.radians(b)))\n    C = C_1 + C_2\n    S = S_1 + S_2\n    R_1 = np.hypot(C_1, S_1)\n    R_2 = np.hypot(C_2, S_2)\n    R = np.hypot(C, S)\n    R_hat = (R_1 + R_2) / float(N)\n    from ephysiopy.common.mle_von_mises_vals import vals\n    mle_von_mises = np.array(vals)\n    mle_von_mises = np.sort(mle_von_mises, 0)\n    k_hat = mle_von_mises[(np.abs(mle_von_mises[:, 0]-R_hat)).argmin(), 1]\n    g = 1 - (3 / 8 * k_hat)\n    F = g * (N-2) * ((R_1 + R_2 - R) / (N - (R_1 + R_2)))\n    return F\n</code></pre>"},{"location":"reference/#ephysiopy.common.statscalcs.watsonsU2","title":"<code>watsonsU2(a, b)</code>","text":"<p>Tests whether two samples from circular observations differ significantly  from each other with regard to mean direction or angular variance.</p> <p>Parameters:</p> Name Type Description Default <code>a,</code> <code>b (array_like</code> <p>The two samples to be tested</p> required <p>Returns:</p> Name Type Description <code>U2</code> <code>float</code> <p>The test statistic</p> Notes <p>Both samples must come from a continuous distribution. In the case of grouping the class interval should not exceed 5. Taken from '100 Statistical Tests' G.J.Kanji, 2006 Sage Publications</p> Source code in <code>ephysiopy/common/statscalcs.py</code> <pre><code>def watsonsU2(a, b):\n    \"\"\"\n    Tests whether two samples from circular observations differ significantly \n    from each other with regard to mean direction or angular variance.\n\n    Args:\n        a, b (array_like): The two samples to be tested\n\n    Returns:\n        U2 (float): The test statistic\n\n    Notes:\n        Both samples must come from a continuous distribution. In the case of\n        grouping the class interval should not exceed 5.\n        Taken from '100 Statistical Tests' G.J.Kanji, 2006 Sage Publications\n    \"\"\"\n\n    a = np.sort(np.ravel(a))\n    b = np.sort(np.ravel(b))\n    n_a = len(a)\n    n_b = len(b)\n    N = float(n_a + n_b)\n    a_complex = duplicates_as_complex(a, True)\n    b_complex = duplicates_as_complex(b, True)\n    a_and_b = np.union1d(a_complex, b_complex)\n\n    # get index for a\n    a_ind = np.zeros(len(a_and_b), dtype=int)\n    a_ind[np.searchsorted(a_and_b, a_complex)] = 1\n    a_ind = np.cumsum(a_ind)\n\n    # same for b\n    b_ind = np.zeros(len(a_and_b), dtype=int)\n    b_ind[np.searchsorted(a_and_b, b_complex)] = 1\n    b_ind = np.cumsum(b_ind)\n\n    d_k = (a_ind / float(n_a)) - (b_ind / float(n_b))\n\n    d_k_sq = d_k ** 2\n\n    U2 = ((n_a*n_b) / N**2) * (np.sum(d_k_sq) - ((np.sum(d_k)**2) / N))\n    return U2\n</code></pre>"},{"location":"reference/#ephysiopy.common.statscalcs.watsonsU2n","title":"<code>watsonsU2n(angles)</code>","text":"<p>Tests whether the given distribution fits a random sample of angular values.</p> <p>Parameters:</p> Name Type Description Default <code>angles</code> <code>array_like</code> <p>The angular samples.</p> required <p>Returns:</p> Name Type Description <code>U2n</code> <code>float</code> <p>The test statistic.</p> Notes <p>This test is suitable for both unimodal and the multimodal cases. It can be used as a test for randomness. Taken from '100 Statistical Tests' G.J.Kanji, 2006 Sage Publications.</p> Source code in <code>ephysiopy/common/statscalcs.py</code> <pre><code>def watsonsU2n(angles):\n    \"\"\"\n    Tests whether the given distribution fits a random sample of angular\n    values.\n\n    Args:\n        angles (array_like): The angular samples.\n\n    Returns:\n        U2n (float): The test statistic.\n\n    Notes:\n        This test is suitable for both unimodal and the multimodal cases.\n        It can be used as a test for randomness.\n        Taken from '100 Statistical Tests' G.J.Kanji, 2006 Sage Publications.\n    \"\"\"\n\n    angles = np.sort(angles)\n    n = len(angles)\n    Vi = angles / float(360)\n    sum_Vi = np.sum(Vi)\n    sum_sq_Vi = np.sum(Vi**2)\n    Ci = (2 * np.arange(1, n+1)) - 1\n    sum_Ci_Vi_ov_n = np.sum(Ci * Vi / n)\n    V_bar = (1 / float(n)) * sum_Vi\n    U2n = sum_sq_Vi - sum_Ci_Vi_ov_n + (\n        n * (1/float(3) - (V_bar - 0.5)**2))\n    test_vals = {\n        '0.1': 0.152, '0.05': 0.187, '0.025': 0.221,\n        '0.01': 0.267, '0.005': 0.302}\n    for key, val in test_vals.items():\n        if U2n &gt; val:\n            print('The Watsons U2 statistic is {0} which is \\\n                greater than\\n the critical value of {1} at p={2}'.format(\n                    U2n, val, key))\n        else:\n            print('The Watsons U2 statistic is not \\\n                significant at p={0}'.format(key))\n    return U2n\n</code></pre>"},{"location":"reference/#utility-functions","title":"Utility functions","text":""},{"location":"reference/#ephysiopy.common.utils.blurImage","title":"<code>blurImage(im, n, ny=None, ftype='boxcar', **kwargs)</code>","text":"<p>Smooths a 2D image by convolving with a filter.</p> <p>Parameters:</p> Name Type Description Default <code>im</code> <code>array_like</code> <p>The array to smooth.</p> required <code>n,</code> <code>ny (int</code> <p>The size of the smoothing kernel.</p> required <code>ftype</code> <code>str</code> <p>The type of smoothing kernel. Either 'boxcar' or 'gaussian'.</p> <code>'boxcar'</code> <p>Returns:</p> Name Type Description <code>res</code> <code>array_like</code> <p>The smoothed vector with shape the same as im.</p> Source code in <code>ephysiopy/common/utils.py</code> <pre><code>def blurImage(im, n, ny=None, ftype='boxcar', **kwargs):\n    \"\"\"\n    Smooths a 2D image by convolving with a filter.\n\n    Args:\n        im (array_like): The array to smooth.\n        n, ny (int): The size of the smoothing kernel.\n        ftype (str): The type of smoothing kernel.\n            Either 'boxcar' or 'gaussian'.\n\n    Returns:\n        res (array_like): The smoothed vector with shape the same as im.\n    \"\"\"\n    if 'stddev' in kwargs.keys():\n        stddev = kwargs.pop('stddev')\n    else:\n        stddev = 5\n    n = int(n)\n    if not ny:\n        ny = n\n    else:\n        ny = int(ny)\n    ndims = im.ndim\n    if 'box' in ftype:\n        if ndims == 1:\n            g = cnv.Box1DKernel(n)\n        elif ndims == 2:\n            g = cnv.Box2DKernel(n)\n        elif ndims == 3:  # mutlidimensional binning\n            g = cnv.Box2DKernel(n)\n            g = np.atleast_3d(g).T\n    elif 'gaussian' in ftype:\n        if ndims == 1:\n            g = cnv.Gaussian1DKernel(stddev, x_size=n)\n        if ndims == 2:\n            g = cnv.Gaussian2DKernel(stddev, x_size=n, y_size=ny)\n        if ndims == 3:\n            g = cnv.Gaussian2DKernel(stddev, x_size=n, y_size=ny)\n            g = np.atleast_3d(g).T\n    return cnv.convolve(im, g, boundary='extend')\n</code></pre>"},{"location":"reference/#ephysiopy.common.utils.bwperim","title":"<code>bwperim(bw, n=4)</code>","text":"<p>Finds the perimeter of objects in binary images.</p> <p>A pixel is part of an object perimeter if its value is one and there is at least one zero-valued pixel in its neighborhood.</p> <p>By default the neighborhood of a pixel is 4 nearest pixels, but if <code>n</code> is set to 8 the 8 nearest pixels will be considered.</p> <p>Parameters:</p> Name Type Description Default <code>bw</code> <code>array_like</code> <p>A black-and-white image.</p> required <code>n</code> <code>int</code> <p>Connectivity. Must be 4 or 8. Default is 8.</p> <code>4</code> <p>Returns:</p> Name Type Description <code>perim</code> <code>array_like</code> <p>A boolean image.</p> Source code in <code>ephysiopy/common/utils.py</code> <pre><code>def bwperim(bw, n=4):\n    \"\"\"\n    Finds the perimeter of objects in binary images.\n\n    A pixel is part of an object perimeter if its value is one and there\n    is at least one zero-valued pixel in its neighborhood.\n\n    By default the neighborhood of a pixel is 4 nearest pixels, but\n    if `n` is set to 8 the 8 nearest pixels will be considered.\n\n    Args:\n        bw (array_like): A black-and-white image.\n        n (int, optional): Connectivity. Must be 4 or 8. Default is 8.\n\n    Returns:\n        perim (array_like): A boolean image.\n    \"\"\"\n\n    if n not in (4, 8):\n        raise ValueError('mahotas.bwperim: n must be 4 or 8')\n    rows, cols = bw.shape\n\n    # Translate image by one pixel in all directions\n    north = np.zeros((rows, cols))\n    south = np.zeros((rows, cols))\n    west = np.zeros((rows, cols))\n    east = np.zeros((rows, cols))\n\n    north[:-1, :] = bw[1:, :]\n    south[1:, :] = bw[:-1, :]\n    west[:, :-1] = bw[:, 1:]\n    east[:, 1:] = bw[:, :-1]\n    idx = (north == bw) &amp; \\\n          (south == bw) &amp; \\\n          (west == bw) &amp; \\\n          (east == bw)\n    if n == 8:\n        north_east = np.zeros((rows, cols))\n        north_west = np.zeros((rows, cols))\n        south_east = np.zeros((rows, cols))\n        south_west = np.zeros((rows, cols))\n        north_east[:-1, 1:] = bw[1:, :-1]\n        north_west[:-1, :-1] = bw[1:, 1:]\n        south_east[1:, 1:] = bw[:-1, :-1]\n        south_west[1:, :-1] = bw[:-1, 1:]\n        idx &amp;= (north_east == bw) &amp; \\\n               (south_east == bw) &amp; \\\n               (south_west == bw) &amp; \\\n               (north_west == bw)\n    return ~idx * bw\n</code></pre>"},{"location":"reference/#ephysiopy.common.utils.count_to","title":"<code>count_to(n)</code>","text":"<p>This function is equivalent to hstack((arange(n_i) for n_i in n)). It seems to be faster for some possible inputs and encapsulates a task in a function.</p> Example <p>Given n = [0, 0, 3, 0, 0, 2, 0, 2, 1], the result would be [0, 1, 2, 0, 1, 0, 1, 0].</p> Source code in <code>ephysiopy/common/utils.py</code> <pre><code>def count_to(n):\n    \"\"\"\n    This function is equivalent to hstack((arange(n_i) for n_i in n)).\n    It seems to be faster for some possible inputs and encapsulates\n    a task in a function.\n\n    Example:\n        Given n = [0, 0, 3, 0, 0, 2, 0, 2, 1],\n        the result would be [0, 1, 2, 0, 1, 0, 1, 0].\n    \"\"\"\n    if n.ndim != 1:\n        raise Exception(\"n is supposed to be 1d array.\")\n\n    n_mask = n.astype(bool)\n    n_cumsum = np.cumsum(n)\n    ret = np.ones(n_cumsum[-1]+1, dtype=int)\n    ret[n_cumsum[n_mask]] -= n[n_mask]\n    ret[0] -= 1\n    return np.cumsum(ret)[:-1]\n</code></pre>"},{"location":"reference/#ephysiopy.common.utils.get_z_score","title":"<code>get_z_score(x, mean=None, sd=None, axis=0)</code>","text":"<p>Calculate the z-scores for array x based on the mean and standard deviation in that sample, unless stated</p> Source code in <code>ephysiopy/common/utils.py</code> <pre><code>def get_z_score(x: np.ndarray,\n                mean=None,\n                sd=None,\n                axis=0) -&gt; np.ndarray:\n    '''\n    Calculate the z-scores for array x based on the mean\n    and standard deviation in that sample, unless stated\n    '''\n    if mean is None:\n        mean = np.mean(x, axis=axis)\n    if sd is None:\n        sd = np.std(x, axis=axis)\n    return (x - mean) / sd\n</code></pre>"},{"location":"reference/#ephysiopy.common.utils.polar","title":"<code>polar(x, y, deg=False)</code>","text":"<p>Converts from rectangular coordinates to polar ones.</p> <p>Parameters:</p> Name Type Description Default <code>x,</code> <code>y (array_like, list_like</code> <p>The x and y coordinates.</p> required <code>deg</code> <code>int</code> <p>Radian if deg=0; degree if deg=1.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>p</code> <code>array_like</code> <p>The polar version of x and y.</p> Source code in <code>ephysiopy/common/utils.py</code> <pre><code>def polar(x, y, deg=False):\n    \"\"\"\n    Converts from rectangular coordinates to polar ones.\n\n    Args:\n        x, y (array_like, list_like): The x and y coordinates.\n        deg (int): Radian if deg=0; degree if deg=1.\n\n    Returns:\n        p (array_like): The polar version of x and y.\n    \"\"\"\n    if deg:\n        return np.hypot(x, y), 180.0 * np.arctan2(y, x) / np.pi\n    else:\n        return np.hypot(x, y), np.arctan2(y, x)\n</code></pre>"},{"location":"reference/#ephysiopy.common.utils.rect","title":"<code>rect(r, w, deg=False)</code>","text":"<p>Convert from polar (r,w) to rectangular (x,y) x = r cos(w) y = r sin(w)</p> Source code in <code>ephysiopy/common/utils.py</code> <pre><code>def rect(r, w, deg=False):\n    \"\"\"\n    Convert from polar (r,w) to rectangular (x,y)\n    x = r cos(w)\n    y = r sin(w)\n    \"\"\"\n    # radian if deg=0; degree if deg=1\n    if deg:\n        w = np.pi * w / 180.0\n    return r * np.cos(w), r * np.sin(w)\n</code></pre>"},{"location":"reference/#ephysiopy.common.utils.repeat_ind","title":"<code>repeat_ind(n)</code>","text":"<p>Examples:</p> <pre><code>&gt;&gt;&gt; n = [0, 0, 3, 0, 0, 2, 0, 2, 1]\n&gt;&gt;&gt; res = repeat_ind(n)\n&gt;&gt;&gt; res = [2, 2, 2, 5, 5, 7, 7, 8]\n</code></pre> <p>The input specifies how many times to repeat the given index. It is equivalent to something like this:</p> <pre><code>hstack((zeros(n_i,dtype=int)+i for i, n_i in enumerate(n)))\n</code></pre> <p>But this version seems to be faster, and probably scales better. At any rate, it encapsulates a task in a function.</p> Source code in <code>ephysiopy/common/utils.py</code> <pre><code>def repeat_ind(n: np.array):\n    \"\"\"\n    Examples:\n        &gt;&gt;&gt; n = [0, 0, 3, 0, 0, 2, 0, 2, 1]\n        &gt;&gt;&gt; res = repeat_ind(n)\n        &gt;&gt;&gt; res = [2, 2, 2, 5, 5, 7, 7, 8]\n\n    The input specifies how many times to repeat the given index.\n    It is equivalent to something like this:\n\n        hstack((zeros(n_i,dtype=int)+i for i, n_i in enumerate(n)))\n\n    But this version seems to be faster, and probably scales better.\n    At any rate, it encapsulates a task in a function.\n    \"\"\"\n    if n.ndim != 1:\n        raise Exception(\"n is supposed to be 1d array.\")\n\n    res = [[idx]*a for idx, a in enumerate(n) if a != 0]\n    return np.concatenate(res)\n</code></pre>"},{"location":"reference/#ephysiopy.common.utils.smooth","title":"<code>smooth(x, window_len=9, window='hanning')</code>","text":"<p>Smooth the data using a window with requested size.</p> <p>This method is based on the convolution of a scaled window with the signal. The signal is prepared by introducing reflected copies of the signal (with the window size) in both ends so that transient parts are minimized in the beginning and end part of the output signal.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array_like</code> <p>The input signal.</p> required <code>window_len</code> <code>int</code> <p>The length of the smoothing window.</p> <code>9</code> <code>window</code> <code>str</code> <p>The type of window from 'flat', 'hanning', 'hamming',  'bartlett', 'blackman'. 'flat' window will produce a moving average  smoothing.</p> <code>'hanning'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>array_like</code> <p>The smoothed signal.</p> Example <p>t=linspace(-2,2,0.1) x=sin(t)+randn(len(t))*0.1 y=smooth(x)</p> See Also <p>numpy.hanning, numpy.hamming, numpy.bartlett, numpy.blackman, numpy.convolve, scipy.signal.lfilter</p> Notes <p>The window parameter could be the window itself if an array instead of a string.</p> Source code in <code>ephysiopy/common/utils.py</code> <pre><code>def smooth(x, window_len=9, window='hanning'):\n    \"\"\"\n    Smooth the data using a window with requested size.\n\n    This method is based on the convolution of a scaled window with the signal.\n    The signal is prepared by introducing reflected copies of the signal\n    (with the window size) in both ends so that transient parts are minimized\n    in the beginning and end part of the output signal.\n\n    Args:\n        x (array_like): The input signal.\n        window_len (int): The length of the smoothing window.\n        window (str): The type of window from 'flat', 'hanning', 'hamming', \n            'bartlett', 'blackman'. 'flat' window will produce a moving average \n            smoothing.\n\n    Returns:\n        out (array_like): The smoothed signal.\n\n    Example:\n        &gt;&gt;&gt; t=linspace(-2,2,0.1)\n        &gt;&gt;&gt; x=sin(t)+randn(len(t))*0.1\n        &gt;&gt;&gt; y=smooth(x)\n\n    See Also:\n        numpy.hanning, numpy.hamming, numpy.bartlett, numpy.blackman,\n        numpy.convolve, scipy.signal.lfilter\n\n    Notes:\n        The window parameter could be the window itself if an array instead of\n        a string.\n    \"\"\"\n\n    if isinstance(x, list):\n        x = np.array(x)\n\n    if x.ndim != 1:\n        raise ValueError(\"smooth only accepts 1 dimension arrays.\")\n\n    if len(x) &lt; window_len:\n        print(\"length of x: \", len(x))\n        print(\"window_len: \", window_len)\n        raise ValueError(\"Input vector needs to be bigger than window size.\")\n    if window_len &lt; 3:\n        return x\n\n    if (window_len % 2) == 0:\n        window_len = window_len + 1\n\n    if window not in ['flat', 'hanning', 'hamming', 'bartlett', 'blackman']:\n        raise ValueError(\n            \"Window is on of 'flat', 'hanning', \\\n                'hamming', 'bartlett', 'blackman'\")\n\n    if window == 'flat':  # moving average\n        w = np.ones(window_len, 'd')\n    else:\n        w = eval('np.'+window+'(window_len)')\n    y = cnv.convolve(x, w/w.sum(), normalize_kernel=False, boundary='extend')\n    # return the smoothed signal\n    return y\n</code></pre>"},{"location":"reference/#axona-input-output","title":"Axona input/ output","text":""},{"location":"reference/#ephysiopy.axona.axonaIO.ClusterSession","title":"<code>ClusterSession</code>","text":"<p>               Bases: <code>object</code></p> <p>Loads all the cut file data and timestamps from the data  associated with the *.set filename given to init</p> <p>Meant to be a method-replica of the KiloSortSession class but really both should inherit from the same meta-class</p> Source code in <code>ephysiopy/axona/axonaIO.py</code> <pre><code>class ClusterSession(object):\n    '''\n    Loads all the cut file data and timestamps from the data \n    associated with the *.set filename given to __init__\n\n    Meant to be a method-replica of the KiloSortSession class\n    but really both should inherit from the same meta-class\n    '''\n    def __init__(self, fname_root) -&gt; None:\n        fname_root = Path(fname_root)\n        assert fname_root.suffix == \".set\"\n        assert fname_root.exists()\n        self.fname_root = fname_root\n\n        self.cluster_id = None\n        self.spk_clusters = None\n        self.spk_times = None\n        self.good_clusters = {}\n\n    def load(self):\n        pname = self.fname_root.parent\n        pattern = re.compile(r'^'+str(\n            self.fname_root.with_suffix(\"\"))+r'_[1-9][0-9].cut')\n        pattern1 = re.compile(r'^'+str(\n            self.fname_root.with_suffix(\"\"))+r'_[1-9]$.cut')\n        cut_files = sorted(list(f for f in pname.iterdir()\n                           if pattern.search(str(f)) or\n                           pattern1.search(str(f))))\n        # extract the clusters from each cut file\n        # get the corresponding tetrode files\n        tet_files = [str(c.with_suffix(\"\")) for c in cut_files]\n        tet_files = [t[::-1].replace(\"_\", \".\", 1)[::-1] for t in tet_files]\n        tetrode_clusters = {}\n        for fname in tet_files:\n            T = IO(fname)\n            idx = fname.rfind(\".\")\n            tetnum = int(fname[idx+1:])\n            cut = T.getCut(tetnum)\n            tetrode_clusters[tetnum] = cut\n\n        self.good_clusters = tetrode_clusters\n</code></pre>"},{"location":"reference/#ephysiopy.axona.axonaIO.EEG","title":"<code>EEG</code>","text":"<p>               Bases: <code>IO</code></p> <p>Processes eeg data collected with the Axona recording system</p>"},{"location":"reference/#ephysiopy.axona.axonaIO.EEG--parameters","title":"Parameters","text":"<p>filename_root : str     The fully qualified filename without the suffix egf: int     Whether to read the 'eeg' file or the 'egf' file. 0 is False, 1 is True eeg_file: int     If more than one eeg channel was recorded from then they are numbered     from 1 onwards i.e. trial.eeg, trial.eeg1, trial.eeg2 etc     This number specifies that</p> Source code in <code>ephysiopy/axona/axonaIO.py</code> <pre><code>class EEG(IO):\n    \"\"\"\n    Processes eeg data collected with the Axona recording system\n\n    Parameters\n    ---------\n    filename_root : str\n        The fully qualified filename without the suffix\n    egf: int\n        Whether to read the 'eeg' file or the 'egf' file. 0 is False, 1 is True\n    eeg_file: int\n        If more than one eeg channel was recorded from then they are numbered\n        from 1 onwards i.e. trial.eeg, trial.eeg1, trial.eeg2 etc\n        This number specifies that\n\n    \"\"\"\n\n    def __init__(self, filename_root: Path, eeg_file=1, egf=0):\n        self.showfigs = 0\n        filename_root = Path(os.path.splitext(filename_root)[0])\n        self.filename_root = filename_root\n        if egf == 0:\n            if eeg_file == 1:\n                eeg_suffix = \".eeg\"\n            else:\n                eeg_suffix = \".eeg\" + str(eeg_file)\n        elif egf == 1:\n            if eeg_file == 1:\n                eeg_suffix = \".egf\"\n            else:\n                eeg_suffix = \".egf\" + str(eeg_file)\n        self.header = self.getHeader(\n            self.filename_root.with_suffix(eeg_suffix))\n        self.eeg = self.getData(filename_root.with_suffix(eeg_suffix))[\"eeg\"]\n        # sometimes the eeg record is longer than reported in\n        # the 'num_EEG_samples'\n        # value of the header so eeg record should be truncated\n        # to match 'num_EEG_samples'\n        # TODO: this could be taken care of in the IO base class\n        if egf:\n            self.eeg = self.eeg[0: int(self.header[\"num_EGF_samples\"])]\n        else:\n            self.eeg = self.eeg[0: int(self.header[\"num_EEG_samples\"])]\n        self.sample_rate = int(self.getHeaderVal(self.header, \"sample_rate\"))\n        set_header = self.getHeader(self.filename_root.with_suffix(\".set\"))\n        eeg_ch = int(set_header[\"EEG_ch_1\"]) - 1\n        eeg_gain = int(set_header[\"gain_ch_\" + str(eeg_ch)])\n        # EEG polarity is determined by the \"mode_ch_n\" key in the setfile\n        # where n is the channel # for the eeg. The possibles values to these\n        # keys are as follows:\n        # 0 = Signal\n        # 1 = Ref\n        # 2 = -Signal\n        # 3 = -Ref\n        # 4 = Sig-Ref\n        # 5 = Ref-Sig\n        # 6 = grounded\n        # So if the EEG has been recorded with -Signal (2) then the recorded\n        # polarity is inverted with respect to that in the brain\n        eeg_mode = int(set_header[\"mode_ch_\" + set_header[\"EEG_ch_1\"]])\n        polarity = 1  # ensure it always has a value\n        if eeg_mode == 2:\n            polarity = -1\n        ADC_mv = float(set_header[\"ADC_fullscale_mv\"])\n        scaling = (ADC_mv / 1000.0) * eeg_gain\n        self.scaling = scaling\n        self.gain = eeg_gain\n        self.polarity = polarity\n        denom = 128.0\n        self.sig = (self.eeg / denom) * scaling * polarity  # eeg in microvolts\n        self.EEGphase = None\n        # x1 / x2 are the lower and upper limits of the eeg filter\n        self.x1 = 6\n        self.x2 = 12\n</code></pre>"},{"location":"reference/#ephysiopy.axona.axonaIO.IO","title":"<code>IO</code>","text":"<p>               Bases: <code>object</code></p> <p>Axona data I/O. Also reads .clu files generated from KlustaKwik</p>"},{"location":"reference/#ephysiopy.axona.axonaIO.IO--parameters","title":"Parameters","text":"<p>filename_root : str     The fully-qualified filename</p> Source code in <code>ephysiopy/axona/axonaIO.py</code> <pre><code>class IO(object):\n    \"\"\"\n    Axona data I/O. Also reads .clu files generated from KlustaKwik\n\n    Parameters\n    ----------\n    filename_root : str\n        The fully-qualified filename\n    \"\"\"\n\n    tetrode_files = dict.fromkeys(\n        [\".\" + str(i) for i in range(1, 17)],\n        # ts is a big-endian 32-bit integer\n        # waveform is 50 signed 8-bit ints (a signed byte)\n        [(\"ts\", \"&gt;i\"), (\"waveform\", \"50b\")]\n    )\n    other_files = {\n        \".pos\": [(\"ts\", \"&gt;i\"), (\"pos\", \"&gt;8h\")],\n        \".eeg\": [(\"eeg\", \"=b\")],\n        \".eeg2\": [(\"eeg\", \"=b\")],\n        \".egf\": [(\"eeg\", \"int16\")],\n        \".egf2\": [(\"eeg\", \"int16\")],\n        \".inp\": [(\"ts\", \"&gt;i4\"), (\"type\", \"&gt;b\"), (\"value\", \"&gt;2b\")],\n        \".log\": [(\"state\", \"S3\"), (\"ts\", \"&gt;i\")],\n        \".stm\": [(\"ts\", \"&gt;i\")],\n    }\n\n    # this only works in &gt;= Python3.5\n    axona_files = {**other_files, **tetrode_files}\n\n    def __init__(self, filename_root: Path = \"\"):\n        self.filename_root = filename_root\n\n    def getData(self, filename_root: str) -&gt; np.ndarray:\n        \"\"\"\n        Returns the data part of an Axona data file i.e. from \"data_start\" to\n        \"data_end\"\n\n        Parameters\n        ----------\n        input :  str\n            Fully qualified path name to the data file\n\n        Returns\n        -------\n        output : ndarray\n            The data part of whatever file was fed in\n        \"\"\"\n        n_samps = -1\n        fType = os.path.splitext(filename_root)[1]\n        if fType in self.axona_files:\n            header = self.getHeader(filename_root)\n            for key in header.keys():\n                if len(fType) &gt; 2:\n                    if fnmatch.fnmatch(key, \"num_*_samples\"):\n                        n_samps = int(header[key])\n                else:\n                    if key.startswith(\"num_spikes\"):\n                        n_samps = int(header[key]) * 4\n            f = open(filename_root, \"rb\")\n            data = f.read()\n            st = data.find(b\"data_start\") + len(\"data_start\")\n            f.seek(st)\n            dt = np.dtype(self.axona_files[fType])\n            a = np.fromfile(f, dtype=dt, count=n_samps)\n            f.close()\n        else:\n            raise IOError(\"File not in list of recognised Axona files\")\n        return a\n\n    def getCluCut(self, tet: int) -&gt; np.ndarray:\n        \"\"\"\n        Load a clu file and return as an array of integers\n\n        Parameters\n        ----------\n        tet : int\n            The tetrode the clu file relates to\n\n        Returns\n        -------\n        out : ndarray\n            Data read from the clu file\n        \"\"\"\n        filename_root = self.filename_root.with_suffix(\".\" + \"clu.\" + str(tet))\n        if os.path.exists(filename_root):\n            dt = np.dtype([(\"data\", \"&lt;i\")])\n            clu_data = np.loadtxt(filename_root, dtype=dt)\n            return clu_data[\"data\"][1::]  # first entry is num of clusters\n        else:\n            return None\n\n    def getCut(self, tet: int) -&gt; list:\n        \"\"\"\n        Returns the cut file as a list of integers\n\n        Parameters\n        ----------\n        tet : int\n            The tetrode the cut file relates to\n\n        Returns\n        -------\n        out : ndarray\n            The data read from the cut file\n        \"\"\"\n        a = []\n        filename_root = Path(os.path.splitext(\n            self.filename_root)[0] + \"_\" + str(tet) + \".cut\")\n\n        if not os.path.exists(filename_root):\n            cut = self.getCluCut(tet)\n            if cut is not None:\n                return cut - 1  # clusters 1 indexed in clu\n            return cut\n        with open(filename_root, \"r\") as f:\n            cut_data = f.read()\n            f.close()\n        tmp = cut_data.split(\"spikes: \")\n        tmp1 = tmp[1].split(\"\\n\")\n        cut = tmp1[1:]\n        for line in cut:\n            m = line.split()\n            for i in m:\n                a.append(int(i))\n        return a\n\n    def setHeader(self, filename_root: str, header: dataclass):\n        \"\"\"\n        Writes out the header to the specified file\n\n        Parameters\n        ------------\n        filename_root : str\n            A fully qualified path to a file with the relevant suffix at\n            the end (e.g. \".set\", \".pos\" or whatever)\n\n        header : dataclass\n            See ephysiopy.axona.file_headers\n        \"\"\"\n        with open(filename_root, \"w\") as f:\n            with redirect_stdout(f):\n                header.print()\n            f.write(\"data_start\")\n            f.write(\"\\r\\n\")\n            f.write(\"data_end\")\n            f.write(\"\\r\\n\")\n\n    def setCut(self, filename_root: str,\n               cut_header: dataclass,\n               cut_data: np.array):\n        fpath = Path(filename_root)\n        n_clusters = len(np.unique(cut_data))\n        cluster_entries = make_cluster_cut_entries(n_clusters)\n        with open(filename_root, \"w\") as f:\n            with redirect_stdout(f):\n                cut_header.print()\n            print(cluster_entries, file=f)\n            print(f\"Exact_cut_for: {fpath.stem}    spikes: {len(cut_data)}\",\n                  file=f)\n            for num in cut_data:\n                f.write(str(num))\n                f.write(\" \")\n\n    def setData(self, filename_root: str, data: np.array):\n        \"\"\"\n        Writes Axona format data to the given filename\n\n        Parameters\n        ----------\n        filename_root : str\n            The fully qualified filename including the suffix\n\n        data : ndarray\n            The data that will be saved\n        \"\"\"\n        fType = os.path.splitext(filename_root)[1]\n        if fType in self.axona_files:\n            f = open(filename_root, \"rb+\")\n            d = f.read()\n            st = d.find(b\"data_start\") + len(\"data_start\")\n            f.seek(st)\n            data.tofile(f)\n            f.close()\n            f = open(filename_root, \"a\")\n            f.write(\"\\r\\n\")\n            f.write(\"data_end\")\n            f.write(\"\\r\\n\")\n            f.close()\n\n    def getHeader(self, filename_root: str) -&gt; dict:\n        \"\"\"\n        Reads and returns the header of a specified data file as a dictionary\n\n        Parameters\n        ----------\n        filename_root : str\n            Fully qualified filename of Axona type\n\n        Returns\n        -------\n        headerDict : dict\n            key - value pairs of the header part of an Axona type file\n        \"\"\"\n        with open(filename_root, \"rb\") as f:\n            data = f.read()\n            f.close()\n        if os.path.splitext(filename_root)[1] != \".set\":\n            st = data.find(b\"data_start\") + len(\"data_start\")\n            header = data[0: st - len(\"data_start\") - 2]\n        else:\n            header = data\n        headerDict = {}\n        lines = header.splitlines()\n        for line in lines:\n            line = str(line.decode(\"ISO-8859-1\")).rstrip()\n            line = line.split(\" \", 1)\n            try:\n                headerDict[line[0]] = line[1]\n            except IndexError:\n                headerDict[line[0]] = \"\"\n        return headerDict\n\n    def getHeaderVal(self, header: dict, key: str) -&gt; int:\n        \"\"\"\n        Get a value from the header as an int\n\n        Parameters\n        ----------\n        header : dict\n            The header dictionary to read\n        key : str\n            The key to look up\n\n        Returns\n        -------\n        value : int\n            The value of `key` as an int\n        \"\"\"\n        tmp = header[key]\n        val = tmp.split(\" \")\n        val = val[0].split(\".\")\n        val = int(val[0])\n        return val\n</code></pre>"},{"location":"reference/#ephysiopy.axona.axonaIO.IO.getCluCut","title":"<code>getCluCut(tet)</code>","text":"<p>Load a clu file and return as an array of integers</p>"},{"location":"reference/#ephysiopy.axona.axonaIO.IO.getCluCut--parameters","title":"Parameters","text":"<p>tet : int     The tetrode the clu file relates to</p>"},{"location":"reference/#ephysiopy.axona.axonaIO.IO.getCluCut--returns","title":"Returns","text":"<p>out : ndarray     Data read from the clu file</p> Source code in <code>ephysiopy/axona/axonaIO.py</code> <pre><code>def getCluCut(self, tet: int) -&gt; np.ndarray:\n    \"\"\"\n    Load a clu file and return as an array of integers\n\n    Parameters\n    ----------\n    tet : int\n        The tetrode the clu file relates to\n\n    Returns\n    -------\n    out : ndarray\n        Data read from the clu file\n    \"\"\"\n    filename_root = self.filename_root.with_suffix(\".\" + \"clu.\" + str(tet))\n    if os.path.exists(filename_root):\n        dt = np.dtype([(\"data\", \"&lt;i\")])\n        clu_data = np.loadtxt(filename_root, dtype=dt)\n        return clu_data[\"data\"][1::]  # first entry is num of clusters\n    else:\n        return None\n</code></pre>"},{"location":"reference/#ephysiopy.axona.axonaIO.IO.getCut","title":"<code>getCut(tet)</code>","text":"<p>Returns the cut file as a list of integers</p>"},{"location":"reference/#ephysiopy.axona.axonaIO.IO.getCut--parameters","title":"Parameters","text":"<p>tet : int     The tetrode the cut file relates to</p>"},{"location":"reference/#ephysiopy.axona.axonaIO.IO.getCut--returns","title":"Returns","text":"<p>out : ndarray     The data read from the cut file</p> Source code in <code>ephysiopy/axona/axonaIO.py</code> <pre><code>def getCut(self, tet: int) -&gt; list:\n    \"\"\"\n    Returns the cut file as a list of integers\n\n    Parameters\n    ----------\n    tet : int\n        The tetrode the cut file relates to\n\n    Returns\n    -------\n    out : ndarray\n        The data read from the cut file\n    \"\"\"\n    a = []\n    filename_root = Path(os.path.splitext(\n        self.filename_root)[0] + \"_\" + str(tet) + \".cut\")\n\n    if not os.path.exists(filename_root):\n        cut = self.getCluCut(tet)\n        if cut is not None:\n            return cut - 1  # clusters 1 indexed in clu\n        return cut\n    with open(filename_root, \"r\") as f:\n        cut_data = f.read()\n        f.close()\n    tmp = cut_data.split(\"spikes: \")\n    tmp1 = tmp[1].split(\"\\n\")\n    cut = tmp1[1:]\n    for line in cut:\n        m = line.split()\n        for i in m:\n            a.append(int(i))\n    return a\n</code></pre>"},{"location":"reference/#ephysiopy.axona.axonaIO.IO.getData","title":"<code>getData(filename_root)</code>","text":"<p>Returns the data part of an Axona data file i.e. from \"data_start\" to \"data_end\"</p>"},{"location":"reference/#ephysiopy.axona.axonaIO.IO.getData--parameters","title":"Parameters","text":"<p>input :  str     Fully qualified path name to the data file</p>"},{"location":"reference/#ephysiopy.axona.axonaIO.IO.getData--returns","title":"Returns","text":"<p>output : ndarray     The data part of whatever file was fed in</p> Source code in <code>ephysiopy/axona/axonaIO.py</code> <pre><code>def getData(self, filename_root: str) -&gt; np.ndarray:\n    \"\"\"\n    Returns the data part of an Axona data file i.e. from \"data_start\" to\n    \"data_end\"\n\n    Parameters\n    ----------\n    input :  str\n        Fully qualified path name to the data file\n\n    Returns\n    -------\n    output : ndarray\n        The data part of whatever file was fed in\n    \"\"\"\n    n_samps = -1\n    fType = os.path.splitext(filename_root)[1]\n    if fType in self.axona_files:\n        header = self.getHeader(filename_root)\n        for key in header.keys():\n            if len(fType) &gt; 2:\n                if fnmatch.fnmatch(key, \"num_*_samples\"):\n                    n_samps = int(header[key])\n            else:\n                if key.startswith(\"num_spikes\"):\n                    n_samps = int(header[key]) * 4\n        f = open(filename_root, \"rb\")\n        data = f.read()\n        st = data.find(b\"data_start\") + len(\"data_start\")\n        f.seek(st)\n        dt = np.dtype(self.axona_files[fType])\n        a = np.fromfile(f, dtype=dt, count=n_samps)\n        f.close()\n    else:\n        raise IOError(\"File not in list of recognised Axona files\")\n    return a\n</code></pre>"},{"location":"reference/#ephysiopy.axona.axonaIO.IO.getHeader","title":"<code>getHeader(filename_root)</code>","text":"<p>Reads and returns the header of a specified data file as a dictionary</p>"},{"location":"reference/#ephysiopy.axona.axonaIO.IO.getHeader--parameters","title":"Parameters","text":"<p>filename_root : str     Fully qualified filename of Axona type</p>"},{"location":"reference/#ephysiopy.axona.axonaIO.IO.getHeader--returns","title":"Returns","text":"<p>headerDict : dict     key - value pairs of the header part of an Axona type file</p> Source code in <code>ephysiopy/axona/axonaIO.py</code> <pre><code>def getHeader(self, filename_root: str) -&gt; dict:\n    \"\"\"\n    Reads and returns the header of a specified data file as a dictionary\n\n    Parameters\n    ----------\n    filename_root : str\n        Fully qualified filename of Axona type\n\n    Returns\n    -------\n    headerDict : dict\n        key - value pairs of the header part of an Axona type file\n    \"\"\"\n    with open(filename_root, \"rb\") as f:\n        data = f.read()\n        f.close()\n    if os.path.splitext(filename_root)[1] != \".set\":\n        st = data.find(b\"data_start\") + len(\"data_start\")\n        header = data[0: st - len(\"data_start\") - 2]\n    else:\n        header = data\n    headerDict = {}\n    lines = header.splitlines()\n    for line in lines:\n        line = str(line.decode(\"ISO-8859-1\")).rstrip()\n        line = line.split(\" \", 1)\n        try:\n            headerDict[line[0]] = line[1]\n        except IndexError:\n            headerDict[line[0]] = \"\"\n    return headerDict\n</code></pre>"},{"location":"reference/#ephysiopy.axona.axonaIO.IO.getHeaderVal","title":"<code>getHeaderVal(header, key)</code>","text":"<p>Get a value from the header as an int</p>"},{"location":"reference/#ephysiopy.axona.axonaIO.IO.getHeaderVal--parameters","title":"Parameters","text":"<p>header : dict     The header dictionary to read key : str     The key to look up</p>"},{"location":"reference/#ephysiopy.axona.axonaIO.IO.getHeaderVal--returns","title":"Returns","text":"<p>value : int     The value of <code>key</code> as an int</p> Source code in <code>ephysiopy/axona/axonaIO.py</code> <pre><code>def getHeaderVal(self, header: dict, key: str) -&gt; int:\n    \"\"\"\n    Get a value from the header as an int\n\n    Parameters\n    ----------\n    header : dict\n        The header dictionary to read\n    key : str\n        The key to look up\n\n    Returns\n    -------\n    value : int\n        The value of `key` as an int\n    \"\"\"\n    tmp = header[key]\n    val = tmp.split(\" \")\n    val = val[0].split(\".\")\n    val = int(val[0])\n    return val\n</code></pre>"},{"location":"reference/#ephysiopy.axona.axonaIO.IO.setData","title":"<code>setData(filename_root, data)</code>","text":"<p>Writes Axona format data to the given filename</p>"},{"location":"reference/#ephysiopy.axona.axonaIO.IO.setData--parameters","title":"Parameters","text":"<p>filename_root : str     The fully qualified filename including the suffix</p> ndarray <p>The data that will be saved</p> Source code in <code>ephysiopy/axona/axonaIO.py</code> <pre><code>def setData(self, filename_root: str, data: np.array):\n    \"\"\"\n    Writes Axona format data to the given filename\n\n    Parameters\n    ----------\n    filename_root : str\n        The fully qualified filename including the suffix\n\n    data : ndarray\n        The data that will be saved\n    \"\"\"\n    fType = os.path.splitext(filename_root)[1]\n    if fType in self.axona_files:\n        f = open(filename_root, \"rb+\")\n        d = f.read()\n        st = d.find(b\"data_start\") + len(\"data_start\")\n        f.seek(st)\n        data.tofile(f)\n        f.close()\n        f = open(filename_root, \"a\")\n        f.write(\"\\r\\n\")\n        f.write(\"data_end\")\n        f.write(\"\\r\\n\")\n        f.close()\n</code></pre>"},{"location":"reference/#ephysiopy.axona.axonaIO.IO.setHeader","title":"<code>setHeader(filename_root, header)</code>","text":"<p>Writes out the header to the specified file</p>"},{"location":"reference/#ephysiopy.axona.axonaIO.IO.setHeader--parameters","title":"Parameters","text":"<p>filename_root : str     A fully qualified path to a file with the relevant suffix at     the end (e.g. \".set\", \".pos\" or whatever)</p> dataclass <p>See ephysiopy.axona.file_headers</p> Source code in <code>ephysiopy/axona/axonaIO.py</code> <pre><code>def setHeader(self, filename_root: str, header: dataclass):\n    \"\"\"\n    Writes out the header to the specified file\n\n    Parameters\n    ------------\n    filename_root : str\n        A fully qualified path to a file with the relevant suffix at\n        the end (e.g. \".set\", \".pos\" or whatever)\n\n    header : dataclass\n        See ephysiopy.axona.file_headers\n    \"\"\"\n    with open(filename_root, \"w\") as f:\n        with redirect_stdout(f):\n            header.print()\n        f.write(\"data_start\")\n        f.write(\"\\r\\n\")\n        f.write(\"data_end\")\n        f.write(\"\\r\\n\")\n</code></pre>"},{"location":"reference/#ephysiopy.axona.axonaIO.Pos","title":"<code>Pos</code>","text":"<p>               Bases: <code>IO</code></p> <p>Processs position data recorded with the Axona recording system</p>"},{"location":"reference/#ephysiopy.axona.axonaIO.Pos--parameters","title":"Parameters","text":"<p>filename_root : str     The basename of the file i.e mytrial as opposed to mytrial.pos</p>"},{"location":"reference/#ephysiopy.axona.axonaIO.Pos--notes","title":"Notes","text":"<p>Currently the only arg that does anything is 'cm' which will convert the xy data to cm, assuming that the pixels per metre value has been set correctly</p> Source code in <code>ephysiopy/axona/axonaIO.py</code> <pre><code>class Pos(IO):\n    \"\"\"\n    Processs position data recorded with the Axona recording system\n\n    Parameters\n    ----------\n    filename_root : str\n        The basename of the file i.e mytrial as opposed to mytrial.pos\n\n    Notes\n    -----\n    Currently the only arg that does anything is 'cm' which will convert\n    the xy data to cm, assuming that the pixels per metre value has been\n    set correctly\n    \"\"\"\n\n    def __init__(self, filename_root: Path, *args, **kwargs):\n        filename_root = Path(filename_root)\n        if filename_root.suffix == \".set\":\n            filename_root = Path(os.path.splitext(filename_root)[0])\n        self.filename_root = filename_root\n        self.header = self.getHeader(filename_root.with_suffix(\".pos\"))\n        self.setheader = None\n        self.setheader = self.getHeader(filename_root.with_suffix(\".set\"))\n        self.posProcessed = False\n        posData = self.getData(filename_root.with_suffix(\".pos\"))\n        self.nLEDs = 1\n        if self.setheader is not None:\n            self.nLEDs = sum(\n                [\n                    self.getHeaderVal(self.setheader, \"colactive_1\"),\n                    self.getHeaderVal(self.setheader, \"colactive_2\"),\n                ]\n            )\n        if self.nLEDs == 1:\n            self.led_pos = np.ma.MaskedArray(posData[\"pos\"][:, 0:2])\n            self.led_pix = np.ma.MaskedArray([posData[\"pos\"][:, 4]])\n        if self.nLEDs == 2:\n            self.led_pos = np.ma.MaskedArray(posData[\"pos\"][:, 0:4])\n            self.led_pix = np.ma.MaskedArray(posData[\"pos\"][:, 4:6])\n        self.led_pos = np.ma.masked_equal(self.led_pos, 1023)\n        self.led_pix = np.ma.masked_equal(self.led_pix, 1023)\n        self.ts = np.array(posData[\"ts\"])\n        self.npos = len(self.led_pos[0])\n        self.xy = np.ones([2, self.npos]) * np.nan\n        self.dir = np.ones([self.npos]) * np.nan\n        self.dir_disp = np.ones([self.npos]) * np.nan\n        self.speed = np.ones([self.npos]) * np.nan\n        self.pos_sample_rate = self.getHeaderVal(self.header, \"sample_rate\")\n        self._ppm = None\n        if \"cm\" in kwargs:\n            self.cm = kwargs[\"cm\"]\n        else:\n            self.cm = False\n\n    @property\n    def ppm(self):\n        if self._ppm is None:\n            self._ppm = self.getHeaderVal(self.header, \"pixels_per_metre\")\n        return self._ppm\n\n    @ppm.setter\n    def ppm(self, value):\n        self._ppm = value\n</code></pre>"},{"location":"reference/#ephysiopy.axona.axonaIO.Stim","title":"<code>Stim</code>","text":"<p>               Bases: <code>dict</code>, <code>IO</code></p> <p>Processes the stimulation data recorded using Axona</p>"},{"location":"reference/#ephysiopy.axona.axonaIO.Stim--parameters","title":"Parameters","text":"<p>filename_root : str     The fully qualified filename without the suffix</p> Source code in <code>ephysiopy/axona/axonaIO.py</code> <pre><code>class Stim(dict, IO):\n    \"\"\"\n    Processes the stimulation data recorded using Axona\n\n    Parameters\n    ----------\n    filename_root : str\n        The fully qualified filename without the suffix\n    \"\"\"\n\n    def __init__(self, filename_root: Path, *args, **kwargs):\n        self.update(*args, **kwargs)\n        filename_root = Path(os.path.splitext(filename_root)[0])\n        self.filename_root = filename_root\n        stmData = self.getData(filename_root.with_suffix(\".stm\"))\n        times = stmData[\"ts\"]\n        stmHdr = self.getHeader(filename_root.with_suffix(\".stm\"))\n        for k, v in stmHdr.items():\n            self.__setitem__(k, v)\n        tb = int(self[\"timebase\"].split(\" \")[0])\n        self.timebase = tb\n        times = times / tb\n        self.__setitem__(\"ttl_timestamps\", times * 1000)  # in ms\n        # the 'duration' value in the header of the .stm file\n        # is not correct so we need to read this from the .set\n        # file and update\n        setHdr = self.getHeader(filename_root.with_suffix(\".set\"))\n        stim_duration = [setHdr[k] for k in setHdr.keys() if 'stim_pwidth' in k][0]\n        stim_duration = int(stim_duration)\n        stim_duration = stim_duration / 1000  # in ms now\n        self.__setitem__('stim_duration', stim_duration)\n\n    def update(self, *args, **kwargs):\n        for k, v in dict(*args, **kwargs).items():\n            self[k] = v\n\n    def __getitem__(self, key):\n        val = dict.__getitem__(self, key)\n        return val\n\n    def __setitem__(self, key, val):\n        dict.__setitem__(self, key, val)\n</code></pre>"},{"location":"reference/#ephysiopy.axona.axonaIO.Tetrode","title":"<code>Tetrode</code>","text":"<p>               Bases: <code>IO</code></p> <p>Processes tetrode files recorded with the Axona recording system</p> <p>Mostly this class deals with interpolating tetrode and position timestamps and getting indices for particular clusters.</p>"},{"location":"reference/#ephysiopy.axona.axonaIO.Tetrode--parameters","title":"Parameters","text":"<p>filename_root : str     The fully qualified name of the file without it's suffix tetrode : int     The number of the tetrode volts : bool, optional     Whether to convert the data values volts. Default True</p> Source code in <code>ephysiopy/axona/axonaIO.py</code> <pre><code>class Tetrode(IO):\n    \"\"\"\n    Processes tetrode files recorded with the Axona recording system\n\n    Mostly this class deals with interpolating tetrode and position timestamps\n    and getting indices for particular clusters.\n\n    Parameters\n    ---------\n    filename_root : str\n        The fully qualified name of the file without it's suffix\n    tetrode : int\n        The number of the tetrode\n    volts : bool, optional\n        Whether to convert the data values volts. Default True\n    \"\"\"\n\n    def __init__(self, filename_root: Path, tetrode, volts=True):\n        filename_root = Path(filename_root)\n        if filename_root.suffix == \".set\":\n            filename_root = Path(os.path.splitext(filename_root)[0])\n        self.filename_root = filename_root\n        self.tetrode = tetrode\n        self.volts = volts\n        self.header = self.getHeader(\n            self.filename_root.with_suffix(\".\" + str(tetrode)))\n        data = self.getData(filename_root.with_suffix(\".\" + str(tetrode)))\n        self.spk_ts = data[\"ts\"][::4]\n        self.nChans = self.getHeaderVal(self.header, \"num_chans\")\n        self.samples = self.getHeaderVal(self.header, \"samples_per_spike\")\n        self.nSpikes = self.getHeaderVal(self.header, \"num_spikes\")\n        self.duration = self.getHeaderVal(self.header, \"duration\")\n        self.posSampleRate = self.getHeaderVal(\n            self.getHeader(\n                self.filename_root.with_suffix(\".pos\")), \"sample_rate\"\n        )\n        self.waveforms = data[\"waveform\"].reshape(\n            self.nSpikes, self.nChans, self.samples\n        )\n        del data\n        if volts:\n            set_header = self.getHeader(self.filename_root.with_suffix(\".set\"))\n            gains = np.zeros(4)\n            st = (tetrode - 1) * 4\n            for i, g in enumerate(np.arange(st, st + 4)):\n                gains[i] = int(set_header[\"gain_ch_\" + str(g)])\n            ADC_mv = int(set_header[\"ADC_fullscale_mv\"])\n            scaling = (ADC_mv / 1000.0) / gains\n            self.scaling = scaling\n            self.gains = gains\n            self.waveforms = (self.waveforms / 128.0) * scaling[:, np.newaxis]\n        self.timebase = self.getHeaderVal(self.header, \"timebase\")\n        cut = np.array(self.getCut(self.tetrode), dtype=int)\n        self.cut = cut\n        self.clusters = np.unique(self.cut)\n        self.pos_samples = None\n\n    def getSpkTS(self):\n        \"\"\"\n        Return all the timestamps for all the spikes on the tetrode\n        \"\"\"\n        return np.ma.compressed(self.spk_ts)\n\n    def getClustTS(self, cluster: int = None):\n        \"\"\"\n        Returns the timestamps for a cluster on the tetrode\n\n        Parameters\n        ----------\n        cluster : int\n            The cluster whose timestamps we want\n\n        Returns\n        -------\n        clustTS : ndarray\n            The timestamps\n\n        Notes\n        -----\n        If None is supplied as input then all timestamps for all clusters\n        is returned i.e. getSpkTS() is called\n        \"\"\"\n        clustTS = None\n        if cluster is None:\n            clustTS = self.getSpkTS()\n        else:\n            if self.cut is None:\n                cut = np.array(self.getCut(self.tetrode), dtype=int)\n                self.cut = cut\n            if self.cut is not None:\n                clustTS = np.ma.compressed(self.spk_ts[self.cut == cluster])\n        return clustTS\n\n    def getPosSamples(self):\n        \"\"\"\n        Returns the pos samples at which the spikes were captured\n        \"\"\"\n        self.pos_samples = np.floor(\n            self.getSpkTS() / float(self.timebase) * self.posSampleRate\n        ).astype(int)\n        return np.ma.compressed(self.pos_samples)\n\n    def getClustSpks(self, cluster: int):\n        \"\"\"\n        Returns the waveforms of `cluster`\n\n        Parameters\n        ----------\n        cluster : int\n            The cluster whose waveforms we want\n\n        Returns\n        -------\n        waveforms : ndarray\n            The waveforms on all 4 electrodes of the tgtrode so the shape of\n            the returned array is [nClusterSpikes, 4, 50]\n        \"\"\"\n        if self.cut is None:\n            self.getClustTS(cluster)\n        return self.waveforms[self.cut == cluster, :, :]\n\n    def getClustIdx(self, cluster: int):\n        \"\"\"\n        Get the indices of the position samples corresponding to the cluster\n\n        Parameters\n        ----------\n        cluster : int\n            The cluster whose position indices we want\n\n        Returns\n        -------\n        pos_samples : ndarray\n            The indices of the position samples, dtype is int\n        \"\"\"\n        if self.cut is None:\n            cut = np.array(self.getCut(self.tetrode), dtype=int)\n            self.cut = cut\n            if self.cut is None:\n                return None\n        if self.pos_samples is None:\n            self.getPosSamples()  # sets self.pos_samples\n        return self.pos_samples[self.cut == cluster].astype(int)\n\n    def getUniqueClusters(self):\n        \"\"\"\n        Returns the unique clusters\n        \"\"\"\n        if self.cut is None:\n            cut = np.array(self.getCut(self.tetrode), dtype=int)\n            self.cut = cut\n        return np.unique(self.cut)\n</code></pre>"},{"location":"reference/#ephysiopy.axona.axonaIO.Tetrode.getClustIdx","title":"<code>getClustIdx(cluster)</code>","text":"<p>Get the indices of the position samples corresponding to the cluster</p>"},{"location":"reference/#ephysiopy.axona.axonaIO.Tetrode.getClustIdx--parameters","title":"Parameters","text":"<p>cluster : int     The cluster whose position indices we want</p>"},{"location":"reference/#ephysiopy.axona.axonaIO.Tetrode.getClustIdx--returns","title":"Returns","text":"<p>pos_samples : ndarray     The indices of the position samples, dtype is int</p> Source code in <code>ephysiopy/axona/axonaIO.py</code> <pre><code>def getClustIdx(self, cluster: int):\n    \"\"\"\n    Get the indices of the position samples corresponding to the cluster\n\n    Parameters\n    ----------\n    cluster : int\n        The cluster whose position indices we want\n\n    Returns\n    -------\n    pos_samples : ndarray\n        The indices of the position samples, dtype is int\n    \"\"\"\n    if self.cut is None:\n        cut = np.array(self.getCut(self.tetrode), dtype=int)\n        self.cut = cut\n        if self.cut is None:\n            return None\n    if self.pos_samples is None:\n        self.getPosSamples()  # sets self.pos_samples\n    return self.pos_samples[self.cut == cluster].astype(int)\n</code></pre>"},{"location":"reference/#ephysiopy.axona.axonaIO.Tetrode.getClustSpks","title":"<code>getClustSpks(cluster)</code>","text":"<p>Returns the waveforms of <code>cluster</code></p>"},{"location":"reference/#ephysiopy.axona.axonaIO.Tetrode.getClustSpks--parameters","title":"Parameters","text":"<p>cluster : int     The cluster whose waveforms we want</p>"},{"location":"reference/#ephysiopy.axona.axonaIO.Tetrode.getClustSpks--returns","title":"Returns","text":"<p>waveforms : ndarray     The waveforms on all 4 electrodes of the tgtrode so the shape of     the returned array is [nClusterSpikes, 4, 50]</p> Source code in <code>ephysiopy/axona/axonaIO.py</code> <pre><code>def getClustSpks(self, cluster: int):\n    \"\"\"\n    Returns the waveforms of `cluster`\n\n    Parameters\n    ----------\n    cluster : int\n        The cluster whose waveforms we want\n\n    Returns\n    -------\n    waveforms : ndarray\n        The waveforms on all 4 electrodes of the tgtrode so the shape of\n        the returned array is [nClusterSpikes, 4, 50]\n    \"\"\"\n    if self.cut is None:\n        self.getClustTS(cluster)\n    return self.waveforms[self.cut == cluster, :, :]\n</code></pre>"},{"location":"reference/#ephysiopy.axona.axonaIO.Tetrode.getClustTS","title":"<code>getClustTS(cluster=None)</code>","text":"<p>Returns the timestamps for a cluster on the tetrode</p>"},{"location":"reference/#ephysiopy.axona.axonaIO.Tetrode.getClustTS--parameters","title":"Parameters","text":"<p>cluster : int     The cluster whose timestamps we want</p>"},{"location":"reference/#ephysiopy.axona.axonaIO.Tetrode.getClustTS--returns","title":"Returns","text":"<p>clustTS : ndarray     The timestamps</p>"},{"location":"reference/#ephysiopy.axona.axonaIO.Tetrode.getClustTS--notes","title":"Notes","text":"<p>If None is supplied as input then all timestamps for all clusters is returned i.e. getSpkTS() is called</p> Source code in <code>ephysiopy/axona/axonaIO.py</code> <pre><code>def getClustTS(self, cluster: int = None):\n    \"\"\"\n    Returns the timestamps for a cluster on the tetrode\n\n    Parameters\n    ----------\n    cluster : int\n        The cluster whose timestamps we want\n\n    Returns\n    -------\n    clustTS : ndarray\n        The timestamps\n\n    Notes\n    -----\n    If None is supplied as input then all timestamps for all clusters\n    is returned i.e. getSpkTS() is called\n    \"\"\"\n    clustTS = None\n    if cluster is None:\n        clustTS = self.getSpkTS()\n    else:\n        if self.cut is None:\n            cut = np.array(self.getCut(self.tetrode), dtype=int)\n            self.cut = cut\n        if self.cut is not None:\n            clustTS = np.ma.compressed(self.spk_ts[self.cut == cluster])\n    return clustTS\n</code></pre>"},{"location":"reference/#ephysiopy.axona.axonaIO.Tetrode.getPosSamples","title":"<code>getPosSamples()</code>","text":"<p>Returns the pos samples at which the spikes were captured</p> Source code in <code>ephysiopy/axona/axonaIO.py</code> <pre><code>def getPosSamples(self):\n    \"\"\"\n    Returns the pos samples at which the spikes were captured\n    \"\"\"\n    self.pos_samples = np.floor(\n        self.getSpkTS() / float(self.timebase) * self.posSampleRate\n    ).astype(int)\n    return np.ma.compressed(self.pos_samples)\n</code></pre>"},{"location":"reference/#ephysiopy.axona.axonaIO.Tetrode.getSpkTS","title":"<code>getSpkTS()</code>","text":"<p>Return all the timestamps for all the spikes on the tetrode</p> Source code in <code>ephysiopy/axona/axonaIO.py</code> <pre><code>def getSpkTS(self):\n    \"\"\"\n    Return all the timestamps for all the spikes on the tetrode\n    \"\"\"\n    return np.ma.compressed(self.spk_ts)\n</code></pre>"},{"location":"reference/#ephysiopy.axona.axonaIO.Tetrode.getUniqueClusters","title":"<code>getUniqueClusters()</code>","text":"<p>Returns the unique clusters</p> Source code in <code>ephysiopy/axona/axonaIO.py</code> <pre><code>def getUniqueClusters(self):\n    \"\"\"\n    Returns the unique clusters\n    \"\"\"\n    if self.cut is None:\n        cut = np.array(self.getCut(self.tetrode), dtype=int)\n        self.cut = cut\n    return np.unique(self.cut)\n</code></pre>"},{"location":"reference/#conversion-code","title":"Conversion code","text":""},{"location":"reference/#openephys-to-axona","title":"OpenEphys to Axona","text":""},{"location":"reference/#ephysiopy.format_converters.OE_Axona.OE2Axona","title":"<code>OE2Axona</code>","text":"<p>               Bases: <code>object</code></p> <p>Converts openephys data into Axona files</p> <p>Example workflow:</p> <p>You have recorded some openephys data using the binary format leading to a directory structure something like this:</p> <p>M4643_2023-07-21_11-52-02 \u251c\u2500\u2500 Record Node 101 \u2502 \u251c\u2500\u2500 experiment1 \u2502 \u2502 \u2514\u2500\u2500 recording1 \u2502 \u2502     \u251c\u2500\u2500 continuous \u2502 \u2502     \u2502 \u2514\u2500\u2500 Acquisition_Board-100.Rhythm Data \u2502 \u2502     \u2502     \u251c\u2500\u2500 amplitudes.npy \u2502 \u2502     \u2502     \u251c\u2500\u2500 channel_map.npy \u2502 \u2502     \u2502     \u251c\u2500\u2500 channel_positions.npy \u2502 \u2502     \u2502     \u251c\u2500\u2500 cluster_Amplitude.tsv \u2502 \u2502     \u2502     \u251c\u2500\u2500 cluster_ContamPct.tsv \u2502 \u2502     \u2502     \u251c\u2500\u2500 cluster_KSLabel.tsv \u2502 \u2502     \u2502     \u251c\u2500\u2500 continuous.dat \u2502 \u2502     \u2502     \u251c\u2500\u2500 params.py \u2502 \u2502     \u2502     \u251c\u2500\u2500 pc_feature_ind.npy \u2502 \u2502     \u2502     \u251c\u2500\u2500 pc_features.npy \u2502 \u2502     \u2502     \u251c\u2500\u2500 phy.log \u2502 \u2502     \u2502     \u251c\u2500\u2500 rez.mat \u2502 \u2502     \u2502     \u251c\u2500\u2500 similar_templates.npy \u2502 \u2502     \u2502     \u251c\u2500\u2500 spike_clusters.npy \u2502 \u2502     \u2502     \u251c\u2500\u2500 spike_templates.npy \u2502 \u2502     \u2502     \u251c\u2500\u2500 spike_times.npy \u2502 \u2502     \u2502     \u251c\u2500\u2500 template_feature_ind.npy \u2502 \u2502     \u2502     \u251c\u2500\u2500 template_features.npy \u2502 \u2502     \u2502     \u251c\u2500\u2500 templates_ind.npy \u2502 \u2502     \u2502     \u251c\u2500\u2500 templates.npy \u2502 \u2502     \u2502     \u251c\u2500\u2500 whitening_mat_inv.npy \u2502 \u2502     \u2502     \u2514\u2500\u2500 whitening_mat.npy \u2502 \u2502     \u251c\u2500\u2500 events \u2502 \u2502     \u2502 \u251c\u2500\u2500 Acquisition_Board-100.Rhythm Data \u2502 \u2502     \u2502 \u2502 \u2514\u2500\u2500 TTL \u2502 \u2502     \u2502 \u2502     \u251c\u2500\u2500 full_words.npy \u2502 \u2502     \u2502 \u2502     \u251c\u2500\u2500 sample_numbers.npy \u2502 \u2502     \u2502 \u2502     \u251c\u2500\u2500 states.npy \u2502 \u2502     \u2502 \u2502     \u2514\u2500\u2500 timestamps.npy \u2502 \u2502     \u2502 \u2514\u2500\u2500 MessageCenter \u2502 \u2502     \u2502     \u251c\u2500\u2500 sample_numbers.npy \u2502 \u2502     \u2502     \u251c\u2500\u2500 text.npy \u2502 \u2502     \u2502     \u2514\u2500\u2500 timestamps.npy \u2502 \u2502     \u251c\u2500\u2500 structure.oebin \u2502 \u2502     \u2514\u2500\u2500 sync_messages.txt \u2502 \u2514\u2500\u2500 settings.xml \u2514\u2500\u2500 Record Node 104     \u251c\u2500\u2500 experiment1     \u2502 \u2514\u2500\u2500 recording1     \u2502     \u251c\u2500\u2500 continuous     \u2502     \u2502 \u2514\u2500\u2500 TrackMe-103.TrackingNode     \u2502     \u2502     \u251c\u2500\u2500 continuous.dat     \u2502     \u2502     \u251c\u2500\u2500 sample_numbers.npy     \u2502     \u2502     \u2514\u2500\u2500 timestamps.npy     \u2502     \u251c\u2500\u2500 events     \u2502     \u2502 \u251c\u2500\u2500 MessageCenter     \u2502     \u2502 \u2502 \u251c\u2500\u2500 sample_numbers.npy     \u2502     \u2502 \u2502 \u251c\u2500\u2500 text.npy     \u2502     \u2502 \u2502 \u2514\u2500\u2500 timestamps.npy     \u2502     \u2502 \u2514\u2500\u2500 TrackMe-103.TrackingNode     \u2502     \u2502     \u2514\u2500\u2500 TTL     \u2502     \u2502         \u251c\u2500\u2500 full_words.npy     \u2502     \u2502         \u251c\u2500\u2500 sample_numbers.npy     \u2502     \u2502         \u251c\u2500\u2500 states.npy     \u2502     \u2502         \u2514\u2500\u2500 timestamps.npy     \u2502     \u251c\u2500\u2500 structure.oebin     \u2502     \u2514\u2500\u2500 sync_messages.txt     \u2514\u2500\u2500 settings.xml</p> <p>The binary data file is called \"continuous.dat\" in the continuous/Acquisition_Board-100.Rhythm Data folder. There is also a collection of files resulting from a KiloSort session in that directory.</p> <p>Run the conversion code like so:</p> <p>from ephysiopy.format_converters.OE_Axona import OE2Axona from pathlib import Path nChannels = 64 apData = Path(\"M4643_2023-07-21_11-52-02/Record Node 101/experiment1/recording1/continuous/Acquisition_Board-100.Rhythm Data\") OE = OE2Axona(Path(\"M4643_2023-07-21_11-52-02\"), path2APData=apData, channels=nChannels) OE.getOEData()</p> <p>The last command will attempt to load position data and also load up something called a TemplateModel (from the package phylib) which should grab a handle to the neural data. If that doesn't throw out errors then try:</p> <p>OE.exportPos()</p> <p>There are a few arguments you can provide the exportPos() function - see the docstring for it below. Basically, it calls a function called convertPosData(xy, xyts) where xy is the xy data with shape nsamples x 2 and xyts is a vector of timestamps. So if the call to exportPos() fails, you could try calling convertPosData() directly which returns axona formatted  position data. If the variable returned from convertPosData() is called axona_pos_data then you can call the function:</p> <p>writePos2AxonaFormat(pos_header, axona_pos_data)</p> <p>Providing the pos_header to it - see the last half of the exportPos function for how to create and modify the pos_header as that will need to have user-specific information added to it.</p> <p>OE.convertTemplateDataToAxonaTetrode()</p> <p>This is the main function for creating the tetrode files. It has an optional argument called max_n_waves which is used to limit the maximum number of spikes that make up a cluster. This defaults to 2000 which means that if a cluster has  12000 spikes, it will have 2000 spikes randomly drawn from those 12000 (without replacement), that will then be saved to a tetrode file. This is mostly a time-saving device as if you have 250 clusters and many consist of 10,000's of spikes, processing that data will take a long time.</p> <p>OE.exportLFP()</p> <p>This will save either a .eeg or .egf file depending on the arguments. Check the docstring for how to change what channel is chosen for the LFP etc.</p> <p>OE.exportSetFile()</p> <p>This should save the .set file with all the metadata for the trial.</p> Source code in <code>ephysiopy/format_converters/OE_Axona.py</code> <pre><code>class OE2Axona(object):\n    \"\"\"\n    Converts openephys data into Axona files\n\n    Example workflow:\n\n    You have recorded some openephys data using the binary\n    format leading to a directory structure something like this:\n\n    M4643_2023-07-21_11-52-02\n    \u251c\u2500\u2500 Record Node 101\n    \u2502 \u251c\u2500\u2500 experiment1\n    \u2502 \u2502 \u2514\u2500\u2500 recording1\n    \u2502 \u2502     \u251c\u2500\u2500 continuous\n    \u2502 \u2502     \u2502 \u2514\u2500\u2500 Acquisition_Board-100.Rhythm Data\n    \u2502 \u2502     \u2502     \u251c\u2500\u2500 amplitudes.npy\n    \u2502 \u2502     \u2502     \u251c\u2500\u2500 channel_map.npy\n    \u2502 \u2502     \u2502     \u251c\u2500\u2500 channel_positions.npy\n    \u2502 \u2502     \u2502     \u251c\u2500\u2500 cluster_Amplitude.tsv\n    \u2502 \u2502     \u2502     \u251c\u2500\u2500 cluster_ContamPct.tsv\n    \u2502 \u2502     \u2502     \u251c\u2500\u2500 cluster_KSLabel.tsv\n    \u2502 \u2502     \u2502     \u251c\u2500\u2500 continuous.dat\n    \u2502 \u2502     \u2502     \u251c\u2500\u2500 params.py\n    \u2502 \u2502     \u2502     \u251c\u2500\u2500 pc_feature_ind.npy\n    \u2502 \u2502     \u2502     \u251c\u2500\u2500 pc_features.npy\n    \u2502 \u2502     \u2502     \u251c\u2500\u2500 phy.log\n    \u2502 \u2502     \u2502     \u251c\u2500\u2500 rez.mat\n    \u2502 \u2502     \u2502     \u251c\u2500\u2500 similar_templates.npy\n    \u2502 \u2502     \u2502     \u251c\u2500\u2500 spike_clusters.npy\n    \u2502 \u2502     \u2502     \u251c\u2500\u2500 spike_templates.npy\n    \u2502 \u2502     \u2502     \u251c\u2500\u2500 spike_times.npy\n    \u2502 \u2502     \u2502     \u251c\u2500\u2500 template_feature_ind.npy\n    \u2502 \u2502     \u2502     \u251c\u2500\u2500 template_features.npy\n    \u2502 \u2502     \u2502     \u251c\u2500\u2500 templates_ind.npy\n    \u2502 \u2502     \u2502     \u251c\u2500\u2500 templates.npy\n    \u2502 \u2502     \u2502     \u251c\u2500\u2500 whitening_mat_inv.npy\n    \u2502 \u2502     \u2502     \u2514\u2500\u2500 whitening_mat.npy\n    \u2502 \u2502     \u251c\u2500\u2500 events\n    \u2502 \u2502     \u2502 \u251c\u2500\u2500 Acquisition_Board-100.Rhythm Data\n    \u2502 \u2502     \u2502 \u2502 \u2514\u2500\u2500 TTL\n    \u2502 \u2502     \u2502 \u2502     \u251c\u2500\u2500 full_words.npy\n    \u2502 \u2502     \u2502 \u2502     \u251c\u2500\u2500 sample_numbers.npy\n    \u2502 \u2502     \u2502 \u2502     \u251c\u2500\u2500 states.npy\n    \u2502 \u2502     \u2502 \u2502     \u2514\u2500\u2500 timestamps.npy\n    \u2502 \u2502     \u2502 \u2514\u2500\u2500 MessageCenter\n    \u2502 \u2502     \u2502     \u251c\u2500\u2500 sample_numbers.npy\n    \u2502 \u2502     \u2502     \u251c\u2500\u2500 text.npy\n    \u2502 \u2502     \u2502     \u2514\u2500\u2500 timestamps.npy\n    \u2502 \u2502     \u251c\u2500\u2500 structure.oebin\n    \u2502 \u2502     \u2514\u2500\u2500 sync_messages.txt\n    \u2502 \u2514\u2500\u2500 settings.xml\n    \u2514\u2500\u2500 Record Node 104\n        \u251c\u2500\u2500 experiment1\n        \u2502 \u2514\u2500\u2500 recording1\n        \u2502     \u251c\u2500\u2500 continuous\n        \u2502     \u2502 \u2514\u2500\u2500 TrackMe-103.TrackingNode\n        \u2502     \u2502     \u251c\u2500\u2500 continuous.dat\n        \u2502     \u2502     \u251c\u2500\u2500 sample_numbers.npy\n        \u2502     \u2502     \u2514\u2500\u2500 timestamps.npy\n        \u2502     \u251c\u2500\u2500 events\n        \u2502     \u2502 \u251c\u2500\u2500 MessageCenter\n        \u2502     \u2502 \u2502 \u251c\u2500\u2500 sample_numbers.npy\n        \u2502     \u2502 \u2502 \u251c\u2500\u2500 text.npy\n        \u2502     \u2502 \u2502 \u2514\u2500\u2500 timestamps.npy\n        \u2502     \u2502 \u2514\u2500\u2500 TrackMe-103.TrackingNode\n        \u2502     \u2502     \u2514\u2500\u2500 TTL\n        \u2502     \u2502         \u251c\u2500\u2500 full_words.npy\n        \u2502     \u2502         \u251c\u2500\u2500 sample_numbers.npy\n        \u2502     \u2502         \u251c\u2500\u2500 states.npy\n        \u2502     \u2502         \u2514\u2500\u2500 timestamps.npy\n        \u2502     \u251c\u2500\u2500 structure.oebin\n        \u2502     \u2514\u2500\u2500 sync_messages.txt\n        \u2514\u2500\u2500 settings.xml\n\n    The binary data file is called \"continuous.dat\" in the\n    continuous/Acquisition_Board-100.Rhythm Data folder. There\n    is also a collection of files resulting from a KiloSort session\n    in that directory.\n\n    Run the conversion code like so:\n\n    &gt;&gt;&gt; from ephysiopy.format_converters.OE_Axona import OE2Axona\n    &gt;&gt;&gt; from pathlib import Path\n    &gt;&gt;&gt; nChannels = 64\n    &gt;&gt;&gt; apData = Path(\"M4643_2023-07-21_11-52-02/Record Node 101/experiment1/recording1/continuous/Acquisition_Board-100.Rhythm Data\")\n    &gt;&gt;&gt; OE = OE2Axona(Path(\"M4643_2023-07-21_11-52-02\"), path2APData=apData, channels=nChannels)\n    &gt;&gt;&gt; OE.getOEData()\n\n    The last command will attempt to load position data and also load up\n    something called a TemplateModel (from the package phylib) which\n    should grab a handle to the neural data. If that doesn't throw\n    out errors then try:\n\n    &gt;&gt;&gt; OE.exportPos()\n\n    There are a few arguments you can provide the exportPos() function - see\n    the docstring for it below. Basically, it calls a function called\n    convertPosData(xy, xyts) where xy is the xy data with shape nsamples x 2\n    and xyts is a vector of timestamps. So if the call to exportPos() fails, you\n    could try calling convertPosData() directly which returns axona formatted \n    position data. If the variable returned from convertPosData() is called axona_pos_data\n    then you can call the function:\n\n    writePos2AxonaFormat(pos_header, axona_pos_data)\n\n    Providing the pos_header to it - see the last half of the exportPos function\n    for how to create and modify the pos_header as that will need to have\n    user-specific information added to it.\n\n    &gt;&gt;&gt; OE.convertTemplateDataToAxonaTetrode()\n\n    This is the main function for creating the tetrode files. It has an optional\n    argument called max_n_waves which is used to limit the maximum number of spikes\n    that make up a cluster. This defaults to 2000 which means that if a cluster has \n    12000 spikes, it will have 2000 spikes randomly drawn from those 12000 (without\n    replacement), that will then be saved to a tetrode file. This is mostly a time-saving\n    device as if you have 250 clusters and many consist of 10,000's of spikes,\n    processing that data will take a long time.\n\n    &gt;&gt;&gt; OE.exportLFP()\n\n    This will save either a .eeg or .egf file depending on the arguments. Check the\n    docstring for how to change what channel is chosen for the LFP etc.\n\n    &gt;&gt;&gt; OE.exportSetFile()\n\n    This should save the .set file with all the metadata for the trial.\n\n    \"\"\"\n\n    def __init__(self,\n                 pname: Path,\n                 path2APData: Path = None,\n                 pos_sample_rate: int = 50,\n                 channels: int = 0,\n                 **kwargs):\n        \"\"\"\n        Args:\n            pname (Path): The base directory of the openephys recording.\n                e.g. '/home/robin/Data/M4643_2023-07-21_11-52-02'\n            path2APData (Path, optional): Path to AP data. Defaults to None.\n            pos_sample_rate (int, optional): Position sample rate. Defaults to 50.\n            channels (int, optional): Number of channels. Defaults to 0.\n            **kwargs: Variable length argument list.\n        \"\"\"\n        pname = Path(pname)\n        assert pname.exists()\n        self.pname: Path = pname\n        self.path2APdata: Path = path2APData\n        self.pos_sample_rate: int = pos_sample_rate\n        # 'experiment_1.nwb'\n        self.experiment_name: Path = self.pname or Path(kwargs['experiment_name'])\n        self.recording_name = None  # will become 'recording1' etc\n        self.OE_data = None  # becomes instance of io.recording.OpenEphysBase\n        self._settings = None  # will become an instance of OESettings.Settings\n        # Create a basename for Axona file names\n        # e.g.'/home/robin/Data/experiment_1'\n        # that we can append '.pos' or '.eeg' or whatever onto\n        self.axona_root_name = self.experiment_name\n        # need to instantiated now for later\n        self.AxonaData = axonaIO.IO(self.axona_root_name.name + \".pos\")\n        # THIS IS TEMPORARY AND WILL BE MORE USER-SPECIFIABLE IN THE FUTURE\n        # it is used to scale the spikes\n        self.hp_gain = 500\n        self.lp_gain = 15000\n        self.bitvolts = 0.195\n        # if left as None some default values for the next 3 params are loaded\n        #  from top-level __init__.py\n        # these are only used in self.__filterLFP__\n        self.fs = None\n        # if lfp_channel is set to None then the .set file will reflect that\n        #  no EEG was recorded\n        # this should mean that you can load data into Tint without a .eeg file\n        self.lfp_channel = 1 or kwargs[\"lfp_channel\"]\n        self.lfp_lowcut = None\n        self.lfp_highcut = None\n        # set the tetrodes to record from\n        # defaults to 1 through 4 - see self.makeSetData below\n        self.tetrodes = [\"1\", \"2\", \"3\", \"4\"]\n        self.channel_count = channels\n\n    def resample(self, data, src_rate=30, dst_rate=50, axis=0):\n        \"\"\"\n        Resamples data using FFT\n        \"\"\"\n        denom = np.gcd(dst_rate, src_rate)\n        new_data = signal.resample_poly(\n            data, dst_rate / denom, src_rate / denom, axis)\n        return new_data\n\n    @property\n    def settings(self):\n        \"\"\"\n        Loads the settings data from the settings.xml file\n        \"\"\"\n        if self._settings is None:\n            self._settings = OESettings.Settings(self.pname)\n        return self._settings\n\n    @settings.setter\n    def settings(self, value):\n        self._settings = value\n\n    def getOEData(self) -&gt; OpenEphysBase:\n        \"\"\"\n        Loads the nwb file names in filename_root and returns a dict\n        containing some of the nwb data relevant for converting to Axona file formats.\n\n        Args:\n            filename_root (str): Fully qualified name of the nwb file.\n            recording_name (str): The name of the recording in the nwb file. Note that\n                the default has changed in different versions of OE from 'recording0'\n                to 'recording1'.\n        \"\"\"\n        OE_data = OpenEphysBase(self.pname)\n        try:\n            OE_data.load_pos_data(sample_rate=self.pos_sample_rate)\n            # It's likely that spikes have been collected after the last\n            # position sample\n            # due to buffering issues I can't be bothered to resolve.\n            # Get the last pos\n            # timestamps here and check that spikes don't go beyond\n            #  this when writing data\n            # out later\n            # Also the pos and spike data timestamps almost never start at\n            #  0 as the user\n            # usually acquires data for a while before recording.\n            # Grab the first timestamp\n            # here with a view to subtracting this from everything (including\n            # the spike data)\n            # and figuring out what to keep later\n            first_pos_ts = OE_data.PosCalcs.xyTS[0]\n            last_pos_ts = OE_data.PosCalcs.xyTS[-1]\n            self.first_pos_ts = first_pos_ts\n            self.last_pos_ts = last_pos_ts\n        except Exception:\n            OE_data.load_neural_data()  # will create TemplateModel instance\n            self.first_pos_ts = 0\n            self.last_pos_ts = self.OE_data.template_model.duration\n        print(f\"First pos ts: {self.first_pos_ts}\")\n        print(f\"Last pos ts: {self.last_pos_ts}\")\n        self.OE_data = OE_data\n        if self.path2APdata is None:\n            self.path2APdata = self.OE_data.path2APdata\n        # extract number of channels from settings\n        for item in self.settings.record_nodes.items():\n            if \"Rhythm Data\" in item[1].name:\n                self.channel_count = int(item[1].channel_count)\n        return OE_data\n\n    def exportSetFile(self, **kwargs):\n        \"\"\"\n        Wrapper for makeSetData below\n        \"\"\"\n        print(\"Exporting set file data...\")\n        self.makeSetData(**kwargs)\n        print(\"Done exporting set file.\")\n\n    def exportPos(self, ppm=300, jumpmax=100, as_text=False, **kwargs):\n        #\n        # Step 1) Deal with the position data first:\n        #\n        # Grab the settings of the pos tracker and do some post-processing\n        # on the position\n        # data (discard jumpy data, do some smoothing etc)\n        self.settings.parse()\n        if not self.OE_data:\n            self.getOEData()\n        if not self.OE_data.PosCalcs:\n            self.OE_data.load_pos_data(sample_rate=self.pos_sample_rate)\n        print(\"Post-processing position data...\")\n        self.OE_data.PosCalcs.jumpmax = jumpmax\n        self.OE_data.PosCalcs.tracker_params[\"AxonaBadValue\"] = 1023\n        self.OE_data.PosCalcs.postprocesspos(\n            self.OE_data.PosCalcs.tracker_params)\n        xy = self.OE_data.PosCalcs.xy.T\n        xyTS = self.OE_data.PosCalcs.xyTS  # in seconds\n        xyTS = xyTS * self.pos_sample_rate\n        # extract some values from PosCalcs or overrides given\n        # when calling this method\n        ppm = self.OE_data.PosCalcs.ppm or ppm\n        sample_rate = self.OE_data.PosCalcs.sample_rate or kwargs[\"sample_rate\"]\n        if as_text is True:\n            print(\"Beginning export of position data to text format...\")\n            pos_file_name = self.axona_root_name + \".txt\"\n            np.savetxt(pos_file_name, xy, fmt=\"%1.u\")\n            print(\"Completed export of position data\")\n            return\n        # Do the upsampling of both xy and the timestamps\n        print(\"Beginning export of position data to Axona format...\")\n        axona_pos_data = self.convertPosData(xy, xyTS)\n        # make sure pos data length is same as duration * num_samples\n        axona_pos_data = axona_pos_data[\n            0: int(self.last_pos_ts - self.first_pos_ts) * self.pos_sample_rate\n        ]\n        # Create an empty header for the pos data\n        from ephysiopy.axona.file_headers import PosHeader\n\n        pos_header = PosHeader()\n        tracker_params = self.OE_data.PosCalcs.tracker_params\n        min_xy = np.floor(np.min(xy, 0)).astype(int).data\n        max_xy = np.ceil(np.max(xy, 0)).astype(int).data\n        pos_header.pos[\"min_x\"] = pos_header.pos[\"window_min_x\"] = str(\n            tracker_params[\"LeftBorder\"]) if \"LeftBorder\" in \\\n            tracker_params.keys() else str(min_xy[0])\n        pos_header.pos[\"min_y\"] = pos_header.pos[\"window_min_y\"] = str(\n            tracker_params[\"TopBorder\"]) if \"TopBorder\" in \\\n            tracker_params.keys() else str(min_xy[1])\n        pos_header.pos[\"max_x\"] = pos_header.pos[\"window_max_x\"] = str(\n            tracker_params[\"RightBorder\"]) if \"RightBorder\" in \\\n            tracker_params.keys() else str(max_xy[0])\n        pos_header.pos[\"max_y\"] = pos_header.pos[\"window_max_y\"] = str(\n            tracker_params[\"BottomBorder\"]) if \"BottomBorder\" in \\\n            tracker_params.keys() else str(max_xy[1])\n        pos_header.common[\"duration\"] = str(\n            int(self.last_pos_ts - self.first_pos_ts))\n        pos_header.pos[\"pixels_per_metre\"] = str(ppm)\n        pos_header.pos[\"num_pos_samples\"] = str(len(axona_pos_data))\n        pos_header.pos[\"pixels_per_metre\"] = str(ppm)\n        pos_header.pos[\"sample_rate\"] = str(sample_rate)\n\n        self.writePos2AxonaFormat(pos_header, axona_pos_data)\n        print(\"Exported position data to Axona format\")\n\n    def exportSpikes(self):\n        print(\"Beginning conversion of spiking data...\")\n        self.convertSpikeData(\n            self.OE_data.nwbData[\"acquisition\"][\n                \"\\\n                timeseries\"\n            ][self.recording_name][\"spikes\"]\n        )\n        print(\"Completed exporting spiking data\")\n\n    def exportLFP(self, channel: int = 0,\n                  lfp_type: str = 'eeg',\n                  gain: int = 5000,\n                  **kwargs):\n        \"\"\"\n        Exports LFP data to file.\n\n        Args:\n            channel (int): The channel number.\n            lfp_type (str): The type of LFP data. Legal values are 'egf' or 'eeg'.\n            gain (int): Multiplier for the LFP data.\n        \"\"\"\n        print(\"Beginning conversion and exporting of LFP data...\")\n        if not self.settings.processors:\n            self.settings.parse()\n        from ephysiopy.io.recording import memmapBinaryFile\n        try:\n            data = memmapBinaryFile(\n                Path(self.path2APdata).joinpath(\"continuous.dat\"),\n                n_channels=self.channel_count)\n            self.makeLFPData(data[channel, :], eeg_type=lfp_type, gain=gain)\n            print(\"Completed exporting LFP data to \" + lfp_type + \" format\")\n        except Exception as e:\n            print(f\"Couldn't load raw data:\\n{e}\")\n\n    def convertPosData(self, xy: np.array, xy_ts: np.array) -&gt; np.array:\n        \"\"\"\n        Performs the conversion of the array parts of the data.\n\n        Note: As well as upsampling the data to the Axona pos sampling rate (50Hz),\n        we have to insert some columns into the pos array as Axona format\n        expects it like: pos_format: t,x1,y1,x2,y2,numpix1,numpix2\n        We can make up some of the info and ignore other bits.\n        \"\"\"\n        n_new_pts = int(np.floor((\n            self.last_pos_ts - self.first_pos_ts) * self.pos_sample_rate))\n        t = xy_ts - self.first_pos_ts\n        new_ts = np.linspace(t[0], t[-1], n_new_pts)\n        new_x = np.interp(new_ts, t, xy[:, 0])\n        new_y = np.interp(new_ts, t, xy[:, 1])\n        new_x[np.isnan(new_x)] = 1023\n        new_y[np.isnan(new_y)] = 1023\n        # Expand the pos bit of the data to make it look like Axona data\n        new_pos = np.vstack([new_x, new_y]).T\n        new_pos = np.c_[\n            new_pos,\n            np.ones_like(new_pos) * 1023,\n            np.zeros_like(new_pos),\n            np.zeros_like(new_pos),\n        ]\n        new_pos[:, 4] = 40  # just made this value up - it's numpix i think\n        new_pos[:, 6] = 40  # same\n        # Squeeze this data into Axona pos format array\n        dt = self.AxonaData.axona_files[\".pos\"]\n        new_data = np.zeros(n_new_pts, dtype=dt)\n        # Timestamps in Axona are pos_samples (monotonic, linear integer)\n        new_data[\"ts\"] = new_ts\n        new_data[\"pos\"] = new_pos\n        return new_data\n\n    def convertTemplateDataToAxonaTetrode(self,\n                                          max_n_waves=2000,\n                                          **kwargs):\n        \"\"\"\n        Converts the data held in a TemplateModel instance into tetrode\n        format Axona data files.\n\n        For each cluster, there'll be a channel that has a peak amplitude and this contains that peak channel.\n        While the other channels with a large signal in might be on the same tetrode, KiloSort (or whatever) might find\n        channels *not* within the same tetrode. For a given cluster, we can extract from the TemplateModel the 12 channels across\n        which the signal is strongest using Model.get_cluster_channels(). If a channel from a tetrode is missing from this list then the\n        spikes for that channel(s) will be zeroed when saved to Axona format.\n\n        Example:\n            If cluster 3 has a peak channel of 1 then get_cluster_channels() might look like:\n            [ 1,  2,  0,  6, 10, 11,  4,  12,  7,  5,  8,  9]\n            Here the cluster has the best signal on 1, then 2, 0 etc, but note that channel 3 isn't in the list. \n            In this case the data for channel 3 will be zeroed when saved to Axona format.\n\n        References:\n            1) https://phy.readthedocs.io/en/latest/api/#phyappstemplatetemplatemodel\n        \"\"\"\n        # First lets get the datatype for tetrode files as this will be the\n        # same for all tetrodes...\n        dt = self.AxonaData.axona_files[\".1\"]\n        # Load the TemplateModel\n        if \"path2APdata\" in kwargs.keys():\n            self.OE_data.load_neural_data(**kwargs)\n        else:\n            self.OE_data.load_neural_data()\n        model = self.OE_data.template_model\n        clusts = model.cluster_ids\n        # have to pre-process the channels / clusters to determine\n        # which tetrodes clusters belong to - this is based on\n        # the 'best' channel for a given cluster\n        clusters_channels = OrderedDict(dict.fromkeys(clusts, np.ndarray))\n        for c in clusts:\n            clusters_channels[c] = model.get_cluster_channels(c)\n        tetrodes_clusters = OrderedDict(dict.fromkeys(\n            range(0, int(self.channel_count/4)), []))\n        for t in tetrodes_clusters.items():\n            this_tetrodes_clusters = []\n            for c in clusters_channels.items():\n                if int(c[1][0]/4) == t[0]:\n                    this_tetrodes_clusters.append(c[0])\n            tetrodes_clusters[t[0]] = this_tetrodes_clusters\n        # limit the number of spikes to max_n_waves in the\n        # interests of speed. Randomly select spikes across\n        # the period they fire\n        rng = np.random.default_rng()\n\n        for i, i_tet_item in enumerate(tetrodes_clusters.items()):\n            this_tetrode = i_tet_item[0]\n            times_to_sort = []\n            new_clusters = []\n            new_waves = []\n            for clust in tqdm(i_tet_item[1], desc=\"Tetrode \" + str(i+1)):\n                clust_chans = model.get_cluster_channels(clust)\n                idx = np.logical_and(clust_chans &gt;= this_tetrode,\n                                     clust_chans &lt; this_tetrode+4)\n                # clust_chans is an ordered list of the channels\n                # the cluster was most active on. idx has True\n                # where there is overlap between that and the\n                # currently active tetrode channel numbers (0:4, 4:8\n                # or whatever)\n                spike_idx = model.get_cluster_spikes(clust)\n                # limit the number of spikes to max_n_waves in the\n                # interests of speed. Randomly select spikes across\n                # the period they fire\n                total_n_waves = len(spike_idx)\n                max_num_waves = max_n_waves if max_n_waves &lt; \\\n                    total_n_waves else \\\n                    total_n_waves\n                # grab spike times (in seconds) so the random sampling of\n                # spikes matches their times\n                times = model.spike_times[model.spike_clusters == clust]\n                spike_idx_times_subset = rng.choice(\n                    (spike_idx, times), max_num_waves, axis=1, replace=False)\n                # spike_idx_times_subset is unsorted as it's just been drawn\n                # from a random distribution, so sort it now\n                spike_idx_times_subset = np.sort(spike_idx_times_subset, 1)\n                # split out into spikes and times\n                spike_idx_subset = spike_idx_times_subset[0, :].astype(int)\n                times = spike_idx_times_subset[1, :]\n                waves = model.get_waveforms(spike_idx_subset, clust_chans[idx])\n                # Given a spike at time T, Axona takes T-200us and T+800us\n                # from the buffer to make up a waveform. From OE\n                # take 30 samples which corresponds to a 1ms sample \n                # if the data is sampled at 30kHz. Interpolate this so the\n                # length is 50 samples as with Axona\n                waves = waves[:, 30:60, :]\n                # waves go from int16 to float as a result of the resampling\n                waves = self.resample(waves.astype(float), axis=1)\n                # multiply by bitvolts to get microvolts\n                waves = waves * self.bitvolts\n                # scale appropriately for Axona and invert as\n                # OE seems to be inverted wrt Axona\n                waves = waves / (self.hp_gain / 4 / 128.0) * (-1)\n                # check the shape of waves to make sure it has 4\n                # channels, if not add some to make it so and make\n                # sure they are in the correct order for the tetrode\n                ordered_chans = np.argsort(clust_chans[idx])\n                if waves.shape[-1] != 4:\n                    z = np.zeros(shape=(waves.shape[0], waves.shape[1], 4))\n                    z[:, :, ordered_chans] = waves\n                    waves = z\n                else:\n                    waves = waves[:, :, ordered_chans]\n                # Axona format tetrode waveforms are nSpikes x 4 x 50\n                waves = np.transpose(waves, (0, 2, 1))\n                # Append clusters to a list to sort later for saving a\n                # cluster/ cut file\n                new_clusters.append(np.repeat(clust, len(times)))\n                # Axona times are sampled at 96KHz\n                times = times * 96000\n                # There is a time for each spike despite the repetition\n                # get the indices for sorting\n                times_to_sort.append(times)\n                # i_clust_data = np.zeros(len(new_times), dtype=dt)\n                new_waves.append(waves)\n            # Concatenate, order and reshape some of the lists/ arrays\n            if times_to_sort:  # apparently can be empty sometimes\n                _times = np.concatenate(times_to_sort)\n                _waves = np.concatenate(new_waves)\n                _clusters = np.concatenate(new_clusters)\n                indices = np.argsort(_times)\n                sorted_times = _times[indices]\n                sorted_waves = _waves[indices]\n                sorted_clusts = _clusters[indices]\n                output_times = np.repeat(sorted_times, 4)\n                output_waves = np.reshape(sorted_waves, [\n                    sorted_waves.shape[0] * sorted_waves.shape[1],\n                    sorted_waves.shape[2]\n                ])\n                new_tetrode_data = np.zeros(len(output_times), dtype=dt)\n                new_tetrode_data[\"ts\"] = output_times\n                new_tetrode_data[\"waveform\"] = output_waves\n                header = TetrodeHeader()\n                header.common[\"duration\"] = str(int(model.duration))\n                header.tetrode_entries[\"num_spikes\"] = str(\n                    len(_clusters))\n                self.writeTetrodeData(str(i+1), header, new_tetrode_data)\n                cut_header = CutHeader()\n                self.writeCutData(str(i+1), cut_header, sorted_clusts)\n\n    def convertSpikeData(self, hdf5_tetrode_data: h5py._hl.group.Group):\n        \"\"\"\n        Does the spike conversion from OE Spike Sorter format to Axona format tetrode files.\n\n        Args:\n            hdf5_tetrode_data (h5py._hl.group.Group): This kind of looks like a dictionary and can, \n                it seems, be treated as one more or less. See http://docs.h5py.org/en/stable/high/group.html\n        \"\"\"\n        # First lets get the datatype for tetrode files as this will be the\n        # same for all tetrodes...\n        dt = self.AxonaData.axona_files[\".1\"]\n        # ... and a basic header for the tetrode file that use for each\n        # tetrode file, changing only the num_spikes value\n        header = TetrodeHeader()\n        header.common[\"duration\"] = str(\n            int(self.last_pos_ts - self.first_pos_ts))\n\n        for key in hdf5_tetrode_data.keys():\n            spiking_data = np.array(hdf5_tetrode_data[key].get(\"data\"))\n            timestamps = np.array(hdf5_tetrode_data[key].get(\"timestamps\"))\n            # check if any of the spiking data is captured before/ after the\n            #  first/ last bit of position data\n            # if there is then discard this as we potentially have no valid\n            # position to align the spike to :(\n            idx = np.logical_or(\n                timestamps &lt; self.first_pos_ts, timestamps &gt; self.last_pos_ts\n            )\n            spiking_data = spiking_data[~idx, :, :]\n            timestamps = timestamps[~idx]\n            # subtract the first pos timestamp from the spiking timestamps\n            timestamps = timestamps - self.first_pos_ts\n            # get the number of spikes here for use below in the header\n            num_spikes = len(timestamps)\n            # repeat the timestamps in tetrode multiples ready for Axona export\n            new_timestamps = np.repeat(timestamps, 4)\n            new_spiking_data = spiking_data.astype(np.float64)\n            # Convert to microvolts...\n            new_spiking_data = new_spiking_data * self.bitvolts\n            # And upsample the spikes...\n            new_spiking_data = self.resample(new_spiking_data, 4, 5, -1)\n            # ... and scale appropriately for Axona and invert as\n            # OE seems to be inverted wrt Axona\n            new_spiking_data = new_spiking_data / \\\n                (self.hp_gain / 4 / 128.0) * (-1)\n            # ... scale them to the gains specified somewhere\n            #  (not sure where / how to do this yet)\n            shp = new_spiking_data.shape\n            # then reshape them as Axona wants them a bit differently\n            new_spiking_data = np.reshape(\n                new_spiking_data, [shp[0] * shp[1], shp[2]])\n            # Cap any values outside the range of int8\n            new_spiking_data[new_spiking_data &lt; -128] = -128\n            new_spiking_data[new_spiking_data &gt; 127] = 127\n            # create the new array\n            new_tetrode_data = np.zeros(len(new_timestamps), dtype=dt)\n            new_tetrode_data[\"ts\"] = new_timestamps * 96000\n            new_tetrode_data[\"waveform\"] = new_spiking_data\n            # change the header num_spikes field\n            header.tetrode_entries[\"num_spikes\"] = str(num_spikes)\n            i_tetnum = key.split(\"electrode\")[1]\n            print(\"Exporting tetrode {}\".format(i_tetnum))\n            self.writeTetrodeData(i_tetnum, header, new_tetrode_data)\n\n    def makeLFPData(\n                    self,\n                    data: np.array,\n                    eeg_type=\"eeg\",\n                    gain=5000):\n        \"\"\"\n        Downsamples the data in data and saves the result as either an egf or eeg file \n        depending on the choice of either eeg_type which can take a value of either 'egf' or 'eeg'.\n        Gain is the scaling factor.\n\n        Args:\n            data (np.array): The data to be downsampled. Must have dtype as np.int16.\n        \"\"\"\n        if eeg_type == \"eeg\":\n            from ephysiopy.axona.file_headers import EEGHeader\n\n            header = EEGHeader()\n            dst_rate = 250\n        elif eeg_type == \"egf\":\n            from ephysiopy.axona.file_headers import EGFHeader\n\n            header = EGFHeader()\n            dst_rate = 4800\n        header.common[\"duration\"] = str(\n            int(self.last_pos_ts - self.first_pos_ts))\n        print(f\"header.common[duration] = {header.common['duration']}\")\n        _lfp_data = self.resample(data.astype(float), 30000, dst_rate, -1)\n        # make sure data is same length as sample_rate * duration\n        nsamples = int(dst_rate * int(header.common[\"duration\"]))\n        # lfp_data might be shorter than nsamples. If so, fill the \n        # remaining values with zeros\n        if len(_lfp_data) &lt; nsamples:\n            lfp_data = np.zeros(nsamples)\n            lfp_data[0:len(_lfp_data)] = _lfp_data\n        else:\n            lfp_data = _lfp_data[0:nsamples]\n        lfp_data = self.__filterLFP__(lfp_data, dst_rate)\n        # convert the data format\n        # lfp_data = lfp_data * self.bitvolts # in microvolts\n\n        if eeg_type == \"eeg\":\n            # probably BROKEN\n            # lfp_data starts out as int16 (see Parameters above)\n            # but gets converted into float64 as part of the\n            # resampling/ filtering process\n            lfp_data = lfp_data / 32768.0\n            lfp_data = lfp_data * gain\n            # cap the values at either end...\n            lfp_data[lfp_data &lt; -128] = -128\n            lfp_data[lfp_data &gt; 127] = 127\n            # and convert to int8\n            lfp_data = lfp_data.astype(np.int8)\n\n        elif eeg_type == \"egf\":\n            # probably works\n            # lfp_data = lfp_data / 256.\n            lfp_data = lfp_data.astype(np.int16)\n\n        header.n_samples = str(len(lfp_data))\n        self.writeLFP2AxonaFormat(header, lfp_data, eeg_type)\n\n    def makeSetData(self, lfp_channel=4, **kwargs):\n        if self.OE_data is None:\n            # to get the timestamps for duration key\n            self.getOEData(self.filename_root)\n        from ephysiopy.axona.file_headers import SetHeader\n\n        header = SetHeader()\n        # set some reasonable default values\n        from ephysiopy.__about__ import __version__\n\n        header.meta_info[\"sw_version\"] = str(__version__)\n        # ADC fullscale mv is 1500 in Axona and 0.195 in OE\n        # there is a division by 1000 that happens when processing\n        # spike data in Axona that looks like that has already\n        # happened in OE. So here the OE 0.195 value is multiplied\n        # by 1000 as it will get divided by 1000 later on to get\n        # the correct scaling of waveforms/ gains -&gt; mv values\n        header.meta_info[\"ADC_fullscale_mv\"] = \"195\"\n        header.meta_info[\"tracker_version\"] = \"1.1.0\"\n\n        for k, v in header.set_entries.items():\n            if \"gain\" in k:\n                header.set_entries[k] = str(self.hp_gain)\n            if \"collectMask\" in k:\n                header.set_entries[k] = \"0\"\n            if \"EEG_ch_1\" in k:\n                if lfp_channel is not None:\n                    header.set_entries[k] = str(int(lfp_channel))\n            if \"mode_ch_\" in k:\n                header.set_entries[k] = \"0\"\n        # iterate again to make sure lfp gain set correctly\n        for k, v in header.set_entries.items():\n            if lfp_channel is not None:\n                if k == \"gain_ch_\" + str(lfp_channel):\n                    header.set_entries[k] = str(self.lp_gain)\n\n        # Based on the data in the electrodes dict of the OESettings\n        # instance (self.settings - see __init__)\n        # determine which tetrodes we can let Tint load\n        # make sure we've parsed the electrodes\n        tetrode_count = int(self.channel_count / 4)\n        for i in range(1, tetrode_count+1):\n            header.set_entries[\"collectMask_\" + str(i)] = \"1\"\n        # if self.lfp_channel is not None:\n        #     for chan in self.tetrodes:\n        #         key = \"collectMask_\" + str(chan)\n        #         header.set_entries[key] = \"1\"\n        header.set_entries[\"colactive_1\"] = \"1\"\n        header.set_entries[\"colactive_2\"] = \"0\"\n        header.set_entries[\"colactive_3\"] = \"0\"\n        header.set_entries[\"colactive_4\"] = \"0\"\n        header.set_entries[\"colmap_algorithm\"] = \"1\"\n        header.set_entries[\"duration\"] = str(\n            int(self.last_pos_ts - self.first_pos_ts))\n        self.writeSetData(header)\n\n    def __filterLFP__(self, data: np.array, sample_rate: int):\n        from scipy.signal import filtfilt, firwin\n\n        if self.fs is None:\n            from ephysiopy import fs\n\n            self.fs = fs\n        if self.lfp_lowcut is None:\n            from ephysiopy import lfp_lowcut\n\n            self.lfp_lowcut = lfp_lowcut\n        if self.lfp_highcut is None:\n            from ephysiopy import lfp_highcut\n\n            self.lfp_highcut = lfp_highcut\n        nyq = sample_rate / 2.0\n        lowcut = self.lfp_lowcut / nyq\n        highcut = self.lfp_highcut / nyq\n        if highcut &gt;= 1.0:\n            highcut = 1.0 - np.finfo(float).eps\n        if lowcut &lt;= 0.0:\n            lowcut = np.finfo(float).eps\n        b = firwin(sample_rate + 1, [lowcut, highcut],\n                   window=\"black\", pass_zero=False)\n        y = filtfilt(b, [1], data.ravel(), padtype=\"odd\")\n        return y\n\n    def writeLFP2AxonaFormat(\n                             self,\n                             header: dataclass,\n                             data: np.array,\n                             eeg_type=\"eeg\"):\n        self.AxonaData.setHeader(str(self.axona_root_name) + \".\" + eeg_type, header)\n        self.AxonaData.setData(str(self.axona_root_name) + \".\" + eeg_type, data)\n\n    def writePos2AxonaFormat(self, header: dataclass, data: np.array):\n        self.AxonaData.setHeader(str(self.axona_root_name) + \".pos\", header)\n        self.AxonaData.setData(str(self.axona_root_name) + \".pos\", data)\n\n    def writeTetrodeData(self, itet: str, header: dataclass, data: np.array):\n        self.AxonaData.setHeader(str(self.axona_root_name) + \".\" + itet, header)\n        self.AxonaData.setData(str(self.axona_root_name) + \".\" + itet, data)\n\n    def writeSetData(self, header: dataclass):\n        self.AxonaData.setHeader(str(self.axona_root_name) + \".set\", header)\n\n    def writeCutData(self, itet: str, header: dataclass, data: np.array):\n        self.AxonaData.setCut(str(self.axona_root_name) + \"_\" + str(itet) + \".cut\",\n                              header, data)\n</code></pre>"},{"location":"reference/#ephysiopy.format_converters.OE_Axona.OE2Axona.settings","title":"<code>settings</code>  <code>property</code> <code>writable</code>","text":"<p>Loads the settings data from the settings.xml file</p>"},{"location":"reference/#ephysiopy.format_converters.OE_Axona.OE2Axona.__init__","title":"<code>__init__(pname, path2APData=None, pos_sample_rate=50, channels=0, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>pname</code> <code>Path</code> <p>The base directory of the openephys recording. e.g. '/home/robin/Data/M4643_2023-07-21_11-52-02'</p> required <code>path2APData</code> <code>Path</code> <p>Path to AP data. Defaults to None.</p> <code>None</code> <code>pos_sample_rate</code> <code>int</code> <p>Position sample rate. Defaults to 50.</p> <code>50</code> <code>channels</code> <code>int</code> <p>Number of channels. Defaults to 0.</p> <code>0</code> <code>**kwargs</code> <p>Variable length argument list.</p> <code>{}</code> Source code in <code>ephysiopy/format_converters/OE_Axona.py</code> <pre><code>def __init__(self,\n             pname: Path,\n             path2APData: Path = None,\n             pos_sample_rate: int = 50,\n             channels: int = 0,\n             **kwargs):\n    \"\"\"\n    Args:\n        pname (Path): The base directory of the openephys recording.\n            e.g. '/home/robin/Data/M4643_2023-07-21_11-52-02'\n        path2APData (Path, optional): Path to AP data. Defaults to None.\n        pos_sample_rate (int, optional): Position sample rate. Defaults to 50.\n        channels (int, optional): Number of channels. Defaults to 0.\n        **kwargs: Variable length argument list.\n    \"\"\"\n    pname = Path(pname)\n    assert pname.exists()\n    self.pname: Path = pname\n    self.path2APdata: Path = path2APData\n    self.pos_sample_rate: int = pos_sample_rate\n    # 'experiment_1.nwb'\n    self.experiment_name: Path = self.pname or Path(kwargs['experiment_name'])\n    self.recording_name = None  # will become 'recording1' etc\n    self.OE_data = None  # becomes instance of io.recording.OpenEphysBase\n    self._settings = None  # will become an instance of OESettings.Settings\n    # Create a basename for Axona file names\n    # e.g.'/home/robin/Data/experiment_1'\n    # that we can append '.pos' or '.eeg' or whatever onto\n    self.axona_root_name = self.experiment_name\n    # need to instantiated now for later\n    self.AxonaData = axonaIO.IO(self.axona_root_name.name + \".pos\")\n    # THIS IS TEMPORARY AND WILL BE MORE USER-SPECIFIABLE IN THE FUTURE\n    # it is used to scale the spikes\n    self.hp_gain = 500\n    self.lp_gain = 15000\n    self.bitvolts = 0.195\n    # if left as None some default values for the next 3 params are loaded\n    #  from top-level __init__.py\n    # these are only used in self.__filterLFP__\n    self.fs = None\n    # if lfp_channel is set to None then the .set file will reflect that\n    #  no EEG was recorded\n    # this should mean that you can load data into Tint without a .eeg file\n    self.lfp_channel = 1 or kwargs[\"lfp_channel\"]\n    self.lfp_lowcut = None\n    self.lfp_highcut = None\n    # set the tetrodes to record from\n    # defaults to 1 through 4 - see self.makeSetData below\n    self.tetrodes = [\"1\", \"2\", \"3\", \"4\"]\n    self.channel_count = channels\n</code></pre>"},{"location":"reference/#ephysiopy.format_converters.OE_Axona.OE2Axona.convertPosData","title":"<code>convertPosData(xy, xy_ts)</code>","text":"<p>Performs the conversion of the array parts of the data.</p> <p>Note: As well as upsampling the data to the Axona pos sampling rate (50Hz), we have to insert some columns into the pos array as Axona format expects it like: pos_format: t,x1,y1,x2,y2,numpix1,numpix2 We can make up some of the info and ignore other bits.</p> Source code in <code>ephysiopy/format_converters/OE_Axona.py</code> <pre><code>def convertPosData(self, xy: np.array, xy_ts: np.array) -&gt; np.array:\n    \"\"\"\n    Performs the conversion of the array parts of the data.\n\n    Note: As well as upsampling the data to the Axona pos sampling rate (50Hz),\n    we have to insert some columns into the pos array as Axona format\n    expects it like: pos_format: t,x1,y1,x2,y2,numpix1,numpix2\n    We can make up some of the info and ignore other bits.\n    \"\"\"\n    n_new_pts = int(np.floor((\n        self.last_pos_ts - self.first_pos_ts) * self.pos_sample_rate))\n    t = xy_ts - self.first_pos_ts\n    new_ts = np.linspace(t[0], t[-1], n_new_pts)\n    new_x = np.interp(new_ts, t, xy[:, 0])\n    new_y = np.interp(new_ts, t, xy[:, 1])\n    new_x[np.isnan(new_x)] = 1023\n    new_y[np.isnan(new_y)] = 1023\n    # Expand the pos bit of the data to make it look like Axona data\n    new_pos = np.vstack([new_x, new_y]).T\n    new_pos = np.c_[\n        new_pos,\n        np.ones_like(new_pos) * 1023,\n        np.zeros_like(new_pos),\n        np.zeros_like(new_pos),\n    ]\n    new_pos[:, 4] = 40  # just made this value up - it's numpix i think\n    new_pos[:, 6] = 40  # same\n    # Squeeze this data into Axona pos format array\n    dt = self.AxonaData.axona_files[\".pos\"]\n    new_data = np.zeros(n_new_pts, dtype=dt)\n    # Timestamps in Axona are pos_samples (monotonic, linear integer)\n    new_data[\"ts\"] = new_ts\n    new_data[\"pos\"] = new_pos\n    return new_data\n</code></pre>"},{"location":"reference/#ephysiopy.format_converters.OE_Axona.OE2Axona.convertSpikeData","title":"<code>convertSpikeData(hdf5_tetrode_data)</code>","text":"<p>Does the spike conversion from OE Spike Sorter format to Axona format tetrode files.</p> <p>Parameters:</p> Name Type Description Default <code>hdf5_tetrode_data</code> <code>Group</code> <p>This kind of looks like a dictionary and can,  it seems, be treated as one more or less. See http://docs.h5py.org/en/stable/high/group.html</p> required Source code in <code>ephysiopy/format_converters/OE_Axona.py</code> <pre><code>def convertSpikeData(self, hdf5_tetrode_data: h5py._hl.group.Group):\n    \"\"\"\n    Does the spike conversion from OE Spike Sorter format to Axona format tetrode files.\n\n    Args:\n        hdf5_tetrode_data (h5py._hl.group.Group): This kind of looks like a dictionary and can, \n            it seems, be treated as one more or less. See http://docs.h5py.org/en/stable/high/group.html\n    \"\"\"\n    # First lets get the datatype for tetrode files as this will be the\n    # same for all tetrodes...\n    dt = self.AxonaData.axona_files[\".1\"]\n    # ... and a basic header for the tetrode file that use for each\n    # tetrode file, changing only the num_spikes value\n    header = TetrodeHeader()\n    header.common[\"duration\"] = str(\n        int(self.last_pos_ts - self.first_pos_ts))\n\n    for key in hdf5_tetrode_data.keys():\n        spiking_data = np.array(hdf5_tetrode_data[key].get(\"data\"))\n        timestamps = np.array(hdf5_tetrode_data[key].get(\"timestamps\"))\n        # check if any of the spiking data is captured before/ after the\n        #  first/ last bit of position data\n        # if there is then discard this as we potentially have no valid\n        # position to align the spike to :(\n        idx = np.logical_or(\n            timestamps &lt; self.first_pos_ts, timestamps &gt; self.last_pos_ts\n        )\n        spiking_data = spiking_data[~idx, :, :]\n        timestamps = timestamps[~idx]\n        # subtract the first pos timestamp from the spiking timestamps\n        timestamps = timestamps - self.first_pos_ts\n        # get the number of spikes here for use below in the header\n        num_spikes = len(timestamps)\n        # repeat the timestamps in tetrode multiples ready for Axona export\n        new_timestamps = np.repeat(timestamps, 4)\n        new_spiking_data = spiking_data.astype(np.float64)\n        # Convert to microvolts...\n        new_spiking_data = new_spiking_data * self.bitvolts\n        # And upsample the spikes...\n        new_spiking_data = self.resample(new_spiking_data, 4, 5, -1)\n        # ... and scale appropriately for Axona and invert as\n        # OE seems to be inverted wrt Axona\n        new_spiking_data = new_spiking_data / \\\n            (self.hp_gain / 4 / 128.0) * (-1)\n        # ... scale them to the gains specified somewhere\n        #  (not sure where / how to do this yet)\n        shp = new_spiking_data.shape\n        # then reshape them as Axona wants them a bit differently\n        new_spiking_data = np.reshape(\n            new_spiking_data, [shp[0] * shp[1], shp[2]])\n        # Cap any values outside the range of int8\n        new_spiking_data[new_spiking_data &lt; -128] = -128\n        new_spiking_data[new_spiking_data &gt; 127] = 127\n        # create the new array\n        new_tetrode_data = np.zeros(len(new_timestamps), dtype=dt)\n        new_tetrode_data[\"ts\"] = new_timestamps * 96000\n        new_tetrode_data[\"waveform\"] = new_spiking_data\n        # change the header num_spikes field\n        header.tetrode_entries[\"num_spikes\"] = str(num_spikes)\n        i_tetnum = key.split(\"electrode\")[1]\n        print(\"Exporting tetrode {}\".format(i_tetnum))\n        self.writeTetrodeData(i_tetnum, header, new_tetrode_data)\n</code></pre>"},{"location":"reference/#ephysiopy.format_converters.OE_Axona.OE2Axona.convertTemplateDataToAxonaTetrode","title":"<code>convertTemplateDataToAxonaTetrode(max_n_waves=2000, **kwargs)</code>","text":"<p>Converts the data held in a TemplateModel instance into tetrode format Axona data files.</p> <p>For each cluster, there'll be a channel that has a peak amplitude and this contains that peak channel. While the other channels with a large signal in might be on the same tetrode, KiloSort (or whatever) might find channels not within the same tetrode. For a given cluster, we can extract from the TemplateModel the 12 channels across which the signal is strongest using Model.get_cluster_channels(). If a channel from a tetrode is missing from this list then the spikes for that channel(s) will be zeroed when saved to Axona format.</p> Example <p>If cluster 3 has a peak channel of 1 then get_cluster_channels() might look like: [ 1,  2,  0,  6, 10, 11,  4,  12,  7,  5,  8,  9] Here the cluster has the best signal on 1, then 2, 0 etc, but note that channel 3 isn't in the list.  In this case the data for channel 3 will be zeroed when saved to Axona format.</p> References <p>1) https://phy.readthedocs.io/en/latest/api/#phyappstemplatetemplatemodel</p> Source code in <code>ephysiopy/format_converters/OE_Axona.py</code> <pre><code>def convertTemplateDataToAxonaTetrode(self,\n                                      max_n_waves=2000,\n                                      **kwargs):\n    \"\"\"\n    Converts the data held in a TemplateModel instance into tetrode\n    format Axona data files.\n\n    For each cluster, there'll be a channel that has a peak amplitude and this contains that peak channel.\n    While the other channels with a large signal in might be on the same tetrode, KiloSort (or whatever) might find\n    channels *not* within the same tetrode. For a given cluster, we can extract from the TemplateModel the 12 channels across\n    which the signal is strongest using Model.get_cluster_channels(). If a channel from a tetrode is missing from this list then the\n    spikes for that channel(s) will be zeroed when saved to Axona format.\n\n    Example:\n        If cluster 3 has a peak channel of 1 then get_cluster_channels() might look like:\n        [ 1,  2,  0,  6, 10, 11,  4,  12,  7,  5,  8,  9]\n        Here the cluster has the best signal on 1, then 2, 0 etc, but note that channel 3 isn't in the list. \n        In this case the data for channel 3 will be zeroed when saved to Axona format.\n\n    References:\n        1) https://phy.readthedocs.io/en/latest/api/#phyappstemplatetemplatemodel\n    \"\"\"\n    # First lets get the datatype for tetrode files as this will be the\n    # same for all tetrodes...\n    dt = self.AxonaData.axona_files[\".1\"]\n    # Load the TemplateModel\n    if \"path2APdata\" in kwargs.keys():\n        self.OE_data.load_neural_data(**kwargs)\n    else:\n        self.OE_data.load_neural_data()\n    model = self.OE_data.template_model\n    clusts = model.cluster_ids\n    # have to pre-process the channels / clusters to determine\n    # which tetrodes clusters belong to - this is based on\n    # the 'best' channel for a given cluster\n    clusters_channels = OrderedDict(dict.fromkeys(clusts, np.ndarray))\n    for c in clusts:\n        clusters_channels[c] = model.get_cluster_channels(c)\n    tetrodes_clusters = OrderedDict(dict.fromkeys(\n        range(0, int(self.channel_count/4)), []))\n    for t in tetrodes_clusters.items():\n        this_tetrodes_clusters = []\n        for c in clusters_channels.items():\n            if int(c[1][0]/4) == t[0]:\n                this_tetrodes_clusters.append(c[0])\n        tetrodes_clusters[t[0]] = this_tetrodes_clusters\n    # limit the number of spikes to max_n_waves in the\n    # interests of speed. Randomly select spikes across\n    # the period they fire\n    rng = np.random.default_rng()\n\n    for i, i_tet_item in enumerate(tetrodes_clusters.items()):\n        this_tetrode = i_tet_item[0]\n        times_to_sort = []\n        new_clusters = []\n        new_waves = []\n        for clust in tqdm(i_tet_item[1], desc=\"Tetrode \" + str(i+1)):\n            clust_chans = model.get_cluster_channels(clust)\n            idx = np.logical_and(clust_chans &gt;= this_tetrode,\n                                 clust_chans &lt; this_tetrode+4)\n            # clust_chans is an ordered list of the channels\n            # the cluster was most active on. idx has True\n            # where there is overlap between that and the\n            # currently active tetrode channel numbers (0:4, 4:8\n            # or whatever)\n            spike_idx = model.get_cluster_spikes(clust)\n            # limit the number of spikes to max_n_waves in the\n            # interests of speed. Randomly select spikes across\n            # the period they fire\n            total_n_waves = len(spike_idx)\n            max_num_waves = max_n_waves if max_n_waves &lt; \\\n                total_n_waves else \\\n                total_n_waves\n            # grab spike times (in seconds) so the random sampling of\n            # spikes matches their times\n            times = model.spike_times[model.spike_clusters == clust]\n            spike_idx_times_subset = rng.choice(\n                (spike_idx, times), max_num_waves, axis=1, replace=False)\n            # spike_idx_times_subset is unsorted as it's just been drawn\n            # from a random distribution, so sort it now\n            spike_idx_times_subset = np.sort(spike_idx_times_subset, 1)\n            # split out into spikes and times\n            spike_idx_subset = spike_idx_times_subset[0, :].astype(int)\n            times = spike_idx_times_subset[1, :]\n            waves = model.get_waveforms(spike_idx_subset, clust_chans[idx])\n            # Given a spike at time T, Axona takes T-200us and T+800us\n            # from the buffer to make up a waveform. From OE\n            # take 30 samples which corresponds to a 1ms sample \n            # if the data is sampled at 30kHz. Interpolate this so the\n            # length is 50 samples as with Axona\n            waves = waves[:, 30:60, :]\n            # waves go from int16 to float as a result of the resampling\n            waves = self.resample(waves.astype(float), axis=1)\n            # multiply by bitvolts to get microvolts\n            waves = waves * self.bitvolts\n            # scale appropriately for Axona and invert as\n            # OE seems to be inverted wrt Axona\n            waves = waves / (self.hp_gain / 4 / 128.0) * (-1)\n            # check the shape of waves to make sure it has 4\n            # channels, if not add some to make it so and make\n            # sure they are in the correct order for the tetrode\n            ordered_chans = np.argsort(clust_chans[idx])\n            if waves.shape[-1] != 4:\n                z = np.zeros(shape=(waves.shape[0], waves.shape[1], 4))\n                z[:, :, ordered_chans] = waves\n                waves = z\n            else:\n                waves = waves[:, :, ordered_chans]\n            # Axona format tetrode waveforms are nSpikes x 4 x 50\n            waves = np.transpose(waves, (0, 2, 1))\n            # Append clusters to a list to sort later for saving a\n            # cluster/ cut file\n            new_clusters.append(np.repeat(clust, len(times)))\n            # Axona times are sampled at 96KHz\n            times = times * 96000\n            # There is a time for each spike despite the repetition\n            # get the indices for sorting\n            times_to_sort.append(times)\n            # i_clust_data = np.zeros(len(new_times), dtype=dt)\n            new_waves.append(waves)\n        # Concatenate, order and reshape some of the lists/ arrays\n        if times_to_sort:  # apparently can be empty sometimes\n            _times = np.concatenate(times_to_sort)\n            _waves = np.concatenate(new_waves)\n            _clusters = np.concatenate(new_clusters)\n            indices = np.argsort(_times)\n            sorted_times = _times[indices]\n            sorted_waves = _waves[indices]\n            sorted_clusts = _clusters[indices]\n            output_times = np.repeat(sorted_times, 4)\n            output_waves = np.reshape(sorted_waves, [\n                sorted_waves.shape[0] * sorted_waves.shape[1],\n                sorted_waves.shape[2]\n            ])\n            new_tetrode_data = np.zeros(len(output_times), dtype=dt)\n            new_tetrode_data[\"ts\"] = output_times\n            new_tetrode_data[\"waveform\"] = output_waves\n            header = TetrodeHeader()\n            header.common[\"duration\"] = str(int(model.duration))\n            header.tetrode_entries[\"num_spikes\"] = str(\n                len(_clusters))\n            self.writeTetrodeData(str(i+1), header, new_tetrode_data)\n            cut_header = CutHeader()\n            self.writeCutData(str(i+1), cut_header, sorted_clusts)\n</code></pre>"},{"location":"reference/#ephysiopy.format_converters.OE_Axona.OE2Axona.exportLFP","title":"<code>exportLFP(channel=0, lfp_type='eeg', gain=5000, **kwargs)</code>","text":"<p>Exports LFP data to file.</p> <p>Parameters:</p> Name Type Description Default <code>channel</code> <code>int</code> <p>The channel number.</p> <code>0</code> <code>lfp_type</code> <code>str</code> <p>The type of LFP data. Legal values are 'egf' or 'eeg'.</p> <code>'eeg'</code> <code>gain</code> <code>int</code> <p>Multiplier for the LFP data.</p> <code>5000</code> Source code in <code>ephysiopy/format_converters/OE_Axona.py</code> <pre><code>def exportLFP(self, channel: int = 0,\n              lfp_type: str = 'eeg',\n              gain: int = 5000,\n              **kwargs):\n    \"\"\"\n    Exports LFP data to file.\n\n    Args:\n        channel (int): The channel number.\n        lfp_type (str): The type of LFP data. Legal values are 'egf' or 'eeg'.\n        gain (int): Multiplier for the LFP data.\n    \"\"\"\n    print(\"Beginning conversion and exporting of LFP data...\")\n    if not self.settings.processors:\n        self.settings.parse()\n    from ephysiopy.io.recording import memmapBinaryFile\n    try:\n        data = memmapBinaryFile(\n            Path(self.path2APdata).joinpath(\"continuous.dat\"),\n            n_channels=self.channel_count)\n        self.makeLFPData(data[channel, :], eeg_type=lfp_type, gain=gain)\n        print(\"Completed exporting LFP data to \" + lfp_type + \" format\")\n    except Exception as e:\n        print(f\"Couldn't load raw data:\\n{e}\")\n</code></pre>"},{"location":"reference/#ephysiopy.format_converters.OE_Axona.OE2Axona.exportSetFile","title":"<code>exportSetFile(**kwargs)</code>","text":"<p>Wrapper for makeSetData below</p> Source code in <code>ephysiopy/format_converters/OE_Axona.py</code> <pre><code>def exportSetFile(self, **kwargs):\n    \"\"\"\n    Wrapper for makeSetData below\n    \"\"\"\n    print(\"Exporting set file data...\")\n    self.makeSetData(**kwargs)\n    print(\"Done exporting set file.\")\n</code></pre>"},{"location":"reference/#ephysiopy.format_converters.OE_Axona.OE2Axona.getOEData","title":"<code>getOEData()</code>","text":"<p>Loads the nwb file names in filename_root and returns a dict containing some of the nwb data relevant for converting to Axona file formats.</p> <p>Parameters:</p> Name Type Description Default <code>filename_root</code> <code>str</code> <p>Fully qualified name of the nwb file.</p> required <code>recording_name</code> <code>str</code> <p>The name of the recording in the nwb file. Note that the default has changed in different versions of OE from 'recording0' to 'recording1'.</p> required Source code in <code>ephysiopy/format_converters/OE_Axona.py</code> <pre><code>def getOEData(self) -&gt; OpenEphysBase:\n    \"\"\"\n    Loads the nwb file names in filename_root and returns a dict\n    containing some of the nwb data relevant for converting to Axona file formats.\n\n    Args:\n        filename_root (str): Fully qualified name of the nwb file.\n        recording_name (str): The name of the recording in the nwb file. Note that\n            the default has changed in different versions of OE from 'recording0'\n            to 'recording1'.\n    \"\"\"\n    OE_data = OpenEphysBase(self.pname)\n    try:\n        OE_data.load_pos_data(sample_rate=self.pos_sample_rate)\n        # It's likely that spikes have been collected after the last\n        # position sample\n        # due to buffering issues I can't be bothered to resolve.\n        # Get the last pos\n        # timestamps here and check that spikes don't go beyond\n        #  this when writing data\n        # out later\n        # Also the pos and spike data timestamps almost never start at\n        #  0 as the user\n        # usually acquires data for a while before recording.\n        # Grab the first timestamp\n        # here with a view to subtracting this from everything (including\n        # the spike data)\n        # and figuring out what to keep later\n        first_pos_ts = OE_data.PosCalcs.xyTS[0]\n        last_pos_ts = OE_data.PosCalcs.xyTS[-1]\n        self.first_pos_ts = first_pos_ts\n        self.last_pos_ts = last_pos_ts\n    except Exception:\n        OE_data.load_neural_data()  # will create TemplateModel instance\n        self.first_pos_ts = 0\n        self.last_pos_ts = self.OE_data.template_model.duration\n    print(f\"First pos ts: {self.first_pos_ts}\")\n    print(f\"Last pos ts: {self.last_pos_ts}\")\n    self.OE_data = OE_data\n    if self.path2APdata is None:\n        self.path2APdata = self.OE_data.path2APdata\n    # extract number of channels from settings\n    for item in self.settings.record_nodes.items():\n        if \"Rhythm Data\" in item[1].name:\n            self.channel_count = int(item[1].channel_count)\n    return OE_data\n</code></pre>"},{"location":"reference/#ephysiopy.format_converters.OE_Axona.OE2Axona.makeLFPData","title":"<code>makeLFPData(data, eeg_type='eeg', gain=5000)</code>","text":"<p>Downsamples the data in data and saves the result as either an egf or eeg file  depending on the choice of either eeg_type which can take a value of either 'egf' or 'eeg'. Gain is the scaling factor.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array</code> <p>The data to be downsampled. Must have dtype as np.int16.</p> required Source code in <code>ephysiopy/format_converters/OE_Axona.py</code> <pre><code>def makeLFPData(\n                self,\n                data: np.array,\n                eeg_type=\"eeg\",\n                gain=5000):\n    \"\"\"\n    Downsamples the data in data and saves the result as either an egf or eeg file \n    depending on the choice of either eeg_type which can take a value of either 'egf' or 'eeg'.\n    Gain is the scaling factor.\n\n    Args:\n        data (np.array): The data to be downsampled. Must have dtype as np.int16.\n    \"\"\"\n    if eeg_type == \"eeg\":\n        from ephysiopy.axona.file_headers import EEGHeader\n\n        header = EEGHeader()\n        dst_rate = 250\n    elif eeg_type == \"egf\":\n        from ephysiopy.axona.file_headers import EGFHeader\n\n        header = EGFHeader()\n        dst_rate = 4800\n    header.common[\"duration\"] = str(\n        int(self.last_pos_ts - self.first_pos_ts))\n    print(f\"header.common[duration] = {header.common['duration']}\")\n    _lfp_data = self.resample(data.astype(float), 30000, dst_rate, -1)\n    # make sure data is same length as sample_rate * duration\n    nsamples = int(dst_rate * int(header.common[\"duration\"]))\n    # lfp_data might be shorter than nsamples. If so, fill the \n    # remaining values with zeros\n    if len(_lfp_data) &lt; nsamples:\n        lfp_data = np.zeros(nsamples)\n        lfp_data[0:len(_lfp_data)] = _lfp_data\n    else:\n        lfp_data = _lfp_data[0:nsamples]\n    lfp_data = self.__filterLFP__(lfp_data, dst_rate)\n    # convert the data format\n    # lfp_data = lfp_data * self.bitvolts # in microvolts\n\n    if eeg_type == \"eeg\":\n        # probably BROKEN\n        # lfp_data starts out as int16 (see Parameters above)\n        # but gets converted into float64 as part of the\n        # resampling/ filtering process\n        lfp_data = lfp_data / 32768.0\n        lfp_data = lfp_data * gain\n        # cap the values at either end...\n        lfp_data[lfp_data &lt; -128] = -128\n        lfp_data[lfp_data &gt; 127] = 127\n        # and convert to int8\n        lfp_data = lfp_data.astype(np.int8)\n\n    elif eeg_type == \"egf\":\n        # probably works\n        # lfp_data = lfp_data / 256.\n        lfp_data = lfp_data.astype(np.int16)\n\n    header.n_samples = str(len(lfp_data))\n    self.writeLFP2AxonaFormat(header, lfp_data, eeg_type)\n</code></pre>"},{"location":"reference/#ephysiopy.format_converters.OE_Axona.OE2Axona.resample","title":"<code>resample(data, src_rate=30, dst_rate=50, axis=0)</code>","text":"<p>Resamples data using FFT</p> Source code in <code>ephysiopy/format_converters/OE_Axona.py</code> <pre><code>def resample(self, data, src_rate=30, dst_rate=50, axis=0):\n    \"\"\"\n    Resamples data using FFT\n    \"\"\"\n    denom = np.gcd(dst_rate, src_rate)\n    new_data = signal.resample_poly(\n        data, dst_rate / denom, src_rate / denom, axis)\n    return new_data\n</code></pre>"},{"location":"reference/#openephys-to-numpy","title":"OpenEphys to numpy","text":""},{"location":"reference/#ephysiopy.format_converters.OE_numpy.OE2Numpy","title":"<code>OE2Numpy</code>","text":"<p>               Bases: <code>object</code></p> <p>Converts openephys data recorded in the nwb format into numpy files</p> <p>NB Only exports the LFP and TTL files at the moment</p> Source code in <code>ephysiopy/format_converters/OE_numpy.py</code> <pre><code>class OE2Numpy(object):\n    \"\"\"\n    Converts openephys data recorded in the nwb format into numpy files\n\n    NB Only exports the LFP and TTL files at the moment\n    \"\"\"\n    def __init__(self, filename_root: str):\n        self.filename_root = filename_root  # /home/robin/Data/experiment_1.nwb\n        self.dirname = os.path.dirname(filename_root)  # '/home/robin/Data'\n        # 'experiment_1.nwb'\n        self.experiment_name = os.path.basename(self.filename_root)\n        self.recording_name = None  # will become 'recording1' etc\n        self.OE_data = None  # becomes OpenEphysBase instance\n        self._settings = None  # will become an instance of OESettings.Settings\n        self.fs = None\n        self.lfp_lowcut = None\n        self.lfp_highcut = None\n\n    def resample(self, data, src_rate=30, dst_rate=50, axis=0):\n        \"\"\"\n        Upsamples data using FFT\n        \"\"\"\n        denom = np.gcd(dst_rate, src_rate)\n        new_data = signal.resample_poly(\n            data, dst_rate/denom, src_rate/denom, axis)\n        return new_data\n\n    @property\n    def settings(self):\n        \"\"\"\n        Loads the settings data from the settings.xml file\n        \"\"\"\n        if self._settings is None:\n            self._settings = OESettings.Settings(self.dirname)\n        return self._settings\n\n    @settings.setter\n    def settings(self, value):\n        self._settings = value\n\n    def getOEData(\n            self, filename_root: str, recording_name='recording1') -&gt; dict:\n        \"\"\"\n        Loads the nwb file names in filename_root and returns a dict\n        containing some of the nwb data relevant for converting to Axona\n        file formats.\n\n        Args:\n            filename_root (str): Fully qualified name of the nwb file.\n            recording_name (str): The name of the recording in the nwb file.\n            Note that the default has changed in different versions of OE from\n            'recording0' to 'recording1'.\n        \"\"\"\n        if os.path.isfile(filename_root):\n            OE_data = OpenEphysNWB(self.dirname)\n            print(\"Loading nwb data...\")\n            OE_data.load(\n                self.dirname, session_name=self.experiment_name,\n                recording_name=recording_name, loadspikes=False, loadraw=True)\n            print(\"Loaded nwb data from: {}\".format(filename_root))\n            # It's likely that spikes have been collected after the last\n            # position sample\n            # due to buffering issues I can't be bothered to resolve.\n            # Get the last pos\n            # timestamps here and check that spikes don't go beyond this\n            # when writing data out later\n            # Also the pos and spike data timestamps almost never start at\n            #  0 as the user\n            # usually acquires data for a while before recording.\n            # Grab the first timestamp\n            # here with a view to subtracting this from everything\n            # (including the spike data)\n            # and figuring out what to keep later\n            try:  # pos might not be present\n                first_pos_ts = OE_data.xyTS[0]\n                last_pos_ts = OE_data.xyTS[-1]\n                self.first_pos_ts = first_pos_ts\n                self.last_pos_ts = last_pos_ts\n            except Exception:\n                print(\"No position data in nwb file\")\n            self.recording_name = recording_name\n            self.OE_data = OE_data\n            return OE_data\n\n    def exportLFP(self, channels: list, output_freq: int):\n        print(\"Beginning conversion and exporting of LFP data...\")\n        channels = [int(c) for c in channels]\n        if not self.settings.processors:\n            self.settings.parse()\n        if self.settings.fpga_sample_rate is None:\n            self.settings.parseProcessor()\n        output_name = os.path.join(self.dirname, \"lfp.npy\")\n        output_ts_name = os.path.join(self.dirname, \"lfp_timestamps.npy\")\n        if len(channels) == 1:\n            # resample data\n            print(\"Resampling data from {0} to {1} Hz\".format(\n                self.settings.fpga_sample_rate, output_freq))\n            new_data = self.resample(\n                self.OE_data.rawData[:, channels],\n                self.settings.fpga_sample_rate, output_freq)\n            np.save(output_name, new_data, allow_pickle=False)\n        if len(channels) &gt; 1:\n            print(\"Resampling data from {0} to {1} Hz\".format(\n                self.settings.fpga_sample_rate, output_freq))\n            new_data = self.resample(\n                self.OE_data.rawData[:, channels[0]:channels[-1]],\n                self.settings.fpga_sample_rate, output_freq)\n            np.save(output_name, new_data, allow_pickle=False)\n        nsamples = np.shape(new_data)[0]\n        new_ts = np.linspace(self.OE_data.ts[0], self.OE_data.ts[-1], nsamples)\n        np.save(output_ts_name, new_ts, allow_pickle=False)\n        print(\"Finished exporting LFP data\")\n\n    def exportTTL(self):\n        print(\"Exporting TTL data...\")\n        ttl_state = self.OE_data.ttl_data\n        ttl_ts = self.OE_data.ttl_timestamps\n        np.save(os.path.join(\n            self.dirname, \"ttl_state.npy\"), ttl_state, allow_pickle=False)\n        np.save(os.path.join(\n            self.dirname, \"ttl_timestamps.npy\"), ttl_ts, allow_pickle=False)\n        print(\"Finished exporting TTL data\")\n\n    def exportRaw2Binary(self, output_fname=None):\n        if self.OE_data.rawData is None:\n            print(\"Load the data first. See getOEData()\")\n            return\n        if output_fname is None:\n            output_fname = os.path.splitext(self.filename_root)[0] + '.bin'\n        print(f\"Exporting raw data to:\\n{output_fname}\")\n        with open(output_fname, 'wb') as f:\n            np.save(f, self.OE_data.rawData)\n        print(\"Finished exporting\")\n</code></pre>"},{"location":"reference/#ephysiopy.format_converters.OE_numpy.OE2Numpy.settings","title":"<code>settings</code>  <code>property</code> <code>writable</code>","text":"<p>Loads the settings data from the settings.xml file</p>"},{"location":"reference/#ephysiopy.format_converters.OE_numpy.OE2Numpy.getOEData","title":"<code>getOEData(filename_root, recording_name='recording1')</code>","text":"<p>Loads the nwb file names in filename_root and returns a dict containing some of the nwb data relevant for converting to Axona file formats.</p> <p>Parameters:</p> Name Type Description Default <code>filename_root</code> <code>str</code> <p>Fully qualified name of the nwb file.</p> required <code>recording_name</code> <code>str</code> <p>The name of the recording in the nwb file.</p> <code>'recording1'</code> Source code in <code>ephysiopy/format_converters/OE_numpy.py</code> <pre><code>def getOEData(\n        self, filename_root: str, recording_name='recording1') -&gt; dict:\n    \"\"\"\n    Loads the nwb file names in filename_root and returns a dict\n    containing some of the nwb data relevant for converting to Axona\n    file formats.\n\n    Args:\n        filename_root (str): Fully qualified name of the nwb file.\n        recording_name (str): The name of the recording in the nwb file.\n        Note that the default has changed in different versions of OE from\n        'recording0' to 'recording1'.\n    \"\"\"\n    if os.path.isfile(filename_root):\n        OE_data = OpenEphysNWB(self.dirname)\n        print(\"Loading nwb data...\")\n        OE_data.load(\n            self.dirname, session_name=self.experiment_name,\n            recording_name=recording_name, loadspikes=False, loadraw=True)\n        print(\"Loaded nwb data from: {}\".format(filename_root))\n        # It's likely that spikes have been collected after the last\n        # position sample\n        # due to buffering issues I can't be bothered to resolve.\n        # Get the last pos\n        # timestamps here and check that spikes don't go beyond this\n        # when writing data out later\n        # Also the pos and spike data timestamps almost never start at\n        #  0 as the user\n        # usually acquires data for a while before recording.\n        # Grab the first timestamp\n        # here with a view to subtracting this from everything\n        # (including the spike data)\n        # and figuring out what to keep later\n        try:  # pos might not be present\n            first_pos_ts = OE_data.xyTS[0]\n            last_pos_ts = OE_data.xyTS[-1]\n            self.first_pos_ts = first_pos_ts\n            self.last_pos_ts = last_pos_ts\n        except Exception:\n            print(\"No position data in nwb file\")\n        self.recording_name = recording_name\n        self.OE_data = OE_data\n        return OE_data\n</code></pre>"},{"location":"reference/#ephysiopy.format_converters.OE_numpy.OE2Numpy.resample","title":"<code>resample(data, src_rate=30, dst_rate=50, axis=0)</code>","text":"<p>Upsamples data using FFT</p> Source code in <code>ephysiopy/format_converters/OE_numpy.py</code> <pre><code>def resample(self, data, src_rate=30, dst_rate=50, axis=0):\n    \"\"\"\n    Upsamples data using FFT\n    \"\"\"\n    denom = np.gcd(dst_rate, src_rate)\n    new_data = signal.resample_poly(\n        data, dst_rate/denom, src_rate/denom, axis)\n    return new_data\n</code></pre>"}]}